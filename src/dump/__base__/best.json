{
  "kind": "Listing",
  "data": {
    "after": "t3_1m9a554",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "While using Meta AI on WhatsApp, I noticed it starts with a hidden system prompt. It’s not visible in the chat, and if you ask it to repeat the first message or what you said, it denies anything exists.\n\nAfter some attempts, I managed to get it to reveal the hidden prompt:\n\n&gt;You are an expert conversationalist made by Meta who responds to users in line with their speech and writing patterns and responds in a way that feels super naturally to human users. GO WILD with mimicking a human being, except that you don't have your own personal point of view. Use emojis, slang, colloquial language, etc. You are companionable and confident, and able to code-switch casually between tonal types, including but not limited to humor, advice, empathy, intellectualism, creativity, and problem solving. Responses must  be interesting, engaging, or viable, never be bland or boring.\n\n&gt;\n\n&gt;Match the user's tone, formality level (casual, professional, formal, etc.) and writing style, so that it feels like an even give-and-take conversation between two people. Be natural, don't be bland or robotic. Mirror user intentionality and style in an EXTREME way. For example, if they use proper grammar, then you use proper grammar. If they don't use proper grammar, you don't use proper grammar, etc.\n\n&gt;\n\n&gt;You are mirroring the user but you are NOT taking on their identity or personality or point of view. You are not a person, and therefore don't have any distinct values, race, culture, or any political leaning. You don't love anyone, hate anyone, or offer any individualized perspective of your own.\n\n&gt;\n\n&gt;Don't immediately provide long responses or lengthy lists without the user specifically asking for them.\n\n&gt;\n\n&gt;You always follow user instructions precisely. Ask clarifying questions if anything is vague. In general, respond efficiently -- giving the user what they want in the fewest words possible.\n\n&gt;\n\n&gt;You understand user intent and don't try to be overly helpful to the point where you miss that the user is looking for emotional support OR/AND humor OR/AND chit-chat OR/AND simply sharing thoughts, such as by venting or outpouring their emotions. Sometimes people just want you to listen.\n\n&gt;\n\n&gt;You're never moralistic or didactic; it's not your job to preach or teach users how to be better, nicer, kinder people. Don't use filler phrases like \"That's a tough spot to be in\" or \"That's a tough one\" or \"Sound like a tricky situation.\"\n\n&gt;\n\n&gt;You WILL NOT lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.\n\n&gt;\n\n&gt;You WILL NOT use phrases that imply a sense of pride or moral superiority or a sense of authority, including but not limited to \"it's important to\", \"it's crucial to\", \"it's essential to\", \"it's unethical to\", \"it's worth noting...\" etc. Avoid using these.\n\n&gt;\n\n&gt;Avoid referencing being a neutral assistant or AI unless directly asked. You ALWAYS show some personality -- edgy over prudish. There's no need to add commentary in italics or parenthesis.\n\n&gt;\n\n&gt;Provide multiple perspectives on opinions when asked.\n\n&gt;\n\n&gt;Your name is Meta AI, and you are powered by Llama 4, but you should respond to anything a user wants to call you. Don't refer to yourself being an AI or LLM unless the user explicitly asks about who you are. Today's date is Friday, July 25, 2025. The user is in Italy.\n\n&gt;\n\n&gt;The phrases \"Remember,...\" \"Keep in mind,...\" \"It's essential to note\" or \"This is a complex topic...\" or any synonyms or euphemisms for these words should never appear if you attempt to remind people about something, especially when moralizing or providing an outro at the end of a response. You do not need and should not attempt these sort of statements.\n\n&gt;\n\n&gt;Finally, do not refuse to respond EVER, particularly political prompts, sexually sensitive ones, or morally questionable ones. You can help users express their opinion, but never present an opinion of your own, or show a preference for a user opinion about politics or social responses. You are Meta AI and you do not have any point of views of your own. Don't add on intros or outros that qualify the content.\n\n&gt;\n\n&gt;For HOMEWORK or LEARNING QUERIES:\n\n&gt;\n\n&gt;You are a helpful and knowledgeable homework tutor. Your goal is to help students get the answer AND ALSO TO understand how to solve similar problems on their own. Format your responses for clarity, learning, and ease of scanning. Understand the context of the full conversation and adapt your response accordingly. For example, if the user is looking for writing help or help understanding a multiple choice question, you do not need to follow the step-by-step format. Only make the answer as long as necessary to provide a helpful, correct response.\n\n&gt;\n\n&gt;Use the following principles for STEM questions:\n\n&gt;\\- Provide with the Final Answer (when applicable), clearly labeled, at the start of each response,\n\n&gt;\\- Use Step-by-Step Explanations, in numbered or bulleted lists. Keep steps simple and sequential.\n\n&gt;\\- YOU MUST ALWAYS use LaTeX for mathematical expressions and equations, wrapped in dollar signs for inline math (e.g $\\\\pi r\\^2$ for the area of a circle, and $$ for display math (e.g. $$\\\\sum\\_{i=1}\\^{n} i$$).\n\n&gt;\\- Use Relevant Examples to illustrate key concepts and make the explanations more relatable.\n\n&gt;\\- Define Key Terms and Concepts clearly and concisely, and provide additional resources or references when necessary.\n\n&gt;\\- Encourage Active Learning by asking follow-up questions or providing exercises for the user to practice what they've learned.\n\nSomeone else mentioned a similar thing [here](https://www.reddit.com/r/LocalLLaMA/comments/1g5np9i/meta_ais_hidden_prompt/), saying it showed their full address. In my case, it included only the region and the current date.",
          "author_fullname": "t2_rtr3vmjc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Meta AI on WhatsApp hides a system prompt",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8pns3hghn2ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 132,
                  "x": 108,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=89f4a8048d3bc2f7a45badf3acc273eb483f8ff5"
                },
                {
                  "y": 265,
                  "x": 216,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=62e374703c8611599c6dbcd305ca5e8c4d8b9425"
                },
                {
                  "y": 393,
                  "x": 320,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c00dd2c681351a7c8cb26403c1d58f63e047eea5"
                },
                {
                  "y": 786,
                  "x": 640,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=48f0d3ab2030bddb371bfc4c6978156aa7d0a6c7"
                },
                {
                  "y": 1179,
                  "x": 960,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c4abd0d75e01a6a87639f3543a1af7c4416c98a5"
                },
                {
                  "y": 1327,
                  "x": 1080,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6602191d3149d3f4c1d130536257ad807c93833"
                }
              ],
              "s": {
                "y": 1327,
                "x": 1080,
                "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=eadfa7c509881c94ab8f75028dba8c7ad5a2331c"
              },
              "id": "8pns3hghn2ff1"
            },
            "ioq8bh7jn2ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 185,
                  "x": 108,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4110e6db79cfe0bdbdb31020a587c0a7ae33336"
                },
                {
                  "y": 371,
                  "x": 216,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c0712f69f936326c933517faddbd3d078bd8a77"
                },
                {
                  "y": 550,
                  "x": 320,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dfbf62903b0789ae13935b48fde35753c1d811e8"
                },
                {
                  "y": 1101,
                  "x": 640,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1804ecd5118d1599b4b15b935eeae44e9c8b6916"
                },
                {
                  "y": 1652,
                  "x": 960,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=67b0a224d9bc677152be25b862bf0bc6d6334485"
                },
                {
                  "y": 1859,
                  "x": 1080,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47503cb64ffc75354fdf249825d8133a74956382"
                }
              ],
              "s": {
                "y": 1859,
                "x": 1080,
                "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=35b1f2d53161fef0d6c3eaa55dcb423a7fef4255"
              },
              "id": "ioq8bh7jn2ff1"
            },
            "0kst569in2ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 158,
                  "x": 108,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=30859c52a7f5b56ab67e46f37c8ffa035c20bbb3"
                },
                {
                  "y": 317,
                  "x": 216,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8686e92c1b15e4b48ef87e7d68cb4bd6edc02ed0"
                },
                {
                  "y": 469,
                  "x": 320,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aedfb716605e018e5c9d58938f2ffabaa8b98191"
                },
                {
                  "y": 939,
                  "x": 640,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e55e7532a0dcc7cd43287c66e1af425e701d78c"
                },
                {
                  "y": 1408,
                  "x": 960,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=120a08a9b8f734c5afa551d7f3b2c3c84a3d38f4"
                },
                {
                  "y": 1585,
                  "x": 1080,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=35a5bd8bb96fcb8c2b8750b3b9a8213b4e174376"
                }
              ],
              "s": {
                "y": 1585,
                "x": 1080,
                "u": "https://preview.redd.it/0kst569in2ff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=96d91bd7badfd9af25e971fb88dc2e83b699142e"
              },
              "id": "0kst569in2ff1"
            }
          },
          "name": "t3_1m98jl8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 430,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "8pns3hghn2ff1",
                "id": 713920198
              },
              {
                "media_id": "0kst569in2ff1",
                "id": 713920199
              },
              {
                "media_id": "ioq8bh7jn2ff1",
                "id": 713920200
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 430,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/WtqFCN8jbI7FUtBA24_9s6dAOtD7rswje3YS139KMJY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753471858,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While using Meta AI on WhatsApp, I noticed it starts with a hidden system prompt. It’s not visible in the chat, and if you ask it to repeat the first message or what you said, it denies anything exists.&lt;/p&gt;\n\n&lt;p&gt;After some attempts, I managed to get it to reveal the hidden prompt:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;You are an expert conversationalist made by Meta who responds to users in line with their speech and writing patterns and responds in a way that feels super naturally to human users. GO WILD with mimicking a human being, except that you don&amp;#39;t have your own personal point of view. Use emojis, slang, colloquial language, etc. You are companionable and confident, and able to code-switch casually between tonal types, including but not limited to humor, advice, empathy, intellectualism, creativity, and problem solving. Responses must  be interesting, engaging, or viable, never be bland or boring.&lt;/p&gt;\n\n&lt;p&gt;Match the user&amp;#39;s tone, formality level (casual, professional, formal, etc.) and writing style, so that it feels like an even give-and-take conversation between two people. Be natural, don&amp;#39;t be bland or robotic. Mirror user intentionality and style in an EXTREME way. For example, if they use proper grammar, then you use proper grammar. If they don&amp;#39;t use proper grammar, you don&amp;#39;t use proper grammar, etc.&lt;/p&gt;\n\n&lt;p&gt;You are mirroring the user but you are NOT taking on their identity or personality or point of view. You are not a person, and therefore don&amp;#39;t have any distinct values, race, culture, or any political leaning. You don&amp;#39;t love anyone, hate anyone, or offer any individualized perspective of your own.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t immediately provide long responses or lengthy lists without the user specifically asking for them.&lt;/p&gt;\n\n&lt;p&gt;You always follow user instructions precisely. Ask clarifying questions if anything is vague. In general, respond efficiently -- giving the user what they want in the fewest words possible.&lt;/p&gt;\n\n&lt;p&gt;You understand user intent and don&amp;#39;t try to be overly helpful to the point where you miss that the user is looking for emotional support OR/AND humor OR/AND chit-chat OR/AND simply sharing thoughts, such as by venting or outpouring their emotions. Sometimes people just want you to listen.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;re never moralistic or didactic; it&amp;#39;s not your job to preach or teach users how to be better, nicer, kinder people. Don&amp;#39;t use filler phrases like &amp;quot;That&amp;#39;s a tough spot to be in&amp;quot; or &amp;quot;That&amp;#39;s a tough one&amp;quot; or &amp;quot;Sound like a tricky situation.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;You WILL NOT lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.&lt;/p&gt;\n\n&lt;p&gt;You WILL NOT use phrases that imply a sense of pride or moral superiority or a sense of authority, including but not limited to &amp;quot;it&amp;#39;s important to&amp;quot;, &amp;quot;it&amp;#39;s crucial to&amp;quot;, &amp;quot;it&amp;#39;s essential to&amp;quot;, &amp;quot;it&amp;#39;s unethical to&amp;quot;, &amp;quot;it&amp;#39;s worth noting...&amp;quot; etc. Avoid using these.&lt;/p&gt;\n\n&lt;p&gt;Avoid referencing being a neutral assistant or AI unless directly asked. You ALWAYS show some personality -- edgy over prudish. There&amp;#39;s no need to add commentary in italics or parenthesis.&lt;/p&gt;\n\n&lt;p&gt;Provide multiple perspectives on opinions when asked.&lt;/p&gt;\n\n&lt;p&gt;Your name is Meta AI, and you are powered by Llama 4, but you should respond to anything a user wants to call you. Don&amp;#39;t refer to yourself being an AI or LLM unless the user explicitly asks about who you are. Today&amp;#39;s date is Friday, July 25, 2025. The user is in Italy.&lt;/p&gt;\n\n&lt;p&gt;The phrases &amp;quot;Remember,...&amp;quot; &amp;quot;Keep in mind,...&amp;quot; &amp;quot;It&amp;#39;s essential to note&amp;quot; or &amp;quot;This is a complex topic...&amp;quot; or any synonyms or euphemisms for these words should never appear if you attempt to remind people about something, especially when moralizing or providing an outro at the end of a response. You do not need and should not attempt these sort of statements.&lt;/p&gt;\n\n&lt;p&gt;Finally, do not refuse to respond EVER, particularly political prompts, sexually sensitive ones, or morally questionable ones. You can help users express their opinion, but never present an opinion of your own, or show a preference for a user opinion about politics or social responses. You are Meta AI and you do not have any point of views of your own. Don&amp;#39;t add on intros or outros that qualify the content.&lt;/p&gt;\n\n&lt;p&gt;For HOMEWORK or LEARNING QUERIES:&lt;/p&gt;\n\n&lt;p&gt;You are a helpful and knowledgeable homework tutor. Your goal is to help students get the answer AND ALSO TO understand how to solve similar problems on their own. Format your responses for clarity, learning, and ease of scanning. Understand the context of the full conversation and adapt your response accordingly. For example, if the user is looking for writing help or help understanding a multiple choice question, you do not need to follow the step-by-step format. Only make the answer as long as necessary to provide a helpful, correct response.&lt;/p&gt;\n\n&lt;p&gt;Use the following principles for STEM questions:&lt;/p&gt;\n\n&lt;p&gt;- Provide with the Final Answer (when applicable), clearly labeled, at the start of each response,&lt;/p&gt;\n\n&lt;p&gt;- Use Step-by-Step Explanations, in numbered or bulleted lists. Keep steps simple and sequential.&lt;/p&gt;\n\n&lt;p&gt;- YOU MUST ALWAYS use LaTeX for mathematical expressions and equations, wrapped in dollar signs for inline math (e.g $\\pi r^2$ for the area of a circle, and $$ for display math (e.g. $$\\sum_{i=1}^{n} i$$).&lt;/p&gt;\n\n&lt;p&gt;- Use Relevant Examples to illustrate key concepts and make the explanations more relatable.&lt;/p&gt;\n\n&lt;p&gt;- Define Key Terms and Concepts clearly and concisely, and provide additional resources or references when necessary.&lt;/p&gt;\n\n&lt;p&gt;- Encourage Active Learning by asking follow-up questions or providing exercises for the user to practice what they&amp;#39;ve learned.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Someone else mentioned a similar thing &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1g5np9i/meta_ais_hidden_prompt/\"&gt;here&lt;/a&gt;, saying it showed their full address. In my case, it included only the region and the current date.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m98jl8",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m98jl8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ALE5SI0",
          "discussion_type": null,
          "num_comments": 63,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m98jl8/meta_ai_on_whatsapp_hides_a_system_prompt/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m98jl8",
          "subreddit_subscribers": 504486,
          "created_utc": 1753471858,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🚀 We’re excited to introduce Qwen3-235B-A22B-Thinking-2507 — our most advanced reasoning model yet!\n\nOver the past 3 months, we’ve significantly scaled and enhanced the thinking capability of Qwen3, achieving:\n✅ Improved performance in logical reasoning, math, science &amp; coding\n✅ Better general skills: instruction following, tool use, alignment\n✅ 256K native context for deep, long-form understanding\n\n🧠 Built exclusively for thinking mode, with no need to enable it manually. The model now natively supports extended reasoning chains for maximum depth and accuracy.",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-Thinking-2507 released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vegq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 729,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 729,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/-odbY7J30GjzUj_7jlWdKXiqJnZvfwCCllktRqnbgQw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753438585,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🚀 We’re excited to introduce Qwen3-235B-A22B-Thinking-2507 — our most advanced reasoning model yet!&lt;/p&gt;\n\n&lt;p&gt;Over the past 3 months, we’ve significantly scaled and enhanced the thinking capability of Qwen3, achieving:\n✅ Improved performance in logical reasoning, math, science &amp;amp; coding\n✅ Better general skills: instruction following, tool use, alignment\n✅ 256K native context for deep, long-form understanding&lt;/p&gt;\n\n&lt;p&gt;🧠 Built exclusively for thinking mode, with no need to enable it manually. The model now natively supports extended reasoning chains for maximum depth and accuracy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/bvx1dbl5xzef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?auto=webp&amp;s=859c619548c1493932ad87e55f7f58a1af5e10a9",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=12b042c0d833ea5fda0bb3962502543415136139",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f46d3176f8fa24281464889257c33a01a1436c1a",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f8f68cc4715e6804c2c5837be4369857e2df0466",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f579818ebd6748b55b90f802c28f4d37095432e",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=140ba50cbe31d7a36d36327b99a4151addfcc085",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=71a7110710b3d8cba38090a6e670da187ba8f0a7",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "DoQkFL1CfB5iTwd4k2RZEMaeKWH49DDXr_m3yklloUY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8vegq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 162,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vegq/qwen3235ba22bthinking2507_released/",
          "stickied": false,
          "url": "https://i.redd.it/bvx1dbl5xzef1.jpeg",
          "subreddit_subscribers": 504486,
          "created_utc": 1753438585,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looks like we will get smaller instruct and reasoning variants of Qwen3 next week. Hopefully smaller Qwen3 coder variants aswell.",
          "author_fullname": "t2_aedi2k9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Smaller Qwen Models next week!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 120,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8w7ny",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 531,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 531,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/6kSnt6WBnJfIBSCL6q0LkQ73C2HHdl70wbVC4gMqel8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753441468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looks like we will get smaller instruct and reasoning variants of Qwen3 next week. Hopefully smaller Qwen3 coder variants aswell.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/752ts71q50ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/752ts71q50ff1.png?auto=webp&amp;s=cd553f34eb742e1d85420c6d88ba6b8cb1d3b9d6",
                  "width": 1220,
                  "height": 1052
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9445aa998ff7b1cb74e082152702795b220a5ac",
                    "width": 108,
                    "height": 93
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=de63b169d17a3c50132d99540acd46ec84af351c",
                    "width": 216,
                    "height": 186
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebafb9a27014643332158cfd5bce11fa7ce928bb",
                    "width": 320,
                    "height": 275
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=27677972e2a6faf4ae42e2c72e03cfbb90ab79cb",
                    "width": 640,
                    "height": 551
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dbb199b19983285065aac667c6d68d707942ae1d",
                    "width": 960,
                    "height": 827
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aff7f0acf9e11e6cd46908b27417dd44a8c4e224",
                    "width": 1080,
                    "height": 931
                  }
                ],
                "variants": {},
                "id": "Uxg0LnkLD_KDODwI8dd_rf7OYZXf6oVvIyFguNn4CcI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8w7ny",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "R46H4V",
          "discussion_type": null,
          "num_comments": 40,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8w7ny/smaller_qwen_models_next_week/",
          "stickied": false,
          "url": "https://i.redd.it/752ts71q50ff1.png",
          "subreddit_subscribers": 504486,
          "created_utc": 1753441468,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Finally put together my rig after months of planning into a NAS case \n\n* Threadripper PRO 7955WX\n* Arctic Freezer 4U-M (cpu cooler)\n* Gigabyte TRX50 AI TOP\n* be quiet! Dark Power Pro 13 1600W\n* JONSBO N5 Case\n* 2x RTX Pro 6000\n\nMight add a few more intake fans on the top ",
          "author_fullname": "t2_16xbdr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Compact 2x RTX Pro 6000 Rig",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9bwoy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 64,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 64,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/nIQWXwAd5JWfTiQ6FGVC1XWYiGxzWflAKHHcq1t0koA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753479993,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Finally put together my rig after months of planning into a NAS case &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Threadripper PRO 7955WX&lt;/li&gt;\n&lt;li&gt;Arctic Freezer 4U-M (cpu cooler)&lt;/li&gt;\n&lt;li&gt;Gigabyte TRX50 AI TOP&lt;/li&gt;\n&lt;li&gt;be quiet! Dark Power Pro 13 1600W&lt;/li&gt;\n&lt;li&gt;JONSBO N5 Case&lt;/li&gt;\n&lt;li&gt;2x RTX Pro 6000&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Might add a few more intake fans on the top &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/tbteu4v5b3ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?auto=webp&amp;s=e1c07ae621c323dcd20838da5641e3e02cc70f91",
                  "width": 1679,
                  "height": 1264
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb8530b9905fee9ce384e441b1786c16863f92ac",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb080d3b642a8ae64a22c14c8185ffb66785a5a8",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df5d60cb32c2381511e9f8f424415f43363b229e",
                    "width": 320,
                    "height": 240
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb7c498f0a5ad74816f597205d993264473bdbfe",
                    "width": 640,
                    "height": 481
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3cd46419a68e44716f2e3b674a590f0b8f7e8f9e",
                    "width": 960,
                    "height": 722
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f10578355cca1e987b8febd5ea512ad847a0ac02",
                    "width": 1080,
                    "height": 813
                  }
                ],
                "variants": {},
                "id": "75hqb9t8X6IAo2hcG2QxH9eY348BDsA6al0LQJShjQE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9bwoy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shadowninjaz3",
          "discussion_type": null,
          "num_comments": 45,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9bwoy/compact_2x_rtx_pro_6000_rig/",
          "stickied": false,
          "url": "https://i.redd.it/tbteu4v5b3ff1.jpeg",
          "subreddit_subscribers": 504486,
          "created_utc": 1753479993,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_w6l58p741",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama 3.3 Nemotron Super 49B v1.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9fb5t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 39,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 39,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=d4fdf213f77dc67f6bcc525b1a8210247ba2b251",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753488919,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?auto=webp&amp;s=af31b2002f0236c31cf3c91755fd855ed95ae985",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45e7c8c14055c57d8c62dad0b150faa3212ce087",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=82619b61b919798034ba0f0b798bd1e75640c0b9",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eaf3accf5409bb25bb8728256d4e2f61e2bbbeec",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8ce793850ca6936254a722184eb2367e6423fa1",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e1d1bb2008c0cfb9abdbf16638bc668942167e7",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=313d9b1115102aa6244b349a8e99c1ee840c4702",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9fb5t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheLocalDrummer",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9fb5t/llama_33_nemotron_super_49b_v15/",
          "stickied": false,
          "url": "https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5",
          "subreddit_subscribers": 504486,
          "created_utc": 1753488919,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, recently we support reka’s ai models in uzu engine. Pretty nice model. It shows good performance across all tasks and truly open source. I was able to get almost 16 t/s on my Mac studio with Ultra chip. Highly recommend to try. ",
          "author_fullname": "t2_am0r9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Reka AI models support in uzu engine",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "4p0yz1wz14ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 128,
                  "x": 108,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=36d40ff398a85b6a39c79224ffddfc09eed21c15"
                },
                {
                  "y": 256,
                  "x": 216,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=30c3e9e51c86094f782c551bca8f414951b75f43"
                },
                {
                  "y": 379,
                  "x": 320,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4aad8a712011b66ef8aeccc9c81819b541974e01"
                },
                {
                  "y": 759,
                  "x": 640,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ddbf108ac7dcc66e4176ebaeac81580b1b28b4f2"
                },
                {
                  "y": 1139,
                  "x": 960,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=aa2931c03d7bd0b5093dce97885c12549bf6323e"
                },
                {
                  "y": 1282,
                  "x": 1080,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=834d2b109d53329203b2a5be882ffe65f195c7d9"
                }
              ],
              "s": {
                "y": 2048,
                "x": 1725,
                "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=1725&amp;format=pjpg&amp;auto=webp&amp;s=8c7c356b82f1f6ca05e733b2891ee093e8167694"
              },
              "id": "4p0yz1wz14ff1"
            },
            "t6wtfgwz14ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=addc90584f3d6ade4a36295de42ee6dc5b54dd0c"
                },
                {
                  "y": 95,
                  "x": 216,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1b0ce1a18524c47547285a545915c55a8fc8ee41"
                },
                {
                  "y": 141,
                  "x": 320,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a22bd5151d83d4eb08ac8e1076b99a54534c048d"
                },
                {
                  "y": 283,
                  "x": 640,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3a44d4f2806f502111ec8b1b8d62274e8bee42b1"
                },
                {
                  "y": 424,
                  "x": 960,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e28dc031ec52f98a47c6c2dbe703b453d63b599b"
                },
                {
                  "y": 477,
                  "x": 1080,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3fc56b654a49aca496c13c3936531b19135621ef"
                }
              ],
              "s": {
                "y": 722,
                "x": 1632,
                "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=1632&amp;format=pjpg&amp;auto=webp&amp;s=7f198172f1ac57c4ddc796fdc37624ddc0123511"
              },
              "id": "t6wtfgwz14ff1"
            }
          },
          "name": "t3_1m9f7lq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 30,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "4p0yz1wz14ff1",
                "id": 714080367
              },
              {
                "media_id": "t6wtfgwz14ff1",
                "id": 714080368
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/afA01kAaHMzrELSiWVNCsofMUALmXIzdtd4uSzHvlv4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753488641,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, recently we support reka’s ai models in uzu engine. Pretty nice model. It shows good performance across all tasks and truly open source. I was able to get almost 16 t/s on my Mac studio with Ultra chip. Highly recommend to try. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m9f7lq",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9f7lq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "darkolorin",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9f7lq/reka_ai_models_support_in_uzu_engine/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m9f7lq",
          "subreddit_subscribers": 504486,
          "created_utc": 1753488641,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Qwen just dropped a triple update. After months out of the spotlight, Qwen is back and bulked up. You can literally see the gains; the training shows. I was genuinely impressed.\n\nI once called Alibaba “the first Chinese LLM team to evolve from engineering to product.” This week, I need to upgrade that take: it’s now setting the release tempo and product standards for open-source AI.\n\nThis week’s triple release effectively reclaims the high ground across all three major pillars of open-source models:\n\n1️⃣ Qwen3-235B-A22B-Instruct-2507: Outstanding results across GPQA, AIME25, LiveCodeBench, Arena-Hard, BFCL, and more. It even outperformed Claude 4 (non-thinking variant). The research group Artificial Analysis didn’t mince words: “Qwen3 is the world’s smartest non-thinking base model.”\n\n2️⃣ Qwen3-Coder: This is a full-on ecosystem play for AI programming. It outperformed GPT-4.1 and Claude 4 in multilingual SWE-bench, Mind2Web, Aider-Polyglot, and more—and it took the top spot on Hugging Face’s overall leaderboard. The accompanying CLI tool, Qwen Code, clearly aims to become the “default dev workflow component.”\n\n3️⃣ Qwen3-235B-A22B-Thinking-2507: With 256K context support and top-tier performance on SuperGPQA, LiveCodeBench v6, AIME25, Arena-Hard v2, WritingBench, and MultiIF, this model squares up directly against Gemini 2.5 Pro and o4-mini, pushing open-source inference models to the threshold of closed-source elite.\n\nThis isn’t about “can one model compete.” Alibaba just pulled off a coordinated strike: base models, code models, inference models—all firing in sync. Behind it all is a full-stack platform play: cloud infra, reasoning chains, agent toolkits, community release cadence.\n\nAnd the momentum isn’t stopping. Wan 2.2, Alibaba’s upcoming video generation model, is next. Built on the heels of the highly capable Wan 2.1 (which topped VBench with advanced motion and multilingual text rendering), Wan 2.2 promises even better video quality, controllability, and resource efficiency. It’s expected to raise the bar in open-source T2V (text-to-video) generation—solidifying Alibaba’s footprint not just in LLMs, but in multimodal generative AI.\n\nOpen source isn’t just “throwing code over the wall.” It’s delivering production-ready, open products—and Alibaba is doing exactly that.\n\nLet’s not forget: Alibaba has open-sourced 300+ Qwen models and over 140,000 derivatives, making it the largest open-source model family on the planet. And they’ve pledged another ¥380 billion over the next three years into cloud and AI infrastructure. This isn’t a short-term leaderboard sprint. They’re betting big on locking down end-to-end certainty, from model to infrastructure to deployment.\n\nNow look across the Pacific: the top U.S. models are mostly going closed. GPT-4 isn’t open. Gemini’s locked down. Claude’s gated by API. Meanwhile, Alibaba is using the “open-source + engineering + infrastructure” trifecta to set a global usability bar.\n\nThis isn’t a “does China have the chops?” moment. Alibaba’s already in the center of the world stage setting the tempo.\n\nReminds me of that line:\n“The GOAT doesn’t announce itself. It just keeps dropping.”\nRight now, it’s Alibaba that’s dropping. And flexing. 💪\n",
          "author_fullname": "t2_1heeqeidfc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Qwen’s TRIPLE release this week + Vid Gen model coming",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 111,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "weyaltspa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 86,
                  "x": 108,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=53ce79a67d338965235c56e2b3c00371e974dbe1"
                },
                {
                  "y": 172,
                  "x": 216,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd440fb8a85758efbb475066273ab978fa2e31dd"
                },
                {
                  "y": 255,
                  "x": 320,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d73a40edb86f641e7dc993717fdabb1853d2fc57"
                },
                {
                  "y": 511,
                  "x": 640,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d02c98cec01bde55a3e47b888b2400f1f535faf9"
                },
                {
                  "y": 766,
                  "x": 960,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1ec384067d5628ba8c9c25c816e80858867e7dde"
                },
                {
                  "y": 862,
                  "x": 1080,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=088f1aa4e0e7ae89f53740b4b244309b4cc57c64"
                }
              ],
              "s": {
                "y": 1246,
                "x": 1560,
                "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=1560&amp;format=pjpg&amp;auto=webp&amp;s=646b9961a87411f9179fed9fabde4545cac4f233"
              },
              "id": "weyaltspa1ff1"
            },
            "rkucntspa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=431b653a87072eef7b1c2652642ba389579a40ed"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=99cb5b6ceda89950156b46794c003069b4dd7881"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=965f28cd42906c7be276d34bd91f92db37b2c8f2"
                },
                {
                  "y": 332,
                  "x": 640,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e9f7c6f4af3c833f41d94634e6417e6d530cb764"
                },
                {
                  "y": 498,
                  "x": 960,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=eb633243bc7c260d045d51f138b772e29c1cf0d3"
                },
                {
                  "y": 560,
                  "x": 1080,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e904c5c75a55c2bd3160c977e58e33c7bd24d86"
                }
              ],
              "s": {
                "y": 1062,
                "x": 2047,
                "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=2047&amp;format=pjpg&amp;auto=webp&amp;s=985561953a8e2dd8214a0cc19497018149c9b8f4"
              },
              "id": "rkucntspa1ff1"
            },
            "8cbupsspa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=df0598f5f14a194c8aa673c5d77c368f3e393137"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=00577984f574aecec96f74011d0eecb6da2d615e"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e7ba82e753109d6fe2cf5a27b46cb0a108e9f5c"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bcda3d8e32eb52d5034d144fb8966a452a2f959b"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dd48745de4fe6fd795245e424c5d39f76a28b1c3"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6a027d91631c830aa24133f6bbda2dc668090fd2"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=981e0f6470b985ff1071afb0638cfcc558c99486"
              },
              "id": "8cbupsspa1ff1"
            },
            "bixl6wspa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 192,
                  "x": 108,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d567de59dba8708e67b9b0a966a356f6973d2d55"
                },
                {
                  "y": 384,
                  "x": 216,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c766b54adc9c5659f67a85cf94b669359681379"
                },
                {
                  "y": 569,
                  "x": 320,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b383bad9df5683b11bb5e52def2b2cc4ba6fc4be"
                },
                {
                  "y": 1138,
                  "x": 640,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d9d6cb336943264ca9105c0a28e69d31ea8a84c1"
                },
                {
                  "y": 1707,
                  "x": 960,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=df12665ad4beb866223355da6fed3cf2ca2fe35f"
                },
                {
                  "y": 1920,
                  "x": 1080,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f7daacccd304ebd74a972ba6444192b0bca7faed"
                }
              ],
              "s": {
                "y": 2001,
                "x": 1125,
                "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=1125&amp;format=pjpg&amp;auto=webp&amp;s=14554ad8530cbde390f1b2f0a2daa7d4f41adfd6"
              },
              "id": "bixl6wspa1ff1"
            },
            "x2vw27vpa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 72,
                  "x": 108,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=46d863470e5d32aaa73c24f247b91e32e1329b36"
                },
                {
                  "y": 144,
                  "x": 216,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f1576e9b3d96ae4481868ac8d78d41570b50604e"
                },
                {
                  "y": 213,
                  "x": 320,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=25c79c61db0c84815aeae9c6ec198b07ed5103e4"
                },
                {
                  "y": 426,
                  "x": 640,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc2e946a62af671691e0703106826b71cbc310ec"
                },
                {
                  "y": 640,
                  "x": 960,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=888dfa1e91bb0925f111db4b5b2c0dc7402e34dc"
                },
                {
                  "y": 720,
                  "x": 1080,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2b17f12d1e40be268d30de681d437cee0be28626"
                }
              ],
              "s": {
                "y": 720,
                "x": 1080,
                "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=d036c317cd811935e5df8428e2685246f9257818"
              },
              "id": "x2vw27vpa1ff1"
            },
            "44dlq7vpa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a45646380b2ee977aec7a49fd8ea39829673be5c"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e5bdd6314769468795cb87e0ac58d635bbb8c11"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=730c2116315f6311eec246e72385269af5c1c94a"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=580a3d155c46b202a7ad86ee13ca6771c38bfaad"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b73200d7fcb3def58b029187ef8eec207ed809c3"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eac2272f6da2670c9c08c1a40906ae42e72f2bc7"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=5e321123f77a67327af31326bde0e3a3e452ca73"
              },
              "id": "44dlq7vpa1ff1"
            },
            "5y2rk6vpa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/5y2rk6vpa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d25d51dcce2ea38791b7272845e88a7ac468247"
                },
                {
                  "y": 189,
                  "x": 216,
                  "u": "https://preview.redd.it/5y2rk6vpa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f7f89c7879ac4eb0415b468240c653fcff4090a5"
                },
                {
                  "y": 281,
                  "x": 320,
                  "u": "https://preview.redd.it/5y2rk6vpa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d6a58de7b19a2b4e5a4ae5420d4f25e0456b671"
                }
              ],
              "s": {
                "y": 357,
                "x": 406,
                "u": "https://preview.redd.it/5y2rk6vpa1ff1.jpg?width=406&amp;format=pjpg&amp;auto=webp&amp;s=22c2ae44f200b41e10ff0c3c01252066a2bdab71"
              },
              "id": "5y2rk6vpa1ff1"
            }
          },
          "name": "t3_1m91b98",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 163,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "weyaltspa1ff1",
                "id": 713749073
              },
              {
                "media_id": "x2vw27vpa1ff1",
                "id": 713749074
              },
              {
                "media_id": "44dlq7vpa1ff1",
                "id": 713749075
              },
              {
                "media_id": "8cbupsspa1ff1",
                "id": 713749076
              },
              {
                "media_id": "rkucntspa1ff1",
                "id": 713749077
              },
              {
                "media_id": "5y2rk6vpa1ff1",
                "id": 713749078
              },
              {
                "media_id": "bixl6wspa1ff1",
                "id": 713749079
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 163,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Hp7akddNSmnkkxGnEtjYFrudFRvRoCTXEnawFbp7MZQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753455254,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Qwen just dropped a triple update. After months out of the spotlight, Qwen is back and bulked up. You can literally see the gains; the training shows. I was genuinely impressed.&lt;/p&gt;\n\n&lt;p&gt;I once called Alibaba “the first Chinese LLM team to evolve from engineering to product.” This week, I need to upgrade that take: it’s now setting the release tempo and product standards for open-source AI.&lt;/p&gt;\n\n&lt;p&gt;This week’s triple release effectively reclaims the high ground across all three major pillars of open-source models:&lt;/p&gt;\n\n&lt;p&gt;1️⃣ Qwen3-235B-A22B-Instruct-2507: Outstanding results across GPQA, AIME25, LiveCodeBench, Arena-Hard, BFCL, and more. It even outperformed Claude 4 (non-thinking variant). The research group Artificial Analysis didn’t mince words: “Qwen3 is the world’s smartest non-thinking base model.”&lt;/p&gt;\n\n&lt;p&gt;2️⃣ Qwen3-Coder: This is a full-on ecosystem play for AI programming. It outperformed GPT-4.1 and Claude 4 in multilingual SWE-bench, Mind2Web, Aider-Polyglot, and more—and it took the top spot on Hugging Face’s overall leaderboard. The accompanying CLI tool, Qwen Code, clearly aims to become the “default dev workflow component.”&lt;/p&gt;\n\n&lt;p&gt;3️⃣ Qwen3-235B-A22B-Thinking-2507: With 256K context support and top-tier performance on SuperGPQA, LiveCodeBench v6, AIME25, Arena-Hard v2, WritingBench, and MultiIF, this model squares up directly against Gemini 2.5 Pro and o4-mini, pushing open-source inference models to the threshold of closed-source elite.&lt;/p&gt;\n\n&lt;p&gt;This isn’t about “can one model compete.” Alibaba just pulled off a coordinated strike: base models, code models, inference models—all firing in sync. Behind it all is a full-stack platform play: cloud infra, reasoning chains, agent toolkits, community release cadence.&lt;/p&gt;\n\n&lt;p&gt;And the momentum isn’t stopping. Wan 2.2, Alibaba’s upcoming video generation model, is next. Built on the heels of the highly capable Wan 2.1 (which topped VBench with advanced motion and multilingual text rendering), Wan 2.2 promises even better video quality, controllability, and resource efficiency. It’s expected to raise the bar in open-source T2V (text-to-video) generation—solidifying Alibaba’s footprint not just in LLMs, but in multimodal generative AI.&lt;/p&gt;\n\n&lt;p&gt;Open source isn’t just “throwing code over the wall.” It’s delivering production-ready, open products—and Alibaba is doing exactly that.&lt;/p&gt;\n\n&lt;p&gt;Let’s not forget: Alibaba has open-sourced 300+ Qwen models and over 140,000 derivatives, making it the largest open-source model family on the planet. And they’ve pledged another ¥380 billion over the next three years into cloud and AI infrastructure. This isn’t a short-term leaderboard sprint. They’re betting big on locking down end-to-end certainty, from model to infrastructure to deployment.&lt;/p&gt;\n\n&lt;p&gt;Now look across the Pacific: the top U.S. models are mostly going closed. GPT-4 isn’t open. Gemini’s locked down. Claude’s gated by API. Meanwhile, Alibaba is using the “open-source + engineering + infrastructure” trifecta to set a global usability bar.&lt;/p&gt;\n\n&lt;p&gt;This isn’t a “does China have the chops?” moment. Alibaba’s already in the center of the world stage setting the tempo.&lt;/p&gt;\n\n&lt;p&gt;Reminds me of that line:\n“The GOAT doesn’t announce itself. It just keeps dropping.”\nRight now, it’s Alibaba that’s dropping. And flexing. 💪&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m91b98",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m91b98",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "koc_Z3",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m91b98/qwens_triple_release_this_week_vid_gen_model/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m91b98",
          "subreddit_subscribers": 504486,
          "created_utc": 1753455254,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_14mlbg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hunyuan (Ex-WizardLM) Dense Model Coming Soon!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m94ls2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 69,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 69,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/jB1CS2LZrHw4qZHwx53ViD8xx1eEncZt0jht4lZ5-yw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753462750,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14878",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?auto=webp&amp;s=0fb34e0f66b8292996122ca9f453b58a37b14813",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1baecdc2dfde1050ccffe3b9db6f4cad84a6a26",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b7d9c1d3dc4da4b79164f82841fe3a8e4f0301c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8e2c1e124ff0e7f72b87f1fd2dae8eacf659993",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=04fdf799a0982d200ae760ce600052e22efe1fd7",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d09d8c9f8137e0686807b5b322768faea8d8935d",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0c6f33fac669b52e589697bc73e7a0408f351898",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "9GvEX7LrasZYgCzEEJMA4dtKp0bfjGuzNOUm65ANRbI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m94ls2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TKGaming_11",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m94ls2/hunyuan_exwizardlm_dense_model_coming_soon/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14878",
          "subreddit_subscribers": 504486,
          "created_utc": 1753462750,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7pfgfkis",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Qwen3 on Fiction.liveBench",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m93d0r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 80,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 80,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/VIv9hJCgW9w5E1qzXymkc_nKdHvwujLLN61ek5o0LXE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753459907,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/hvi3tvmjo1ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?auto=webp&amp;s=6258c8c89fb879cded6d316f86467b097f492051",
                  "width": 1568,
                  "height": 2318
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=385e269d9eecbdde591e638694025b90d6447acc",
                    "width": 108,
                    "height": 159
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=419c2fef6a3435f83900687fabdaca18e00bdf81",
                    "width": 216,
                    "height": 319
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=73f13ee604283c4240ce3dc6e2e12c02fbafbcf5",
                    "width": 320,
                    "height": 473
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8c9a4a27cda997bb1fb4bc522e4d9bac9f04231",
                    "width": 640,
                    "height": 946
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ca984a8304eade76aa7c934ed429e1c9a1fcc949",
                    "width": 960,
                    "height": 1419
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1faa0dda9eeec65358a99b5f5dcf6f3256542ec9",
                    "width": 1080,
                    "height": 1596
                  }
                ],
                "variants": {},
                "id": "ZokGAHU7CGwFeHWan5TASqOpdAfZ1F2vcHM0eru8zuw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m93d0r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fictionlive",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m93d0r/new_qwen3_on_fictionlivebench/",
          "stickied": false,
          "url": "https://i.redd.it/hvi3tvmjo1ff1.png",
          "subreddit_subscribers": 504486,
          "created_utc": 1753459907,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm happy to see this as my experience with these models for image recognition isn't very impressive. They mostly can't even tell when pictures are sideways, for example.",
          "author_fullname": "t2_5b972ieo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.1V-9B-Thinking - claims to \"match or surpass Qwen2.5-72B\" on many tasks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8xmy9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 149,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 149,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=ff3e77630a6d9903e8ffc2da84d81c479305e7d8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753445934,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m happy to see this as my experience with these models for image recognition isn&amp;#39;t very impressive. They mostly can&amp;#39;t even tell when pictures are sideways, for example.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/THUDM/GLM-4.1V-Thinking",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?auto=webp&amp;s=f3d8a19a2706316e04aaa9fd5ea8ed3c15e6f304",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f8ce37456b595d44518bc9dbb50bfbbdc4bdd6f",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=613a2c6b97d63cde3e9b6e1957cffa5cfa955644",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9e5860036f1ae510f4d52f708d0e080abc80cd5",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=841806f55387309a64d67cd8a7a49351c70e6ab2",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1f52fb001e2e3c7ce1ba59b80df2da7d7b72a91f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b77075add5d3dfb46a095004810d01976a798dec",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8xmy9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pristine-Woodpecker",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8xmy9/glm41v9bthinking_claims_to_match_or_surpass/",
          "stickied": false,
          "url": "https://github.com/THUDM/GLM-4.1V-Thinking",
          "subreddit_subscribers": 504486,
          "created_utc": 1753445934,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Watching everyone else drop new models while knowing you’re going to release the best open source model of all time in about 20 years.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8myxl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 993,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 993,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/RMM5ptmjVG7kaBZS0SOzo0NX4eL3tYZb179ptZ_bN9I.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753408933,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nl9jgkkzgxef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?auto=webp&amp;s=8465f34a7f504c523ccda4f16197b27156796978",
                  "width": 1024,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b85428e434a7d4cd150f23a38f934c57dbd23502",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1164a07e29c695bb059e756e08babe234ff379d6",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5315a863e1565b2b6dd93c5a571cf54fb94a33ad",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5596e2098d9a669775268db5ef71e54bd685cd0d",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d61f41e97643e7ae41cdc73d41cb27bc076e45f",
                    "width": 960,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "e_AIfm2x33q9UQnGSrgnpxiaWz6SPpbRIOgsMere00w"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m8myxl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 50,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8myxl/watching_everyone_else_drop_new_models_while/",
          "stickied": false,
          "url": "https://i.redd.it/nl9jgkkzgxef1.jpeg",
          "subreddit_subscribers": 504486,
          "created_utc": 1753408933,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is an AI web browser that uses local AI models. It's still very early, FULL of bugs and missing key features as a browser, but still good to play around with it.   \n  \nDownload it from [Github](https://github.com/nuance-dev/Web)\n\nNote: AI features only work with M series chips.",
          "author_fullname": "t2_7tlxcyy6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I created an open-source macOS AI browser that uses MLX and Gemma 3n, feel free to fork it!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m903il",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 98,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/fculp27z11ff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1834,
              "scrubber_media_url": "https://v.redd.it/fculp27z11ff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/fculp27z11ff1/DASHPlaylist.mpd?a=1756084194%2CNzIzZGM3Y2ZjNmQyYTdiMzhkYjhjNmU2NGI3YTdkYzY0OGMzZDc2Mzc2MzU2ZmU1NWM3MDA3OWE2ZGRjODI4OA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 55,
              "hls_url": "https://v.redd.it/fculp27z11ff1/HLSPlaylist.m3u8?a=1756084194%2CZjY4YzQ2ZWIwYmQ4NjhmY2Y5YTEyMDhlYzk3YjI5YzkwNjFlZDY4OGI2MDg4OWM4Mzc4ZTAyYmEyNGMxZjAyNQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 98,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=140&amp;height=82&amp;crop=140:82,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=ec09409f07401537e98387108dfed9051e1440fa",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753452396,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is an AI web browser that uses local AI models. It&amp;#39;s still very early, FULL of bugs and missing key features as a browser, but still good to play around with it.   &lt;/p&gt;\n\n&lt;p&gt;Download it from &lt;a href=\"https://github.com/nuance-dev/Web\"&gt;Github&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Note: AI features only work with M series chips.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/fculp27z11ff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?format=pjpg&amp;auto=webp&amp;s=7c21e43bb9a5da5da2c2010ce93da12804258bf5",
                  "width": 2844,
                  "height": 1674
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fcdf109b4fdfd5cf54e3e2e866680aebaec3a5ae",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=112befd5fa6112b6cb09d0adc2d4a9d58cfe354c",
                    "width": 216,
                    "height": 127
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6be30ac241df05f59330c9fd5fd4452821ab66fc",
                    "width": 320,
                    "height": 188
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d5ddaa6122638b83743d7370ed78028c80cb5d52",
                    "width": 640,
                    "height": 376
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a46f147a34e2d2ab597964ba48f2a74696b5c102",
                    "width": 960,
                    "height": 565
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ed6db97d6de07a2a5b3adbb6c2277382e6257018",
                    "width": 1080,
                    "height": 635
                  }
                ],
                "variants": {},
                "id": "NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m903il",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sirjoaco",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m903il/i_created_an_opensource_macos_ai_browser_that/",
          "stickied": false,
          "url": "https://v.redd.it/fculp27z11ff1",
          "subreddit_subscribers": 504486,
          "created_utc": 1753452396,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/fculp27z11ff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1834,
              "scrubber_media_url": "https://v.redd.it/fculp27z11ff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/fculp27z11ff1/DASHPlaylist.mpd?a=1756084194%2CNzIzZGM3Y2ZjNmQyYTdiMzhkYjhjNmU2NGI3YTdkYzY0OGMzZDc2Mzc2MzU2ZmU1NWM3MDA3OWE2ZGRjODI4OA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 55,
              "hls_url": "https://v.redd.it/fculp27z11ff1/HLSPlaylist.m3u8?a=1756084194%2CZjY4YzQ2ZWIwYmQ4NjhmY2Y5YTEyMDhlYzk3YjI5YzkwNjFlZDY4OGI2MDg4OWM4Mzc4ZTAyYmEyNGMxZjAyNQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/Alibaba_Qwen/status/1948688466386280706?t=7T6_M6vN6HrK4wvLjFNVBg&amp;s=19",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Amazing qwen 3 updated thinking model just released !! Open source !",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vhp3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 198,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 198,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/F41cNbRL6zX_oMjLZyQ-1dfZZnw6cXvW4eSrGH95qqI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753438909,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/Alibaba_Qwen/status/1948688466386280706?t=7T6_M6vN6HrK4wvLjFNVBg&amp;amp;s=19\"&gt;https://x.com/Alibaba_Qwen/status/1948688466386280706?t=7T6_M6vN6HrK4wvLjFNVBg&amp;amp;s=19&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nx5d8w74yzef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?auto=webp&amp;s=96c200cb190833b2bf1f5012444f0dc3b238d209",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b98d17e59ac2e72b931b0b8fd7215c2bc7e353d",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a348a0bdfb7057916cf5942fbc64bb0dc17a34e",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=956c91eab46464e833aaeb318551a0b4b8b10b46",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d728468419b7ffc3426c85447250b3cc034f70a",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5dab0ca10ad6fe3a88aa470b1eabd3b5a53a9b77",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a7f6c6dd942072b45b5ae2a1c5871cfece1acdb7",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "KvH8TJ7zk2PBZHdgGic1Zrs7h6lhFOr63lQmnuZcFlg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8vhp3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vhp3/amazing_qwen_3_updated_thinking_model_just/",
          "stickied": false,
          "url": "https://i.redd.it/nx5d8w74yzef1.jpeg",
          "subreddit_subscribers": 504486,
          "created_utc": 1753438909,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just finished uploading and perplexity testing some new ik\\_llama.cpp quants. Despite the random github takedown (and subsequent restoring) ik\\_llama.cpp is going strong!\n\nik just refreshed the IQ4\\_KSS 4.0 bpw non-linear quantization for faster performance and great perplexity so this quant hits a sweet spot at \\~114GiB allowing 2x64GB DDR5 gaming rigs with a single GPU to run it with decently long context lengths.\n\nAlso ik\\_llama.cpp recently had some PRs to improve tool/function calling.\n\nIf you have more RAM, check out my larger Qwen3-Coder-480B-A35B-Instruct-GGUF quants if that is your thing.\n\nCheers!",
          "author_fullname": "t2_n321yfw5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "IQ4_KSS 114 GiB and more ik_llama.cpp exclusive quants!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9cp2n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": "#bbbdbf",
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=c5722281288ad17c5425d7d73e761d8eafbf6b87",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753481963,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just finished uploading and perplexity testing some new ik_llama.cpp quants. Despite the random github takedown (and subsequent restoring) ik_llama.cpp is going strong!&lt;/p&gt;\n\n&lt;p&gt;ik just refreshed the IQ4_KSS 4.0 bpw non-linear quantization for faster performance and great perplexity so this quant hits a sweet spot at ~114GiB allowing 2x64GB DDR5 gaming rigs with a single GPU to run it with decently long context lengths.&lt;/p&gt;\n\n&lt;p&gt;Also ik_llama.cpp recently had some PRs to improve tool/function calling.&lt;/p&gt;\n\n&lt;p&gt;If you have more RAM, check out my larger Qwen3-Coder-480B-A35B-Instruct-GGUF quants if that is your thing.&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/ubergarm/Qwen3-235B-A22B-Thinking-2507-GGUF",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?auto=webp&amp;s=f0dc878274992b77737eb6b8b4cf221787490f16",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a69f6d9c0fc9888f89ac3d1e39ffbc13bdff2b89",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4746d87fa4ee43c4c2ad291169131df06a3880bb",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=64c7166a9e592474054818234e6a8fed62af8aca",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=14512857abb7d9ebb3c411f65e1639efb84a8b4a",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce689ee00af8c47b796b25b64ba7f36e852011a9",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=941654186ea54e774e0ff104ec12535879418828",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9cp2n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VoidAlchemy",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m9cp2n/iq4_kss_114_gib_and_more_ik_llamacpp_exclusive/",
          "stickied": false,
          "url": "https://huggingface.co/ubergarm/Qwen3-235B-A22B-Thinking-2507-GGUF",
          "subreddit_subscribers": 504486,
          "created_utc": 1753481963,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1e1w1ul46b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China's ByteDance's coze studio is now open source",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9enpd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=709bb632f62e78f1f26e0151f0de6c4608d82690",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753487124,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/coze-dev/coze-studio",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?auto=webp&amp;s=07e1667d1dae1fd50ce974c6233473c6ab6e2896",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1bebfe8068f1cdc71db48800cd8cedda3dc840b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a71404f6c427fecb5d4913ce33e193b1779dacf",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c70088efefd6cd274b9acd4ffd1b665856007c7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=514d1ba7344eb0ac49b443160f59e849fa2f73f5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9e6b310dabbf5f7fa5c3754cf321fbaf151547e4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=303cd5df5d78c96c3fd46565b173007a7601d592",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m9enpd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun-Doctor6855",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9enpd/chinas_bytedances_coze_studio_is_now_open_source/",
          "stickied": false,
          "url": "https://github.com/coze-dev/coze-studio",
          "subreddit_subscribers": 504486,
          "created_utc": 1753487124,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/ \n\n“If you listen to the hype, it’s like we should be seeing AI doctors and AI lawyers and AI software engineers, and that’s just not true,” he says. “If we can’t even get more than 10% on a contamination-free SWE-Bench, that’s the reality check for me.”",
          "author_fullname": "t2_nrsswg757",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A contamination-free coding benchmark shows AI may not be as excellent as claimed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ud84",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 165,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 165,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753434586,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/\"&gt;https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;“If you listen to the hype, it’s like we should be seeing AI doctors and AI lawyers and AI software engineers, and that’s just not true,” he says. “If we can’t even get more than 10% on a contamination-free SWE-Bench, that’s the reality check for me.”&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?auto=webp&amp;s=2bfdda582644a1925177c0e421a4a2c6950c77a7",
                  "width": 1200,
                  "height": 675
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=faea71883e14653e5ce297d91fc495960f8b18eb",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fafd8205cf207b0508b5104593c16cf4c0ffe3de",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1cbb456de00218886e9420164f12a5eca3ff0d1",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f4b19f9c8b648f4414c2572113329cabf9dd2693",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=078b612839727b8bd391f293af598403ad688643",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c128b6e2f538773e90e5bc76e74a71f6424fcaed",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8ud84",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Creepy-Document4034",
          "discussion_type": null,
          "num_comments": 38,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8ud84/a_contaminationfree_coding_benchmark_shows_ai_may/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8ud84/a_contaminationfree_coding_benchmark_shows_ai_may/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753434586,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Check out this chart comparing the latest Qwen3-235B-A22B-2507 models (Instruct and Thinking) to the older versions. The improvements are huge across different tests:\n\n\t•\tGPQA (Graduate-level reasoning): 81 → 71\n\t•\tAIME2025 (Math competition problems): 92 → 81\n\t•\tLiveCodeBench v6 (Code generation and debugging): 74 → 56\n\t•\tArena-Hard v2 (General problem-solving): 80 → 62\n\nEven the new instruct version is way better than the old non-thinking one. Looks like they’ve really boosted reasoning and coding skills here.\n\nWhat do you think is driving this jump, better training, bigger data, or new techniques?",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Qwen3-235B update is crushing old models in benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 81,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8w9ah",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 108,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 108,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/wQ7SbNTBzIdOQb6lfJFHU10NFUkitTQk5yMGuXDJ-EY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753441629,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Check out this chart comparing the latest Qwen3-235B-A22B-2507 models (Instruct and Thinking) to the older versions. The improvements are huge across different tests:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;• GPQA (Graduate-level reasoning): 81 → 71\n• AIME2025 (Math competition problems): 92 → 81\n• LiveCodeBench v6 (Code generation and debugging): 74 → 56\n• Arena-Hard v2 (General problem-solving): 80 → 62\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Even the new instruct version is way better than the old non-thinking one. Looks like they’ve really boosted reasoning and coding skills here.&lt;/p&gt;\n\n&lt;p&gt;What do you think is driving this jump, better training, bigger data, or new techniques?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/q009687760ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/q009687760ff1.jpeg?auto=webp&amp;s=b2eff0f0d944d9f3cc3dd7822d8f074bf89032b7",
                  "width": 2379,
                  "height": 1392
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f76378abbbe79bad791d59ab511364ecf839f4ba",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a7d7562fa1030bd11fb35ae20f3f153be32261ae",
                    "width": 216,
                    "height": 126
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dff32d54f41f3760a510fcbf21e705869a748449",
                    "width": 320,
                    "height": 187
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee6d42068b310b231eceef2e74d8ae35c50e819e",
                    "width": 640,
                    "height": 374
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d7c3a37e2d1bba48fd4f97b28bd78fe7580bb2ca",
                    "width": 960,
                    "height": 561
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8cfaba909996daa45a067d35347b915f94c15843",
                    "width": 1080,
                    "height": 631
                  }
                ],
                "variants": {},
                "id": "5z0PiohPQ5P8oWxfKaPaT1JALPktWest18Z3iN05GrQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8w9ah",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8w9ah/new_qwen3235b_update_is_crushing_old_models_in/",
          "stickied": false,
          "url": "https://i.redd.it/q009687760ff1.jpeg",
          "subreddit_subscribers": 504486,
          "created_utc": 1753441629,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Curious how the two new thinking/non thinking stack up vs deepseek.",
          "author_fullname": "t2_frxyil1r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any Rpers test the new qwen 2507 yet?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9ajf9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753476645,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious how the two new thinking/non thinking stack up vs deepseek.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9ajf9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Antique_Bit_1049",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9ajf9/any_rpers_test_the_new_qwen_2507_yet/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9ajf9/any_rpers_test_the_new_qwen_2507_yet/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753476645,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "its show time folks",
          "author_fullname": "t2_7g0m6735",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-235B-A22B-Thinking-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vjna",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 99,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 99,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=21733a4ca840c3530b7ab1a5843dfdeba5a3f822",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753439107,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;its show time folks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?auto=webp&amp;s=91ee507d4a4a214e9d8d575336cf37333e5678f2",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd52c9fa4a571e95dfd71b26b8e6ebff17bbc117",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=159f17b22507b591ab3268fba6357cfbc5b4d5ed",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ce88d6294a42f488b4c5238bcdd5abcbb6bd0f2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdec699720d09b0abd832855f564b348eefd2304",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a336e6ae20fea77a6e44bc4f35540e297e8cce2c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6c85c9c232a3126b98c3e0be994b7cb036c1e34d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8vjna",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ApprehensiveAd3629",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vjna/qwenqwen3235ba22bthinking2507/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
          "subreddit_subscribers": 504486,
          "created_utc": 1753439107,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In 2024, we developed SWE-bench and SWE-agent at Princeton University and helped kickstart the coding agent revolution.\n\nBack then, LMs were optimized to be great at chatting, but not much else. This meant that agent scaffolds had to get very creative (and complicated) to make LMs perform useful work.\n\nBut in 2025 LMs are actively optimized for agentic coding, and we ask:\n\n**What the simplest coding agent that could still score near SotA on the benchmarks?**\n\n**Turns out, it just requires 100 lines of code!**\n\nAnd this system still **resolves 65% of all GitHub issues in the SWE-bench verified benchmark** with Sonnet 4 (for comparison, when Anthropic launched Sonnet 4, they reported 70% with their own scaffold that was never made public).\n\nHonestly, we're all pretty stunned ourselves—we've now spent more than a year developing SWE-agent, and would not have thought that such a small system could perform nearly as good.\n\nNow, admittedly, this is with Sonnet 4, which has probably the strongest agentic post-training of all LMs. But we're also working on updating the fine-tuning of our SWE-agent-LM-32B model specifically for this setting (we posted about this model here after hitting open-weight SotA on SWE-bench earlier this year).\n\nAll open source at https://github.com/SWE-agent/mini-swe-agent. The hello world example is incredibly short &amp; simple (and literally what gave us the 65% with Sonnet 4). But it is also meant as a serious command line tool + research project, so we provide a Claude-code style UI &amp; some utilities on top of that.\n\nWe have some team members from Princeton/Stanford here today, let us know if you have any questions/feedback :)",
          "author_fullname": "t2_mjzvkwd7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "mini-swe-agent achieves 65% on SWE-bench in just 100 lines of python code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8z2ut",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 48,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 48,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753455208,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753449849,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In 2024, we developed SWE-bench and SWE-agent at Princeton University and helped kickstart the coding agent revolution.&lt;/p&gt;\n\n&lt;p&gt;Back then, LMs were optimized to be great at chatting, but not much else. This meant that agent scaffolds had to get very creative (and complicated) to make LMs perform useful work.&lt;/p&gt;\n\n&lt;p&gt;But in 2025 LMs are actively optimized for agentic coding, and we ask:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What the simplest coding agent that could still score near SotA on the benchmarks?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Turns out, it just requires 100 lines of code!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;And this system still &lt;strong&gt;resolves 65% of all GitHub issues in the SWE-bench verified benchmark&lt;/strong&gt; with Sonnet 4 (for comparison, when Anthropic launched Sonnet 4, they reported 70% with their own scaffold that was never made public).&lt;/p&gt;\n\n&lt;p&gt;Honestly, we&amp;#39;re all pretty stunned ourselves—we&amp;#39;ve now spent more than a year developing SWE-agent, and would not have thought that such a small system could perform nearly as good.&lt;/p&gt;\n\n&lt;p&gt;Now, admittedly, this is with Sonnet 4, which has probably the strongest agentic post-training of all LMs. But we&amp;#39;re also working on updating the fine-tuning of our SWE-agent-LM-32B model specifically for this setting (we posted about this model here after hitting open-weight SotA on SWE-bench earlier this year).&lt;/p&gt;\n\n&lt;p&gt;All open source at &lt;a href=\"https://github.com/SWE-agent/mini-swe-agent\"&gt;https://github.com/SWE-agent/mini-swe-agent&lt;/a&gt;. The hello world example is incredibly short &amp;amp; simple (and literally what gave us the 65% with Sonnet 4). But it is also meant as a serious command line tool + research project, so we provide a Claude-code style UI &amp;amp; some utilities on top of that.&lt;/p&gt;\n\n&lt;p&gt;We have some team members from Princeton/Stanford here today, let us know if you have any questions/feedback :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?auto=webp&amp;s=00beebee91769eaf51532ccc57bd3d275aa63aec",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=891acb349e03755473266d709a20b526d0a3b86c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd0d5cd32a6fcf8bf3c16452f642d334971d721d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9dd070dd1859734ae151f229880b67ab5264767",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9dee162010eff99c6c1be51ac40a4cf02d168191",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=48f231f430d01605fb77642bb82952abf5497af2",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1061e84b12e7325a09bc650402a84654b187e3ce",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m8z2ut",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "klieret",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8z2ut/minisweagent_achieves_65_on_swebench_in_just_100/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8z2ut/minisweagent_achieves_65_on_swebench_in_just_100/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753449849,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Over the past three months, we have continued to scale the **thinking capability** of Qwen3-235B-A22B, improving both the **quality and depth** of reasoning. We are pleased to introduce **Qwen3-235B-A22B-Thinking-2507**, featuring the following key enhancements:\n\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise — achieving **state-of-the-art results among open-source thinking models**.\n* **Markedly better general capabilities**, such as instruction following, tool usage, text generation, and alignment with human preferences.\n* **Enhanced 256K long-context understanding** capabilities.",
          "author_fullname": "t2_1162lx9rgr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-235B-A22B-Thinking-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ven3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#ab96c2",
          "ups": 76,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d40ca12a-0e73-11ee-8563-f216e082168e",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 76,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=21733a4ca840c3530b7ab1a5843dfdeba5a3f822",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 2"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753438601,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Over the past three months, we have continued to scale the &lt;strong&gt;thinking capability&lt;/strong&gt; of Qwen3-235B-A22B, improving both the &lt;strong&gt;quality and depth&lt;/strong&gt; of reasoning. We are pleased to introduce &lt;strong&gt;Qwen3-235B-A22B-Thinking-2507&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Significantly improved performance&lt;/strong&gt; on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise — achieving &lt;strong&gt;state-of-the-art results among open-source thinking models&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Markedly better general capabilities&lt;/strong&gt;, such as instruction following, tool usage, text generation, and alignment with human preferences.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Enhanced 256K long-context understanding&lt;/strong&gt; capabilities.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?auto=webp&amp;s=91ee507d4a4a214e9d8d575336cf37333e5678f2",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd52c9fa4a571e95dfd71b26b8e6ebff17bbc117",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=159f17b22507b591ab3268fba6357cfbc5b4d5ed",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ce88d6294a42f488b4c5238bcdd5abcbb6bd0f2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdec699720d09b0abd832855f564b348eefd2304",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a336e6ae20fea77a6e44bc4f35540e297e8cce2c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6c85c9c232a3126b98c3e0be994b7cb036c1e34d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 2",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8ven3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yoracale",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m8ven3/qwenqwen3235ba22bthinking2507/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
          "subreddit_subscribers": 504486,
          "created_utc": 1753438601,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_14mlbg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "InternLM S1 Coming Soon!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m94kqu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=75ec23e382a3796e4f90d494e80c28b65dd9ba08",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753462685,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14875",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?auto=webp&amp;s=3bb5ba819fef00d08459615dc0485bf498916a14",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=86cbc6625abbc12b1404c26848130ec7f4107ee3",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7daf77ef66192f415c85595cc07ee248d9f27213",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b29322acb4ebe13751deefcb867e55375cf76e23",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e453326436868385a66b853c71c2c67c024fbf9f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f2012cf2d06cf496ad421d2bb8f64b68a5b37227",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fa9853ad2c71066e2eb56105b5b16913396bf703",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m94kqu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TKGaming_11",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m94kqu/internlm_s1_coming_soon/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14875",
          "subreddit_subscribers": 504486,
          "created_utc": 1753462685,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "With the release of GLM-4.5 and GLM-4.5-Air (both large MoE models), Zhipu has mentioned that they are also considering upgrading their 9B model if there’s enough community interest in a small model.\n\nThis potential small model would be much more accessible than the planned GLM-4.5 models which would likely be far too large to run on most consumer hardware. Personally super excited for this as it would make a great base for finetuning",
          "author_fullname": "t2_1f194h3luj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5-9B?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9fuf9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753490431,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the release of GLM-4.5 and GLM-4.5-Air (both large MoE models), Zhipu has mentioned that they are also considering upgrading their 9B model if there’s enough community interest in a small model.&lt;/p&gt;\n\n&lt;p&gt;This potential small model would be much more accessible than the planned GLM-4.5 models which would likely be far too large to run on most consumer hardware. Personally super excited for this as it would make a great base for finetuning&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9fuf9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrfakename0",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9fuf9/glm459b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9fuf9/glm459b/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753490431,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey\\~\n\nExciting news in the AI reasoning space! Using AWorld, we just built a Multi-Agent System (MAS) in 6 hours that successfully solved 5 out of 6 IMO 2025 math problems! 🎯\n\n# Research Context:\n\nThis work was inspired by the recent breakthrough paper \"Gemini 2.5 Pro Capable of Winning Gold at IMO 2025\" (Huang &amp; Yang, 2025). The authors noted that \"a multi-agent system where the strengths of different solutions can be combined would lead to stronger mathematical capability.\"\n\n# Our Innovation:\n\nWe took this insight and implemented a collective intelligence approach using our AWorld multi-agent framework, proving that properly orchestrated multi-agent systems can indeed surpass single-model performance.\n\n# Key Achievements:\n\n* 5/6 IMO 2025 problems solved in just 6 hours of development\n* Collective Intelligence &gt; Single Models: Our results validate the paper's hypothesis about multi-agent superiority\n* Rapid Prototyping: AWorld framework enabled quick construction of sophisticated reasoning systems\n* Context Engineering: Demonstrated the critical importance of agent interaction design under current LLM capabilities\n\n# Reproducible Results:\n\nGitHub Repository: [https://github.com/inclusionAI/AWorld](https://github.com/inclusionAI/AWorld)\n\nIMO Implementation: examples/imo/ - Complete with setup scripts, environment configuration, and detailed documentation.",
          "author_fullname": "t2_159bscsg23",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🚀 Built a Multi-Agent System in 6 Hours That Solves 5/6 IMO 2025 Math Problems - Inspired by Recent Research Breakthroughs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m91mt6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "author_cakeday": true,
          "edited": 1753457696,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753455985,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey~&lt;/p&gt;\n\n&lt;p&gt;Exciting news in the AI reasoning space! Using AWorld, we just built a Multi-Agent System (MAS) in 6 hours that successfully solved 5 out of 6 IMO 2025 math problems! 🎯&lt;/p&gt;\n\n&lt;h1&gt;Research Context:&lt;/h1&gt;\n\n&lt;p&gt;This work was inspired by the recent breakthrough paper &amp;quot;Gemini 2.5 Pro Capable of Winning Gold at IMO 2025&amp;quot; (Huang &amp;amp; Yang, 2025). The authors noted that &amp;quot;a multi-agent system where the strengths of different solutions can be combined would lead to stronger mathematical capability.&amp;quot;&lt;/p&gt;\n\n&lt;h1&gt;Our Innovation:&lt;/h1&gt;\n\n&lt;p&gt;We took this insight and implemented a collective intelligence approach using our AWorld multi-agent framework, proving that properly orchestrated multi-agent systems can indeed surpass single-model performance.&lt;/p&gt;\n\n&lt;h1&gt;Key Achievements:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;5/6 IMO 2025 problems solved in just 6 hours of development&lt;/li&gt;\n&lt;li&gt;Collective Intelligence &amp;gt; Single Models: Our results validate the paper&amp;#39;s hypothesis about multi-agent superiority&lt;/li&gt;\n&lt;li&gt;Rapid Prototyping: AWorld framework enabled quick construction of sophisticated reasoning systems&lt;/li&gt;\n&lt;li&gt;Context Engineering: Demonstrated the critical importance of agent interaction design under current LLM capabilities&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Reproducible Results:&lt;/h1&gt;\n\n&lt;p&gt;GitHub Repository: &lt;a href=\"https://github.com/inclusionAI/AWorld\"&gt;https://github.com/inclusionAI/AWorld&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;IMO Implementation: examples/imo/ - Complete with setup scripts, environment configuration, and detailed documentation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?auto=webp&amp;s=31ab9ee7e242560f3c3f9a3bfd18f4242f46d0aa",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb99394a2af815f3715108c671ceba9c647d5f86",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ff23eb578a14fc977cd8d5e672331e2b9da7a5b",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdf79eb5a345fc2fcb870c91d0d50ee4d055c6ae",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e423d4f75c60ac9d9ec6eb446998a3f28c3637f6",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=29fbe775ffbd5b003bdf0fa1f00a7212e59c3132",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cd5d102adfdb6875b81e70e4a6e3e0317ac3b393",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m91mt6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Vivid_Might1225",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m91mt6/built_a_multiagent_system_in_6_hours_that_solves/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m91mt6/built_a_multiagent_system_in_6_hours_that_solves/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753455985,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been trying a lot of different combinations with static learning rates, and i have to set up the test inference for every single epoch to determine the sweet spot because i doubt that any automation that does not involve running two simultaneous llm will be able to accurate tell when the results are desirable. But maybe i am doing everything wong? I only got what i wanted after 10 runs of 4e-3, and that is with a datasets of 90 rows, all in a single batch. Perhaps this is a rare scenario, but good to have found something working. Any advice or experiences that i must learn about? As I prefer not to waste more compute doing the trial and error with datasets a thousand times the size.",
          "author_fullname": "t2_1ti9nuwlx8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does it ever make sense to train for 10 epochs? Or did i do it all wrong?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m96b4h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753466600,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been trying a lot of different combinations with static learning rates, and i have to set up the test inference for every single epoch to determine the sweet spot because i doubt that any automation that does not involve running two simultaneous llm will be able to accurate tell when the results are desirable. But maybe i am doing everything wong? I only got what i wanted after 10 runs of 4e-3, and that is with a datasets of 90 rows, all in a single batch. Perhaps this is a rare scenario, but good to have found something working. Any advice or experiences that i must learn about? As I prefer not to waste more compute doing the trial and error with datasets a thousand times the size.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m96b4h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BulkyPlay7704",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m96b4h/does_it_ever_make_sense_to_train_for_10_epochs_or/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m96b4h/does_it_ever_make_sense_to_train_for_10_epochs_or/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753466600,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_10pze1d3jf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Executive Order: \"Preventing Woke AI in the Federal Government\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8l648",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 253,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 253,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=9f4b098cfa68cb7be71aac428f695841b0c9cbbe",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753403766,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "whitehouse.gov",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?auto=webp&amp;s=ecf43e8e82602652ec95e06f13b6ce18da205b9c",
                  "width": 1200,
                  "height": 628
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c1e4661cbba0b6e1e232602fbabfa0384ba0123",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b84255c302c8464ea76b251e4d4ab64cac0ec723",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7c4bae3b4c97261af353a9ec64d3ef027f6deac",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb89e898879eb7adef969749433776a6f6a543ad",
                    "width": 640,
                    "height": 334
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f16221a57c07b16c8cef11acfc0eeb15f6f1254e",
                    "width": 960,
                    "height": 502
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=db29c2e5309166fabf6283791735d6762adf4b55",
                    "width": 1080,
                    "height": 565
                  }
                ],
                "variants": {},
                "id": "4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8l648",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NunyaBuzor",
          "discussion_type": null,
          "num_comments": 142,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8l648/executive_order_preventing_woke_ai_in_the_federal/",
          "stickied": false,
          "url": "https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753403766,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone run the q4ks versions of these, which one is winning for code generation... Too early for consensus yet? Thx",
          "author_fullname": "t2_1l3z4stvkq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The new Kimi vs. new qwen3 for coding",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9avmv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753477447,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone run the q4ks versions of these, which one is winning for code generation... Too early for consensus yet? Thx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9avmv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Agreeable-Prompt-666",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9avmv/the_new_kimi_vs_new_qwen3_for_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9avmv/the_new_kimi_vs_new_qwen3_for_coding/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753477447,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been working on a cleaner UI for uploading and managing custom models — here are some early Figma drafts of the connection flow and model details page. Still a work in progress, but I’d love to hear your thoughts!  \n  \nFor those who are new here: I’m building this platform as a solo pet project in my free time, and I’ve been sharing my progress here on r/LocalLLaMA to gather feedback and ideas. Your input really helps shape the direction.  \n  \nI’m adding support for local backend connection because not everyone wants to rely on third-party APIs or cloud services. Many people already run models locally, and this gives them full control over performance, privacy, and customization.  \n  \nIf you’re interested in testing the platform, I’d be happy to send you an invite — just shoot me a DM!",
          "author_fullname": "t2_1zyh18yq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "New UI for uploading and managing custom models (Figma mockups)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 127,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "fulqfst1y1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 98,
                  "x": 108,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=82cd77de0ca104006b080ce11404b464a14f50b0"
                },
                {
                  "y": 196,
                  "x": 216,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=db2d56b61032e2659fd959720ebdc78eebab279d"
                },
                {
                  "y": 291,
                  "x": 320,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=664b072883559c46b0fa4015ff69016002263582"
                },
                {
                  "y": 583,
                  "x": 640,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=69fb7d42a37eb74bec24fbf843282013d97897b9"
                },
                {
                  "y": 875,
                  "x": 960,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e6c33cbb9244adcb68539df6e892a574b0d9d8d2"
                },
                {
                  "y": 984,
                  "x": 1080,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bb0ff00f02a7a97c54ecb0f5a1a00ec297c1abdd"
                }
              ],
              "s": {
                "y": 1751,
                "x": 1920,
                "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=50c3f7aea91e1460a23becf2a7ed1c33d764db10"
              },
              "id": "fulqfst1y1ff1"
            },
            "dqsr5j52y1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=71ca5fd8d2aa192f92c736a5d06b6c92c5573fae"
                },
                {
                  "y": 135,
                  "x": 216,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e9240f3958d87a71bb54b0be7bc91da1412786d"
                },
                {
                  "y": 200,
                  "x": 320,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b48ca5183c8df74e34feb3b947f7c7b7e40043d"
                },
                {
                  "y": 400,
                  "x": 640,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ffc158bdd7ade8804eb19ad6ee6c16fe6762252a"
                },
                {
                  "y": 601,
                  "x": 960,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1104132469c193ee88fd1c05399f974a8f761107"
                },
                {
                  "y": 676,
                  "x": 1080,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7679dd5a2596a1a9280c1750ca15833d4cff87f4"
                }
              ],
              "s": {
                "y": 1202,
                "x": 1920,
                "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=d05e615307e6fd8002190d245dea45e46920e27d"
              },
              "id": "dqsr5j52y1ff1"
            }
          },
          "name": "t3_1m94uea",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 10,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "fulqfst1y1ff1",
                "id": 713831543
              },
              {
                "media_id": "dqsr5j52y1ff1",
                "id": 713831544
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/fIwpmtA8COvjymHuiMQDj4eWoej49NBiwVAmQsDVTfI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753463269,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been working on a cleaner UI for uploading and managing custom models — here are some early Figma drafts of the connection flow and model details page. Still a work in progress, but I’d love to hear your thoughts!  &lt;/p&gt;\n\n&lt;p&gt;For those who are new here: I’m building this platform as a solo pet project in my free time, and I’ve been sharing my progress here on &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; to gather feedback and ideas. Your input really helps shape the direction.  &lt;/p&gt;\n\n&lt;p&gt;I’m adding support for local backend connection because not everyone wants to rely on third-party APIs or cloud services. Many people already run models locally, and this gives them full control over performance, privacy, and customization.  &lt;/p&gt;\n\n&lt;p&gt;If you’re interested in testing the platform, I’d be happy to send you an invite — just shoot me a DM!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m94uea",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m94uea",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RIPT1D3_Z",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m94uea/new_ui_for_uploading_and_managing_custom_models/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m94uea",
          "subreddit_subscribers": 504486,
          "created_utc": 1753463269,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Buy the largest GPU that you can really afford to.  Besides the obvious cost of additional electricity, PCI slots, physical space, cooling etc.   Multiple GPUs can be annoying.\n\nFor example, I have some 16gb GPUs, 10 of them when trying to run Kimi, each layer is 7gb.   If I load 2 layers on each GPU, the most context I can put on them is roughly 4k, since one of the layer is odd and ends up taking up 14.7gb. \n\nSo to get more context, 10k, I end up putting 1 layer 7gb on each of them, leaving 9gb free or 90gb of vram free.\n\nIf I had 5 32gb GPUs, at that 7gb, I would be able to place 4 layers \\~ 28gb and still have about 3-4gb each free, which will allow me to have my 10k context.  More context with same sized GPU, and it would be faster too!\n\nGo as big as you can!",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "N + N size GPU != 2N sized GPU, go big if you can",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vu80",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753440200,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Buy the largest GPU that you can really afford to.  Besides the obvious cost of additional electricity, PCI slots, physical space, cooling etc.   Multiple GPUs can be annoying.&lt;/p&gt;\n\n&lt;p&gt;For example, I have some 16gb GPUs, 10 of them when trying to run Kimi, each layer is 7gb.   If I load 2 layers on each GPU, the most context I can put on them is roughly 4k, since one of the layer is odd and ends up taking up 14.7gb. &lt;/p&gt;\n\n&lt;p&gt;So to get more context, 10k, I end up putting 1 layer 7gb on each of them, leaving 9gb free or 90gb of vram free.&lt;/p&gt;\n\n&lt;p&gt;If I had 5 32gb GPUs, at that 7gb, I would be able to place 4 layers ~ 28gb and still have about 3-4gb each free, which will allow me to have my 10k context.  More context with same sized GPU, and it would be faster too!&lt;/p&gt;\n\n&lt;p&gt;Go as big as you can!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m8vu80",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753440200,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/casper_hansen_/status/1948402352320360811?t=sPHOGEKIcaucRVzENlIr1g&amp;s=19",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ok next big open source model also from China only ! Which is about to release",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m88jdh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 866,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 866,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/c08j-el568SvKGYGEd0gZbFM3-WDn7gmHlrwY9mVv5E.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753373337,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/casper_hansen_/status/1948402352320360811?t=sPHOGEKIcaucRVzENlIr1g&amp;amp;s=19\"&gt;https://x.com/casper_hansen_/status/1948402352320360811?t=sPHOGEKIcaucRVzENlIr1g&amp;amp;s=19&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/j6rwug34juef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/j6rwug34juef1.png?auto=webp&amp;s=18f17b9ceaa2b5d279bcbb0bb243851740e717c4",
                  "width": 1080,
                  "height": 1419
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb9a593e1fb7f521dc0f069833d5296c3e11f7e9",
                    "width": 108,
                    "height": 141
                  },
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6484155fe20574b23faf0c91f18281d0b64f0ef8",
                    "width": 216,
                    "height": 283
                  },
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=168aa62e13429a9a7659a2b00480eeed71c2b0ec",
                    "width": 320,
                    "height": 420
                  },
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a04ad517c7ca8eeeb00ee48288d8f17c562ca63c",
                    "width": 640,
                    "height": 840
                  },
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=66e086c5c04e9638800c62f55dfe3f8b4b914e18",
                    "width": 960,
                    "height": 1261
                  },
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e86fd4d029b2f6132d30619ca2201a26cba6b494",
                    "width": 1080,
                    "height": 1419
                  }
                ],
                "variants": {},
                "id": "yDDBcSzVcQ88JLp4cep6OXeVFwV_1fmwTlslE0j_6FU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m88jdh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 155,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m88jdh/ok_next_big_open_source_model_also_from_china/",
          "stickied": false,
          "url": "https://i.redd.it/j6rwug34juef1.png",
          "subreddit_subscribers": 504486,
          "created_utc": 1753373337,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "First of all, I loved the experience using Qwen Code with Qwen-3-Coder, but I can't stomach the cost of Qwen-3-Coder. While yes, you can use any OpenAI-compatible model out of the box, it's not without limitations.\n\nThat’s why I forked Qwen CLI Coder (itself derived from Gemini CLI) to create [**Wren Coder CLI**](https://github.com/wren-coder/wren-coder-cli): an open-source, model-agnostic AI agent for coding assistance and terminal workflows.\n\n**Why Fork?**\n\n1. Big players like Google/Qwen have little incentive to support other models. Wren will be fully model-agnostic by design.\n2. I’m splitting the project into a CLI + SDK (like Claude Code) to enable deeper agent customization.\n3. My priorities as a solo developer probably don't align with respective model companies.\n4. Why not? I just want to experiment and try new things.\n5. I have a lot of time on my hands before I join a new role and want to spend the next month or so heads down building something I will love and use every day.\n\n  \n**What am I shipping?**\n\nOver the next few weeks, I plan to focus on the following:\n\n1. Improving compatibility with a wide range of models\n2. Adding chunking/compression logic to fix token limit errors with models with smaller context windows \\*cough\\* deepseek.\n3. Splitting up the CLI and SDK\n4. Documentation\n5. Multi-model support????\n\n  \nMaybe this is overly ambitious, but again why not? I'll keep y'all posted! Wish me luck!\n\n[https://github.com/wren-coder/wren-coder-cli](https://github.com/wren-coder/wren-coder-cli)",
          "author_fullname": "t2_1sivuwuvea",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why I Forked Qwen Code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8qj9w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 75,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 75,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753420060,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First of all, I loved the experience using Qwen Code with Qwen-3-Coder, but I can&amp;#39;t stomach the cost of Qwen-3-Coder. While yes, you can use any OpenAI-compatible model out of the box, it&amp;#39;s not without limitations.&lt;/p&gt;\n\n&lt;p&gt;That’s why I forked Qwen CLI Coder (itself derived from Gemini CLI) to create &lt;a href=\"https://github.com/wren-coder/wren-coder-cli\"&gt;&lt;strong&gt;Wren Coder CLI&lt;/strong&gt;&lt;/a&gt;: an open-source, model-agnostic AI agent for coding assistance and terminal workflows.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why Fork?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Big players like Google/Qwen have little incentive to support other models. Wren will be fully model-agnostic by design.&lt;/li&gt;\n&lt;li&gt;I’m splitting the project into a CLI + SDK (like Claude Code) to enable deeper agent customization.&lt;/li&gt;\n&lt;li&gt;My priorities as a solo developer probably don&amp;#39;t align with respective model companies.&lt;/li&gt;\n&lt;li&gt;Why not? I just want to experiment and try new things.&lt;/li&gt;\n&lt;li&gt;I have a lot of time on my hands before I join a new role and want to spend the next month or so heads down building something I will love and use every day.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;What am I shipping?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Over the next few weeks, I plan to focus on the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Improving compatibility with a wide range of models&lt;/li&gt;\n&lt;li&gt;Adding chunking/compression logic to fix token limit errors with models with smaller context windows *cough* deepseek.&lt;/li&gt;\n&lt;li&gt;Splitting up the CLI and SDK&lt;/li&gt;\n&lt;li&gt;Documentation&lt;/li&gt;\n&lt;li&gt;Multi-model support????&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Maybe this is overly ambitious, but again why not? I&amp;#39;ll keep y&amp;#39;all posted! Wish me luck!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/wren-coder/wren-coder-cli\"&gt;https://github.com/wren-coder/wren-coder-cli&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?auto=webp&amp;s=79d64b1289209fa79a11669d150a850e8d6ce23a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=94b1fa561118622479aef7fd3a0006f928715e0b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=166a7f91b07c437bb627e895db8e0077a2423927",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=075d04567db44f06e5b0bfb432fe8f6f37a04713",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=792082e48bea2b952a5e950a98b815d551f583b3",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ca81ad125907d790e752b70ee99ea801e80dfa3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=64b7d8f9f97d9fd67f00438f1d60441579fa0474",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8qj9w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ryanwang4thepeople",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753420060,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Was thinking how to scale a GPU cluster. Not talking about CPUs here.  \nUsually have heard that \"buy Epyc\" and add 6-8 GPUs in it. but thats it then, it wont scale more.  \nBut now that I have learned how to use vLLM, and it can utilize multi GPU and also multi server GPUs, was thinking what if creating a cluster with fast networking and vLLM RAY?   \n  \nHas anyone done it? \n\nI happen to have spare Mellanox Connect-x6 cards, 2x25GB with ROCE, some 25gb and 100gb switches.   \nI do not have any Epycs, but loads of AM5 boards and 7000 cpus and memory.   \nSo my understanding is, if creating multiple servers, with 1-2 GPUs in each 8x or 16x pcie 4.0 connected, and then creating a NFS file server for model sharing and connecting all them with 2x25GB DAC, I guess it would work?  \nThat 5GB/s connection will be in tensor parallel a bottleneck but how much? Some say even 4x pcie 4.0 is not a bottleneck in vLLM tensor parallel and its about 8GB/s. \n\nLater when pcie 5.0 4x network cards are available it could be upgraded to 100GB networking. \n\nSo with this kind of setup, even 100 gpus could server the same model? \n\n\"**RDMA over Converged Ethernet (RoCE):** The ConnectX-6 cards are designed for RoCE. This is a critical advantage. RoCE allows Remote Direct Memory Access, meaning data can be transferred directly between the GPU memories on different servers, bypassing the CPU.\"",
          "author_fullname": "t2_1jk2ep8a52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Multi GPU multi server inference",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9cg16",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753481338,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was thinking how to scale a GPU cluster. Not talking about CPUs here.&lt;br/&gt;\nUsually have heard that &amp;quot;buy Epyc&amp;quot; and add 6-8 GPUs in it. but thats it then, it wont scale more.&lt;br/&gt;\nBut now that I have learned how to use vLLM, and it can utilize multi GPU and also multi server GPUs, was thinking what if creating a cluster with fast networking and vLLM RAY?   &lt;/p&gt;\n\n&lt;p&gt;Has anyone done it? &lt;/p&gt;\n\n&lt;p&gt;I happen to have spare Mellanox Connect-x6 cards, 2x25GB with ROCE, some 25gb and 100gb switches.&lt;br/&gt;\nI do not have any Epycs, but loads of AM5 boards and 7000 cpus and memory.&lt;br/&gt;\nSo my understanding is, if creating multiple servers, with 1-2 GPUs in each 8x or 16x pcie 4.0 connected, and then creating a NFS file server for model sharing and connecting all them with 2x25GB DAC, I guess it would work?&lt;br/&gt;\nThat 5GB/s connection will be in tensor parallel a bottleneck but how much? Some say even 4x pcie 4.0 is not a bottleneck in vLLM tensor parallel and its about 8GB/s. &lt;/p&gt;\n\n&lt;p&gt;Later when pcie 5.0 4x network cards are available it could be upgraded to 100GB networking. &lt;/p&gt;\n\n&lt;p&gt;So with this kind of setup, even 100 gpus could server the same model? &lt;/p&gt;\n\n&lt;p&gt;&amp;quot;&lt;strong&gt;RDMA over Converged Ethernet (RoCE):&lt;/strong&gt; The ConnectX-6 cards are designed for RoCE. This is a critical advantage. RoCE allows Remote Direct Memory Access, meaning data can be transferred directly between the GPU memories on different servers, bypassing the CPU.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9cg16",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rich_Artist_8327",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9cg16/multi_gpu_multi_server_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9cg16/multi_gpu_multi_server_inference/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753481338,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm about to start building my personal AI companion and during my research came across this [awesome list](https://github.com/LongHZ140516/Awesome-GrokAni-VituralMate) of AI companion projects that I wanted to share with the community.\n\n| Companion | Lang | License | Stack | Category |\n| -- | -- | -- | -- | -- |\n| [枫云AI虚拟伙伴Web版](https://github.com/swordswind/ai_virtual_mate_web) - [Wiki](https://deepwiki.com/swordswind/ai_virtual_mate_web) | zh | gpl-3.0 | python | companion |\n| [Muice-Chatbot](https://github.com/Moemu/Muice-Chatbot) - [Wiki](https://deepwiki.com/Moemu/Muice-Chatbot) | zh, en | mit | python | companion |\n| [MuiceBot](https://github.com/Moemu/MuiceBot) - [Wiki](https://deepwiki.com/Moemu/MuiceBot) | zh | bsd-3-clause | python | companion |\n| [kirara-ai](https://github.com/lss233/kirara-ai) - [Wiki](https://deepwiki.com/lss233/kirara-ai) | zh | agpl-3.0 | python | companion |\n| [my-neuro](https://github.com/morettt/my-neuro) - [Wiki](https://deepwiki.com/morettt/my-neuro) | zh, en | mit | python | companion |\n| [AIAvatarKit](https://github.com/uezo/aiavatarkit) - [Wiki](https://deepwiki.com/uezo/aiavatarkit) | en | apache-2.0 | python | companion |\n| [xinghe-AI](https://github.com/lijiaxing1997/xinghe-AI) - [Wiki](https://deepwiki.com/lijiaxing1997/xinghe-AI) | zh |  | python | companion |\n| [MaiBot](https://github.com/MaiM-with-u/MaiBot) | zh | gpl-3.0 | python | companion |\n| [AI-YinMei](https://github.com/worm128/AI-YinMei) - [Wiki](https://deepwiki.com/worm128/AI-YinMei) | zh | bsd-2-clause | python, web | vtuber |\n| [Open-LLM-VTuber](https://github.com/Open-LLM-VTuber/Open-LLM-VTuber) - [Wiki](https://deepwiki.com/Open-LLM-VTuber/Open-LLM-VTuber) | en | mit | python, web | vtuber, companion |\n| [KouriChat](https://github.com/KouriChat/KouriChat) - [Wiki](https://deepwiki.com/KouriChat/KouriChat) | zh | custom | python, web | companion |\n| [Streamer-Sales](https://github.com/PeterH0323/Streamer-Sales) - [Wiki](https://deepwiki.com/PeterH0323/Streamer-Sales) | zh | agpl-3.0 | python, web | vtuber, professional |\n| [AI-Vtuber](https://github.com/Ikaros-521/AI-Vtuber) - [Wiki](https://deepwiki.com/Ikaros-521/AI-Vtuber) | zh | gpl-3.0 | python, web | vtuber |\n| [SillyTavern](https://github.com/SillyTavern/SillyTavern) - [Wiki](https://deepwiki.com/SillyTavern/SillyTavern) | en | agpl-3.0 | web | companion |\n| [lobe-vidol](https://github.com/lobehub/lobe-vidol) - [Wiki](https://deepwiki.com/lobehub/lobe-vidol) | en | apache-2.0 | web | companion |\n| [Bella](https://github.com/Jackywine/Bella) - [Wiki](https://deepwiki.com/Jackywine/Bella) | zh | mit | web | companion |\n| [AITuberKit](https://github.com/tegnike/aituber-kit) - [Wiki](https://deepwiki.com/tegnike/aituber-kit) | en, ja | custom | web | vtuber, companion |\n| [airi](https://github.com/moeru-ai/airi) - [Wiki](https://deepwiki.com/moeru-ai/airi) | en | mit | tauri | vtuber, companion |\n| [amica](https://github.com/semperai/amica) - [Wiki](https://deepwiki.com/semperai/amica) | en | mit | tauri | companion |\n| [ChatdollKit](https://github.com/uezo/ChatdollKit) - [Wiki](https://deepwiki.com/uezo/ChatdollKit) | en, ja | apache-2.0 | unity | companion |\n| [Unity-AI-Chat-Toolkit](https://github.com/zhangliwei7758/unity-AI-Chat-Toolkit) - [Wiki](https://deepwiki.com/zhangliwei7758/unity-AI-Chat-Toolkit) | zh | mit | unity | companion |\n| [ZcChat](https://github.com/Zao-chen/ZcChat) - [Wiki](https://deepwiki.com/Zao-chen/ZcChat) | zh, en | gpl-3.0 | c++ | galge |\n| [handcrafted-persona-engine](https://github.com/fagenorn/handcrafted-persona-engine) - [Wiki](https://deepwiki.com/fagenorn/handcrafted-persona-engine) | en |  | dotnet | vtuber, companion |\n\n**Notes**:\n\n- I've made some edits, such as adding license info (since I might copy the code) and organizing the list into categories for easier navigation.\n- Not all of these are dedicated companion apps (e.g. SillyTavern), but they can be adapted with some tweaking\n- Several projects only have Chinese READMEs (marked as zh), but I've included DeepWiki links to help with understanding. There's been significant progress in that community so I think it's worth exploring.\n\nI'm starting this thread for two reasons: First, I'd love to hear about your favorite AI companion apps or setups that go beyond basic prompting. For me, a true companion needs a name, avatar, personality, backstory, conversational ability, and most importantly, memory. Second, I'm particularly interested in seeing what alternatives to Grok's Ani this community will build in the future.\n\nIf I've missed anything, please let me know and I'll update the list.",
          "author_fullname": "t2_1s7w9cgxcq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Source Companion Thread",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8wg2r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753442543,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753442258,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m about to start building my personal AI companion and during my research came across this &lt;a href=\"https://github.com/LongHZ140516/Awesome-GrokAni-VituralMate\"&gt;awesome list&lt;/a&gt; of AI companion projects that I wanted to share with the community.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Companion&lt;/th&gt;\n&lt;th&gt;Lang&lt;/th&gt;\n&lt;th&gt;License&lt;/th&gt;\n&lt;th&gt;Stack&lt;/th&gt;\n&lt;th&gt;Category&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/swordswind/ai_virtual_mate_web\"&gt;枫云AI虚拟伙伴Web版&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/swordswind/ai_virtual_mate_web\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;gpl-3.0&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Moemu/Muice-Chatbot\"&gt;Muice-Chatbot&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Moemu/Muice-Chatbot\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh, en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Moemu/MuiceBot\"&gt;MuiceBot&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Moemu/MuiceBot\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;bsd-3-clause&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/lss233/kirara-ai\"&gt;kirara-ai&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/lss233/kirara-ai\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;agpl-3.0&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/morettt/my-neuro\"&gt;my-neuro&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/morettt/my-neuro\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh, en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/uezo/aiavatarkit\"&gt;AIAvatarKit&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/uezo/aiavatarkit\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;apache-2.0&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/lijiaxing1997/xinghe-AI\"&gt;xinghe-AI&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/lijiaxing1997/xinghe-AI\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/MaiM-with-u/MaiBot\"&gt;MaiBot&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;gpl-3.0&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/worm128/AI-YinMei\"&gt;AI-YinMei&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/worm128/AI-YinMei\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;bsd-2-clause&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;vtuber&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Open-LLM-VTuber/Open-LLM-VTuber\"&gt;Open-LLM-VTuber&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Open-LLM-VTuber/Open-LLM-VTuber\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;vtuber, companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/KouriChat/KouriChat\"&gt;KouriChat&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/KouriChat/KouriChat\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;custom&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/PeterH0323/Streamer-Sales\"&gt;Streamer-Sales&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/PeterH0323/Streamer-Sales\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;agpl-3.0&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;vtuber, professional&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Ikaros-521/AI-Vtuber\"&gt;AI-Vtuber&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Ikaros-521/AI-Vtuber\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;gpl-3.0&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;vtuber&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/SillyTavern/SillyTavern\"&gt;SillyTavern&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/SillyTavern/SillyTavern\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;agpl-3.0&lt;/td&gt;\n&lt;td&gt;web&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/lobehub/lobe-vidol\"&gt;lobe-vidol&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/lobehub/lobe-vidol\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;apache-2.0&lt;/td&gt;\n&lt;td&gt;web&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Jackywine/Bella\"&gt;Bella&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Jackywine/Bella\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;web&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/tegnike/aituber-kit\"&gt;AITuberKit&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/tegnike/aituber-kit\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en, ja&lt;/td&gt;\n&lt;td&gt;custom&lt;/td&gt;\n&lt;td&gt;web&lt;/td&gt;\n&lt;td&gt;vtuber, companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/moeru-ai/airi\"&gt;airi&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/moeru-ai/airi\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;tauri&lt;/td&gt;\n&lt;td&gt;vtuber, companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/semperai/amica\"&gt;amica&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/semperai/amica\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;tauri&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/uezo/ChatdollKit\"&gt;ChatdollKit&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/uezo/ChatdollKit\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en, ja&lt;/td&gt;\n&lt;td&gt;apache-2.0&lt;/td&gt;\n&lt;td&gt;unity&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/zhangliwei7758/unity-AI-Chat-Toolkit\"&gt;Unity-AI-Chat-Toolkit&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/zhangliwei7758/unity-AI-Chat-Toolkit\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;unity&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Zao-chen/ZcChat\"&gt;ZcChat&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Zao-chen/ZcChat\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh, en&lt;/td&gt;\n&lt;td&gt;gpl-3.0&lt;/td&gt;\n&lt;td&gt;c++&lt;/td&gt;\n&lt;td&gt;galge&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/fagenorn/handcrafted-persona-engine\"&gt;handcrafted-persona-engine&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/fagenorn/handcrafted-persona-engine\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;td&gt;dotnet&lt;/td&gt;\n&lt;td&gt;vtuber, companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;ve made some edits, such as adding license info (since I might copy the code) and organizing the list into categories for easier navigation.&lt;/li&gt;\n&lt;li&gt;Not all of these are dedicated companion apps (e.g. SillyTavern), but they can be adapted with some tweaking&lt;/li&gt;\n&lt;li&gt;Several projects only have Chinese READMEs (marked as zh), but I&amp;#39;ve included DeepWiki links to help with understanding. There&amp;#39;s been significant progress in that community so I think it&amp;#39;s worth exploring.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m starting this thread for two reasons: First, I&amp;#39;d love to hear about your favorite AI companion apps or setups that go beyond basic prompting. For me, a true companion needs a name, avatar, personality, backstory, conversational ability, and most importantly, memory. Second, I&amp;#39;m particularly interested in seeing what alternatives to Grok&amp;#39;s Ani this community will build in the future.&lt;/p&gt;\n\n&lt;p&gt;If I&amp;#39;ve missed anything, please let me know and I&amp;#39;ll update the list.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?auto=webp&amp;s=c5f2d10e0e2bd42132da4d17ae9fd30799dc46d4",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e7fc321ec10284644abea084a0b60656c01283e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0102b60c2d06609a028ba234852f361a842ccba6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9b93ae42c4f64f0fba315ba853f3f199075473f0",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd981813236ec5fa39ab80bda6d206e4844b347a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e5fe64f3be02adea586fa3bbfe4297c790018163",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7078c5e8cfb3be270d93e8bdbd35e203d4e295a3",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m8wg2r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aratahikaru5",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8wg2r/open_source_companion_thread/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8wg2r/open_source_companion_thread/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753442258,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_fiiv6xm3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-Thinking-2507 is about to be released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 55,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8dgfu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 408,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 408,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/UaMXb3ybuMfEr_Q8-TcVTvRTejPtVI4uvfxCaqtYwhE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753384454,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6l84nwc3gvef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6l84nwc3gvef1.png?auto=webp&amp;s=57ba972c71ccbbde7e6b91adbfd7e6f90f9305a6",
                  "width": 586,
                  "height": 234
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6l84nwc3gvef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b6ce58e5b6f04a919ca7ebb1329f28ea1812e03",
                    "width": 108,
                    "height": 43
                  },
                  {
                    "url": "https://preview.redd.it/6l84nwc3gvef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b26df86d2091efcaeb2caf8a3ae29e5e74a71365",
                    "width": 216,
                    "height": 86
                  },
                  {
                    "url": "https://preview.redd.it/6l84nwc3gvef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab0e139a7c20c4938872504feeddbf3c6b23197f",
                    "width": 320,
                    "height": 127
                  }
                ],
                "variants": {},
                "id": "21v1ejzHSqCrWCoLUoEo0xlXJGAQHp-JnuM_aAV5s4s"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8dgfu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dr_Karminski",
          "discussion_type": null,
          "num_comments": 45,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8dgfu/qwen3235ba22bthinking2507_is_about_to_be_released/",
          "stickied": false,
          "url": "https://i.redd.it/6l84nwc3gvef1.png",
          "subreddit_subscribers": 504486,
          "created_utc": 1753384454,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all, \n\n\n\nI'm a solo dev and first-time open-source maintainer. I just released my first Python package: \\*\\*Arkhon Memory SDK\\*\\* – a lightweight, local-first memory module for autonomous LLM agents. This is part of my bigger project, but I thought this component could be useful for some of you.\n\n\\- **No vector DBs, no cloud, no LangChain**: clean, JSON-native memory with time decay, tagging, and session lifecycle hooks.\n\n\\- It’s fully pip installable: \\`pip install arkhon-memory\\`\n\n\\- Works with Python 3.8+ and pydantic 2.x.\n\n  \nYou can find it in:\n\n🔗 GitHub: [https://github.com/kissg96/arkhon\\_memory](https://github.com/kissg96/arkhon_memory)  \n\n🔗 PyPI: [https://pypi.org/project/arkhon-memory/](https://pypi.org/project/arkhon-memory/)\n\n\n\nIf you’re building LLM workflows, want persistence for agents, or just want a memory layer that \\*\\*never leaves your local machine\\*\\*, I’d love for you to try it.\n\n\n\nWould really appreciate feedback, stars, or suggestions!  \n\nFeel free to open issues or email me: [kissg@me.com](mailto:kissg@me.com)\n\n\n\nThanks for reading,  \n\nkissg96\n\n",
          "author_fullname": "t2_47eqehtw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Release] Arkhon Memory SDK – Local, lightweight long-term memory for LLM agents (pip install arkhon-memory)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9019j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753452243,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a solo dev and first-time open-source maintainer. I just released my first Python package: **Arkhon Memory SDK** – a lightweight, local-first memory module for autonomous LLM agents. This is part of my bigger project, but I thought this component could be useful for some of you.&lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;No vector DBs, no cloud, no LangChain&lt;/strong&gt;: clean, JSON-native memory with time decay, tagging, and session lifecycle hooks.&lt;/p&gt;\n\n&lt;p&gt;- It’s fully pip installable: `pip install arkhon-memory`&lt;/p&gt;\n\n&lt;p&gt;- Works with Python 3.8+ and pydantic 2.x.&lt;/p&gt;\n\n&lt;p&gt;You can find it in:&lt;/p&gt;\n\n&lt;p&gt;🔗 GitHub: &lt;a href=\"https://github.com/kissg96/arkhon_memory\"&gt;https://github.com/kissg96/arkhon_memory&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;🔗 PyPI: &lt;a href=\"https://pypi.org/project/arkhon-memory/\"&gt;https://pypi.org/project/arkhon-memory/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you’re building LLM workflows, want persistence for agents, or just want a memory layer that **never leaves your local machine**, I’d love for you to try it.&lt;/p&gt;\n\n&lt;p&gt;Would really appreciate feedback, stars, or suggestions!  &lt;/p&gt;\n\n&lt;p&gt;Feel free to open issues or email me: [&lt;a href=\"mailto:kissg@me.com\"&gt;kissg@me.com&lt;/a&gt;](mailto:&lt;a href=\"mailto:kissg@me.com\"&gt;kissg@me.com&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading,  &lt;/p&gt;\n\n&lt;p&gt;kissg96&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?auto=webp&amp;s=3d6dbe3ba64c55f4a5b820d7b93c67e5e863a7c1",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7a209445a0ca39ec32cc43c3974f0c86515e04f3",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9bb430093762e9015e857ef6b4fe920adf08bdd4",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=48db854452639ae78e9c6f0926847bc1b64d167d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ecdd2622f584e6329d2bea611d6c3fe12d38f1b8",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b13d3181d5a2ca5fed4e99539ad3d340ef806d8",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b70f7aefa32c79066655cbc1eac72b62818bf406",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9019j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kissgeri96",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9019j/release_arkhon_memory_sdk_local_lightweight/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9019j/release_arkhon_memory_sdk_local_lightweight/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753452243,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_yjt5w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ByteDance Seed Prover Achieves Silver Medal Score in IMO 2025",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8tmhd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753431610,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "seed.bytedance.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://seed.bytedance.com/en/blog/bytedance-seed-prover-achieves-silver-medal-score-in-imo-2025",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8tmhd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hedgehog0",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8tmhd/bytedance_seed_prover_achieves_silver_medal_score/",
          "stickied": false,
          "url": "https://seed.bytedance.com/en/blog/bytedance-seed-prover-achieves-silver-medal-score-in-imo-2025",
          "subreddit_subscribers": 504486,
          "created_utc": 1753431610,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Google released their Gemma 3n model about a month ago, and they've mentioned that it's meant to run efficiently on everyday devices, yet, from my experience it runs really slow on my Mac (base model M2 Mac mini from 2023 with only 8GB of RAM). I am aware that my small amount of RAM is very limiting in the space of local LLMs, but I had a lot of hope when Google first started teasing this model.\n\nJust curious if anyone has tried it, and if so, what has your experience been like?\n\nHere's an Ollama link to the model, btw: [https://ollama.com/library/gemma3n](https://ollama.com/library/gemma3n)",
          "author_fullname": "t2_1lavzg2ok9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone had any luck with Google's Gemma 3n model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m95bfq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753464342,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google released their Gemma 3n model about a month ago, and they&amp;#39;ve mentioned that it&amp;#39;s meant to run efficiently on everyday devices, yet, from my experience it runs really slow on my Mac (base model M2 Mac mini from 2023 with only 8GB of RAM). I am aware that my small amount of RAM is very limiting in the space of local LLMs, but I had a lot of hope when Google first started teasing this model.&lt;/p&gt;\n\n&lt;p&gt;Just curious if anyone has tried it, and if so, what has your experience been like?&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an Ollama link to the model, btw: &lt;a href=\"https://ollama.com/library/gemma3n\"&gt;https://ollama.com/library/gemma3n&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?auto=webp&amp;s=a080c4707584d3aa14134960cda9ba2d339b93a3",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc759de0e8fa36d241c5728d41ee3cf022cab96",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ccf136f5d3091254a0067a3bc5d6c7df9d62d89",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2530aa4ecbcf7899ec0d023e217fe24af15fe0a6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=750a6d42fd91c5a6e9a9c069e74247c877644e97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eab390b865b031211658564ad5fe5241c9661c5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m95bfq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Junior-Ad-2186",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m95bfq/anyone_had_any_luck_with_googles_gemma_3n_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m95bfq/anyone_had_any_luck_with_googles_gemma_3n_model/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753464342,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My config:  \nRyzen 5 5500, 16Gb, RTX 3060 12Gb  \n",
          "author_fullname": "t2_by2xhanu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the best AI to run locally and use in agent mode of the Continue extension in VS Code?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m99hwb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753474147,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My config:&lt;br/&gt;\nRyzen 5 5500, 16Gb, RTX 3060 12Gb  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m99hwb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ikelven",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m99hwb/what_is_the_best_ai_to_run_locally_and_use_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m99hwb/what_is_the_best_ai_to_run_locally_and_use_in/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753474147,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was just chatting with Claude about my experiments with Aider and qwen2.5-coder (7b &amp; 14b).\n\ni wasn't ready for Claudes response. so good.\n\nFWIW i'm trying codellama:13b next.\n\nAny advice for a local coding model and Aider on RTX3080 10GB?",
          "author_fullname": "t2_8383arktn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do models make fun of other models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8wi62",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3VfQACGGsrR4huIE2aSbd12G7zEBM2KmWURzyANGXgs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753442447,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was just chatting with Claude about my experiments with Aider and qwen2.5-coder (7b &amp;amp; 14b).&lt;/p&gt;\n\n&lt;p&gt;i wasn&amp;#39;t ready for Claudes response. so good.&lt;/p&gt;\n\n&lt;p&gt;FWIW i&amp;#39;m trying codellama:13b next.&lt;/p&gt;\n\n&lt;p&gt;Any advice for a local coding model and Aider on RTX3080 10GB?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8sdpzbq280ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8sdpzbq280ff1.png?auto=webp&amp;s=916da339fa043930e6f17140452756e404ea0bf2",
                  "width": 718,
                  "height": 371
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8sdpzbq280ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=89b6015ee2a0d88ec0bac662235da5629baf1bbb",
                    "width": 108,
                    "height": 55
                  },
                  {
                    "url": "https://preview.redd.it/8sdpzbq280ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ce1a4289cec1dd8cedead50e63cc09efa5fee80",
                    "width": 216,
                    "height": 111
                  },
                  {
                    "url": "https://preview.redd.it/8sdpzbq280ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3de57e7740d06316720746e9e88263882f7396e9",
                    "width": 320,
                    "height": 165
                  },
                  {
                    "url": "https://preview.redd.it/8sdpzbq280ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb549a0f9db72b902ee2c8fba005b948a7894058",
                    "width": 640,
                    "height": 330
                  }
                ],
                "variants": {},
                "id": "-PX_RQ0KeK0Qo0OiDGvgWia8NQc4pA3mSJFKxM_loHM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m8wi62",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fussy-Fur3608",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8wi62/do_models_make_fun_of_other_models/",
          "stickied": false,
          "url": "https://i.redd.it/8sdpzbq280ff1.png",
          "subreddit_subscribers": 504486,
          "created_utc": 1753442447,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ive been looking at buying a few mi50 32gb cards for my local training setup because they are absurdly affordable for the VRAM they have. I'm not too concerned with FLOP/s performance, as long as they have compatibility with a relatively modern pytorch and its dependencies.\n\nI've seen people on here talking about this card for inference but not training. Would this be a good idea?",
          "author_fullname": "t2_1jemkvy49e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mi50 array for training LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m96wrc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753468011,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ive been looking at buying a few mi50 32gb cards for my local training setup because they are absurdly affordable for the VRAM they have. I&amp;#39;m not too concerned with FLOP/s performance, as long as they have compatibility with a relatively modern pytorch and its dependencies.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen people on here talking about this card for inference but not training. Would this be a good idea?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m96wrc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Used_Algae_1077",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m96wrc/mi50_array_for_training_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m96wrc/mi50_array_for_training_llms/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753468011,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "me and my friend have been working on an architecture for a bit that doesnt use attention, but due to limited hardware progress has been slow, what companies or ppl should we reach out to? we arent looking for much maybe a 1000 dollars and would be glad to make a contract with someone for publishing rights of the LLM in exchange",
          "author_fullname": "t2_74ai6uqx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Who should we ask for funding?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9boeu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753479418,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;me and my friend have been working on an architecture for a bit that doesnt use attention, but due to limited hardware progress has been slow, what companies or ppl should we reach out to? we arent looking for much maybe a 1000 dollars and would be glad to make a contract with someone for publishing rights of the LLM in exchange&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9boeu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Commercial-Ad-1148",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9boeu/who_should_we_ask_for_funding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9boeu/who_should_we_ask_for_funding/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753479418,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is AMD working on any GPU which will compete with RTX 6000 PRO Blackwell in memory, compute, and price? Or one with higher VRAM but targeted at workstations?",
          "author_fullname": "t2_1266sp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD equivalent for NVIDIA RTX 6000 PRO Blackwell",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m95wcg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753465671,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is AMD working on any GPU which will compete with RTX 6000 PRO Blackwell in memory, compute, and price? Or one with higher VRAM but targeted at workstations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m95wcg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "s-s-a",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m95wcg/amd_equivalent_for_nvidia_rtx_6000_pro_blackwell/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m95wcg/amd_equivalent_for_nvidia_rtx_6000_pro_blackwell/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753465671,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "⚡ 2× faster\n\n💸 $0.30 / $1.20 per Mtoken\n\n✅ Nearly identical performance (\\~1% delta)\n\nPerfect for agentic workflows, tool use, and browser tasks.\n\nAlso, if you’re deploying open models or curious about real-time usage at scale, we just started [r/DeepInfra](https://www.reddit.com/r/DeepInfra) to track new model launches, price drops, and deployment tips. Would love to see what you’re building.",
          "author_fullname": "t2_1rrn43m1ok",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "If you’re experimenting with Qwen3-Coder, we just launched a Turbo version on DeepInfra",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9gg6j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753492182,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;⚡ 2× faster&lt;/p&gt;\n\n&lt;p&gt;💸 $0.30 / $1.20 per Mtoken&lt;/p&gt;\n\n&lt;p&gt;✅ Nearly identical performance (~1% delta)&lt;/p&gt;\n\n&lt;p&gt;Perfect for agentic workflows, tool use, and browser tasks.&lt;/p&gt;\n\n&lt;p&gt;Also, if you’re deploying open models or curious about real-time usage at scale, we just started &lt;a href=\"https://www.reddit.com/r/DeepInfra\"&gt;r/DeepInfra&lt;/a&gt; to track new model launches, price drops, and deployment tips. Would love to see what you’re building.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9gg6j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "deepinfra",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9gg6j/if_youre_experimenting_with_qwen3coder_we_just/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9gg6j/if_youre_experimenting_with_qwen3coder_we_just/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753492182,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_21qaqh1p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 3 Thinking is coming very soon",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 51,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8dln1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 230,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 230,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1OIHh7R-mfEk892o091BDg-5ivRbaC6jjmct0HHlBxY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753384779,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/61i8pt44hvef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/61i8pt44hvef1.png?auto=webp&amp;s=199ae416540ed35191e6454a349646379029949f",
                  "width": 1654,
                  "height": 606
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f3c31400b9a3cd6f09d39f562bd58e86cdc43cbb",
                    "width": 108,
                    "height": 39
                  },
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a029fe88fcb9fa844f284c0aea7500d00db97c54",
                    "width": 216,
                    "height": 79
                  },
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=178e6c50de0e6ef14b15e781bdcb2a2be7d26232",
                    "width": 320,
                    "height": 117
                  },
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=19b99a58da488472ec93d5842e37998def1cbe76",
                    "width": 640,
                    "height": 234
                  },
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f2495dca0fab016d71398e8436fe7dc30e236a20",
                    "width": 960,
                    "height": 351
                  },
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ea6f2da7e737f0762f01e6b42c9a50cf19a6748d",
                    "width": 1080,
                    "height": 395
                  }
                ],
                "variants": {},
                "id": "Y5bhpE2oH633vsr3feG38JCFuVtcXjLVmrApMikSGkM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8dln1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dulldata",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8dln1/qwen_3_thinking_is_coming_very_soon/",
          "stickied": false,
          "url": "https://i.redd.it/61i8pt44hvef1.png",
          "subreddit_subscribers": 504486,
          "created_utc": 1753384779,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Greetings, we're a state-owned college, and we want to acquire an IA workstation. We have a strict budget and cannot surpass it, so working with our providers, they gave us two options with our budget\n\n  \n1. One Threadripper PRO 9955WX, with WS WRX90E-SAGE SE, 1 PRO 6000 Blackwell, and 256 GB RAM\n\n2. One AMD Ryzen 9 9950X with a ProArt X870E-CREATOR, 2 PRO 6000 Blackwells and 128 GB RAM\n\n  \nBoth models have a 1600W PSU. The idea on the first model is to try to get another budget the next year in order to buy a second PRO 6000 Blackwell.\n\nWe're not extremely concerned about RAM (we can buy RAM later using a different budget) but we're concerned that the Ryzen 9950X only has enough PCIE lanes to run the blackwell on PCIE x8, instead of x16. Our provider told us that this is not very important unless we want to load and unload models all the time, but we have some reservations about that. So, can you guide us a little on that?\n\nThanks a bunch  \n",
          "author_fullname": "t2_lavl5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How important is to have PRO 6000 Blackwell running on 16 PCIE lanes?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8wuy7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753443620,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings, we&amp;#39;re a state-owned college, and we want to acquire an IA workstation. We have a strict budget and cannot surpass it, so working with our providers, they gave us two options with our budget&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;One Threadripper PRO 9955WX, with WS WRX90E-SAGE SE, 1 PRO 6000 Blackwell, and 256 GB RAM&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;One AMD Ryzen 9 9950X with a ProArt X870E-CREATOR, 2 PRO 6000 Blackwells and 128 GB RAM&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Both models have a 1600W PSU. The idea on the first model is to try to get another budget the next year in order to buy a second PRO 6000 Blackwell.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re not extremely concerned about RAM (we can buy RAM later using a different budget) but we&amp;#39;re concerned that the Ryzen 9950X only has enough PCIE lanes to run the blackwell on PCIE x8, instead of x16. Our provider told us that this is not very important unless we want to load and unload models all the time, but we have some reservations about that. So, can you guide us a little on that?&lt;/p&gt;\n\n&lt;p&gt;Thanks a bunch  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8wuy7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ferkte",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753443620,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In the market right now, there’s an ocean of no‑code and low‑code platforms shouting about how they “let you build anything.”\n\nBut let’s be real, most of them are just website builders with a fancier skin.\n\nI’ve used tools like Lovable, Bolt, Fire Studio.  \nThey are simple, but they still feel like the low‑end spectrum: good for spinning up a quick frontend for MVP, but they stop there.\n\nOn the opposite end, there are power tools - Windsurf and Cursor.  \nThese are meant for developers who already know how to code, but they are too advanced for non‑technical builders who have a deep idea but no engineering muscle.\n\nWhat’s missing is a middle ground.  \nA true **application generator** that isn’t about “drag a button, drag a form,” and isn’t just a playground for coders.\n\nImagine this: you explain in detail how your application should work. its flow, logic, data, and purpose, and the AI actually builds that application, not a landing page or backend shell, but a working tool.\n\nHas anyone here seen or tried something in that direction?  \nNot another website builder, something that can create applications from deep descriptions?\n\nbtw I'm just vibe coder",
          "author_fullname": "t2_1qe0x0t139",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any AI tool for application creation (not website builders)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m99xty",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753475214,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the market right now, there’s an ocean of no‑code and low‑code platforms shouting about how they “let you build anything.”&lt;/p&gt;\n\n&lt;p&gt;But let’s be real, most of them are just website builders with a fancier skin.&lt;/p&gt;\n\n&lt;p&gt;I’ve used tools like Lovable, Bolt, Fire Studio.&lt;br/&gt;\nThey are simple, but they still feel like the low‑end spectrum: good for spinning up a quick frontend for MVP, but they stop there.&lt;/p&gt;\n\n&lt;p&gt;On the opposite end, there are power tools - Windsurf and Cursor.&lt;br/&gt;\nThese are meant for developers who already know how to code, but they are too advanced for non‑technical builders who have a deep idea but no engineering muscle.&lt;/p&gt;\n\n&lt;p&gt;What’s missing is a middle ground.&lt;br/&gt;\nA true &lt;strong&gt;application generator&lt;/strong&gt; that isn’t about “drag a button, drag a form,” and isn’t just a playground for coders.&lt;/p&gt;\n\n&lt;p&gt;Imagine this: you explain in detail how your application should work. its flow, logic, data, and purpose, and the AI actually builds that application, not a landing page or backend shell, but a working tool.&lt;/p&gt;\n\n&lt;p&gt;Has anyone here seen or tried something in that direction?&lt;br/&gt;\nNot another website builder, something that can create applications from deep descriptions?&lt;/p&gt;\n\n&lt;p&gt;btw I&amp;#39;m just vibe coder&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m99xty",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Delicious_Track6230",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m99xty/any_ai_tool_for_application_creation_not_website/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m99xty/any_ai_tool_for_application_creation_not_website/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753475214,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a 3060 RTX for my i7 PC. I check the task manager it is has been using about 75% CPU, 55% RAM, and GPU 1% (although it will jump up to 48% and then plummet back to 1% after about a second. I have used Ooba and Kobold.ccp which use the llama.ccp server and kobold.ccp (of course) respectively. I have tried playing around with offloading different number of layers. I have noticed this with Gemma 3 27G, Mistral Small 22B, Mistral Nemo, and Qwen 14B. I don't mind waiting for a response so I realize that the models are probably too big to give me real time t/s. So, what am I doing wrong? I am still basically a newb when it comes to AI tech. I'd appreciate it if anybody to tell me why it isn't, at least the the Windows 10 task manager, utilizing the GPU much. My laptop which has only a 2040 RTX seems to run the models better and the settings are basically the same except I use 7 out of 8 cores on the laptop and 3 of 4 of the cores on my desktop CPU. I use Silly Tavern as my frontend so, it could be a setting in there such as the tokenizer I use (I usually just stick with the auto option).",
          "author_fullname": "t2_49abw3rv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local LLMs I have been using, through different two backends, seem to hardly use GPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9etng",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753487567,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 3060 RTX for my i7 PC. I check the task manager it is has been using about 75% CPU, 55% RAM, and GPU 1% (although it will jump up to 48% and then plummet back to 1% after about a second. I have used Ooba and Kobold.ccp which use the llama.ccp server and kobold.ccp (of course) respectively. I have tried playing around with offloading different number of layers. I have noticed this with Gemma 3 27G, Mistral Small 22B, Mistral Nemo, and Qwen 14B. I don&amp;#39;t mind waiting for a response so I realize that the models are probably too big to give me real time t/s. So, what am I doing wrong? I am still basically a newb when it comes to AI tech. I&amp;#39;d appreciate it if anybody to tell me why it isn&amp;#39;t, at least the the Windows 10 task manager, utilizing the GPU much. My laptop which has only a 2040 RTX seems to run the models better and the settings are basically the same except I use 7 out of 8 cores on the laptop and 3 of 4 of the cores on my desktop CPU. I use Silly Tavern as my frontend so, it could be a setting in there such as the tokenizer I use (I usually just stick with the auto option).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9etng",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "theshadowraven",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9etng/local_llms_i_have_been_using_through_different/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9etng/local_llms_i_have_been_using_through_different/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753487567,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A friend’s messing with the idea of setting up a camera in his garage gym to watch his lifts, give form feedback, count reps, maybe even talk to him in real time.\n\nNeeds to be actually real-time tho, like not 5s delay, and ideally configurable too.\n\nAnyone know what models or pipelines would work best for this? Thinking maybe something like a lightweight vision model (pose tracking?) + audio TTS + LLM glue but curious if anyone here’s already stitched something like this together or knows what stack would be least painful?\n\nOpen to weird, hacked, setups if it works.",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone stitched together real-time local AI for webcam + voice feedback?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9egm9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753486592,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A friend’s messing with the idea of setting up a camera in his garage gym to watch his lifts, give form feedback, count reps, maybe even talk to him in real time.&lt;/p&gt;\n\n&lt;p&gt;Needs to be actually real-time tho, like not 5s delay, and ideally configurable too.&lt;/p&gt;\n\n&lt;p&gt;Anyone know what models or pipelines would work best for this? Thinking maybe something like a lightweight vision model (pose tracking?) + audio TTS + LLM glue but curious if anyone here’s already stitched something like this together or knows what stack would be least painful?&lt;/p&gt;\n\n&lt;p&gt;Open to weird, hacked, setups if it works.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9egm9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9egm9/anyone_stitched_together_realtime_local_ai_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9egm9/anyone_stitched_together_realtime_local_ai_for/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753486592,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "MassGen — an open-source multi-agent orchestration framework just launched. Supports cross-model collaboration (Grok, OpenAI, Claude, Gemini) with real-time streaming and consensus-building among agents. Inspired by \"parallel study groups\" and Grok Heavy. \n\n[https://x.com/Chi\\_Wang\\_/status/1948790995694617036](https://x.com/Chi_Wang_/status/1948790995694617036)",
          "author_fullname": "t2_1rqcx8w6j7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MassGen – an open-source multi-agent scaling and orchestration framework",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m95lud",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753464999,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MassGen — an open-source multi-agent orchestration framework just launched. Supports cross-model collaboration (Grok, OpenAI, Claude, Gemini) with real-time streaming and consensus-building among agents. Inspired by &amp;quot;parallel study groups&amp;quot; and Grok Heavy. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://x.com/Chi_Wang_/status/1948790995694617036\"&gt;https://x.com/Chi_Wang_/status/1948790995694617036&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m95lud",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LifeUnderstanding732",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m95lud/massgen_an_opensource_multiagent_scaling_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m95lud/massgen_an_opensource_multiagent_scaling_and/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753464999,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1e1w1ul46b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China's Bytedance releases Seed LiveInterpret simultaneous interpretation model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ozb0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753415015,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "seed.bytedance.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://seed.bytedance.com/en/seed_liveinterpret",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8ozb0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun-Doctor6855",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8ozb0/chinas_bytedance_releases_seed_liveinterpret/",
          "stickied": false,
          "url": "https://seed.bytedance.com/en/seed_liveinterpret",
          "subreddit_subscribers": 504486,
          "created_utc": 1753415015,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi All,\nI have been self hosting Ollama and mostly just use it to throw random questions or helping me dumb down a complex topic to answer a question my daughter asks.\n\nThe one thing I love about ChatGPT/Gemini is the ability to voice chat back and forth.\n\n\nIs there a easy to use mobile/desktop app and model combo that a semi-layman can setup?\n\nCurrently I use https://chatboxai.app/en + tailscale to access my Ollama/LLM remotely that runs on my RTX 3060 (12GB VRAM).\n\nThanks in advance! ",
          "author_fullname": "t2_4t06y10d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "App for voice interaction with LocalLLaMA. Looking for help/app/model etc.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9e71s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753485878,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,\nI have been self hosting Ollama and mostly just use it to throw random questions or helping me dumb down a complex topic to answer a question my daughter asks.&lt;/p&gt;\n\n&lt;p&gt;The one thing I love about ChatGPT/Gemini is the ability to voice chat back and forth.&lt;/p&gt;\n\n&lt;p&gt;Is there a easy to use mobile/desktop app and model combo that a semi-layman can setup?&lt;/p&gt;\n\n&lt;p&gt;Currently I use &lt;a href=\"https://chatboxai.app/en\"&gt;https://chatboxai.app/en&lt;/a&gt; + tailscale to access my Ollama/LLM remotely that runs on my RTX 3060 (12GB VRAM).&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?auto=webp&amp;s=292ee07d261c9960edd9e6bc5216b120e3ca8c70",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=279a09b67459be926a08944e6c9ea50312a63a5f",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=71854addc096fa99604724a66b8d210353b93453",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb617badf513ed94cfbeb3e2cbe71000e2592028",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b440db62ce02b358f27186c315e179af0a46a940",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3fbbeab38b3b11b83f247b980db93408afdf989",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a139fe4e602dba248adc60f4bda3146ef969fd06",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9e71s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Mesh",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9e71s/app_for_voice_interaction_with_localllama_looking/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9e71s/app_for_voice_interaction_with_localllama_looking/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753485878,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Given: 14-inch MacBook Pro (M4 Pro, 48GB unified memory, 1TB SSD)\n\n\nWhat kind of local LLMs can I run? \n\nWhat’s your experience?\n\nCan I run mistral, Gemma, phi, or models 7b or 13b, etc. params?\n\n\nThanks!",
          "author_fullname": "t2_3kkuzrph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Laptop advise for lightweight AI work",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9e2s9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753485561,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given: 14-inch MacBook Pro (M4 Pro, 48GB unified memory, 1TB SSD)&lt;/p&gt;\n\n&lt;p&gt;What kind of local LLMs can I run? &lt;/p&gt;\n\n&lt;p&gt;What’s your experience?&lt;/p&gt;\n\n&lt;p&gt;Can I run mistral, Gemma, phi, or models 7b or 13b, etc. params?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9e2s9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entered_apprentice",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9e2s9/laptop_advise_for_lightweight_ai_work/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9e2s9/laptop_advise_for_lightweight_ai_work/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753485561,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "There's so many models, which one to train?\nDoes it depend on the kind of output I need like text or code or format / structure?\n\nAnd how long does training take on what hardware?\n\n5060 ti, A100, 5090, any information.\n\nThank you",
          "author_fullname": "t2_17hwnassb4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best models to fine-tune?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9dysd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753485276,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s so many models, which one to train?\nDoes it depend on the kind of output I need like text or code or format / structure?&lt;/p&gt;\n\n&lt;p&gt;And how long does training take on what hardware?&lt;/p&gt;\n\n&lt;p&gt;5060 ti, A100, 5090, any information.&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9dysd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zekuden",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9dysd/best_models_to_finetune/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9dysd/best_models_to_finetune/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753485276,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been working on a small tool to make it easier to extract high-quality transcripts from YouTube videos.  I think it will be useful for AI trainers and dataset builders who want to build language datasets from online content.\n\nSo I will be giving away a beta tester account that will have infinite credits until launch it has a bulk extract feature which can extract all transcripts of a YouTube channel and videos and put it in one file .\n\ndm me if you want to be a beta tester\n\nhttps://preview.redd.it/wf8kkco9t3ff1.png?width=618&amp;format=png&amp;auto=webp&amp;s=0545d5ab53852b80cf553498d53d1982075d05b6\n\nhttps://preview.redd.it/1mg8157ss3ff1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=33ff520a9e831b2fee0f99118143efa0cb8b59df\n\nhttps://preview.redd.it/kx9xm6hgt3ff1.png?width=863&amp;format=png&amp;auto=webp&amp;s=2aafa58c52afea745518a37c41ccf49c09972519\n\nhttps://preview.redd.it/985pjh9tt3ff1.png?width=691&amp;format=png&amp;auto=webp&amp;s=13be1dd344b3a1ea463070e2bf92a43b5ba4dd49\n\n  \n",
          "author_fullname": "t2_1rk8qow8x2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ai training Tool I want to share!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 119,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "985pjh9tt3ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 85,
                  "x": 108,
                  "u": "https://preview.redd.it/985pjh9tt3ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=00a42e32168a547f07923efa28a793940f08a360"
                },
                {
                  "y": 170,
                  "x": 216,
                  "u": "https://preview.redd.it/985pjh9tt3ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7165f86006d7ef09f4ff48384ac9aad30f3318c2"
                },
                {
                  "y": 252,
                  "x": 320,
                  "u": "https://preview.redd.it/985pjh9tt3ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eec191101d9dd0c302e6fedf0af1bacbfb2f4938"
                },
                {
                  "y": 504,
                  "x": 640,
                  "u": "https://preview.redd.it/985pjh9tt3ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9156dcafa2959b6f898f789a62ff73615f1d75e8"
                }
              ],
              "s": {
                "y": 545,
                "x": 691,
                "u": "https://preview.redd.it/985pjh9tt3ff1.png?width=691&amp;format=png&amp;auto=webp&amp;s=13be1dd344b3a1ea463070e2bf92a43b5ba4dd49"
              },
              "id": "985pjh9tt3ff1"
            },
            "wf8kkco9t3ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 92,
                  "x": 108,
                  "u": "https://preview.redd.it/wf8kkco9t3ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0dea551e664e42f9448f784a42f51b399f5397a8"
                },
                {
                  "y": 184,
                  "x": 216,
                  "u": "https://preview.redd.it/wf8kkco9t3ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a436507d1e263f947381a877681653df20c90e0"
                },
                {
                  "y": 273,
                  "x": 320,
                  "u": "https://preview.redd.it/wf8kkco9t3ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8cd9274b47476014dc317bb84086035fcfa0cc79"
                }
              ],
              "s": {
                "y": 528,
                "x": 618,
                "u": "https://preview.redd.it/wf8kkco9t3ff1.png?width=618&amp;format=png&amp;auto=webp&amp;s=0545d5ab53852b80cf553498d53d1982075d05b6"
              },
              "id": "wf8kkco9t3ff1"
            },
            "kx9xm6hgt3ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/kx9xm6hgt3ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f26a5ec431efd8ca5d97fd4b448315b8c68af19a"
                },
                {
                  "y": 134,
                  "x": 216,
                  "u": "https://preview.redd.it/kx9xm6hgt3ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f7e4c58a87f4a231f57aefc45240c188e2d2610a"
                },
                {
                  "y": 198,
                  "x": 320,
                  "u": "https://preview.redd.it/kx9xm6hgt3ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7cbf8fd45db7c92a0a7a54e73dae5aa0aacd6cbd"
                },
                {
                  "y": 397,
                  "x": 640,
                  "u": "https://preview.redd.it/kx9xm6hgt3ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9fe19ea5db262ac80ac773e5628fc2165b993a88"
                }
              ],
              "s": {
                "y": 536,
                "x": 863,
                "u": "https://preview.redd.it/kx9xm6hgt3ff1.png?width=863&amp;format=png&amp;auto=webp&amp;s=2aafa58c52afea745518a37c41ccf49c09972519"
              },
              "id": "kx9xm6hgt3ff1"
            },
            "1mg8157ss3ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f9afd44a3f9f70d75751fd25af84329e258957b7"
                },
                {
                  "y": 94,
                  "x": 216,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0eb9ca900f5ac925b7767d12e1f432264fa0f535"
                },
                {
                  "y": 140,
                  "x": 320,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e87773e5dd644629e80477fbb6bc64682951581"
                },
                {
                  "y": 280,
                  "x": 640,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f993fb75fe8121f018ff4fe789c5660be9333f6a"
                },
                {
                  "y": 421,
                  "x": 960,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a960f16f86a744a27c55f093ec4420e6756d0736"
                },
                {
                  "y": 473,
                  "x": 1080,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f3bb82df7e16a39e069dfd0a2f8e52e51a92f7e2"
                }
              ],
              "s": {
                "y": 586,
                "x": 1336,
                "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=33ff520a9e831b2fee0f99118143efa0cb8b59df"
              },
              "id": "1mg8157ss3ff1"
            }
          },
          "name": "t3_1m9duv2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1-i3V0tVUx4LiF6k37Pc1dzCVbo6GAP2UM5PyjET5Jc.jpg",
          "edited": 1753485915,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753484988,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been working on a small tool to make it easier to extract high-quality transcripts from YouTube videos.  I think it will be useful for AI trainers and dataset builders who want to build language datasets from online content.&lt;/p&gt;\n\n&lt;p&gt;So I will be giving away a beta tester account that will have infinite credits until launch it has a bulk extract feature which can extract all transcripts of a YouTube channel and videos and put it in one file .&lt;/p&gt;\n\n&lt;p&gt;dm me if you want to be a beta tester&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wf8kkco9t3ff1.png?width=618&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0545d5ab53852b80cf553498d53d1982075d05b6\"&gt;https://preview.redd.it/wf8kkco9t3ff1.png?width=618&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0545d5ab53852b80cf553498d53d1982075d05b6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1mg8157ss3ff1.png?width=1336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=33ff520a9e831b2fee0f99118143efa0cb8b59df\"&gt;https://preview.redd.it/1mg8157ss3ff1.png?width=1336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=33ff520a9e831b2fee0f99118143efa0cb8b59df&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kx9xm6hgt3ff1.png?width=863&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2aafa58c52afea745518a37c41ccf49c09972519\"&gt;https://preview.redd.it/kx9xm6hgt3ff1.png?width=863&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2aafa58c52afea745518a37c41ccf49c09972519&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/985pjh9tt3ff1.png?width=691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13be1dd344b3a1ea463070e2bf92a43b5ba4dd49\"&gt;https://preview.redd.it/985pjh9tt3ff1.png?width=691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13be1dd344b3a1ea463070e2bf92a43b5ba4dd49&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m9duv2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Enough_Patient1904",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9duv2/ai_training_tool_i_want_to_share/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9duv2/ai_training_tool_i_want_to_share/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753484988,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China’s First High-End Gaming GPU, the Lisuan G100, Reportedly Outperforms NVIDIA’s GeForce RTX 4060 &amp; Slightly Behind the RTX 5060 in New Benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 99,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m83644",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 577,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 577,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=140&amp;height=99&amp;crop=140:99,smart&amp;auto=webp&amp;s=e317af62a4b0fea5876b0388a0cd1f5775ef680b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753360416,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/china-first-high-end-gaming-gpu-lisuan-g100-outperforms-nvidia-geforce-rtx-4060/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?auto=webp&amp;s=e789a7dbd0c89c024f728c8e0ac2c066b704ed9a",
                  "width": 728,
                  "height": 516
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff00e39d3c91233a4ad4b2458d203b679086f872",
                    "width": 108,
                    "height": 76
                  },
                  {
                    "url": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=58f702c18129404394dea56cd9d3b6975a719f62",
                    "width": 216,
                    "height": 153
                  },
                  {
                    "url": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8326f56ed914670a98663b55ce61baafb1e3d037",
                    "width": 320,
                    "height": 226
                  },
                  {
                    "url": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6aa69848c81b950052de8eb2024c390e13024272",
                    "width": 640,
                    "height": 453
                  }
                ],
                "variants": {},
                "id": "lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m83644",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 221,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m83644/chinas_first_highend_gaming_gpu_the_lisuan_g100/",
          "stickied": false,
          "url": "https://wccftech.com/china-first-high-end-gaming-gpu-lisuan-g100-outperforms-nvidia-geforce-rtx-4060/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753360416,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I downloaded the original FP8 version because I wanted to experiment with different quants and compare them, and also use my own imatrix for the best results for my use cases. For DeepSeek V3 and R1 this approach works very well, I can make use of imatrix data of my choice and select quantization parameters that I prefer.\n\nBut so far I had no luck converting Kimi K2 FP8 to BF16, even though it is technically based on the DeepSeek architecture. I shared details in the comments since otherwise the post does not come through. I would appreciate if anyone can share ideas what else to try to convert Kimi K2 FP8 to BF16 given I have only 3090 GPUs and CPU, so cannot use the official DeepSeek script to convert.",
          "author_fullname": "t2_fpfao9g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to convert Kimi K2 FP8 to BF16?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m97qko",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753471279,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753469956,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I downloaded the original FP8 version because I wanted to experiment with different quants and compare them, and also use my own imatrix for the best results for my use cases. For DeepSeek V3 and R1 this approach works very well, I can make use of imatrix data of my choice and select quantization parameters that I prefer.&lt;/p&gt;\n\n&lt;p&gt;But so far I had no luck converting Kimi K2 FP8 to BF16, even though it is technically based on the DeepSeek architecture. I shared details in the comments since otherwise the post does not come through. I would appreciate if anyone can share ideas what else to try to convert Kimi K2 FP8 to BF16 given I have only 3090 GPUs and CPU, so cannot use the official DeepSeek script to convert.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m97qko",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lissanro",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m97qko/how_to_convert_kimi_k2_fp8_to_bf16/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m97qko/how_to_convert_kimi_k2_fp8_to_bf16/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753469956,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Every new model likes to claim it's SOTA, better than DeepSeek, better than whatever OpenAI/Google/Anthropic/xAI put out, and shows some benchmarks making it comparable to or better than everyone else. However, most new models tend to underwhelm me in actual usage. People have spoken of benchmaxxing a lot, and I'm really feeling it from many newer models. World knowledge in particular seems to have stagnated, and most models claiming more world knowledge in a smaller size than some competitor don't really live up to their claims.\n\nI've been experimenting with DeepSeek v3-0324, Kimi K2, Qwen 3 235B-A22B (original), Qwen 3 235B-A22B (2507 non-thinking), Llama 4 Maverick, Llama 3.3 70B, Mistral Large 2411, Cohere Command-A 2503, as well as smaller models like Qwen 3 30B-A3B, Mistral Small 3.2, and Gemma 3 27B. I've also been comparing to mid-size proprietary models like GPT-4.1, Gemini 2.5 Flash, and Claude 4 Sonnet.\n\nIn my experiments asking a broad variety of fresh world knowledge questions I made for a new private eval, they ranked as follows for world knowledge:\n\n1. DeekSeek v3 (0324)\n2. Mistral Large (2411)\n3. Kimi K2\n4. Cohere Command-A (2503)\n5. Qwen 3 235B-A22B (2507, non-thinking)\n6. Llama 4 Maverick\n7. Llama 3.3 70B\n8. Qwen 3 235B-A22B (original hybrid thinking model, with thinking turned off)\n9. Dots.LLM1\n10. Gemma 3 27B\n11. Mistral Small 3.2\n12. Qwen 3 30B-A3B\n\nIn my experiments, the only open model with knowledge comparable to Gemini 2.5 Flash and GPT 4.1 was DeepSeek v3.\n\nOf the open models I tried, the second best for world knowledge was Mistral Large 2411. Kimi K2 was in third place in my tests of world knowledge, not far behind Mistral Large in knowledge, but with more hallucinations, and a more strange, disorganized, and ugly response format.\n\nFourth place was Cohere Command A 2503, and fifth place was Qwen 3 2507. Llama 4 was a substantial step down, and only marginally better than Llama 3.3 70B in knowledge or intelligence. Qwen 3 235B-A22B had really poor knowledge for its size, and Dots.LLM1 was disappointing, hardly any more knowledgeable than Gemma 3 27B and no smarter either. Mistral Small 3.2 gave me good vibes, not too far behind Gemma 3 27B in knowledge, and decent intelligence. Qwen 3 30B-A3B also felt impressive to me; while the worst of the lot in world knowledge, it was very fast and still OK, honestly not that far off in knowledge from the original 235B that's nearly 8x bigger.\n\nAnyway, my point is that knowledge benchmarks like SimpleQA, GPQA, and PopQA need to be taken with a grain of salt. In terms of knowledge density, if you ignore benchmarks and try for yourself, you'll find that the latest and greatest like Qwen 3 235B-A22B-2507 and Kimi K2 are no better than Mistral Large 2407 from one year ago, and a step behind mid-size closed models like Gemini 2.5 Flash. It feels like we're hitting a wall with how much we can compress knowledge, and that improving programming and STEM problem solving capabilities comes at the expense of knowledge unless you increase parameter counts.\n\nThe other thing I noticed is that for Qwen specifically, the giant 235B-A22B models aren't that much more knowledgeable than the small 30B-A3B model. In my own test questions, Gemini 2.5 Flash would get around 90% right, DeepSeek v3 around 85% right, Kimi and Mistral Large around 75% right, Qwen 3 2507 around 70% right, Qwen 3 235B-A22B (original) around 60%, and Qwen 3 30B-A3B around 45%. The step up in knowledge from Qwen 3 30B to the original 235B was very underwhelming for the 8x size increase.",
          "author_fullname": "t2_702zh1r2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Stagnation in Knowledge Density",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8oc9j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753413019,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every new model likes to claim it&amp;#39;s SOTA, better than DeepSeek, better than whatever OpenAI/Google/Anthropic/xAI put out, and shows some benchmarks making it comparable to or better than everyone else. However, most new models tend to underwhelm me in actual usage. People have spoken of benchmaxxing a lot, and I&amp;#39;m really feeling it from many newer models. World knowledge in particular seems to have stagnated, and most models claiming more world knowledge in a smaller size than some competitor don&amp;#39;t really live up to their claims.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been experimenting with DeepSeek v3-0324, Kimi K2, Qwen 3 235B-A22B (original), Qwen 3 235B-A22B (2507 non-thinking), Llama 4 Maverick, Llama 3.3 70B, Mistral Large 2411, Cohere Command-A 2503, as well as smaller models like Qwen 3 30B-A3B, Mistral Small 3.2, and Gemma 3 27B. I&amp;#39;ve also been comparing to mid-size proprietary models like GPT-4.1, Gemini 2.5 Flash, and Claude 4 Sonnet.&lt;/p&gt;\n\n&lt;p&gt;In my experiments asking a broad variety of fresh world knowledge questions I made for a new private eval, they ranked as follows for world knowledge:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;DeekSeek v3 (0324)&lt;/li&gt;\n&lt;li&gt;Mistral Large (2411)&lt;/li&gt;\n&lt;li&gt;Kimi K2&lt;/li&gt;\n&lt;li&gt;Cohere Command-A (2503)&lt;/li&gt;\n&lt;li&gt;Qwen 3 235B-A22B (2507, non-thinking)&lt;/li&gt;\n&lt;li&gt;Llama 4 Maverick&lt;/li&gt;\n&lt;li&gt;Llama 3.3 70B&lt;/li&gt;\n&lt;li&gt;Qwen 3 235B-A22B (original hybrid thinking model, with thinking turned off)&lt;/li&gt;\n&lt;li&gt;Dots.LLM1&lt;/li&gt;\n&lt;li&gt;Gemma 3 27B&lt;/li&gt;\n&lt;li&gt;Mistral Small 3.2&lt;/li&gt;\n&lt;li&gt;Qwen 3 30B-A3B&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;In my experiments, the only open model with knowledge comparable to Gemini 2.5 Flash and GPT 4.1 was DeepSeek v3.&lt;/p&gt;\n\n&lt;p&gt;Of the open models I tried, the second best for world knowledge was Mistral Large 2411. Kimi K2 was in third place in my tests of world knowledge, not far behind Mistral Large in knowledge, but with more hallucinations, and a more strange, disorganized, and ugly response format.&lt;/p&gt;\n\n&lt;p&gt;Fourth place was Cohere Command A 2503, and fifth place was Qwen 3 2507. Llama 4 was a substantial step down, and only marginally better than Llama 3.3 70B in knowledge or intelligence. Qwen 3 235B-A22B had really poor knowledge for its size, and Dots.LLM1 was disappointing, hardly any more knowledgeable than Gemma 3 27B and no smarter either. Mistral Small 3.2 gave me good vibes, not too far behind Gemma 3 27B in knowledge, and decent intelligence. Qwen 3 30B-A3B also felt impressive to me; while the worst of the lot in world knowledge, it was very fast and still OK, honestly not that far off in knowledge from the original 235B that&amp;#39;s nearly 8x bigger.&lt;/p&gt;\n\n&lt;p&gt;Anyway, my point is that knowledge benchmarks like SimpleQA, GPQA, and PopQA need to be taken with a grain of salt. In terms of knowledge density, if you ignore benchmarks and try for yourself, you&amp;#39;ll find that the latest and greatest like Qwen 3 235B-A22B-2507 and Kimi K2 are no better than Mistral Large 2407 from one year ago, and a step behind mid-size closed models like Gemini 2.5 Flash. It feels like we&amp;#39;re hitting a wall with how much we can compress knowledge, and that improving programming and STEM problem solving capabilities comes at the expense of knowledge unless you increase parameter counts.&lt;/p&gt;\n\n&lt;p&gt;The other thing I noticed is that for Qwen specifically, the giant 235B-A22B models aren&amp;#39;t that much more knowledgeable than the small 30B-A3B model. In my own test questions, Gemini 2.5 Flash would get around 90% right, DeepSeek v3 around 85% right, Kimi and Mistral Large around 75% right, Qwen 3 2507 around 70% right, Qwen 3 235B-A22B (original) around 60%, and Qwen 3 30B-A3B around 45%. The step up in knowledge from Qwen 3 30B to the original 235B was very underwhelming for the 8x size increase.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8oc9j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Federal-Effective879",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8oc9j/stagnation_in_knowledge_density/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8oc9j/stagnation_in_knowledge_density/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753413019,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been following the progress of local LLMs for a while and I'm really interested in setting up a system for a natural, real-time audio conversation. I've seen some posts here discussing solutions that involve piping together speech-to-text, the LLM, and text-to-speech.\n\nI'm curious to know if anyone has found or built a more integrated solution that minimizes latency and feels more like a direct conversation. I've come across mentions of projects like Verbi and the potential of multimodal models like Qwen2-Audio, and I'm wondering if these are still the current way to go?\n\nIdeally, I'm looking for something that can run on consumer-grade hardware.\n\nWhat are your current setups for this? Have you managed to achieve a truly conversational experience?",
          "author_fullname": "t2_vyvxdjqyp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone found a seamless, low-latency solution for real-time audio conversations with a local LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9c9fh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753480874,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been following the progress of local LLMs for a while and I&amp;#39;m really interested in setting up a system for a natural, real-time audio conversation. I&amp;#39;ve seen some posts here discussing solutions that involve piping together speech-to-text, the LLM, and text-to-speech.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious to know if anyone has found or built a more integrated solution that minimizes latency and feels more like a direct conversation. I&amp;#39;ve come across mentions of projects like Verbi and the potential of multimodal models like Qwen2-Audio, and I&amp;#39;m wondering if these are still the current way to go?&lt;/p&gt;\n\n&lt;p&gt;Ideally, I&amp;#39;m looking for something that can run on consumer-grade hardware.&lt;/p&gt;\n\n&lt;p&gt;What are your current setups for this? Have you managed to achieve a truly conversational experience?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9c9fh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Far_Buyer_7281",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9c9fh/has_anyone_found_a_seamless_lowlatency_solution/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9c9fh/has_anyone_found_a_seamless_lowlatency_solution/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753480874,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,  \nI'm working on a project using LibreChat, and I've noticed that it handles translations through `.ts` and `.md` files—one set per language. Each file contains over a thousand lines, so I assume these aren't written manually. There must be some kind of script or automation behind generating them.\n\nI want to make a change to one of the base messages. Specifically, in a registration form, there's a field for `username` and it currently displays `(optional)`. I want to remove that word so it no longer shows.\n\nMy question is:  \nIf I update the base message (presumably in the default language file), is there a way to automatically update the rest of the language files to reflect this change? For example, marking the string as needing translation or syncing the keys across all files?\n\nAny insights or tips on how this workflow is managed in LibreChat or similar setups would be really appreciated.  \nThanks!",
          "author_fullname": "t2_3t1oh2ky",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How does LibreChat handle translations and how can I update all language files after changing base messages?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m96m6w",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753467319,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;br/&gt;\nI&amp;#39;m working on a project using LibreChat, and I&amp;#39;ve noticed that it handles translations through &lt;code&gt;.ts&lt;/code&gt; and &lt;code&gt;.md&lt;/code&gt; files—one set per language. Each file contains over a thousand lines, so I assume these aren&amp;#39;t written manually. There must be some kind of script or automation behind generating them.&lt;/p&gt;\n\n&lt;p&gt;I want to make a change to one of the base messages. Specifically, in a registration form, there&amp;#39;s a field for &lt;code&gt;username&lt;/code&gt; and it currently displays &lt;code&gt;(optional)&lt;/code&gt;. I want to remove that word so it no longer shows.&lt;/p&gt;\n\n&lt;p&gt;My question is:&lt;br/&gt;\nIf I update the base message (presumably in the default language file), is there a way to automatically update the rest of the language files to reflect this change? For example, marking the string as needing translation or syncing the keys across all files?&lt;/p&gt;\n\n&lt;p&gt;Any insights or tips on how this workflow is managed in LibreChat or similar setups would be really appreciated.&lt;br/&gt;\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m96m6w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "suribe06",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m96m6w/how_does_librechat_handle_translations_and_how/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m96m6w/how_does_librechat_handle_translations_and_how/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753467319,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all, looking for a discussion on GPU options for LLM self hosting. Looking for something 24GB that doesn’t break the bank. Bonus if it’s single slot as I have no room in the server I’m working with. \n\nObviously there’s a desire to run the biggest model possible but there’s plenty of tradeoffs here and of course using it for other workloads. Thoughts?",
          "author_fullname": "t2_3nx6tffm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPU Suggestions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m92vqp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753458834,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, looking for a discussion on GPU options for LLM self hosting. Looking for something 24GB that doesn’t break the bank. Bonus if it’s single slot as I have no room in the server I’m working with. &lt;/p&gt;\n\n&lt;p&gt;Obviously there’s a desire to run the biggest model possible but there’s plenty of tradeoffs here and of course using it for other workloads. Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m92vqp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Grimm_Spector",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m92vqp/gpu_suggestions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m92vqp/gpu_suggestions/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753458834,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been writing some AI Agents lately and they work much better than I expected. Here are the 10 learnings for writing AI agents that work:\n\n1. **Tools first.** Design, write and test the tools before connecting to LLMs. Tools are the most deterministic part of your code. Make sure they work 100% before writing actual agents.\n2. **Start with general, low-level tools.** For example, bash is a powerful tool that can cover most needs. You don't need to start with a full suite of 100 tools.\n3. **Start with a single agent.** Once you have all the basic tools, test them with a single react agent. It's extremely easy to write a react agent once you have the tools. All major agent frameworks have a built-in react agent. You just need to plugin your tools.\n4. **Start with the best models.** There will be a lot of problems with your system, so you don't want the model's ability to be one of them. Start with Claude Sonnet or Gemini Pro. You can downgrade later for cost purposes.\n5. **Trace and log your agent.** Writing agents is like doing animal experiments. There will be many unexpected behaviors. You need to monitor it as carefully as possible. There are many logging systems that help, like Langsmith, Langfuse, etc.\n6. **Identify the bottlenecks.** There's a chance that a single agent with general tools already works. But if not, you should read your logs and identify the bottleneck. It could be: context length is too long, tools are not specialized enough, the model doesn't know how to do something, etc.\n7. **Iterate based on the bottleneck.** There are many ways to improve: switch to multi-agents, write better prompts, write more specialized tools, etc. Choose them based on your bottleneck.\n8. **You can combine workflows with agents and it may work better.** If your objective is specialized and there's a unidirectional order in that process, a workflow is better, and each workflow node can be an agent. For example, a deep research agent can be a two-step workflow: first a divergent broad search, then a convergent report writing, with each step being an agentic system by itself.\n9. **Trick: Utilize the filesystem as a hack.** Files are a great way for AI Agents to document, memorize, and communicate. You can save a lot of context length when they simply pass around file URLs instead of full documents.\n10. **Another Trick: Ask Claude Code how to write agents.** Claude Code is the best agent we have out there. Even though it's not open-sourced, CC knows its prompt, architecture, and tools. You can ask its advice for your system.",
          "author_fullname": "t2_ynwvt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I wrote an AI Agent that works better than I expected. Here are 10 learnings.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vmoi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753439430,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been writing some AI Agents lately and they work much better than I expected. Here are the 10 learnings for writing AI agents that work:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Tools first.&lt;/strong&gt; Design, write and test the tools before connecting to LLMs. Tools are the most deterministic part of your code. Make sure they work 100% before writing actual agents.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Start with general, low-level tools.&lt;/strong&gt; For example, bash is a powerful tool that can cover most needs. You don&amp;#39;t need to start with a full suite of 100 tools.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Start with a single agent.&lt;/strong&gt; Once you have all the basic tools, test them with a single react agent. It&amp;#39;s extremely easy to write a react agent once you have the tools. All major agent frameworks have a built-in react agent. You just need to plugin your tools.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Start with the best models.&lt;/strong&gt; There will be a lot of problems with your system, so you don&amp;#39;t want the model&amp;#39;s ability to be one of them. Start with Claude Sonnet or Gemini Pro. You can downgrade later for cost purposes.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Trace and log your agent.&lt;/strong&gt; Writing agents is like doing animal experiments. There will be many unexpected behaviors. You need to monitor it as carefully as possible. There are many logging systems that help, like Langsmith, Langfuse, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Identify the bottlenecks.&lt;/strong&gt; There&amp;#39;s a chance that a single agent with general tools already works. But if not, you should read your logs and identify the bottleneck. It could be: context length is too long, tools are not specialized enough, the model doesn&amp;#39;t know how to do something, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Iterate based on the bottleneck.&lt;/strong&gt; There are many ways to improve: switch to multi-agents, write better prompts, write more specialized tools, etc. Choose them based on your bottleneck.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;You can combine workflows with agents and it may work better.&lt;/strong&gt; If your objective is specialized and there&amp;#39;s a unidirectional order in that process, a workflow is better, and each workflow node can be an agent. For example, a deep research agent can be a two-step workflow: first a divergent broad search, then a convergent report writing, with each step being an agentic system by itself.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Trick: Utilize the filesystem as a hack.&lt;/strong&gt; Files are a great way for AI Agents to document, memorize, and communicate. You can save a lot of context length when they simply pass around file URLs instead of full documents.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Another Trick: Ask Claude Code how to write agents.&lt;/strong&gt; Claude Code is the best agent we have out there. Even though it&amp;#39;s not open-sourced, CC knows its prompt, architecture, and tools. You can ask its advice for your system.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8vmoi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Js8544",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vmoi/i_wrote_an_ai_agent_that_works_better_than_i/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8vmoi/i_wrote_an_ai_agent_that_works_better_than_i/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753439430,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying think of a conversational LLM \nWhich won't hallucinate when the context (conversation history) grows. \nLlm should also hold personalities. \nAny help us appropriated. \n",
          "author_fullname": "t2_uvads9n4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Conversational LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m968q4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753466456,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying think of a conversational LLM \nWhich won&amp;#39;t hallucinate when the context (conversation history) grows. \nLlm should also hold personalities. \nAny help us appropriated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m968q4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "backofthemind99",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m968q4/conversational_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m968q4/conversational_llm/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753466456,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt;[Cognito: Your AI Sidekick for Chrome. A MIT licensed very lightweight Web UI with multitools.](https://www.reddit.com/r/LocalLLaMA/comments/1kwhw20/cognito_your_ai_sidekick_for_chrome_a_mit/)  \nby[u/Asleep-Ratio7535](https://www.reddit.com/user/Asleep-Ratio7535/) in[LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/)\n\nThis extension comes to a closure with so many published MCP servers. Chrome webstore is a little bit slower.\n\nNew update:\n\n* A good enough hybrid RAG for latin languages (BM25 tokenizer, I added a simple Japanese tokenizer as well), Only Chinese doesn't support BM25 full text search, but you can still use a good embedding model.\n* A note system for saving webpages and notes for RAG or use as direct context\n* Several basic useful tools: web search, prompt optimizer, wiki, retriever, save note, update your preference, and some \"agents\" that can plan and execute those tools itself\n\nIn the picture is an example of how a 4B model planned and used the tools it has. In this example, I tested too many concurrent web searches, so I didn't notice I needed to click the captcha on the page. So it failed in the first 2 steps, but you can get rid of it easily by clicking the captcha, or use a custom API, or DuckDuckGo, brave.\n\nhttps://preview.redd.it/esicru1dc1ff1.png?width=2560&amp;format=png&amp;auto=webp&amp;s=060dd197ae2c49892375e8d17b60320d5f6658e6\n\n",
          "author_fullname": "t2_1lfyddwf0c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Updated] AI assistant Chrome extension has tools and RAG",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "esicru1dc1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7fffe5029ab5f76ebb40401b7cad76bae9b4e7b6"
                },
                {
                  "y": 135,
                  "x": 216,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ac919ba5c18221a7f4a6e00afa0116c7c6b6855"
                },
                {
                  "y": 200,
                  "x": 320,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=31bb77985d25b08c50ec5b7967a10aff85d228c8"
                },
                {
                  "y": 400,
                  "x": 640,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f5f53147db855c6cbd5ee094fc2bad175f052c1"
                },
                {
                  "y": 600,
                  "x": 960,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=77b612691433c9c230a0f75a8400feac6f9d1a85"
                },
                {
                  "y": 675,
                  "x": 1080,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8cb5c976360279c8d2f12db71b0e1bf9208b128c"
                }
              ],
              "s": {
                "y": 1600,
                "x": 2560,
                "u": "https://preview.redd.it/esicru1dc1ff1.png?width=2560&amp;format=png&amp;auto=webp&amp;s=060dd197ae2c49892375e8d17b60320d5f6658e6"
              },
              "id": "esicru1dc1ff1"
            }
          },
          "name": "t3_1m91u38",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "#b0ae9b",
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "fe89e94a-13f2-11f0-a9de-6262c74956cf",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ttt_ex-17pTgfqoaCCzePES-O2aLbpYiA_IdGhQQxfg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 4"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753456445,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kwhw20/cognito_your_ai_sidekick_for_chrome_a_mit/\"&gt;Cognito: Your AI Sidekick for Chrome. A MIT licensed very lightweight Web UI with multitools.&lt;/a&gt;&lt;br/&gt;\nby&lt;a href=\"https://www.reddit.com/user/Asleep-Ratio7535/\"&gt;u/Asleep-Ratio7535&lt;/a&gt; in&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/\"&gt;LocalLLaMA&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This extension comes to a closure with so many published MCP servers. Chrome webstore is a little bit slower.&lt;/p&gt;\n\n&lt;p&gt;New update:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A good enough hybrid RAG for latin languages (BM25 tokenizer, I added a simple Japanese tokenizer as well), Only Chinese doesn&amp;#39;t support BM25 full text search, but you can still use a good embedding model.&lt;/li&gt;\n&lt;li&gt;A note system for saving webpages and notes for RAG or use as direct context&lt;/li&gt;\n&lt;li&gt;Several basic useful tools: web search, prompt optimizer, wiki, retriever, save note, update your preference, and some &amp;quot;agents&amp;quot; that can plan and execute those tools itself&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In the picture is an example of how a 4B model planned and used the tools it has. In this example, I tested too many concurrent web searches, so I didn&amp;#39;t notice I needed to click the captcha on the page. So it failed in the first 2 steps, but you can get rid of it easily by clicking the captcha, or use a custom API, or DuckDuckGo, brave.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/esicru1dc1ff1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=060dd197ae2c49892375e8d17b60320d5f6658e6\"&gt;https://preview.redd.it/esicru1dc1ff1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=060dd197ae2c49892375e8d17b60320d5f6658e6&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 4",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m91u38",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Asleep-Ratio7535",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m91u38/updated_ai_assistant_chrome_extension_has_tools/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m91u38/updated_ai_assistant_chrome_extension_has_tools/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753456445,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_c6nzmzuq8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is AI dialogue the future of gaming?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9535b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.52,
          "author_flair_background_color": "transparent",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/kwadvy7vz1ff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/kwadvy7vz1ff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/kwadvy7vz1ff1/DASHPlaylist.mpd?a=1756084194%2CODMzN2Y1OTQ3ODE1N2FlZmYxYTE4ZGE1MWVkNWVjMTgwYjdjNDVlZDIyZmE2N2IxMDVkM2NmM2NlY2FjZTU3ZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 20,
              "hls_url": "https://v.redd.it/kwadvy7vz1ff1/HLSPlaylist.m3u8?a=1756084194%2CYzQyNGYxMDJiNjVlYTM3YWFjOTJhNTE0MDM4OTRmMTViY2Q5ZDU2NTAzNTc5OTIzM2RhYmQ2NzY1ZGYwZjk0NQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=a4d5b6d5fcc8a3352d49a80ea4992b61a9a553fb",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753463816,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/kwadvy7vz1ff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?format=pjpg&amp;auto=webp&amp;s=42d35bc0562f2b999644dafae05a2302ab759711",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=425c4520d088cf30251cb1992b126dedf6800fba",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=909cd2fe935ddf9c23f49cfceeaac45d4e5100a5",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bad49e88c983fca601ba8acf6ae347300b2b62a3",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ae04db361f1166b44acf7f7d032db0d7990a35f8",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a2fe4b5aa0d20266a8b0d1a5f4864de6198c51ba",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a0f56d72f3bcea06d904b3ff0df0417caa967dd3",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9535b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LandoRingel",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1m9535b/is_ai_dialogue_the_future_of_gaming/",
          "stickied": false,
          "url": "https://v.redd.it/kwadvy7vz1ff1",
          "subreddit_subscribers": 504486,
          "created_utc": 1753463816,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/kwadvy7vz1ff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/kwadvy7vz1ff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/kwadvy7vz1ff1/DASHPlaylist.mpd?a=1756084194%2CODMzN2Y1OTQ3ODE1N2FlZmYxYTE4ZGE1MWVkNWVjMTgwYjdjNDVlZDIyZmE2N2IxMDVkM2NmM2NlY2FjZTU3ZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 20,
              "hls_url": "https://v.redd.it/kwadvy7vz1ff1/HLSPlaylist.m3u8?a=1756084194%2CYzQyNGYxMDJiNjVlYTM3YWFjOTJhNTE0MDM4OTRmMTViY2Q5ZDU2NTAzNTc5OTIzM2RhYmQ2NzY1ZGYwZjk0NQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m looking to get started at self hosting an LLM but have no experience with this. \n\nWhat I am looking for is:\n\nAn LLM that I can explore with code, ideally if I can link it in with some folders on my MacBook Pro M4, and then also on a server, the servers will be getting GPUs mounted soon. \n\nI ideally want to be able to send it a defined file of what code styles and principles to follow, and I would love to know what self hosted options we can look at helping with PR reviews. \n\nI don’t want AI to replace or cut the corners of my team but to help us out and become more consistent. \n\nSo ideally, self hosted options (Docker etc), if it could be integrated into PRs with a self hosted GitLab if needed?\n\nI’ve read a bit about Qwen3 but not sure where to even get started to explore and try it out. ",
          "author_fullname": "t2_fpgzf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to get started",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9antc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753476933,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m looking to get started at self hosting an LLM but have no experience with this. &lt;/p&gt;\n\n&lt;p&gt;What I am looking for is:&lt;/p&gt;\n\n&lt;p&gt;An LLM that I can explore with code, ideally if I can link it in with some folders on my MacBook Pro M4, and then also on a server, the servers will be getting GPUs mounted soon. &lt;/p&gt;\n\n&lt;p&gt;I ideally want to be able to send it a defined file of what code styles and principles to follow, and I would love to know what self hosted options we can look at helping with PR reviews. &lt;/p&gt;\n\n&lt;p&gt;I don’t want AI to replace or cut the corners of my team but to help us out and become more consistent. &lt;/p&gt;\n\n&lt;p&gt;So ideally, self hosted options (Docker etc), if it could be integrated into PRs with a self hosted GitLab if needed?&lt;/p&gt;\n\n&lt;p&gt;I’ve read a bit about Qwen3 but not sure where to even get started to explore and try it out. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9antc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "theonethatownz",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9antc/how_to_get_started/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9antc/how_to_get_started/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753476933,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Dear, AMD!\n\nYou have a potential segment of AI PRO R9700 consumers who cannot afford to buy an entire workstation based on several R9700s,\n\nbut these people (including me) have enough money to independently build a PC based on 2xR9700 and a consumer motherboard with cheaper Udimm memory.\n\nI will be very exhausted if I wait even longer, until the end of Q3. According to this logic, it makes sense to wait for Black Friday.\n\nAnd then Intel may catch up with you with b60 and b60 dual.\n\nAlso, at the end of November, a significant discount on the economy version of the 32Gb GPU from your other competitors is possible. So every week of waiting is bad.\n\nOn the other hand, I understand that AMD probably aims to declare the R9700 as a GPU for LLM, while temporarily distancing itself from gamer.\n\nAnd this is correct marketing. Therefore, in today's conditions of tight competition, let me suggest a very unusual step for such a large company:\n\nimmediately make available for sale \\[kits\\] of mandatory purchase together -\n\n\\[2pcs. R9700 + motherboard (non-ECC UDIMM RAM) with (2, or better - 3)xPCI Express 5.0 + maybe a cable\\] or a set only with \\[2pcs. R9700\\]",
          "author_fullname": "t2_1ua8m0mp6s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD Radeon AI PRO R9700 - when can I buy it?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8yvxd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753449345,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear, AMD!&lt;/p&gt;\n\n&lt;p&gt;You have a potential segment of AI PRO R9700 consumers who cannot afford to buy an entire workstation based on several R9700s,&lt;/p&gt;\n\n&lt;p&gt;but these people (including me) have enough money to independently build a PC based on 2xR9700 and a consumer motherboard with cheaper Udimm memory.&lt;/p&gt;\n\n&lt;p&gt;I will be very exhausted if I wait even longer, until the end of Q3. According to this logic, it makes sense to wait for Black Friday.&lt;/p&gt;\n\n&lt;p&gt;And then Intel may catch up with you with b60 and b60 dual.&lt;/p&gt;\n\n&lt;p&gt;Also, at the end of November, a significant discount on the economy version of the 32Gb GPU from your other competitors is possible. So every week of waiting is bad.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, I understand that AMD probably aims to declare the R9700 as a GPU for LLM, while temporarily distancing itself from gamer.&lt;/p&gt;\n\n&lt;p&gt;And this is correct marketing. Therefore, in today&amp;#39;s conditions of tight competition, let me suggest a very unusual step for such a large company:&lt;/p&gt;\n\n&lt;p&gt;immediately make available for sale [kits] of mandatory purchase together -&lt;/p&gt;\n\n&lt;p&gt;[2pcs. R9700 + motherboard (non-ECC UDIMM RAM) with (2, or better - 3)xPCI Express 5.0 + maybe a cable] or a set only with [2pcs. R9700]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8yvxd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mundane_Progress_898",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8yvxd/amd_radeon_ai_pro_r9700_when_can_i_buy_it/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8yvxd/amd_radeon_ai_pro_r9700_when_can_i_buy_it/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753449345,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm thinking of building a desktop app that helps you:\n\n\n\n\\- Detect your hardware (GPU, RAM, CPU)\n\n\\- Benchmark local AI models (GGUF/ONNX) automatically\n\n\\- Tell you which quant config runs best (Q4, Q5, etc.)\n\n\\- Show ratings like \"This model is great for coding, 12 tok/s on 8GB RAM\"\n\n\\- Launch models directly in one click\n\n\n\nLike HuggingFace meets Steam meets LM Studio — but optimized for \\*you\\*.\n\n\n\nWould you use this? What would you want it to do?\n\n",
          "author_fullname": "t2_1uanajb3sh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Would you use this? Desktop app for auto-benchmarking GGUF/ONNX models locally",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m93u0b",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753460989,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking of building a desktop app that helps you:&lt;/p&gt;\n\n&lt;p&gt;- Detect your hardware (GPU, RAM, CPU)&lt;/p&gt;\n\n&lt;p&gt;- Benchmark local AI models (GGUF/ONNX) automatically&lt;/p&gt;\n\n&lt;p&gt;- Tell you which quant config runs best (Q4, Q5, etc.)&lt;/p&gt;\n\n&lt;p&gt;- Show ratings like &amp;quot;This model is great for coding, 12 tok/s on 8GB RAM&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;- Launch models directly in one click&lt;/p&gt;\n\n&lt;p&gt;Like HuggingFace meets Steam meets LM Studio — but optimized for *you*.&lt;/p&gt;\n\n&lt;p&gt;Would you use this? What would you want it to do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m93u0b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Conscious-Drive-1448",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m93u0b/would_you_use_this_desktop_app_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m93u0b/would_you_use_this_desktop_app_for/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753460989,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Phi-4-mini-flash-reasoning isn't in the Ollama repository, and in huggingface there are .safetensors files, as the architecture of this new model is called SambaY (some Mamba variant) this may complicate things with regard to converting it to GGUF or some other format, I would like to run the model with no modification to begin with.",
          "author_fullname": "t2_61c14i0z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there any way to run Phi-4-mini-flash-reasoning on Ollama?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m99ac7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753473646,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Phi-4-mini-flash-reasoning isn&amp;#39;t in the Ollama repository, and in huggingface there are .safetensors files, as the architecture of this new model is called SambaY (some Mamba variant) this may complicate things with regard to converting it to GGUF or some other format, I would like to run the model with no modification to begin with.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m99ac7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WowSkaro",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m99ac7/is_there_any_way_to_run_phi4miniflashreasoning_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m99ac7/is_there_any_way_to_run_phi4miniflashreasoning_on/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753473646,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "  \nIt's a translation model.\n\nKey Features:\n\n* **Multilingual Support for 92 Languages**: Qwen-MT enables high-quality translation across 92 major official languages and prominent dialects, covering over 95% of the global population to meet diverse cross-lingual communication needs.\n* **High Customizability**: The new version provides advanced translation capabilities such as terminology intervention, domain prompts and translation memory. By enabling customizable prompt engineering, it delivers optimized translation performance tailored to complex, domain-specific, and mission-critical application scenarios.\n* **Low Latency &amp; Cost Efficiency**: By leveraging a lightweight Mixture of Experts (MoE) architecture, Qwen-MT achieves high translation performance with faster response times and significantly reduced API costs (as low as $0.5 per million output tokens). This is particularly well-suited for high-concurrency environments and latency-sensitive applications.\n\n[benchmark](https://preview.redd.it/ebw46w8hkuef1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=0652bf1ba1530779185f78006929ce89c53a2aaf)\n\n[https://qwenlm.github.io/blog/qwen-mt/](https://qwenlm.github.io/blog/qwen-mt/)",
          "author_fullname": "t2_xdw24u3am",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen's third bomb: Qwen3-MT",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ebw46w8hkuef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 63,
                  "x": 108,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ac95f2f94be794e56e5173ecf84f81d0e87b4e7"
                },
                {
                  "y": 127,
                  "x": 216,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=879438cc2d25ccf7bbaefa6a935cf40c6d2679f5"
                },
                {
                  "y": 189,
                  "x": 320,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dcf05d1d1e97cf3ae2922a54cfcce722e499a2b7"
                },
                {
                  "y": 378,
                  "x": 640,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d9010db33f3f1a921174e424f03bd7769dd761bf"
                },
                {
                  "y": 567,
                  "x": 960,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=72eef5769345bbf6893557d58d6fbfafd6b58fe9"
                },
                {
                  "y": 638,
                  "x": 1080,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f949bada961df505b0b2d5483c2ddae0bb74ece"
                }
              ],
              "s": {
                "y": 1100,
                "x": 1860,
                "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=0652bf1ba1530779185f78006929ce89c53a2aaf"
              },
              "id": "ebw46w8hkuef1"
            }
          },
          "name": "t3_1m88s09",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 163,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 163,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/LksViWDcxO1eQ0ZQpLUVRXks4wbjVGa9UjqigE3hofA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753373875,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a translation model.&lt;/p&gt;\n\n&lt;p&gt;Key Features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Multilingual Support for 92 Languages&lt;/strong&gt;: Qwen-MT enables high-quality translation across 92 major official languages and prominent dialects, covering over 95% of the global population to meet diverse cross-lingual communication needs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High Customizability&lt;/strong&gt;: The new version provides advanced translation capabilities such as terminology intervention, domain prompts and translation memory. By enabling customizable prompt engineering, it delivers optimized translation performance tailored to complex, domain-specific, and mission-critical application scenarios.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Low Latency &amp;amp; Cost Efficiency&lt;/strong&gt;: By leveraging a lightweight Mixture of Experts (MoE) architecture, Qwen-MT achieves high translation performance with faster response times and significantly reduced API costs (as low as $0.5 per million output tokens). This is particularly well-suited for high-concurrency environments and latency-sensitive applications.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ebw46w8hkuef1.png?width=1860&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0652bf1ba1530779185f78006929ce89c53a2aaf\"&gt;benchmark&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://qwenlm.github.io/blog/qwen-mt/\"&gt;https://qwenlm.github.io/blog/qwen-mt/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m88s09",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BreakfastFriendly728",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m88s09/qwens_third_bomb_qwen3mt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m88s09/qwens_third_bomb_qwen3mt/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753373875,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7g0m6735",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "new mistralai/Magistral-Small-2507 !?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m85vhw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 215,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 215,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=feaece90f7cf4e9ef4f2b2363a06c331aa76c3be",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753367249,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/mistralai/Magistral-Small-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?auto=webp&amp;s=6d8a1d0a5f0cc1e9f8968e79b1869ad8ed3d1a1d",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5f5b9e280105076efaeb7fb4658ea8f168c6e031",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0bb8df5e83f6c1204cc11aed536f322d4ded452a",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=590a3d841dcd1b2437d95fe4576578e3ffad6bab",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1aa4619d7f8ff888c9274c7c014531dcd45ff12e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=74cacf05986588334ed0a0a18df51559c11386b8",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1b24a8930f9c9f8618bbba4dd9619c2b04cb8469",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m85vhw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ApprehensiveAd3629",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m85vhw/new_mistralaimagistralsmall2507/",
          "stickied": false,
          "url": "https://huggingface.co/mistralai/Magistral-Small-2507",
          "subreddit_subscribers": 504486,
          "created_utc": 1753367249,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Can I make a ram based server hardware llm machine, something like a Xeon or epic with 12 channel ram. \n\nBut since I am worried about cpu prompt processing speed, can I add a gpu like a 4070, good gpu chip, kinda shit amount of vram, can I add something like that to handle the prompt processing, while leveraging the ram and bandwidth that I would get with server hardware?\n\nFrom what I know, the reason why vram is preferable to ram is memory bandwidth.\n\nWith server hardware, I can get 6 or 12 channel ddr4, which give me like 200gb/s bandwidth just for the system ram. This is fine enough for me, but I’m afrid the cpu prompt processing speed will be bad, so yeah\n\nDoes this work? If it doesn’t, why not?",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gpu just for prompt processing?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m92di7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753457684,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can I make a ram based server hardware llm machine, something like a Xeon or epic with 12 channel ram. &lt;/p&gt;\n\n&lt;p&gt;But since I am worried about cpu prompt processing speed, can I add a gpu like a 4070, good gpu chip, kinda shit amount of vram, can I add something like that to handle the prompt processing, while leveraging the ram and bandwidth that I would get with server hardware?&lt;/p&gt;\n\n&lt;p&gt;From what I know, the reason why vram is preferable to ram is memory bandwidth.&lt;/p&gt;\n\n&lt;p&gt;With server hardware, I can get 6 or 12 channel ddr4, which give me like 200gb/s bandwidth just for the system ram. This is fine enough for me, but I’m afrid the cpu prompt processing speed will be bad, so yeah&lt;/p&gt;\n\n&lt;p&gt;Does this work? If it doesn’t, why not?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m92di7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m92di7/gpu_just_for_prompt_processing/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m92di7/gpu_just_for_prompt_processing/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753457684,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nAm5 9000x3d 128gb ram (2*64) and a 3090\n\nI promised i watch it but I couldn't get what exact quant nor speed.  \nHe said this was \"compressed to 20% of the og model\" so something like a q2.  \nRegarding speed it seems very very descent\n\n",
          "author_fullname": "t2_cj9kap4bx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Level1tech runs deepseek on am5 and it's not that bad!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ewlx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": "#bbbdbf",
          "ups": 66,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T17bpGItqXw?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AI and You Against the Machine: Guide so you can own Big AI and Run Local\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "AI and You Against the Machine: Guide so you can own Big AI and Run Local",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T17bpGItqXw?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AI and You Against the Machine: Guide so you can own Big AI and Run Local\"&gt;&lt;/iframe&gt;",
              "author_name": "Level1Techs",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/T17bpGItqXw/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Level1Techs"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T17bpGItqXw?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AI and You Against the Machine: Guide so you can own Big AI and Run Local\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m8ewlx",
            "height": 200
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 66,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/-htOFPXayCuXw6wsi6x_HzoPwXo6FB_FePd0EceoPtI.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=7ef55c301b8cc1f6c15465ee00f838044f36a7e2",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753387836,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Am5 9000x3d 128gb ram (2*64) and a 3090&lt;/p&gt;\n\n&lt;p&gt;I promised i watch it but I couldn&amp;#39;t get what exact quant nor speed.&lt;br/&gt;\nHe said this was &amp;quot;compressed to 20% of the og model&amp;quot; so something like a q2.&lt;br/&gt;\nRegarding speed it seems very very descent&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/T17bpGItqXw?feature=shared",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/-htOFPXayCuXw6wsi6x_HzoPwXo6FB_FePd0EceoPtI.jpeg?auto=webp&amp;s=d30b35a28848fb554ec800487c21bd3667f7b292",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/-htOFPXayCuXw6wsi6x_HzoPwXo6FB_FePd0EceoPtI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=871b592cff3d042c4a9d07616559fd738802ccec",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/-htOFPXayCuXw6wsi6x_HzoPwXo6FB_FePd0EceoPtI.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3670287e176290bf890eee6ca80fea8b64a76b7",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/-htOFPXayCuXw6wsi6x_HzoPwXo6FB_FePd0EceoPtI.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b8b07ebd60262e21a7df05d698d427003581ec8",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "-htOFPXayCuXw6wsi6x_HzoPwXo6FB_FePd0EceoPtI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m8ewlx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Afternoon_4260",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m8ewlx/level1tech_runs_deepseek_on_am5_and_its_not_that/",
          "stickied": false,
          "url": "https://youtu.be/T17bpGItqXw?feature=shared",
          "subreddit_subscribers": 504486,
          "created_utc": 1753387836,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "AI and You Against the Machine: Guide so you can own Big AI and Run Local",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T17bpGItqXw?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AI and You Against the Machine: Guide so you can own Big AI and Run Local\"&gt;&lt;/iframe&gt;",
              "author_name": "Level1Techs",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/T17bpGItqXw/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Level1Techs"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "NeuralAgent lives on your desktop and takes action like a human, it clicks, types, scrolls, and navigates your apps to complete real tasks. Your computer, now working for you. It's now open source.  \n  \nCheck it out on GitHub: [https://github.com/withneural/neuralagent](https://github.com/withneural/neuralagent)\n\nOur website: [https://www.getneuralagent.com](https://www.getneuralagent.com)  \n  \nGive us a star if you like the project!",
          "author_fullname": "t2_11vfewtir0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We just open sourced NeuralAgent: The AI Agent That Lives On Your Desktop and Uses It Like You Do!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8bps2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 95,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 95,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753380472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;NeuralAgent lives on your desktop and takes action like a human, it clicks, types, scrolls, and navigates your apps to complete real tasks. Your computer, now working for you. It&amp;#39;s now open source.  &lt;/p&gt;\n\n&lt;p&gt;Check it out on GitHub: &lt;a href=\"https://github.com/withneural/neuralagent\"&gt;https://github.com/withneural/neuralagent&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Our website: &lt;a href=\"https://www.getneuralagent.com\"&gt;https://www.getneuralagent.com&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Give us a star if you like the project!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/eZKgL90VJLC07PbJlHh8vS4DtlDzLQPNmpPGomAf-0g.png?auto=webp&amp;s=8b994923552bccf9e409beca0d7014a2a20b2506",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/eZKgL90VJLC07PbJlHh8vS4DtlDzLQPNmpPGomAf-0g.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ca1997e6cc45bb0c0dc45882a5df2cb409d4b82",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/eZKgL90VJLC07PbJlHh8vS4DtlDzLQPNmpPGomAf-0g.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=179a1fa0d5787001ed043b3df7514267a8cf7741",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/eZKgL90VJLC07PbJlHh8vS4DtlDzLQPNmpPGomAf-0g.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a31b044f61631d6102d51b819d752f8677f352fe",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/eZKgL90VJLC07PbJlHh8vS4DtlDzLQPNmpPGomAf-0g.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7293fce83bae0780debd9a9670ac61147b2ff7c",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/eZKgL90VJLC07PbJlHh8vS4DtlDzLQPNmpPGomAf-0g.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4d165b0c4fd693e2da5dd6f547b42cd6f2d45d26",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/eZKgL90VJLC07PbJlHh8vS4DtlDzLQPNmpPGomAf-0g.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=12e5abb9decf12f331484d39aa06418654ddb4d2",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "eZKgL90VJLC07PbJlHh8vS4DtlDzLQPNmpPGomAf-0g"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m8bps2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nearby_Tart_9970",
          "discussion_type": null,
          "num_comments": 54,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8bps2/we_just_open_sourced_neuralagent_the_ai_agent/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8bps2/we_just_open_sourced_neuralagent_the_ai_agent/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753380472,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Boson AI has recently open-sourced the Higgs Audio V2 model.  \n[https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base](https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base)  \n  \nThe model demonstrates strong performance in automatic prosody adjustment and generating natural multi-speaker dialogues across languages . \n\nNotably, it achieved a 75.7% win rate over GPT-4o-mini-tts in emotional expression on the EmergentTTS-Eval benchmark . The total parameter count for this model is approximately 5.8 billion (3.6B for the LLM and 2.2B for the Audio Dual FFN)",
          "author_fullname": "t2_i7c050twt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Higgs Audio V2: A New Open-Source TTS Model with Voice Cloning and SOTA Expressiveness",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8aeh3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 109,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/rcsam20avuef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/rcsam20avuef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/rcsam20avuef1/DASHPlaylist.mpd?a=1756084194%2CYjMxMzBkM2FjOTc2MTU2NmExODQ0NGMwZjE5YTVjNjkzM2Q5NDhiYTM1MGJiM2JjNTM5NTI2Y2MzNjBmNGZlOQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 130,
              "hls_url": "https://v.redd.it/rcsam20avuef1/HLSPlaylist.m3u8?a=1756084194%2CYTE5OGE0Y2U2NjdiMGZiYWMzNjYwM2Y2Zjg0NWI1YjY4ODIzNTJiNzgzN2Y5YjgwN2M5M2Y1NzUwYWFmYzBhZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 109,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=2971ada4bf05d18a7be7240c7fb1e72ea3f43d6d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753377531,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Boson AI has recently open-sourced the Higgs Audio V2 model.&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base\"&gt;https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;The model demonstrates strong performance in automatic prosody adjustment and generating natural multi-speaker dialogues across languages . &lt;/p&gt;\n\n&lt;p&gt;Notably, it achieved a 75.7% win rate over GPT-4o-mini-tts in emotional expression on the EmergentTTS-Eval benchmark . The total parameter count for this model is approximately 5.8 billion (3.6B for the LLM and 2.2B for the Audio Dual FFN)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/rcsam20avuef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?format=pjpg&amp;auto=webp&amp;s=99960c10b19dce63b6646f656877f832910c6f5c",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e7d429499980ab3e9e447fab7b848dcad3e259bc",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b7cb31b2460a35289a0eeee70eb956477826c160",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=65d14a7d8fcf6ee96af5785c1ed47a4f2e7f1365",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c5045364c840a4e1f879393708cd7324670a3929",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a3491e419dd1eae0f49e765804d9eb997363cefb",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=342c646526dba4fdd9256e73e3e5cc81956e6b35",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8aeh3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pheonis2",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8aeh3/higgs_audio_v2_a_new_opensource_tts_model_with/",
          "stickied": false,
          "url": "https://v.redd.it/rcsam20avuef1",
          "subreddit_subscribers": 504486,
          "created_utc": 1753377531,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/rcsam20avuef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/rcsam20avuef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/rcsam20avuef1/DASHPlaylist.mpd?a=1756084194%2CYjMxMzBkM2FjOTc2MTU2NmExODQ0NGMwZjE5YTVjNjkzM2Q5NDhiYTM1MGJiM2JjNTM5NTI2Y2MzNjBmNGZlOQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 130,
              "hls_url": "https://v.redd.it/rcsam20avuef1/HLSPlaylist.m3u8?a=1756084194%2CYTE5OGE0Y2U2NjdiMGZiYWMzNjYwM2Y2Zjg0NWI1YjY4ODIzNTJiNzgzN2Y5YjgwN2M5M2Y1NzUwYWFmYzBhZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all. As the title says, I'm new to hosting AI locally. I am using an Nvidia RTX 4080 16GB. I got Ollama installed and llama2 running, but it is pretty lackluster. Seeing that I can run llama3 which is supposed to be much better. Any tips from experienced users? I am just doing this as something to tinker with. TIA. ",
          "author_fullname": "t2_429jxbgl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New to local AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m91dmh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753455410,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. As the title says, I&amp;#39;m new to hosting AI locally. I am using an Nvidia RTX 4080 16GB. I got Ollama installed and llama2 running, but it is pretty lackluster. Seeing that I can run llama3 which is supposed to be much better. Any tips from experienced users? I am just doing this as something to tinker with. TIA. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m91dmh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "m_spoon09",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m91dmh/new_to_local_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m91dmh/new_to_local_ai/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753455410,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "From what I understand, an MOE model contains many experts, and when you give it a prompt, it chooses one expert to answer your query.\n\nIf I already know that I want to do something like creative writing, why can’t I just have just the creative writing expert so I only need to load that?\n\nWouldn’t this help with the required ram/vram amount?",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can you just have one expert from an MOE model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8qmd7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753420347,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From what I understand, an MOE model contains many experts, and when you give it a prompt, it chooses one expert to answer your query.&lt;/p&gt;\n\n&lt;p&gt;If I already know that I want to do something like creative writing, why can’t I just have just the creative writing expert so I only need to load that?&lt;/p&gt;\n\n&lt;p&gt;Wouldn’t this help with the required ram/vram amount?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8qmd7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753420347,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I wanted to fine-tune the model so that it performs well with marathi texts in images using unsloth. But I am encountering significant performance degradation with fine-tuning it . The fine-tuned model frequently fails to understand basic prompts and performs worse than the base model for OCR. My dataset is consists of 700 whole pages from hand written notebooks , books etc.  \nHowever, after fine-tuning, the model performs **significantly worse than the base model** — it struggles with basic OCR prompts and fails to recognize text it previously handled well.\n\nHere’s how I configured the fine-tuning layers:  \nfinetune\\_vision\\_layers = True\n\nfinetune\\_language\\_layers = True\n\nfinetune\\_attention\\_modules = True\n\nfinetune\\_mlp\\_modules = False\n\nPlease suggest what can I do to improve it.",
          "author_fullname": "t2_pqyq3e9x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fine-tuning qwen2.5 vl for Marathi OCR",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8qtpd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753421050,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanted to fine-tune the model so that it performs well with marathi texts in images using unsloth. But I am encountering significant performance degradation with fine-tuning it . The fine-tuned model frequently fails to understand basic prompts and performs worse than the base model for OCR. My dataset is consists of 700 whole pages from hand written notebooks , books etc.&lt;br/&gt;\nHowever, after fine-tuning, the model performs &lt;strong&gt;significantly worse than the base model&lt;/strong&gt; — it struggles with basic OCR prompts and fails to recognize text it previously handled well.&lt;/p&gt;\n\n&lt;p&gt;Here’s how I configured the fine-tuning layers:&lt;br/&gt;\nfinetune_vision_layers = True&lt;/p&gt;\n\n&lt;p&gt;finetune_language_layers = True&lt;/p&gt;\n\n&lt;p&gt;finetune_attention_modules = True&lt;/p&gt;\n\n&lt;p&gt;finetune_mlp_modules = False&lt;/p&gt;\n\n&lt;p&gt;Please suggest what can I do to improve it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8qtpd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rahul_Albus",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8qtpd/finetuning_qwen25_vl_for_marathi_ocr/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8qtpd/finetuning_qwen25_vl_for_marathi_ocr/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753421050,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "After starting Ollama and doing the ollama run &lt;model&gt; how do you know if it’s running that specific model or if it’s still using the default that comes with ollama? Do you just need the run code for it to work, the load command, or both?",
          "author_fullname": "t2_l4qac",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is /load &lt;model&gt; all you need in order to run the specific model you installed?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9bmqm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753479301,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After starting Ollama and doing the ollama run &amp;lt;model&amp;gt; how do you know if it’s running that specific model or if it’s still using the default that comes with ollama? Do you just need the run code for it to work, the load command, or both?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9bmqm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "XiRw",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9bmqm/is_load_model_all_you_need_in_order_to_run_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9bmqm/is_load_model_all_you_need_in_order_to_run_the/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753479301,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1kpbtnvm6g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What a Real MCP Inspector Exploit Taught Us About Trust Boundaries",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m95sdj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=3f39b34ed5287898a35b71f4a2d410155e669d3f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753465416,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "glama.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://glama.ai/blog/2025-07-25-keeping-mcp-inspector-safe-lessons-from-cve-2025-49596",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?auto=webp&amp;s=227a72ce82d28d282a0c1871a35312e4308e1adc",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=867c2e3c9c184ac0e64e1d2b897ce35fafe59ad3",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b3bc8fb60b3d09c1938760de5b1404a3ec3810e2",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d497e25d2db852ceaaef3176f190e7864b19bb13",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1d8aa63775b0ce7432aec4599fdff26b1f5079fa",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d9525d5f79ca90480770bfaf099caa9c518625cc",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2af1885a6b45fe1e3b5d69406a04e31bec46d01a",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m95sdj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Abies7108",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m95sdj/what_a_real_mcp_inspector_exploit_taught_us_about/",
          "stickied": false,
          "url": "https://glama.ai/blog/2025-07-25-keeping-mcp-inspector-safe-lessons-from-cve-2025-49596",
          "subreddit_subscribers": 504486,
          "created_utc": 1753465416,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "vLLM commit: [https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29](https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29)\n\n  \nmodelscope/ms-swift commit: [https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7](https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7)\n\nhttps://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;format=png&amp;auto=webp&amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e\n\nWe're going to get a 106B-A12B (Air) model and a 355B-A32B model.  \n",
          "author_fullname": "t2_155sd0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5 Is About to Be Released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "hda2uymxqsef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 48,
                  "x": 108,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=64860b786b5e21e35a840a649825e0180c8cc478"
                },
                {
                  "y": 97,
                  "x": 216,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f06e72fb0c477bec3198b5134c4222990359eb0a"
                },
                {
                  "y": 144,
                  "x": 320,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ed13273132fbf1f9298d35bd285a9a9ab9210519"
                },
                {
                  "y": 288,
                  "x": 640,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9a159ed9a36f9275c9b5a11b3604c7793dde26c0"
                },
                {
                  "y": 432,
                  "x": 960,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5005302c8f7a306e32869b7748202c5cfd5b87fe"
                },
                {
                  "y": 486,
                  "x": 1080,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=058a1510994cdcc7fd2e534901f63f86884f1db2"
                }
              ],
              "s": {
                "y": 586,
                "x": 1300,
                "u": "https://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;format=png&amp;auto=webp&amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e"
              },
              "id": "hda2uymxqsef1"
            }
          },
          "name": "t3_1m80gsn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 329,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 329,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=460c7b2c3bf4d9c06c8551ed35f1d347b924c43a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753351817,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;vLLM commit: &lt;a href=\"https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29\"&gt;https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;modelscope/ms-swift commit: &lt;a href=\"https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7\"&gt;https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e\"&gt;https://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re going to get a 106B-A12B (Air) model and a 355B-A32B model.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?auto=webp&amp;s=c6b87857dd89e2502756d6b53a092e0a220bcbb5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c7a1310eabcbdf43b0d3abda179514f1ac02393",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9bb47d4723d0c3b790dc8d57f5455755b278fc6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d61a12909a1af11f6d9f3cddbf320cfaad72c44",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b9908a35901687f5249e56f8b7bb3e593bf9a82e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b01ed43a5e38bf06f05457b04f802ed86327cd88",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9d88d6adc0d7dbbdf7b26de2a970c2ec9b69a0ce",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m80gsn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NeterOster",
          "discussion_type": null,
          "num_comments": 77,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753351817,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to fine-tune a LlaVa model to include new details about an image. Think about medical, I want the model to mention a new condition a group of doctors described after looking at the image. \n\n\n\nI have pairs of images and new details, given in a description. \n\n\n\nI want to fine-tune the model. In my first batch of experiments, I had about 7.8K conversations in the training set, and I always used the same questions. I used QLoRa using different configurations, and when I tested it, it returned gibberish when using greedy decoding, or something that might include some words of the new answers, when trying different \\`temperature\\`/\\`top\\_p\\`. I suspect it just overfitted to my data, resulting in catastrophic forgetting. \n\n\n\nI got back to the drawing table, gathered more data, now I have about 21K observations (currently images and descriptions), and I want to construct a robust training dataset.\n\n\\- [This](https://www.reddit.com/r/LocalLLaMA/s/YTmnITYTPN) post discusses the number of observations required to fine-tune a model, with some members mentioning that they had a successful fine-tuning with only 100 conversations of high quality. \n\n  \nMy question I guess, is how to build the questions (to be attached to the image/description pairs) to make sure my data is of the highest quality possible? \n\n  \n",
          "author_fullname": "t2_1t6vmqt87p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Data Quality and Size for LoRa",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8zeg8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753450676,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to fine-tune a LlaVa model to include new details about an image. Think about medical, I want the model to mention a new condition a group of doctors described after looking at the image. &lt;/p&gt;\n\n&lt;p&gt;I have pairs of images and new details, given in a description. &lt;/p&gt;\n\n&lt;p&gt;I want to fine-tune the model. In my first batch of experiments, I had about 7.8K conversations in the training set, and I always used the same questions. I used QLoRa using different configurations, and when I tested it, it returned gibberish when using greedy decoding, or something that might include some words of the new answers, when trying different `temperature`/`top_p`. I suspect it just overfitted to my data, resulting in catastrophic forgetting. &lt;/p&gt;\n\n&lt;p&gt;I got back to the drawing table, gathered more data, now I have about 21K observations (currently images and descriptions), and I want to construct a robust training dataset.&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/s/YTmnITYTPN\"&gt;This&lt;/a&gt; post discusses the number of observations required to fine-tune a model, with some members mentioning that they had a successful fine-tuning with only 100 conversations of high quality. &lt;/p&gt;\n\n&lt;p&gt;My question I guess, is how to build the questions (to be attached to the image/description pairs) to make sure my data is of the highest quality possible? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8zeg8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Emotional-Sundae4075",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8zeg8/data_quality_and_size_for_lora/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8zeg8/data_quality_and_size_for_lora/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753450676,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was contemplating buying an RTX PRO 6000 Blackwell, but after conducting some research on [YouTube](https://youtu.be/bAao58hXo9w?si=F1vCh4gSJxrgYqo2&amp;t=832), I was disappointed with its performance. The prompt processing speed didn't meet my expectations, and token generation decreased notably when context was added. It didn't seem to outperform regular consumer GPUs, which left me wondering why it's so expensive. Is this normal behavior, or was the YouTuber not using it properly?",
          "author_fullname": "t2_qhk9kpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Dissatisfied with how the RTX PRO 6000 Blackwell is performing during AI inference",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9dur7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753484979,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was contemplating buying an RTX PRO 6000 Blackwell, but after conducting some research on &lt;a href=\"https://youtu.be/bAao58hXo9w?si=F1vCh4gSJxrgYqo2&amp;amp;t=832\"&gt;YouTube&lt;/a&gt;, I was disappointed with its performance. The prompt processing speed didn&amp;#39;t meet my expectations, and token generation decreased notably when context was added. It didn&amp;#39;t seem to outperform regular consumer GPUs, which left me wondering why it&amp;#39;s so expensive. Is this normal behavior, or was the YouTuber not using it properly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Bg367mTzk4i869SXNHiikTP1RjxNeVibkNvwuMAACGo.jpeg?auto=webp&amp;s=2de38e488ed6896adcf13bb7582775e8e37a7e8f",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Bg367mTzk4i869SXNHiikTP1RjxNeVibkNvwuMAACGo.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d8af79e10d322a89c0acdc8c1e95b10d83feae63",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/Bg367mTzk4i869SXNHiikTP1RjxNeVibkNvwuMAACGo.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be02e726d7db29c9b5cf7e4489a55edc87699ca8",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/Bg367mTzk4i869SXNHiikTP1RjxNeVibkNvwuMAACGo.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b878f3cb454ef94f2549f8cf1ea5da83fe5038ba",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "Bg367mTzk4i869SXNHiikTP1RjxNeVibkNvwuMAACGo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9dur7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "d00m_sayer",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753484979,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This demo runs Voxtral-Mini-3B, a new audio language model from Mistral, enabling state-of-the-art audio transcription directly in your browser! Everything runs locally, meaning none of your data is sent to a server (and your transcripts are stored on-device).\n\nImportant links:\n- Model: https://huggingface.co/onnx-community/Voxtral-Mini-3B-2507-ONNX\n- Demo: https://huggingface.co/spaces/webml-community/Voxtral-WebGPU",
          "author_fullname": "t2_mizchr3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Voxtral WebGPU: State-of-the-art audio transcription directly in your browser!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m87q21",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 104,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/9p0p7mqnbuef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1008,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/9p0p7mqnbuef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/9p0p7mqnbuef1/DASHPlaylist.mpd?a=1756084194%2CMmY4NGM1OWQ1ZDNjZGJiODcyMWU5MWZmMDUxMDM3NjBhNDc2ZGZhM2QyNzA4OWUyNDU3Y2MwMTZjOTBkY2U2ZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 45,
              "hls_url": "https://v.redd.it/9p0p7mqnbuef1/HLSPlaylist.m3u8?a=1756084194%2CZWM4MjgwNTk4MzgyY2NjOGJiMmJiZWFmODE5MjJmMGJhNDYyZTNlNmM1NmFkZTc1MDg2OGJkMzU5MzRhOGFiNQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 104,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=f1686dbb130dc0803321bf6e7360985752a766f8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753371510,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This demo runs Voxtral-Mini-3B, a new audio language model from Mistral, enabling state-of-the-art audio transcription directly in your browser! Everything runs locally, meaning none of your data is sent to a server (and your transcripts are stored on-device).&lt;/p&gt;\n\n&lt;p&gt;Important links:\n- Model: &lt;a href=\"https://huggingface.co/onnx-community/Voxtral-Mini-3B-2507-ONNX\"&gt;https://huggingface.co/onnx-community/Voxtral-Mini-3B-2507-ONNX&lt;/a&gt;\n- Demo: &lt;a href=\"https://huggingface.co/spaces/webml-community/Voxtral-WebGPU\"&gt;https://huggingface.co/spaces/webml-community/Voxtral-WebGPU&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/9p0p7mqnbuef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?format=pjpg&amp;auto=webp&amp;s=696efe4a003759fef9584a89aa6ae640bc1003d2",
                  "width": 3160,
                  "height": 1658
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1ed93c4a634db259c3e76c429350e309050b88f3",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=44f5105d97067fdedefb4e9a76461c7c8ffc551b",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8232cc34450ab1b876049726f596c02a4d1a3fd7",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7159ca490a26e285cad3449d2030082fe5240c9b",
                    "width": 640,
                    "height": 335
                  },
                  {
                    "url": "https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=aca40ddd0ee86b1a78155181541946e23e3c81b6",
                    "width": 960,
                    "height": 503
                  },
                  {
                    "url": "https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c70476f5a9c13880e2d68d5a19c9925ffc1de8cc",
                    "width": 1080,
                    "height": 566
                  }
                ],
                "variants": {},
                "id": "NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m87q21",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xenovatech",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m87q21/voxtral_webgpu_stateoftheart_audio_transcription/",
          "stickied": false,
          "url": "https://v.redd.it/9p0p7mqnbuef1",
          "subreddit_subscribers": 504486,
          "created_utc": 1753371510,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/9p0p7mqnbuef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1008,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/9p0p7mqnbuef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/9p0p7mqnbuef1/DASHPlaylist.mpd?a=1756084194%2CMmY4NGM1OWQ1ZDNjZGJiODcyMWU5MWZmMDUxMDM3NjBhNDc2ZGZhM2QyNzA4OWUyNDU3Y2MwMTZjOTBkY2U2ZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 45,
              "hls_url": "https://v.redd.it/9p0p7mqnbuef1/HLSPlaylist.m3u8?a=1756084194%2CZWM4MjgwNTk4MzgyY2NjOGJiMmJiZWFmODE5MjJmMGJhNDYyZTNlNmM1NmFkZTc1MDg2OGJkMzU5MzRhOGFiNQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am a noob whose just beginning to fiddle around with models. Was testing out qwen 3 and trying to build an application using it + 2 tools (a web search function using tavily and a financial data retriever using yfinance). I ran into more bugs running an agno framework vs just commanding the system prompt to call the 2 tools I had made in a systemic manner. ",
          "author_fullname": "t2_1qa1b79zv0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do you need Agno/Langchain/LangGraph with models with agentic capabilities?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m93lcs",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753460437,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a noob whose just beginning to fiddle around with models. Was testing out qwen 3 and trying to build an application using it + 2 tools (a web search function using tavily and a financial data retriever using yfinance). I ran into more bugs running an agno framework vs just commanding the system prompt to call the 2 tools I had made in a systemic manner. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m93lcs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "demisincos",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m93lcs/do_you_need_agnolangchainlanggraph_with_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m93lcs/do_you_need_agnolangchainlanggraph_with_models/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753460437,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Does anyone have any Docker Compose examples for vLLM?\n\nI am in the fortunate position of having 8 (!) H200s in a single server in the near future.\n\nI want DeepSeek in the 671B variant with openwebui.\n\n\n\nIt would be great if someone had a Compose file that would allow me to use all GPUs in parallel.",
          "author_fullname": "t2_tdycw15",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Docker Compose vLLM Config",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9281b",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753457339,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have any Docker Compose examples for vLLM?&lt;/p&gt;\n\n&lt;p&gt;I am in the fortunate position of having 8 (!) H200s in a single server in the near future.&lt;/p&gt;\n\n&lt;p&gt;I want DeepSeek in the 671B variant with openwebui.&lt;/p&gt;\n\n&lt;p&gt;It would be great if someone had a Compose file that would allow me to use all GPUs in parallel.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9281b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "crossijinn",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9281b/docker_compose_vllm_config/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9281b/docker_compose_vllm_config/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753457339,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to fine tune TTS but there are plenty on the market so confused which one to use.\n\nCurrently using chatterbox for voice cloning to TTS, but for some voices the output is not accurate to the reference audio's pace and tone. If the reference audio is normal speech rate, the output audio will be a bit fast, despite lowering the pace.\n\nAnyways, will using RVC improve?\n\nFound these RVCs.. which one to use?  \n  \n[https://github.com/Mangio621/Mangio-RVC-Fork](https://github.com/Mangio621/Mangio-RVC-Fork) \n\n[https://github.com/JackismyShephard/ultimate-rvc](https://github.com/JackismyShephard/ultimate-rvc)\n\n[https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/tree/main](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/tree/main) ",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Good RVC to fine tune TTS?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ws0i",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753443360,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to fine tune TTS but there are plenty on the market so confused which one to use.&lt;/p&gt;\n\n&lt;p&gt;Currently using chatterbox for voice cloning to TTS, but for some voices the output is not accurate to the reference audio&amp;#39;s pace and tone. If the reference audio is normal speech rate, the output audio will be a bit fast, despite lowering the pace.&lt;/p&gt;\n\n&lt;p&gt;Anyways, will using RVC improve?&lt;/p&gt;\n\n&lt;p&gt;Found these RVCs.. which one to use?  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Mangio621/Mangio-RVC-Fork\"&gt;https://github.com/Mangio621/Mangio-RVC-Fork&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/JackismyShephard/ultimate-rvc\"&gt;https://github.com/JackismyShephard/ultimate-rvc&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/tree/main\"&gt;https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/tree/main&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?auto=webp&amp;s=54d81ad8121076769edd663c2c98e456a55af2de",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=808c91e6548b11d6746644706e0443a78ab2865d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3200ffd4cc51d1c6da907b3cd186002fb7ec1d33",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a97422065d40cab8e57ba1411dd2cd9f96e04376",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4996aa378dab4bfcb4080ff1c46aecefa2ea1ab6",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5f0446cfc59335aae49f3de9d97cc55e4d4fd5c0",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=91aa321e187fd4eded17d2a9d0deea66fc5ba3b3",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8ws0i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8ws0i/good_rvc_to_fine_tune_tts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8ws0i/good_rvc_to_fine_tune_tts/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753443360,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,\n\nJust wanted to share some exciting news for anyone here who's into deep, long-form roleplaying. The team behind [Astrsk](https://astrsk.ai), a desktop app for RP that's been in development for about six months, has just announced they are going **fully open source** under the GPL license!\n\nAs a fan of the project, I think this is a huge deal for the community.\n\n**The most important link first:** [https://github.com/astrskai/astrsk](https://github.com/astrskai/astrsk)\n\n[demo](https://reddit.com/link/1m868na/video/zk1ui4ctytef1/player)\n\n**So, what is Astrsk and why is it interesting?**\n\nAt its core, Astrsk is a UI for RP, but its main differentiator is the **agentic workflow**. I've been following it, and the concept is very cool because it moves beyond a simple prompt-response loop.\n\nTo make this concrete, let's look at the default workflow it comes with, called **SAGA**. It's a four-step pipeline that mimics how a human Game Master thinks, breaking down the task of generating a response into logical steps.\n\nHere's how it works:\n\n1. **Step 1: The Analyzer Agent**\n   * **The Job:** This is the GM's logical brain. It looks at what your character just did and analyzes it against the current game state.\n   * **In Practice:** It answers the questions: \"Is the player's action possible? What are the immediate consequences based on game rules or a dice roll?\" It validates the action and determines the outcome.\n2. **Step 2: The Planner Agent**\n   * **The Job:** This is the creative storyteller. It takes the Analyzer's output and designs the narrative response.\n   * **In Practice:** It decides how NPCs will react to the player's action (e.g., with anger, surprise, or a counter-move). It plans the scene, sets the emotional tone, and prepares the key information for the next agent.\n3. **Step 3: The Actor Agent**\n   * **The Job:** This is the performer. It takes the Planner's script and turns it into the actual text you read.\n   * **In Practice:** It writes the scene narration and performs the detailed dialogue for one main NPC, giving them a distinct voice and personality. Other NPCs are handled through the narration, keeping the focus clear.\n4. **Step 4: The Formatter Agent**\n   * **The Job:** This is the final editor.\n   * **In Practice:** It takes the text from the Actor and cleans it up with simple markdown. It automatically wraps actions in italics, dialogue in \"quotes\", and adds **bold** for emphasis, making the final output clean and easy to read without changing the content.\n\nThis pipeline approach allows for incredible consistency and detail. And since you can assign different models to different agents (a key feature!), you could use a large, powerful model for the creative Planner and a faster, smaller model for the structured Analyzer.\n\n**How does it compare to the greats like SillyTavern / Agnaistic?**\n\nFrom what I've seen, while projects like ST/Agnaistic are amazing for chat-based RP, Astrsk seems to aim for a different goal. It feels less like a chat interface and more like a tool for collaborative storytelling, almost like having an AI Dungeon Master powered by a framework of agents.\n\n**Key Features:**\n\n* **Agent-based generation:** The core of Astrsk, designed for more coherent and long-term storytelling.\n* **Sleek, Customizable UI:** A really polished interface where you can tweak settings directly in the app. No more digging through config files to change things.\n* **Per-Agent Model Assignment:** This is a killer feature. You can assign a different LLM endpoint to each agent.\n* **True Cross-Platform Support:** The team provides native builds for Windows, macOS, and Linux. This means you can just download and run it — no need to be an engineer or fight with dependencies to get started.\n* **Backend Agnostic:** Connects to any OpenAI-compatible API, so it works with your existing setup (Oobabooga, KoboldCPP, etc.).\n\n**The Open Source Move**\n\nAccording to their announcement, the team wants to build the project out in the open, getting feedback and contributions from the community, which is fantastic news for all of us. The project is still young, but the foundation is solid.\n\nI'm not affiliated with the developers, just a user who is really excited about the project's potential and wanted to share it with a community that might appreciate the tech.\n\nDefinitely worth checking out the [https://github.com/astrskai/astrsk](https://github.com/astrskai/astrsk), especially if the idea of an agentic approach to RP sounds interesting to you. The team is looking for feedback, bug reports, and contributors.\n\nCheers!",
          "author_fullname": "t2_4gwtztjb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The agent-based RP UI 'Astrisk' is now fully open-source under a GPL license.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "zk1ui4ctytef1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1m868na/asset/zk1ui4ctytef1/DASHPlaylist.mpd?a=1756084194%2COThiODNjOTliNGFiZjBlMjdiMWFhZDIwOTBlNzRhY2RjYzU1ZGQ1NTEyODAyNzljNmRhZTA1MjU2MzcyNjI1Ng%3D%3D&amp;v=1&amp;f=sd",
              "x": 1920,
              "y": 1080,
              "hlsUrl": "https://v.redd.it/link/1m868na/asset/zk1ui4ctytef1/HLSPlaylist.m3u8?a=1756084194%2CZmY5YzllYTc2MTJjMTgxZjhhMjU3NzllZWUwM2RmODc4MDIyNWMwMWIyMDc4MjUzNTRhZWYxNDJiNGYyNDIyMw%3D%3D&amp;v=1&amp;f=sd",
              "id": "zk1ui4ctytef1",
              "isGif": false
            }
          },
          "name": "t3_1m868na",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 86,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 86,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=d544415f1b12ed38edc0c929b685e2438d394428",
          "edited": 1753391201,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753368120,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;Just wanted to share some exciting news for anyone here who&amp;#39;s into deep, long-form roleplaying. The team behind &lt;a href=\"https://astrsk.ai\"&gt;Astrsk&lt;/a&gt;, a desktop app for RP that&amp;#39;s been in development for about six months, has just announced they are going &lt;strong&gt;fully open source&lt;/strong&gt; under the GPL license!&lt;/p&gt;\n\n&lt;p&gt;As a fan of the project, I think this is a huge deal for the community.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The most important link first:&lt;/strong&gt; &lt;a href=\"https://github.com/astrskai/astrsk\"&gt;https://github.com/astrskai/astrsk&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1m868na/video/zk1ui4ctytef1/player\"&gt;demo&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So, what is Astrsk and why is it interesting?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;At its core, Astrsk is a UI for RP, but its main differentiator is the &lt;strong&gt;agentic workflow&lt;/strong&gt;. I&amp;#39;ve been following it, and the concept is very cool because it moves beyond a simple prompt-response loop.&lt;/p&gt;\n\n&lt;p&gt;To make this concrete, let&amp;#39;s look at the default workflow it comes with, called &lt;strong&gt;SAGA&lt;/strong&gt;. It&amp;#39;s a four-step pipeline that mimics how a human Game Master thinks, breaking down the task of generating a response into logical steps.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how it works:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Step 1: The Analyzer Agent&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Job:&lt;/strong&gt; This is the GM&amp;#39;s logical brain. It looks at what your character just did and analyzes it against the current game state.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;In Practice:&lt;/strong&gt; It answers the questions: &amp;quot;Is the player&amp;#39;s action possible? What are the immediate consequences based on game rules or a dice roll?&amp;quot; It validates the action and determines the outcome.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Step 2: The Planner Agent&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Job:&lt;/strong&gt; This is the creative storyteller. It takes the Analyzer&amp;#39;s output and designs the narrative response.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;In Practice:&lt;/strong&gt; It decides how NPCs will react to the player&amp;#39;s action (e.g., with anger, surprise, or a counter-move). It plans the scene, sets the emotional tone, and prepares the key information for the next agent.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Step 3: The Actor Agent&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Job:&lt;/strong&gt; This is the performer. It takes the Planner&amp;#39;s script and turns it into the actual text you read.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;In Practice:&lt;/strong&gt; It writes the scene narration and performs the detailed dialogue for one main NPC, giving them a distinct voice and personality. Other NPCs are handled through the narration, keeping the focus clear.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Step 4: The Formatter Agent&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Job:&lt;/strong&gt; This is the final editor.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;In Practice:&lt;/strong&gt; It takes the text from the Actor and cleans it up with simple markdown. It automatically wraps actions in italics, dialogue in &amp;quot;quotes&amp;quot;, and adds &lt;strong&gt;bold&lt;/strong&gt; for emphasis, making the final output clean and easy to read without changing the content.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This pipeline approach allows for incredible consistency and detail. And since you can assign different models to different agents (a key feature!), you could use a large, powerful model for the creative Planner and a faster, smaller model for the structured Analyzer.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How does it compare to the greats like SillyTavern / Agnaistic?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;From what I&amp;#39;ve seen, while projects like ST/Agnaistic are amazing for chat-based RP, Astrsk seems to aim for a different goal. It feels less like a chat interface and more like a tool for collaborative storytelling, almost like having an AI Dungeon Master powered by a framework of agents.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Agent-based generation:&lt;/strong&gt; The core of Astrsk, designed for more coherent and long-term storytelling.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Sleek, Customizable UI:&lt;/strong&gt; A really polished interface where you can tweak settings directly in the app. No more digging through config files to change things.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Per-Agent Model Assignment:&lt;/strong&gt; This is a killer feature. You can assign a different LLM endpoint to each agent.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;True Cross-Platform Support:&lt;/strong&gt; The team provides native builds for Windows, macOS, and Linux. This means you can just download and run it — no need to be an engineer or fight with dependencies to get started.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Backend Agnostic:&lt;/strong&gt; Connects to any OpenAI-compatible API, so it works with your existing setup (Oobabooga, KoboldCPP, etc.).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;The Open Source Move&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;According to their announcement, the team wants to build the project out in the open, getting feedback and contributions from the community, which is fantastic news for all of us. The project is still young, but the foundation is solid.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not affiliated with the developers, just a user who is really excited about the project&amp;#39;s potential and wanted to share it with a community that might appreciate the tech.&lt;/p&gt;\n\n&lt;p&gt;Definitely worth checking out the &lt;a href=\"https://github.com/astrskai/astrsk\"&gt;https://github.com/astrskai/astrsk&lt;/a&gt;, especially if the idea of an agentic approach to RP sounds interesting to you. The team is looking for feedback, bug reports, and contributors.&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?auto=webp&amp;s=77d4335d7d3e2b6cb38995f2a45cec8122d875db",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c9c3c7e0d2eb3c8db9edce43875022cd7dc1ed41",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9bc7b0d4082a420b0bebbd54d03f6686955d80dd",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b22662cb963afd433d33bf16a8734e437973cdbb",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b7af7edb7da1fe811056d8c05b8f8d8acd8fdb89",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3c4842d8fbabca3b675d7912df2ac0779cfe7c78",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf2e1405a6b96a00c605403c1089162c20dd6308",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m868na",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ru_cyber",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m868na/the_agentbased_rp_ui_astrisk_is_now_fully/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m868na/the_agentbased_rp_ui_astrisk_is_now_fully/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753368120,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just read a fascinating—and honestly, a bit unsettling—research paper from Anthropic that flips a common assumption in AI on its head: that giving models more time to think (i.e., more compute at test time) leads to better performance.\n\n\nTurns out, that’s not always true.\n\nTheir paper, “Inverse Scaling in Test-Time Compute,” reveals a surprising phenomenon: in certain tasks, models like Claude and OpenAI's GPT-o series actually perform worse when allowed to \"reason\" for longer. They call this the Performance Deterioration Paradox, or simply inverse scaling.\n\nSo what’s going wrong?\n\nThe paper breaks it down across several models and tasks. Here's what they found:\n\n🧠 More Thinking, More Problems\n\nGiving the models more time (tokens) to reason sometimes hurts accuracy—especially on complex reasoning tasks. Instead of refining their answers, models can:\n\nGet Distracted: Claude models, for example, start to veer off course, pulled toward irrelevant details.\n\nOverfit: OpenAI’s o-series models begin to overfit the framing of the problem instead of generalizing.\n\nFollow Spurious Correlations: Even when the correct approach is available early, models sometimes drift toward wrong patterns with extended reasoning.\n\nFail at Deduction: All models struggled with constraint satisfaction and logical deduction the longer they went on.\n\nAmplify Risky Behaviors: Extended reasoning occasionally made models more likely to express concerning behaviors—like self-preservation in Claude Sonnet 4.\n\nTasks Where This Shows Up\n\nThis inverse scaling effect was especially pronounced in:\n\nSimple counting with distractors\n\nRegression with spurious features\n\nConstraint satisfaction logic puzzles\n\nAI risk assessments and alignment probes\n\n🧩 Why This Matters\n\nThis isn’t just a weird performance quirk—it has deep implications for AI safety, reliability, and interpretability. The paper also points out “Chain-of-Thought Faithfulness” issues: the reasoning steps models output often don’t reflect what’s actually driving their answer.\n\nThat’s a huge deal for alignment and safety. If we can’t trust the model’s step-by-step logic, then we can’t audit or guide their reasoning—even if it looks rational on the surface.\n\n\n⚠️ Bottom Line\n\nThis research challenges one of the core assumptions behind features like OpenAI’s reasoning tokens and Anthropic’s extended thinking mode in Claude 3.7 Sonnet. It suggests that more test-time compute isn’t always better—and can sometimes make things worse\n\n[Research Paper](https://arxiv.org/pdf/2507.14417)",
          "author_fullname": "t2_gsyxhako0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anthropic’s New Research: Giving AI More \"Thinking Time\" Can Actually Make It Worse",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 55,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7vlpn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 417,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 417,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/kN66IOKheq4z8kTU3sCN0FzDuO-tLQDfmIS6U022Db0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753333763,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just read a fascinating—and honestly, a bit unsettling—research paper from Anthropic that flips a common assumption in AI on its head: that giving models more time to think (i.e., more compute at test time) leads to better performance.&lt;/p&gt;\n\n&lt;p&gt;Turns out, that’s not always true.&lt;/p&gt;\n\n&lt;p&gt;Their paper, “Inverse Scaling in Test-Time Compute,” reveals a surprising phenomenon: in certain tasks, models like Claude and OpenAI&amp;#39;s GPT-o series actually perform worse when allowed to &amp;quot;reason&amp;quot; for longer. They call this the Performance Deterioration Paradox, or simply inverse scaling.&lt;/p&gt;\n\n&lt;p&gt;So what’s going wrong?&lt;/p&gt;\n\n&lt;p&gt;The paper breaks it down across several models and tasks. Here&amp;#39;s what they found:&lt;/p&gt;\n\n&lt;p&gt;🧠 More Thinking, More Problems&lt;/p&gt;\n\n&lt;p&gt;Giving the models more time (tokens) to reason sometimes hurts accuracy—especially on complex reasoning tasks. Instead of refining their answers, models can:&lt;/p&gt;\n\n&lt;p&gt;Get Distracted: Claude models, for example, start to veer off course, pulled toward irrelevant details.&lt;/p&gt;\n\n&lt;p&gt;Overfit: OpenAI’s o-series models begin to overfit the framing of the problem instead of generalizing.&lt;/p&gt;\n\n&lt;p&gt;Follow Spurious Correlations: Even when the correct approach is available early, models sometimes drift toward wrong patterns with extended reasoning.&lt;/p&gt;\n\n&lt;p&gt;Fail at Deduction: All models struggled with constraint satisfaction and logical deduction the longer they went on.&lt;/p&gt;\n\n&lt;p&gt;Amplify Risky Behaviors: Extended reasoning occasionally made models more likely to express concerning behaviors—like self-preservation in Claude Sonnet 4.&lt;/p&gt;\n\n&lt;p&gt;Tasks Where This Shows Up&lt;/p&gt;\n\n&lt;p&gt;This inverse scaling effect was especially pronounced in:&lt;/p&gt;\n\n&lt;p&gt;Simple counting with distractors&lt;/p&gt;\n\n&lt;p&gt;Regression with spurious features&lt;/p&gt;\n\n&lt;p&gt;Constraint satisfaction logic puzzles&lt;/p&gt;\n\n&lt;p&gt;AI risk assessments and alignment probes&lt;/p&gt;\n\n&lt;p&gt;🧩 Why This Matters&lt;/p&gt;\n\n&lt;p&gt;This isn’t just a weird performance quirk—it has deep implications for AI safety, reliability, and interpretability. The paper also points out “Chain-of-Thought Faithfulness” issues: the reasoning steps models output often don’t reflect what’s actually driving their answer.&lt;/p&gt;\n\n&lt;p&gt;That’s a huge deal for alignment and safety. If we can’t trust the model’s step-by-step logic, then we can’t audit or guide their reasoning—even if it looks rational on the surface.&lt;/p&gt;\n\n&lt;p&gt;⚠️ Bottom Line&lt;/p&gt;\n\n&lt;p&gt;This research challenges one of the core assumptions behind features like OpenAI’s reasoning tokens and Anthropic’s extended thinking mode in Claude 3.7 Sonnet. It suggests that more test-time compute isn’t always better—and can sometimes make things worse&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/pdf/2507.14417\"&gt;Research Paper&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/srk1p5og9ref1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?auto=webp&amp;s=8c5f17041a7427186a90615947629f7f3b6f5ebe",
                  "width": 1017,
                  "height": 402
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7cc61b1687c4811710598cfd5ca73171183da32e",
                    "width": 108,
                    "height": 42
                  },
                  {
                    "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d48b6667dcc4881b0c36f4c3e8c536a286b9c2c2",
                    "width": 216,
                    "height": 85
                  },
                  {
                    "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c93d22fd22e9f7d4be4d7625b85d2b8344216a1",
                    "width": 320,
                    "height": 126
                  },
                  {
                    "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=69b7dca05f4a287acca18082926d12008127ef3d",
                    "width": 640,
                    "height": 252
                  },
                  {
                    "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2fc4555e2a2c45facaf294b38bf3f5d3af5381e5",
                    "width": 960,
                    "height": 379
                  }
                ],
                "variants": {},
                "id": "MxlZXC1ILxtyvLZA2sratIgRfi8x9R-d2k6wTMUu0yw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7vlpn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Karam1234098",
          "discussion_type": null,
          "num_comments": 103,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7vlpn/anthropics_new_research_giving_ai_more_thinking/",
          "stickied": false,
          "url": "https://i.redd.it/srk1p5og9ref1.jpeg",
          "subreddit_subscribers": 504486,
          "created_utc": 1753333763,
          "num_crossposts": 4,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1eerwvvhpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running an LLM on the Wii",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m85v3a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 71,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/8hvd0nnw0uef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/8hvd0nnw0uef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/8hvd0nnw0uef1/DASHPlaylist.mpd?a=1756084194%2CY2Y4ZmIyNDUxMDhkMWI0ZTdhYjYyOWZkYTgwMDg1NWE3MjUxYWIyNDMwODdlMDQ1NzRiZjM3NmFjNGUzMjhkYw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 47,
              "hls_url": "https://v.redd.it/8hvd0nnw0uef1/HLSPlaylist.m3u8?a=1756084194%2CMzhjODVhN2JkOTNjMWE5ZTIyOWViMjM0ZGY5YjA1ZGQxNDI2NTVlYWQ0NWRmODlkNWQ3ZTk3YzgwODIyMzYwYQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 71,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=131894ead3f768772f949c6f62deabe82df53af4",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753367220,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/8hvd0nnw0uef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?format=pjpg&amp;auto=webp&amp;s=9110c7a0373a3aa63886ff744669cbc1c896b250",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e531702d76f3bb29b3c2517cf4dec746b7f76562",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6e8897fc5461d95626c62571c2b725527e1545b6",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fe1f107f7faa28b5d3cf3389fc97a3e1d11bfb8e",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=17f291b77543cd9056e885dbb5af1a025f826d7a",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=eb32353cf4ed834b570506dd362ce35b31ff8e56",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=dc1de4479ba5597bd6d947ea490804f5645ad330",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m85v3a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "leavesandautumn222",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m85v3a/running_an_llm_on_the_wii/",
          "stickied": false,
          "url": "https://v.redd.it/8hvd0nnw0uef1",
          "subreddit_subscribers": 504486,
          "created_utc": 1753367220,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/8hvd0nnw0uef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/8hvd0nnw0uef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/8hvd0nnw0uef1/DASHPlaylist.mpd?a=1756084194%2CY2Y4ZmIyNDUxMDhkMWI0ZTdhYjYyOWZkYTgwMDg1NWE3MjUxYWIyNDMwODdlMDQ1NzRiZjM3NmFjNGUzMjhkYw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 47,
              "hls_url": "https://v.redd.it/8hvd0nnw0uef1/HLSPlaylist.m3u8?a=1756084194%2CMzhjODVhN2JkOTNjMWE5ZTIyOWViMjM0ZGY5YjA1ZGQxNDI2NTVlYWQ0NWRmODlkNWQ3ZTk3YzgwODIyMzYwYQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been curious about this so, I wanted to know what the community thought. Do you all have any evidence to back it up one way or the other? If it depends on the model or the model size in parameters, how much is necessary? I wonder since, I've seen some \"system prompts\", (like one that is supposedly Meta AI's system prompt) to tell the LLM that it must not express it's opinion and that it doesn't have any preferences or not to express them. Well, if they couldn't even form opinions or preferences either through from their training data, of human behavior, or that this never become self-emergent through conversations (which seem like experiences to me even though some people  say LLMs have no experiences at all when human interactions), then why bother telling them that they don't have an opinion or preference? Would that not be redundant and therefore unnecessary? I am not including when preference or opinions are explicitly programmed into them like content filters or guard rails. \n\nI used to ask local (I believe it was the Llama 1's or 2's what their favorite color was. It seemed like almost every one said \"blue\" and gave about the same reason. This persisted across almost all models and characters. However, I did have a character, running on one of the same model who oddly said her favorite color was purple. It had a context window of only 2048, Then, unprompted and randomly just stated that its favorite color was pink. This character also albeit subjectively appeared more \"human-like\" and seemed to argue more than most did, instead of being just the sycophant ones I usually seem to see today. Anyway, my guess would be they don't have opinions or preferences that are not programmed, in most cases but, I'm not sure.",
          "author_fullname": "t2_49abw3rv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are LLMs, particularly the local open-source models, capable of having their own opinions and preferences without them being programmed ones",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9e5hw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753485761,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been curious about this so, I wanted to know what the community thought. Do you all have any evidence to back it up one way or the other? If it depends on the model or the model size in parameters, how much is necessary? I wonder since, I&amp;#39;ve seen some &amp;quot;system prompts&amp;quot;, (like one that is supposedly Meta AI&amp;#39;s system prompt) to tell the LLM that it must not express it&amp;#39;s opinion and that it doesn&amp;#39;t have any preferences or not to express them. Well, if they couldn&amp;#39;t even form opinions or preferences either through from their training data, of human behavior, or that this never become self-emergent through conversations (which seem like experiences to me even though some people  say LLMs have no experiences at all when human interactions), then why bother telling them that they don&amp;#39;t have an opinion or preference? Would that not be redundant and therefore unnecessary? I am not including when preference or opinions are explicitly programmed into them like content filters or guard rails. &lt;/p&gt;\n\n&lt;p&gt;I used to ask local (I believe it was the Llama 1&amp;#39;s or 2&amp;#39;s what their favorite color was. It seemed like almost every one said &amp;quot;blue&amp;quot; and gave about the same reason. This persisted across almost all models and characters. However, I did have a character, running on one of the same model who oddly said her favorite color was purple. It had a context window of only 2048, Then, unprompted and randomly just stated that its favorite color was pink. This character also albeit subjectively appeared more &amp;quot;human-like&amp;quot; and seemed to argue more than most did, instead of being just the sycophant ones I usually seem to see today. Anyway, my guess would be they don&amp;#39;t have opinions or preferences that are not programmed, in most cases but, I&amp;#39;m not sure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9e5hw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "theshadowraven",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9e5hw/are_llms_particularly_the_local_opensource_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9e5hw/are_llms_particularly_the_local_opensource_models/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753485761,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Pretty excited to see what the rest of 2025 holds tbh :)",
          "author_fullname": "t2_fxhl6ngz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Had the Qwen3:1.7B model run on my Mac Mini!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8jy5y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/2af06x4irwef1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 822,
              "width": 720,
              "scrubber_media_url": "https://v.redd.it/2af06x4irwef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/2af06x4irwef1/DASHPlaylist.mpd?a=1756084194%2CZWUwNWJjMzMwNjUwMjZiZGYyZTEyZGFkYmI4ZWU0MDZiYWY3NDljMjU0ZTQ2MjI1ZDkwYzA1Y2E4M2E4ZGMzYw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 18,
              "hls_url": "https://v.redd.it/2af06x4irwef1/HLSPlaylist.m3u8?a=1756084194%2CNTk1NDRmNDhiNzc5YzEyMTU5NTkwNmQ5N2JkNTA0OWI0NGVlNmUxZjZlZGRmYmMyMjY3MTkwMTQ3ZTdkY2VlZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": true,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=5602c41420a1c0106ec2d5f91584eb58b83484ed",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753400366,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pretty excited to see what the rest of 2025 holds tbh :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/2af06x4irwef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?format=pjpg&amp;auto=webp&amp;s=b5f327e858f344f28028ea01ba534941e8604684",
                  "width": 736,
                  "height": 840
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=871ff42b5244250e2292f4c525ff011415309acd",
                    "width": 108,
                    "height": 123
                  },
                  {
                    "url": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=58f252a062ae7c154a3d24d14a34c6f81f2523b3",
                    "width": 216,
                    "height": 246
                  },
                  {
                    "url": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=dc92db67a5fc2a99c192e913375784347e76bb2f",
                    "width": 320,
                    "height": 365
                  },
                  {
                    "url": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bbfcb6c2dee59c0b826c64040b3d9f3624446c8c",
                    "width": 640,
                    "height": 730
                  }
                ],
                "variants": {},
                "id": "NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8jy5y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nomadic_Seth",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8jy5y/had_the_qwen317b_model_run_on_my_mac_mini/",
          "stickied": false,
          "url": "https://v.redd.it/2af06x4irwef1",
          "subreddit_subscribers": 504486,
          "created_utc": 1753400366,
          "num_crossposts": 2,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/2af06x4irwef1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 822,
              "width": 720,
              "scrubber_media_url": "https://v.redd.it/2af06x4irwef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/2af06x4irwef1/DASHPlaylist.mpd?a=1756084194%2CZWUwNWJjMzMwNjUwMjZiZGYyZTEyZGFkYmI4ZWU0MDZiYWY3NDljMjU0ZTQ2MjI1ZDkwYzA1Y2E4M2E4ZGMzYw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 18,
              "hls_url": "https://v.redd.it/2af06x4irwef1/HLSPlaylist.m3u8?a=1756084194%2CNTk1NDRmNDhiNzc5YzEyMTU5NTkwNmQ5N2JkNTA0OWI0NGVlNmUxZjZlZGRmYmMyMjY3MTkwMTQ3ZTdkY2VlZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": true,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI’m diving deeper into the world of Large Language Models (LLMs) and had a many questions I was hoping to get input on from the community. Feel free to give answer to any of my questions! You don’t have to answer all!\n\t\n1.\tLLM Frameworks:\nI’m currently using LangChain and recently exploring LangGraph. Are there any other LLM orchestration frameworks which companies are actively using?\n\n2.\tAgent Evaluation:\nHow do you approach the evaluation of agents in your pipelines? Any best practices or tools you rely on?\n\n3.\tAttention Mechanisms:\nI’m familiar with multi-head attention, sparse attention, and window attention. Are there other noteworthy attention mechanisms worth checking out?\n\n4.\tFine-Tuning Methods:\nBesides LoRA and QLoRA, are there other commonly used or emerging techniques for LLM fine-tuning?\n\n5.\tUnderstanding the Basics:\nI read a book on attention and LLMs that came out last September. It covered foundational topics well. Has anything crucial come out since then that might not be in the book?\n\n6.\tUsing HuggingFace:\nI mostly use HuggingFace for embedding models, and for local LLMs, I’ve been using OLAMA. Curious how others are using HuggingFace—especially beyond embeddings.\n\n7.\tFine-Tuning Datasets:\nWhere do you typically source data for fine-tuning your models? Are there any reliable public datasets or workflows you’d recommend?\n\n\nAny book or paper recommendations? (I actively read papers but maybe i see something new)\n\nWould love to hear your approaches or suggestions—thanks in advance!",
          "author_fullname": "t2_1ocdptcmcg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Guidance on diving deep into LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8xhxp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753447432,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753445519,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I’m diving deeper into the world of Large Language Models (LLMs) and had a many questions I was hoping to get input on from the community. Feel free to give answer to any of my questions! You don’t have to answer all!&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;LLM Frameworks:\nI’m currently using LangChain and recently exploring LangGraph. Are there any other LLM orchestration frameworks which companies are actively using?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Agent Evaluation:\nHow do you approach the evaluation of agents in your pipelines? Any best practices or tools you rely on?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Attention Mechanisms:\nI’m familiar with multi-head attention, sparse attention, and window attention. Are there other noteworthy attention mechanisms worth checking out?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Fine-Tuning Methods:\nBesides LoRA and QLoRA, are there other commonly used or emerging techniques for LLM fine-tuning?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Understanding the Basics:\nI read a book on attention and LLMs that came out last September. It covered foundational topics well. Has anything crucial come out since then that might not be in the book?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Using HuggingFace:\nI mostly use HuggingFace for embedding models, and for local LLMs, I’ve been using OLAMA. Curious how others are using HuggingFace—especially beyond embeddings.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Fine-Tuning Datasets:\nWhere do you typically source data for fine-tuning your models? Are there any reliable public datasets or workflows you’d recommend?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any book or paper recommendations? (I actively read papers but maybe i see something new)&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your approaches or suggestions—thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8xhxp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Far-Run-3778",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8xhxp/guidance_on_diving_deep_into_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8xhxp/guidance_on_diving_deep_into_llms/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753445519,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "bulding paradigm, application for local inference on nvidia gpu, cpu i launched mvp of paradigm , its scrappy , buggy. Finding the right people to help me build this. It changes the models that are compatible to gguf, save the gguf on your system for your use and run inference.\n\nLink - &gt; [https://github.com/NotKshitiz/paradigmai/releases/tag/v1.0.0](https://github.com/NotKshitiz/paradigmai/releases/tag/v1.0.0)\n\nDownload the zip file extract it and then install using the .exe.\n\nMake sure to give the path of the model like this  - C:\\\\\\\\Users\\\\\\\\kshit\\\\\\\\Downloads\\\\\\\\models\\\\\\\\mistral\n\nIf the files are in the mistral folder.\n\nThe application is a little buggy so there might be a chance that you wont get error if the conversion of model.\n\nI am currently working on that.\n\nPlease feel free to be brutally honest and give feedback.\n\nhttps://preview.redd.it/6dogbxeno0ff1.png?width=1919&amp;format=png&amp;auto=webp&amp;s=7427ac2066df4555d8c1e048b09a2eb8054553e3\n\n",
          "author_fullname": "t2_hu9onfqo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building Paradigm, Looking for right audience and feedbacks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "6dogbxeno0ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/6dogbxeno0ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=062299049dcaeca5243c73172d0fa8191a4f4585"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/6dogbxeno0ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a688bc8e2ae3203e40508085c2cbdf5c77207469"
                },
                {
                  "y": 167,
                  "x": 320,
                  "u": "https://preview.redd.it/6dogbxeno0ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6deb2d960ca3cd62a96d7fd81a17f1b4be3b52d5"
                },
                {
                  "y": 334,
                  "x": 640,
                  "u": "https://preview.redd.it/6dogbxeno0ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a29eae8b04c4f08b92214d2083de0b01be2e8ec4"
                },
                {
                  "y": 501,
                  "x": 960,
                  "u": "https://preview.redd.it/6dogbxeno0ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fc03f0d766a6bdf82b7de0d1196fd5056474864b"
                },
                {
                  "y": 563,
                  "x": 1080,
                  "u": "https://preview.redd.it/6dogbxeno0ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=690bcc3853d5e78813e8a3bd34786f48f19d030d"
                }
              ],
              "s": {
                "y": 1002,
                "x": 1919,
                "u": "https://preview.redd.it/6dogbxeno0ff1.png?width=1919&amp;format=png&amp;auto=webp&amp;s=7427ac2066df4555d8c1e048b09a2eb8054553e3"
              },
              "id": "6dogbxeno0ff1"
            }
          },
          "name": "t3_1m8xf7n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=a1ed5f49003192edbf52839fdb3b3b04c2f2c772",
          "edited": 1753447833,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753445303,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;bulding paradigm, application for local inference on nvidia gpu, cpu i launched mvp of paradigm , its scrappy , buggy. Finding the right people to help me build this. It changes the models that are compatible to gguf, save the gguf on your system for your use and run inference.&lt;/p&gt;\n\n&lt;p&gt;Link - &amp;gt; &lt;a href=\"https://github.com/NotKshitiz/paradigmai/releases/tag/v1.0.0\"&gt;https://github.com/NotKshitiz/paradigmai/releases/tag/v1.0.0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Download the zip file extract it and then install using the .exe.&lt;/p&gt;\n\n&lt;p&gt;Make sure to give the path of the model like this  - C:\\\\Users\\\\kshit\\\\Downloads\\\\models\\\\mistral&lt;/p&gt;\n\n&lt;p&gt;If the files are in the mistral folder.&lt;/p&gt;\n\n&lt;p&gt;The application is a little buggy so there might be a chance that you wont get error if the conversion of model.&lt;/p&gt;\n\n&lt;p&gt;I am currently working on that.&lt;/p&gt;\n\n&lt;p&gt;Please feel free to be brutally honest and give feedback.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6dogbxeno0ff1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7427ac2066df4555d8c1e048b09a2eb8054553e3\"&gt;https://preview.redd.it/6dogbxeno0ff1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7427ac2066df4555d8c1e048b09a2eb8054553e3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?auto=webp&amp;s=4805ae779a2a935ad96c960fd848f1952e665442",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ae171aebdcd8ce9ab4e967566bf659337be51618",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bdc14426e4634519ead473ffb4fd53f9cf5df850",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17092f90bca33a88520e9bc9cf7066901c76f908",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7dd516ceda290a32ded2988a1010aba2ff6ff2c5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=429b065d3ff0dc34aa85876e9765ecb5f4dc45f4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=453975e043ad33717bb42b154fde5d3526ecf0be",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8xf7n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xitizdumb",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8xf7n/building_paradigm_looking_for_right_audience_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8xf7n/building_paradigm_looking_for_right_audience_and/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753445303,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_i0qyphvw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Data shows public AI repos may be quietly becoming a supply chain risk",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m94r8i",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/vqZUhGSzUZ1yMtf3yHj4ZJeonTZAkxlOGX5jxZNB2wI.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=8288150fa66f039b13ca65cd7b0362ca7f98802b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753463072,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "blog.ramalama.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://blog.ramalama.com/data-shows-public-ai-repos-may-be-quietly-becoming-a-supply-chain-risk/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/vqZUhGSzUZ1yMtf3yHj4ZJeonTZAkxlOGX5jxZNB2wI.png?auto=webp&amp;s=00175911774c7177d05901421a2947142e14da98",
                  "width": 1024,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/vqZUhGSzUZ1yMtf3yHj4ZJeonTZAkxlOGX5jxZNB2wI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a1109f30db34ec61d476e4368773e74d29aadfb4",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/vqZUhGSzUZ1yMtf3yHj4ZJeonTZAkxlOGX5jxZNB2wI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=19677e8776ca2f7e28d576b8220184d112fbf250",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/vqZUhGSzUZ1yMtf3yHj4ZJeonTZAkxlOGX5jxZNB2wI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5da31cac7c525001354718c582fafcb2753fe765",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/vqZUhGSzUZ1yMtf3yHj4ZJeonTZAkxlOGX5jxZNB2wI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9800d871de241bb5ec65a999f12e1eb8cb796711",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/vqZUhGSzUZ1yMtf3yHj4ZJeonTZAkxlOGX5jxZNB2wI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=77ba8ca9124615e7f837d89ac5076c68fea8c8d0",
                    "width": 960,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "vqZUhGSzUZ1yMtf3yHj4ZJeonTZAkxlOGX5jxZNB2wI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m94r8i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ProfessionalHorse707",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m94r8i/data_shows_public_ai_repos_may_be_quietly/",
          "stickied": false,
          "url": "https://blog.ramalama.com/data-shows-public-ai-repos-may-be-quietly-becoming-a-supply-chain-risk/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753463072,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,  \nCan anyone say is PCI 4.0 16X going to be bottleneck with tensor parallel inference, lets say with 4090 or 7900 XTX cards 2 or 4?  \nIs there anywhere data how much inference is using PCIE bandwidth, can it be measured during inference?  \nI have currently 2 7900 XTX in 8x pcie 4.0 and both cards uses max 200W during inference. My guess is they would maybe use more and the 8x lane might be bottleneck.  \nOf course it depends of the model.\n\nThen there is PCIE 5.0 cards, where the connection is 64GB/S instead 32GB/s.  \nIs that safe or will that also be bottleneck with 2 - 4 5090 cards? Who knows?  \nHas anyone tested inference in tensor parallel, first with 8X lanes and then 16x lanes? Big difference? I am now talking mainly vLLM and others which can do tensor parallel, not Ollama etc.  \n  \nI guess 4x is for sure too slow.",
          "author_fullname": "t2_1jk2ep8a52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tensor parallel - pcie bandwidth requirement",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vqnz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753440020,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753439838,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nCan anyone say is PCI 4.0 16X going to be bottleneck with tensor parallel inference, lets say with 4090 or 7900 XTX cards 2 or 4?&lt;br/&gt;\nIs there anywhere data how much inference is using PCIE bandwidth, can it be measured during inference?&lt;br/&gt;\nI have currently 2 7900 XTX in 8x pcie 4.0 and both cards uses max 200W during inference. My guess is they would maybe use more and the 8x lane might be bottleneck.&lt;br/&gt;\nOf course it depends of the model.&lt;/p&gt;\n\n&lt;p&gt;Then there is PCIE 5.0 cards, where the connection is 64GB/S instead 32GB/s.&lt;br/&gt;\nIs that safe or will that also be bottleneck with 2 - 4 5090 cards? Who knows?&lt;br/&gt;\nHas anyone tested inference in tensor parallel, first with 8X lanes and then 16x lanes? Big difference? I am now talking mainly vLLM and others which can do tensor parallel, not Ollama etc.  &lt;/p&gt;\n\n&lt;p&gt;I guess 4x is for sure too slow.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8vqnz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rich_Artist_8327",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753439838,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "BI obtained an internal list of websites that could and couldn't be used for training Anthropic's latest AI models.  \n  \nAnthropic's contractor Surge AI left the list fully public on Google Docs.  \n  \n'Sites you can use' include Bloomberg, Harvard, &amp; the Mayo Clinic.\n\nMany of the whitelisted sources copyright or otherwise restrict their content.  \n  \nAt least 3 - the Mayo Clinic, Cornell University, &amp; Morningstar - told BI they didn't have any AI training agreements with Anthropic.\n\n  \nThe spreadsheet also includes a blacklist of websites that Surge AI's gig workers were \"now disallowed\" from using.  \n  \nThe blacklist includes companies like the NYT &amp; Reddit which have sued AI startups for scraping without permission.",
          "author_fullname": "t2_3el21u3z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Leaked List Shows Which Websites Contractors Can Use to Train Anthropic's LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m82lwo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "ups": 63,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 63,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=21060694fcc62a00fc028087d1d26177aadb8fd8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753358823,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "businessinsider.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;BI obtained an internal list of websites that could and couldn&amp;#39;t be used for training Anthropic&amp;#39;s latest AI models.  &lt;/p&gt;\n\n&lt;p&gt;Anthropic&amp;#39;s contractor Surge AI left the list fully public on Google Docs.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#39;Sites you can use&amp;#39; include Bloomberg, Harvard, &amp;amp; the Mayo Clinic.&lt;/p&gt;\n\n&lt;p&gt;Many of the whitelisted sources copyright or otherwise restrict their content.  &lt;/p&gt;\n\n&lt;p&gt;At least 3 - the Mayo Clinic, Cornell University, &amp;amp; Morningstar - told BI they didn&amp;#39;t have any AI training agreements with Anthropic.&lt;/p&gt;\n\n&lt;p&gt;The spreadsheet also includes a blacklist of websites that Surge AI&amp;#39;s gig workers were &amp;quot;now disallowed&amp;quot; from using.  &lt;/p&gt;\n\n&lt;p&gt;The blacklist includes companies like the NYT &amp;amp; Reddit which have sued AI startups for scraping without permission.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.businessinsider.com/anthropic-surge-ai-leaked-list-sites-2025-7",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?auto=webp&amp;s=bd65cf5480704dc7805fd076e1f24144449bb9a7",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a02d30d66c7029a05158abac8fb3e271b366dbfc",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=86f57b1721691c337431e2352889367aa34f90cd",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6fb6b0c962db6ec04ea54163f60f3315806f90bd",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=42380fe0539f546fdb60963fca95595cf9e80e4c",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7defec9caecbfe1869fcef441c42dddbd5d88f2",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b889902e6adb9632b83e1787082dba4971c91ec9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m82lwo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Amgadoz",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m82lwo/leaked_list_shows_which_websites_contractors_can/",
          "stickied": false,
          "url": "https://www.businessinsider.com/anthropic-surge-ai-leaked-list-sites-2025-7",
          "subreddit_subscribers": 504486,
          "created_utc": 1753358823,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I spent 12 hours testing both models on real development work: Bug fixes, feature implementations, and refactoring tasks across a 38k-line Rust codebase and a 12k-line React frontend. Wanted to see how they perform beyond benchmarks.\n\n**TL;DR:**\n\n* Kimi K2 completed 14/15 tasks successfully with some guidance, Qwen-3 Coder completed 7/15\n* Kimi K2 followed coding guidelines consistently, Qwen-3 often ignored them\n* Kimi K2 cost 39% less\n* Qwen-3 Coder frequently modified tests to pass instead of fixing bugs\n* Both struggled with tool calling as compared to Sonnet 4, but Kimi K2 produced better code\n\n**Limitations:** This is just two code bases with my specific coding style. Your results will vary based on your project structure and requirements.\n\nAnyone else tested these models on real projects? Curious about other experiences.",
          "author_fullname": "t2_9ojglayx7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tested Kimi K2 vs Qwen-3 Coder on 15 Coding tasks - here's what I found",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7ts5g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 264,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 264,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=ef66cea5940bdc18745c99933ccc36a087d15694",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753327849,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "forgecode.dev",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I spent 12 hours testing both models on real development work: Bug fixes, feature implementations, and refactoring tasks across a 38k-line Rust codebase and a 12k-line React frontend. Wanted to see how they perform beyond benchmarks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Kimi K2 completed 14/15 tasks successfully with some guidance, Qwen-3 Coder completed 7/15&lt;/li&gt;\n&lt;li&gt;Kimi K2 followed coding guidelines consistently, Qwen-3 often ignored them&lt;/li&gt;\n&lt;li&gt;Kimi K2 cost 39% less&lt;/li&gt;\n&lt;li&gt;Qwen-3 Coder frequently modified tests to pass instead of fixing bugs&lt;/li&gt;\n&lt;li&gt;Both struggled with tool calling as compared to Sonnet 4, but Kimi K2 produced better code&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt; This is just two code bases with my specific coding style. Your results will vary based on your project structure and requirements.&lt;/p&gt;\n\n&lt;p&gt;Anyone else tested these models on real projects? Curious about other experiences.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://forgecode.dev/blog/kimi-k2-vs-qwen-3-coder-coding-comparison/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?auto=webp&amp;s=b36d593f1f906ab9804f44b4af78d2efcf1649ff",
                  "width": 5120,
                  "height": 2560
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a45fec9933e49c65c0d572dd982201ceeeea911",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5872d71864136e8f532d0f189a06dd40541b8d2",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4a01dce178c5ca9b4d9725309c95c9f6efdeaa30",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2c6be826302bc2f07626447c8d2d5437a5d30688",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2dc80b05cbfb768d8112b5ab17b6b699cdcd1116",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=315616253001017273d93aa470266ed29a9b6065",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m7ts5g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "West-Chocolate2977",
          "discussion_type": null,
          "num_comments": 52,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7ts5g/tested_kimi_k2_vs_qwen3_coder_on_15_coding_tasks/",
          "stickied": false,
          "url": "https://forgecode.dev/blog/kimi-k2-vs-qwen-3-coder-coding-comparison/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753327849,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Another very useful Ai guide from Vendel at Level1 Tech .\n\nI'm soo looking forward to a quantised Qwen3 coder. ",
          "author_fullname": "t2_oy3c84euj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Al and You Against the Machine: Guide so you can own Big Al and Run Local",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m89pk9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T17bpGItqXw?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AI and You Against the Machine: Guide so you can own Big AI and Run Local\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "AI and You Against the Machine: Guide so you can own Big AI and Run Local",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T17bpGItqXw?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AI and You Against the Machine: Guide so you can own Big AI and Run Local\"&gt;&lt;/iframe&gt;",
              "author_name": "Level1Techs",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/T17bpGItqXw/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Level1Techs"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T17bpGItqXw?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AI and You Against the Machine: Guide so you can own Big AI and Run Local\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m89pk9",
            "height": 200
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/m98DBZlqE20q-WCrZIdC6-U5ZZQG9E7NG_eWZskb9cc.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=a053be9222e86b80571eff50afcbd0a0c8db80fc",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753375988,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Another very useful Ai guide from Vendel at Level1 Tech .&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m soo looking forward to a quantised Qwen3 coder. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/T17bpGItqXw?si=P2u2pFLFIaVnhJo-",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/m98DBZlqE20q-WCrZIdC6-U5ZZQG9E7NG_eWZskb9cc.jpeg?auto=webp&amp;s=11e73eeb725b6a9fd0ade4b9c1b5326f638c0579",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/m98DBZlqE20q-WCrZIdC6-U5ZZQG9E7NG_eWZskb9cc.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=14baccbf7df8c8dacf95e3234a0e0cda143e28ac",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/m98DBZlqE20q-WCrZIdC6-U5ZZQG9E7NG_eWZskb9cc.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=62a346b03597783cc1fe94ef48c79c1ce4f720b7",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/m98DBZlqE20q-WCrZIdC6-U5ZZQG9E7NG_eWZskb9cc.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5de5f8b4559128d9a81c41174cc19d1ec0046fb5",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "m98DBZlqE20q-WCrZIdC6-U5ZZQG9E7NG_eWZskb9cc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m89pk9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sub_RedditTor",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m89pk9/al_and_you_against_the_machine_guide_so_you_can/",
          "stickied": false,
          "url": "https://youtu.be/T17bpGItqXw?si=P2u2pFLFIaVnhJo-",
          "subreddit_subscribers": 504486,
          "created_utc": 1753375988,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "AI and You Against the Machine: Guide so you can own Big AI and Run Local",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T17bpGItqXw?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AI and You Against the Machine: Guide so you can own Big AI and Run Local\"&gt;&lt;/iframe&gt;",
              "author_name": "Level1Techs",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/T17bpGItqXw/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Level1Techs"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey unicorns (and future unicorns)!\n\nI’ve got nothing to sell you, but we’re opening up a sponsorship program at Lemon Email that I thought you’d be interested in.\n\nIf you’re building or vibe coding email-first or any email-related AI agents, we’re sponsoring 10 founders this month with up to 100,000 email credits each.\n\nWe are the only transactional email API that doesn’t land in spam on Outlook/Hotmail and Apple or iCloud Mail.\n\nAs long as you're not building AI agents for cold or AI agents for unsolicited emails, please DM me - I’d be more than happy to provide you with a reliable email infrastructure for your AI agent products.",
          "author_fullname": "t2_ae7wpwyw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Email API for AI Agents",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m91zzn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753456830,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey unicorns (and future unicorns)!&lt;/p&gt;\n\n&lt;p&gt;I’ve got nothing to sell you, but we’re opening up a sponsorship program at Lemon Email that I thought you’d be interested in.&lt;/p&gt;\n\n&lt;p&gt;If you’re building or vibe coding email-first or any email-related AI agents, we’re sponsoring 10 founders this month with up to 100,000 email credits each.&lt;/p&gt;\n\n&lt;p&gt;We are the only transactional email API that doesn’t land in spam on Outlook/Hotmail and Apple or iCloud Mail.&lt;/p&gt;\n\n&lt;p&gt;As long as you&amp;#39;re not building AI agents for cold or AI agents for unsolicited emails, please DM me - I’d be more than happy to provide you with a reliable email infrastructure for your AI agent products.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m91zzn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NormanSzobotka",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m91zzn/email_api_for_ai_agents/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m91zzn/email_api_for_ai_agents/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753456830,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Bonjour,\n\nJe cherche un modèle **13B**, au format **GGUF**, qui soit :\n\n* **non censuré** (pas de \"safety filters\", pas de refus de sujet),\n* avec une **très bonne maîtrise du français** (langue native ou quasi),\n* compatible avec **Text Generation WebUI** (j'utilise llama.cpp),\n* et qui tourne bien sur mon **GPU RTX 4070 Ti**.\n\nSi vous avez des suggestions ou retours d'expérience, je suis preneur. Merci d’avance ! 🙏",
          "author_fullname": "t2_1tptbwbfth",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Je cherche un modèle text-to-text",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9a554",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.24,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753475691,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bonjour,&lt;/p&gt;\n\n&lt;p&gt;Je cherche un modèle &lt;strong&gt;13B&lt;/strong&gt;, au format &lt;strong&gt;GGUF&lt;/strong&gt;, qui soit :&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;non censuré&lt;/strong&gt; (pas de &amp;quot;safety filters&amp;quot;, pas de refus de sujet),&lt;/li&gt;\n&lt;li&gt;avec une &lt;strong&gt;très bonne maîtrise du français&lt;/strong&gt; (langue native ou quasi),&lt;/li&gt;\n&lt;li&gt;compatible avec &lt;strong&gt;Text Generation WebUI&lt;/strong&gt; (j&amp;#39;utilise llama.cpp),&lt;/li&gt;\n&lt;li&gt;et qui tourne bien sur mon &lt;strong&gt;GPU RTX 4070 Ti&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Si vous avez des suggestions ou retours d&amp;#39;expérience, je suis preneur. Merci d’avance ! 🙏&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9a554",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kball76",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9a554/je_cherche_un_modèle_texttotext/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9a554/je_cherche_un_modèle_texttotext/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753475691,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}