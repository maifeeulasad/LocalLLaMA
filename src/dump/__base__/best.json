{
  "kind": "Listing",
  "data": {
    "after": "t3_1lulbd7",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Blog post:¬†[https://huggingface.co/blog/reachy-mini](https://huggingface.co/blog/reachy-mini)  \nThomas Wolf on ùïè:¬†[https://x.com/Thom\\_Wolf/status/1942887160983466096](https://x.com/Thom_Wolf/status/1942887160983466096)",
          "author_fullname": "t2_agjaq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "First Hugging Face robot: Reachy Mini. Hackable yet easy to use, powered by open-source and the community",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "vawnwkkirtbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 51,
                  "x": 108,
                  "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e25da71b200b62f080fe87b898cabedc3f980035"
                },
                {
                  "y": 103,
                  "x": 216,
                  "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=689f3896a6483ecf0301a3e41bee281f6b631af1"
                },
                {
                  "y": 152,
                  "x": 320,
                  "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2cd9f39ffce549fbd969f0abaebfa1827db28721"
                },
                {
                  "y": 305,
                  "x": 640,
                  "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0b0fb97447ff6826b04e66433cfd08d7a784895b"
                },
                {
                  "y": 457,
                  "x": 960,
                  "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=71f3d912f7a1d080feabcf3624d847e3e891184f"
                }
              ],
              "s": {
                "y": 508,
                "x": 1065,
                "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=1065&amp;format=pjpg&amp;auto=webp&amp;s=84d0bc38a028c0f9aa81287381a7357fd7d3e396"
              },
              "id": "vawnwkkirtbf1"
            },
            "z3ecxmnjrtbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 34,
                  "x": 108,
                  "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=65b0f4599ecfcc445909635838034b7beed0949d"
                },
                {
                  "y": 69,
                  "x": 216,
                  "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a90861be827c9441c1ca57a5f9b780139e157edc"
                },
                {
                  "y": 103,
                  "x": 320,
                  "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b071bf753389fe4c3b6390ad5599cfc5214e2850"
                },
                {
                  "y": 206,
                  "x": 640,
                  "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=059b14661bbbaf197136062fb66169ed5137df02"
                },
                {
                  "y": 309,
                  "x": 960,
                  "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3af8de2000740e8be6ccc4c79ab6ff6f8b038313"
                }
              ],
              "s": {
                "y": 335,
                "x": 1038,
                "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=1038&amp;format=pjpg&amp;auto=webp&amp;s=a9b51090153990903d5faa7b87923aba926ec717"
              },
              "id": "z3ecxmnjrtbf1"
            },
            "pxk6rpahrtbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 104,
                  "x": 108,
                  "u": "https://preview.redd.it/pxk6rpahrtbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19d1d5fe149dcc7847ec46e2699a5da8b36eeeaf"
                },
                {
                  "y": 209,
                  "x": 216,
                  "u": "https://preview.redd.it/pxk6rpahrtbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f16d7699048d9ed6d1da7f0ed5da87f297a3ad10"
                },
                {
                  "y": 310,
                  "x": 320,
                  "u": "https://preview.redd.it/pxk6rpahrtbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cba6c6952a333c15a7d428404219d57e972c9b4a"
                },
                {
                  "y": 620,
                  "x": 640,
                  "u": "https://preview.redd.it/pxk6rpahrtbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=42454fe128800677fcaa673135082649ecb884c0"
                }
              ],
              "s": {
                "y": 783,
                "x": 807,
                "u": "https://preview.redd.it/pxk6rpahrtbf1.jpg?width=807&amp;format=pjpg&amp;auto=webp&amp;s=83b123f23064a54273ada207fea811308831fa33"
              },
              "id": "pxk6rpahrtbf1"
            },
            "4d11lsmgrtbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2c6ed682689657a97879863830f5813ff276269a"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb0c530b3eeaaffde0162c03101dfb511c09267c"
                },
                {
                  "y": 179,
                  "x": 320,
                  "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d357f79375b5c35ba498487ba228e6bd7f4da0ce"
                },
                {
                  "y": 359,
                  "x": 640,
                  "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=484d4e8e25389cce7460880aa913c7d32b1ae604"
                },
                {
                  "y": 539,
                  "x": 960,
                  "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=efa51dcac1349e3cae27a4943c55ce6a8e1e7f30"
                }
              ],
              "s": {
                "y": 564,
                "x": 1003,
                "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=1003&amp;format=pjpg&amp;auto=webp&amp;s=a155b0a39487f4ce3621d2cf68531e5f0bf1232b"
              },
              "id": "4d11lsmgrtbf1"
            },
            "pgm76w4krtbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 37,
                  "x": 108,
                  "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d0f424a518cee0f36acf8ad0f186995599febcda"
                },
                {
                  "y": 74,
                  "x": 216,
                  "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58999912ce24f6b9bc822b7b0739223da5153a5e"
                },
                {
                  "y": 110,
                  "x": 320,
                  "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d958a86291f4b94112a565f6d5befb9e1ca8ed63"
                },
                {
                  "y": 221,
                  "x": 640,
                  "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7eb1cab5b9bf99df7cd81f37cda08cb68d5ac380"
                },
                {
                  "y": 331,
                  "x": 960,
                  "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c21de42549bb521f84cc5fcfd37bd1cc55f9a47"
                }
              ],
              "s": {
                "y": 369,
                "x": 1067,
                "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=1067&amp;format=pjpg&amp;auto=webp&amp;s=11985df412c1a2dca7432a762a2c2c8e2a87c786"
              },
              "id": "pgm76w4krtbf1"
            }
          },
          "name": "t3_1lvf7ww",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 142,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "4d11lsmgrtbf1",
                "id": 702004774
              },
              {
                "media_id": "pxk6rpahrtbf1",
                "id": 702004775
              },
              {
                "media_id": "vawnwkkirtbf1",
                "id": 702004776
              },
              {
                "media_id": "z3ecxmnjrtbf1",
                "id": 702004777
              },
              {
                "media_id": "pgm76w4krtbf1",
                "id": 702004778
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 142,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "https://b.thumbs.redditmedia.com/q6tonUvBmagrUz-fog-jtYbG7HMQjflqMjdSdWnuk1o.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752056540,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Blog post:¬†&lt;a href=\"https://huggingface.co/blog/reachy-mini\"&gt;https://huggingface.co/blog/reachy-mini&lt;/a&gt;&lt;br/&gt;\nThomas Wolf on ùïè:¬†&lt;a href=\"https://x.com/Thom_Wolf/status/1942887160983466096\"&gt;https://x.com/Thom_Wolf/status/1942887160983466096&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lvf7ww",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lvf7ww",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nunki08",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvf7ww/first_hugging_face_robot_reachy_mini_hackable_yet/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lvf7ww",
          "subreddit_subscribers": 496591,
          "created_utc": 1752056540,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Models have been converging on \"not x, but y\" type phrases to an absurd degree. So here's a leaderboard for it.  \n  \nI don't think many labs are targeting this kind of slop in their training set filtering, so it gets compounded with subsequent model generations.",
          "author_fullname": "t2_pp9qh5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "\"Not x, but y\" Slop Leaderboard",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Post of the day  "
            },
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv2t7n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "transparent",
          "ups": 664,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Post of the day  :X:",
          "can_mod_post": false,
          "score": 664,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/uL3gwRdCk8J-WNVPdRm8RK50kebervSdJ936_btCrQw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752014921,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Models have been converging on &amp;quot;not x, but y&amp;quot; type phrases to an absurd degree. So here&amp;#39;s a leaderboard for it.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think many labs are targeting this kind of slop in their training set filtering, so it gets compounded with subsequent model generations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nxw6fmegaqbf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nxw6fmegaqbf1.png?auto=webp&amp;s=5f72913393ebeea55d572d59030a515d7e026ec0",
                  "width": 989,
                  "height": 1010
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nxw6fmegaqbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8120f376d154d7a8c30aa965a5d4aec99b2eee8b",
                    "width": 108,
                    "height": 110
                  },
                  {
                    "url": "https://preview.redd.it/nxw6fmegaqbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=50db8e8d3a69f67391ef203884d0459d28cb0f36",
                    "width": 216,
                    "height": 220
                  },
                  {
                    "url": "https://preview.redd.it/nxw6fmegaqbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1cda200654cca3e31e4c92e08cc4e54eb814d342",
                    "width": 320,
                    "height": 326
                  },
                  {
                    "url": "https://preview.redd.it/nxw6fmegaqbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f634168f40782641454db362ee799df6971e84f",
                    "width": 640,
                    "height": 653
                  },
                  {
                    "url": "https://preview.redd.it/nxw6fmegaqbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c9b95ecbe7aea07c72952b87efcbf28616d0137",
                    "width": 960,
                    "height": 980
                  }
                ],
                "variants": {},
                "id": "kkZ-LaZPi1jHaYXALY32ZrWs3vzskemUWmI2c7oZUfE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5563f7e6-52bf-11f0-a755-7266d77e32bb",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#58a7a4",
          "id": "1lv2t7n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_sqrkl",
          "discussion_type": null,
          "num_comments": 140,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/",
          "stickied": false,
          "url": "https://i.redd.it/nxw6fmegaqbf1.png",
          "subreddit_subscribers": 496591,
          "created_utc": 1752014921,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Falcon-H1 Family of Hybrid-Head Language Models (Transformer-SSM), including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B (pretrained &amp; instruction-tuned).\n\nggufs uploaded by Falcon team:\n\n[https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF)\n\n[https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF)\n\n[https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF)\n\n[https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF)\n\n[https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF)\n\n[https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF)\n\n  \n",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "support for Falcon-H1 model family has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvd7z4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "#bbbdbf",
          "ups": 65,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 65,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=3581c83ed94b0a9065771a3ff0877476cc75691f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752048624,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Falcon-H1 Family of Hybrid-Head Language Models (Transformer-SSM), including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B (pretrained &amp;amp; instruction-tuned).&lt;/p&gt;\n\n&lt;p&gt;ggufs uploaded by Falcon team:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14534",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?auto=webp&amp;s=af826696823ccdf7c774b5b780e08660ad5f28d9",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa53a1d81fbb58306dfc5225b4e021e5cd8b5556",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d6b5835790e56a1933151e63ec75c62748607d96",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec7cda0503a4bf7f5bd996c7cffa0de7975e6083",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e1eba07cf9ee71a811133c3ac69643f88b0846c",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1eab70d9d1e180a23543f7080507bbf0676ff940",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0fbc3db39d35ac44a02ca54f1f888871170b49d9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lvd7z4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lvd7z4/support_for_falconh1_model_family_has_been_merged/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14534",
          "subreddit_subscribers": 496591,
          "created_utc": 1752048624,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nJust dropped our paper on a simple but effective approach that got us an 8.7% accuracy boost over baseline (58.4% vs 49.7%) and absolutely crushed GPT-4.1's zero-shot performance (32%) on emotion classification.\n\nThis tutorial comes in 3 different formats:\n1. This LocalLLaMA post - summary and discussion\n2. Our blog post - [Beating ChatGPT with a dollar and a dream](https://syv.ai/viden/beating-chatgpt-dollar-dream)\n3. Our research paper - [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)\n\nThe TL;DR: Instead of training models to just spit out labels, we taught a seperate model to output ONLY reasoning given a instruction and answer. We then use that reasoning to augment other datasets. Think chain-of-thought but generated by a model optimized to generate the reasoning.\n\nWhat we did:\n\nStage 1: Fine-tuned Llama-3.2-1B on a general reasoning dataset (350k examples) to create \"Llama-R-Gen\" - basically a reasoning generator that can take any (Question, Answer) pair and explain why that answer makes sense.\n\nStage 2: Used Llama-R-Gen to augment our emotion classification dataset by generating reasoning for each text-emotion pair. Then trained a downstream classifier to output reasoning + prediction in one go.\n\nKey results:\n- 58.4% accuracy vs 49.7% baseline (statistically significant, p &lt; .001)\n- Massive gains on sadness (+19.6%), fear (+18.2%), anger (+4.0%)\n- Built-in interpretability - model explains its reasoning for every prediction\n- Domain transfer works - reasoning learned from math/code/science transferred beautifully to emotion classification\n\nThe interesting bits:\n\nWhat worked:\n- The reasoning generator trained on logical problems (math, code, science) transferred surprisingly well to the fuzzy world of emotion classification\n- Models that \"think out loud\" during training seem to learn more robust representations\n- Single model outputs both explanation and prediction - no separate explainability module needed\n\nWhat didn't:\n- Completely collapsed on the \"surprise\" class (66 samples, 3.3% of data) - likely due to poor reasoning generation for severely underrepresented classes\n- More computationally expensive than standard fine-tuning\n- Quality heavily depends on the initial reasoning generator\n\nTechnical details:\n- Base model: Llama-3.2-1B-Instruct (both stages)\n- Reasoning dataset: [syvai/reasoning-gen](https://huggingface.co/datasets/syvai/reasoning-gen) (derived from Mixture-of-Thoughts)\n- Target task: dair-ai/emotion (6 basic emotions)\n- Training: Axolotl framework on A40 GPU\n- Reasoning generator model: [syvai/reasoning-gen-1b](https://huggingface.co/syvai/reasoning-gen-1b)\n- Datasets: [syvai/emotion-reasoning](https://huggingface.co/datasets/syvai/emotion-reasoning) and [syvai/no-emotion-reasoning](https://huggingface.co/datasets/syvai/no-emotion-reasoning)\n\nThe approach is pretty generalizable - we're thinking about applying it to other classification tasks where intermediate reasoning steps could help (NLI, QA, multi-label classification, etc.).",
          "author_fullname": "t2_62puf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Here is how we beat ChatGPT at classification with 1 dollar in cloud compute",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvcb72",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 67,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 67,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752045180,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752044873,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Just dropped our paper on a simple but effective approach that got us an 8.7% accuracy boost over baseline (58.4% vs 49.7%) and absolutely crushed GPT-4.1&amp;#39;s zero-shot performance (32%) on emotion classification.&lt;/p&gt;\n\n&lt;p&gt;This tutorial comes in 3 different formats:\n1. This LocalLLaMA post - summary and discussion\n2. Our blog post - &lt;a href=\"https://syv.ai/viden/beating-chatgpt-dollar-dream\"&gt;Beating ChatGPT with a dollar and a dream&lt;/a&gt;\n3. Our research paper - &lt;a href=\"https://arxiv.org/abs/2507.00214\"&gt;Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The TL;DR: Instead of training models to just spit out labels, we taught a seperate model to output ONLY reasoning given a instruction and answer. We then use that reasoning to augment other datasets. Think chain-of-thought but generated by a model optimized to generate the reasoning.&lt;/p&gt;\n\n&lt;p&gt;What we did:&lt;/p&gt;\n\n&lt;p&gt;Stage 1: Fine-tuned Llama-3.2-1B on a general reasoning dataset (350k examples) to create &amp;quot;Llama-R-Gen&amp;quot; - basically a reasoning generator that can take any (Question, Answer) pair and explain why that answer makes sense.&lt;/p&gt;\n\n&lt;p&gt;Stage 2: Used Llama-R-Gen to augment our emotion classification dataset by generating reasoning for each text-emotion pair. Then trained a downstream classifier to output reasoning + prediction in one go.&lt;/p&gt;\n\n&lt;p&gt;Key results:\n- 58.4% accuracy vs 49.7% baseline (statistically significant, p &amp;lt; .001)\n- Massive gains on sadness (+19.6%), fear (+18.2%), anger (+4.0%)\n- Built-in interpretability - model explains its reasoning for every prediction\n- Domain transfer works - reasoning learned from math/code/science transferred beautifully to emotion classification&lt;/p&gt;\n\n&lt;p&gt;The interesting bits:&lt;/p&gt;\n\n&lt;p&gt;What worked:\n- The reasoning generator trained on logical problems (math, code, science) transferred surprisingly well to the fuzzy world of emotion classification\n- Models that &amp;quot;think out loud&amp;quot; during training seem to learn more robust representations\n- Single model outputs both explanation and prediction - no separate explainability module needed&lt;/p&gt;\n\n&lt;p&gt;What didn&amp;#39;t:\n- Completely collapsed on the &amp;quot;surprise&amp;quot; class (66 samples, 3.3% of data) - likely due to poor reasoning generation for severely underrepresented classes\n- More computationally expensive than standard fine-tuning\n- Quality heavily depends on the initial reasoning generator&lt;/p&gt;\n\n&lt;p&gt;Technical details:\n- Base model: Llama-3.2-1B-Instruct (both stages)\n- Reasoning dataset: &lt;a href=\"https://huggingface.co/datasets/syvai/reasoning-gen\"&gt;syvai/reasoning-gen&lt;/a&gt; (derived from Mixture-of-Thoughts)\n- Target task: dair-ai/emotion (6 basic emotions)\n- Training: Axolotl framework on A40 GPU\n- Reasoning generator model: &lt;a href=\"https://huggingface.co/syvai/reasoning-gen-1b\"&gt;syvai/reasoning-gen-1b&lt;/a&gt;\n- Datasets: &lt;a href=\"https://huggingface.co/datasets/syvai/emotion-reasoning\"&gt;syvai/emotion-reasoning&lt;/a&gt; and &lt;a href=\"https://huggingface.co/datasets/syvai/no-emotion-reasoning\"&gt;syvai/no-emotion-reasoning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The approach is pretty generalizable - we&amp;#39;re thinking about applying it to other classification tasks where intermediate reasoning steps could help (NLI, QA, multi-label classification, etc.).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?auto=webp&amp;s=a9d24f583d7b2574603ae8d72c49b280f34bbbd4",
                  "width": 965,
                  "height": 1386
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d3bc762d2895449456d8ae731ab05d4a9ff08669",
                    "width": 108,
                    "height": 155
                  },
                  {
                    "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=93346608db5cd935a59fa4d5a5eda50804747968",
                    "width": 216,
                    "height": 310
                  },
                  {
                    "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d19468e5d7c0abac69fa5e7da790d84234eecbe8",
                    "width": 320,
                    "height": 459
                  },
                  {
                    "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=693b7aba21c7c17a6c3a895ec1f03306cc7e5a41",
                    "width": 640,
                    "height": 919
                  },
                  {
                    "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=375f73f70720f070c5e1baae29ec6e1e97a1bd85",
                    "width": 960,
                    "height": 1378
                  }
                ],
                "variants": {},
                "id": "RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lvcb72",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "iamMess",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752044873,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been working on a Deep Researcher Agent that does multi-step web research and report generation. I wanted to share my stack and approach in case anyone else wants to build similar multi-agent workflows.  \nSo, the agent has 3 main stages:\n\n* **Searcher:**¬†Uses Scrapegraph to crawl and extract live data\n* **Analyst:**¬†Processes and refines the raw data using DeepSeek R1\n* **Writer:**¬†Crafts a clean final report\n\nTo make it easy to use anywhere, I wrapped the whole flow with an MCP Server. So you can run it from Claude Desktop, Cursor, or any MCP-compatible tool. There‚Äôs also a simple Streamlit UI if you want a local dashboard.\n\nHere‚Äôs what I used to build it:\n\n* Scrapegraph for web scraping\n* Nebius AI for open-source models\n* Agno for agent orchestration\n* Streamlit for the UI\n\nThe project is still basic by design, but it's a solid starting point if you're thinking about building your own deep research workflow.\n\nIf you‚Äôre curious, I put a full video tutorial here:¬†[demo](https://www.youtube.com/watch?v=pdsk6yldZGI)\n\nAnd the code is here if you want to try it or fork it:¬†[Full Code](https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/deep_researcher_agent)\n\nWould love to get your feedback on what to add next or how I can improve it",
          "author_fullname": "t2_vnmiyiza",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a Deep Researcher agent and exposed it as an MCP server!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lvj98v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752068897,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a Deep Researcher Agent that does multi-step web research and report generation. I wanted to share my stack and approach in case anyone else wants to build similar multi-agent workflows.&lt;br/&gt;\nSo, the agent has 3 main stages:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Searcher:&lt;/strong&gt;¬†Uses Scrapegraph to crawl and extract live data&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Analyst:&lt;/strong&gt;¬†Processes and refines the raw data using DeepSeek R1&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Writer:&lt;/strong&gt;¬†Crafts a clean final report&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To make it easy to use anywhere, I wrapped the whole flow with an MCP Server. So you can run it from Claude Desktop, Cursor, or any MCP-compatible tool. There‚Äôs also a simple Streamlit UI if you want a local dashboard.&lt;/p&gt;\n\n&lt;p&gt;Here‚Äôs what I used to build it:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Scrapegraph for web scraping&lt;/li&gt;\n&lt;li&gt;Nebius AI for open-source models&lt;/li&gt;\n&lt;li&gt;Agno for agent orchestration&lt;/li&gt;\n&lt;li&gt;Streamlit for the UI&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The project is still basic by design, but it&amp;#39;s a solid starting point if you&amp;#39;re thinking about building your own deep research workflow.&lt;/p&gt;\n\n&lt;p&gt;If you‚Äôre curious, I put a full video tutorial here:¬†&lt;a href=\"https://www.youtube.com/watch?v=pdsk6yldZGI\"&gt;demo&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And the code is here if you want to try it or fork it:¬†&lt;a href=\"https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/deep_researcher_agent\"&gt;Full Code&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to get your feedback on what to add next or how I can improve it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/lcaCNxV_x8GGK9DcZl5R32XXYG1Qwa-DwlfowV5-_M8.jpeg?auto=webp&amp;s=02d60d4af5a2c1e4f51f9a5defaecede4faaa4c3",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/lcaCNxV_x8GGK9DcZl5R32XXYG1Qwa-DwlfowV5-_M8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19bfee8b9dd015cc1cd888971f96965311df82d7",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/lcaCNxV_x8GGK9DcZl5R32XXYG1Qwa-DwlfowV5-_M8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab26a64d50f2bf5d62e51fc4d45b6ec28e777213",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/lcaCNxV_x8GGK9DcZl5R32XXYG1Qwa-DwlfowV5-_M8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd241f0d0944dbab19df916e77528fae4730f062",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "lcaCNxV_x8GGK9DcZl5R32XXYG1Qwa-DwlfowV5-_M8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lvj98v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Arindam_200",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752068897,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1t21btu57w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A language model built for the public good",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvbzpx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 58,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 58,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=37c8e0d9b8323c08d8237825d59c0900f3f7d9cc",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752043626,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "actu.epfl.ch",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://actu.epfl.ch/news/a-language-model-built-for-the-public-good/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?auto=webp&amp;s=24a7dfaa2ffaf2dd392e95399be0e0db46b8d179",
                  "width": 1440,
                  "height": 810
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=183bb3a598c8292f078b413b7f17ceb3b9776157",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f788f52551ae7d9e3603d0bae4dfea152c121d7",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d36eba20b3e7ae5d0a2050b48dd96dd99dd92e46",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3e2bdb7993787cf621700b4cb1686ec01dbb9041",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e50fc045ba9b2205567a10f0a7c7fea2f4f4ede",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5942032fc2302b924091e905dcf0c9d9ffa12cdb",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lvbzpx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PotatoFormal8751",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvbzpx/a_language_model_built_for_the_public_good/",
          "stickied": false,
          "url": "https://actu.epfl.ch/news/a-language-model-built-for-the-public-good/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752043626,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "user_reports": [],
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's local about this?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv53nn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 175,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "author_fullname": "t2_gi7a36v6",
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 175,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/M9nqhox0h181pZd-C5TPbg7bc7em0clENAL-R3A-2U4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "crosspost_parent_list": [
            {
              "approved_at_utc": null,
              "subreddit": "ChatGPT",
              "selftext": "",
              "author_fullname": "t2_1305uo",
              "saved": false,
              "mod_reason_title": null,
              "gilded": 0,
              "clicked": false,
              "title": "getting rejected by local models must be brutal",
              "link_flair_richtext": [
                {
                  "e": "text",
                  "t": "Funny "
                }
              ],
              "subreddit_name_prefixed": "r/ChatGPT",
              "hidden": false,
              "pwls": 6,
              "link_flair_css_class": "",
              "downs": 0,
              "thumbnail_height": 140,
              "top_awarded_type": null,
              "hide_score": false,
              "name": "t3_1luudp3",
              "quarantine": false,
              "link_flair_text_color": "light",
              "upvote_ratio": 0.96,
              "author_flair_background_color": "",
              "ups": 949,
              "total_awards_received": 0,
              "media_embed": {},
              "thumbnail_width": 140,
              "author_flair_template_id": null,
              "is_original_content": false,
              "user_reports": [],
              "secure_media": null,
              "is_reddit_media_domain": true,
              "is_meta": false,
              "category": null,
              "secure_media_embed": {},
              "link_flair_text": "Funny ",
              "can_mod_post": false,
              "score": 949,
              "approved_by": null,
              "is_created_from_ads_ui": false,
              "author_premium": false,
              "thumbnail": "https://a.thumbs.redditmedia.com/M9nqhox0h181pZd-C5TPbg7bc7em0clENAL-R3A-2U4.jpg",
              "edited": false,
              "author_flair_css_class": null,
              "author_flair_richtext": [
                {
                  "a": ":Discord:",
                  "e": "emoji",
                  "u": "https://emoji.redditmedia.com/0zlhaela6zub1_t5_7hqomg/Discord"
                }
              ],
              "gildings": {},
              "post_hint": "image",
              "content_categories": null,
              "is_self": false,
              "subreddit_type": "public",
              "created": 1751995008,
              "link_flair_type": "richtext",
              "wls": 6,
              "removed_by_category": null,
              "banned_by": null,
              "author_flair_type": "richtext",
              "domain": "i.redd.it",
              "allow_live_comments": false,
              "selftext_html": null,
              "likes": null,
              "suggested_sort": null,
              "banned_at_utc": null,
              "url_overridden_by_dest": "https://i.redd.it/rqrg67unoobf1.jpeg",
              "view_count": null,
              "archived": false,
              "no_follow": false,
              "is_crosspostable": false,
              "pinned": false,
              "over_18": false,
              "preview": {
                "images": [
                  {
                    "source": {
                      "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?auto=webp&amp;s=b06b7a4c6077025a13cab3b669b4fbbc06483324",
                      "width": 720,
                      "height": 992
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=78877e3893da49a4298497e65291068e096bd5c6",
                        "width": 108,
                        "height": 148
                      },
                      {
                        "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=642fd11e74684525d7e78fdc561233ed832763a8",
                        "width": 216,
                        "height": 297
                      },
                      {
                        "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=697478bd8ddbc503bb8ec2cfb50b942e942aa274",
                        "width": 320,
                        "height": 440
                      },
                      {
                        "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9a79114d00e031982e11ecfee91ec34ce4a3dbc1",
                        "width": 640,
                        "height": 881
                      }
                    ],
                    "variants": {},
                    "id": "xC373qc6EerFn_qLV-HUabx3XUSwRr-yszT4hsn6ILg"
                  }
                ],
                "enabled": true
              },
              "all_awardings": [],
              "awarders": [],
              "media_only": false,
              "link_flair_template_id": "935162a0-7be9-11ed-913e-6a257d69e3b3",
              "can_gild": false,
              "spoiler": false,
              "locked": false,
              "author_flair_text": ":Discord:",
              "treatment_tags": [],
              "visited": false,
              "removed_by": null,
              "mod_note": null,
              "distinguished": null,
              "subreddit_id": "t5_7hqomg",
              "author_is_blocked": false,
              "mod_reason_by": null,
              "num_reports": null,
              "removal_reason": null,
              "link_flair_background_color": "#0dd3bb",
              "id": "1luudp3",
              "is_robot_indexable": true,
              "report_reasons": null,
              "author": "ewelumokeke",
              "discussion_type": null,
              "num_comments": 109,
              "send_replies": true,
              "contest_mode": false,
              "mod_reports": [],
              "author_patreon_flair": false,
              "author_flair_text_color": "dark",
              "permalink": "/r/ChatGPT/comments/1luudp3/getting_rejected_by_local_models_must_be_brutal/",
              "stickied": false,
              "url": "https://i.redd.it/rqrg67unoobf1.jpeg",
              "subreddit_subscribers": 10865701,
              "created_utc": 1751995008,
              "num_crossposts": 3,
              "media": null,
              "is_video": false
            }
          ],
          "created": 1752021152,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/rqrg67unoobf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?auto=webp&amp;s=b06b7a4c6077025a13cab3b669b4fbbc06483324",
                  "width": 720,
                  "height": 992
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=78877e3893da49a4298497e65291068e096bd5c6",
                    "width": 108,
                    "height": 148
                  },
                  {
                    "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=642fd11e74684525d7e78fdc561233ed832763a8",
                    "width": 216,
                    "height": 297
                  },
                  {
                    "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=697478bd8ddbc503bb8ec2cfb50b942e942aa274",
                    "width": 320,
                    "height": 440
                  },
                  {
                    "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9a79114d00e031982e11ecfee91ec34ce4a3dbc1",
                    "width": 640,
                    "height": 881
                  }
                ],
                "variants": {},
                "id": "xC373qc6EerFn_qLV-HUabx3XUSwRr-yszT4hsn6ILg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lv53nn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HOLUPREDICTIONS",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "crosspost_parent": "t3_1luudp3",
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv53nn/whats_local_about_this/",
          "stickied": false,
          "url": "https://i.redd.it/rqrg67unoobf1.jpeg",
          "subreddit_subscribers": 496591,
          "created_utc": 1752021152,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It is great news for all of us, but at the same time, it will put a lot of pressure on other similar paid projects, like Msty, as in my opinion, LM Studio is one of the best AI front ends at the moment.\n\n[LM Studio is free for use at work | LM Studio Blog](https://lmstudio.ai/blog/free-for-work)",
          "author_fullname": "t2_gct10",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LM Studio is now free for use at work",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lux0q2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 397,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 397,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752000985,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It is great news for all of us, but at the same time, it will put a lot of pressure on other similar paid projects, like Msty, as in my opinion, LM Studio is one of the best AI front ends at the moment.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://lmstudio.ai/blog/free-for-work\"&gt;LM Studio is free for use at work | LM Studio Blog&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SuDY5sfZG1VpbWVeog-gTtG3kPGfhsfAsatkTWl8Lvs.png?auto=webp&amp;s=fa43b6b72b6b58e8ab6acc84231cb4e1178a30f3",
                  "width": 3356,
                  "height": 1760
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SuDY5sfZG1VpbWVeog-gTtG3kPGfhsfAsatkTWl8Lvs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=59744800bd5d1e6874dc4dccab87d0a697f6ded9",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/SuDY5sfZG1VpbWVeog-gTtG3kPGfhsfAsatkTWl8Lvs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d8cb60bfac540472b460977b3f40619d8a1d3f87",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/SuDY5sfZG1VpbWVeog-gTtG3kPGfhsfAsatkTWl8Lvs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f8b80faf042dfbce009f356d57264d2551c60f6",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/SuDY5sfZG1VpbWVeog-gTtG3kPGfhsfAsatkTWl8Lvs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=33ef5439f78376b9eeee8813780095903d214aa0",
                    "width": 640,
                    "height": 335
                  },
                  {
                    "url": "https://external-preview.redd.it/SuDY5sfZG1VpbWVeog-gTtG3kPGfhsfAsatkTWl8Lvs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c63b3af212d02734d551f63d92169baff59b4bd8",
                    "width": 960,
                    "height": 503
                  },
                  {
                    "url": "https://external-preview.redd.it/SuDY5sfZG1VpbWVeog-gTtG3kPGfhsfAsatkTWl8Lvs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8477a8979358df66c25707c814eb7c79750b4793",
                    "width": 1080,
                    "height": 566
                  }
                ],
                "variants": {},
                "id": "SuDY5sfZG1VpbWVeog-gTtG3kPGfhsfAsatkTWl8Lvs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lux0q2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mtomas7",
          "discussion_type": null,
          "num_comments": 94,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752000985,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm thinking about buying a GMKTEK Evo-2.\nWhich models (in terms of B parameters) can I expect to run at a decent speed (&gt; 10tk/s)? I'm undecided between the 64 GB and 128 GB RAM versions, but I'm leaning towards the 64 GB since even slightly larger models (Llama 3.1 70B) run at a painfully slow speed.\n\nEDIT: Thank you all so much for the great answers! I'm new to this, and, to be honest, my main concern is privacy. I plan to use a local AI for research purposes, ( e.g., Which were the causes of WWI) and perhaps for some coding assistance. If I understand the comments correctly, MoE (mixture of experts) models are larger models but only part of the model is activated and can therefore run faster. If so, then maybe the 128 GB is worth it. Thanks again to everyone! ",
          "author_fullname": "t2_bk6b6yhm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What modes can expect I run on an AMD Ryzen AI Max+ 395?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvh87a",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752066643,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752063277,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking about buying a GMKTEK Evo-2.\nWhich models (in terms of B parameters) can I expect to run at a decent speed (&amp;gt; 10tk/s)? I&amp;#39;m undecided between the 64 GB and 128 GB RAM versions, but I&amp;#39;m leaning towards the 64 GB since even slightly larger models (Llama 3.1 70B) run at a painfully slow speed.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Thank you all so much for the great answers! I&amp;#39;m new to this, and, to be honest, my main concern is privacy. I plan to use a local AI for research purposes, ( e.g., Which were the causes of WWI) and perhaps for some coding assistance. If I understand the comments correctly, MoE (mixture of experts) models are larger models but only part of the model is activated and can therefore run faster. If so, then maybe the 128 GB is worth it. Thanks again to everyone! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvh87a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "electrickangaroo31",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752063277,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Benchmarking Inference Engines and talking about metrics like TTFT, TPOT, and ITL.",
          "author_fullname": "t2_6ort7d94",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "vLLM vs SGLang vs MAX ‚Äî Who's the fastest?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvglk7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=7213745313f6282ed8d97a9461d3203fa8b93b47",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752061332,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "ersteiger.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Benchmarking Inference Engines and talking about metrics like TTFT, TPOT, and ITL.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.ersteiger.com/posts/vllm-vs-max/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?auto=webp&amp;s=07b7750f7f07c57f909c2e58365c9545864a9bd6",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=372cf45b99b811f00477201d8509803c02ad5701",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0357d9d8ce069daeaebd8339c89c4474809a9e86",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f5dee7f01d95844ae616fb9a02dda03d3792b254",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e424426a6b9ec4c92da8acc5c9c81fb4ecc20805",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b42e77f502000ed0f3eb155266e4158533a3cf97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c8bd55c4d861d06343f61d261e5b22b43bacd0b0",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lvglk7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rkstgr",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/",
          "stickied": false,
          "url": "https://www.ersteiger.com/posts/vllm-vs-max/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752061332,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "2 years ago, I left Windows mainly because of the creepy Copilot-type stuff ‚Äî always-on apps that watch everything, take screenshots every 5 seconds, and offer \"smart\" help in return. Felt like a trade: my privacy for their convenience.\n\nNow I‚Äôm on Linux, running my local models (Ollama, etc.), and I‚Äôm wondering ‚Äî what‚Äôs out there that gives that same kind of \"wow, this is scary, but actually useful\" feeling, but runs completely offline? Something which actually sort of breaches my privacy (but locally).\n\nNot just screen-watching ‚Äî anything that improves workflow or feels magically helpful... but because it‚Äôs all local I can keep my hand on my heart and say \"all is well\".\n\nLooking for tool recs or project links if anyone‚Äôs already doing this.",
          "author_fullname": "t2_bul2x6po",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What impressive (borderline creepy) local AI tools can I run now that everything is local?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lvk1ms",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752070899,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;2 years ago, I left Windows mainly because of the creepy Copilot-type stuff ‚Äî always-on apps that watch everything, take screenshots every 5 seconds, and offer &amp;quot;smart&amp;quot; help in return. Felt like a trade: my privacy for their convenience.&lt;/p&gt;\n\n&lt;p&gt;Now I‚Äôm on Linux, running my local models (Ollama, etc.), and I‚Äôm wondering ‚Äî what‚Äôs out there that gives that same kind of &amp;quot;wow, this is scary, but actually useful&amp;quot; feeling, but runs completely offline? Something which actually sort of breaches my privacy (but locally).&lt;/p&gt;\n\n&lt;p&gt;Not just screen-watching ‚Äî anything that improves workflow or feels magically helpful... but because it‚Äôs all local I can keep my hand on my heart and say &amp;quot;all is well&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Looking for tool recs or project links if anyone‚Äôs already doing this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvk1ms",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PeithonKing",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752070899,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi there, I'm Elie from the smollm team at huggingface, sharing this new model we built for local/on device use!   \n  \nblog: [https://huggingface.co/blog/smollm3](https://huggingface.co/blog/smollm3)  \nGGUF/ONIX ckpt are being uploaded here: [https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23](https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23) \n\nLet us know what you think!!",
          "author_fullname": "t2_169jzqdxe5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmolLM3: reasoning, long context and multilinguality for 3B parameter only",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lusr7l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 334,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 334,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Pgo7DLY-b9pkx5rEmpEdJXAScPnm7YQcCBljCmubJHo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751991256,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I&amp;#39;m Elie from the smollm team at huggingface, sharing this new model we built for local/on device use!   &lt;/p&gt;\n\n&lt;p&gt;blog: &lt;a href=\"https://huggingface.co/blog/smollm3\"&gt;https://huggingface.co/blog/smollm3&lt;/a&gt;&lt;br/&gt;\nGGUF/ONIX ckpt are being uploaded here: &lt;a href=\"https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23\"&gt;https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Let us know what you think!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/njam3shfcobf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/njam3shfcobf1.png?auto=webp&amp;s=878940560256d58bede1ec736ad4c2822215c7c1",
                  "width": 2048,
                  "height": 1229
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/njam3shfcobf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=02512340691c024aa56fdfadf2cf00ed3eaa8f6c",
                    "width": 108,
                    "height": 64
                  },
                  {
                    "url": "https://preview.redd.it/njam3shfcobf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=25ba7f9ae299557841788002dd85f2bfb310dcf0",
                    "width": 216,
                    "height": 129
                  },
                  {
                    "url": "https://preview.redd.it/njam3shfcobf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e95743ece71a6073e3f1f54779ed3a9ed19335f5",
                    "width": 320,
                    "height": 192
                  },
                  {
                    "url": "https://preview.redd.it/njam3shfcobf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac0783544f10bf513ae61c3adb68fd4ef3c75281",
                    "width": 640,
                    "height": 384
                  },
                  {
                    "url": "https://preview.redd.it/njam3shfcobf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7bc6aa4dcf1326bad4d74b5ab8d700075183cc35",
                    "width": 960,
                    "height": 576
                  },
                  {
                    "url": "https://preview.redd.it/njam3shfcobf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d6eafa575097ea3c5327a9d03316297602f59fdd",
                    "width": 1080,
                    "height": 648
                  }
                ],
                "variants": {},
                "id": "arz-YdPLxSkV6C1oumi84U9PyaRfc_uq0EOj0ruohzc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lusr7l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "eliebakk",
          "discussion_type": null,
          "num_comments": 37,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/",
          "stickied": false,
          "url": "https://i.redd.it/njam3shfcobf1.png",
          "subreddit_subscribers": 496591,
          "created_utc": 1751991256,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi r/LocalLLaMA does anyone have a good tensor override for hunyuan a13b? I get around 12 t/s on ddr4 3600 and with different offloads to a 3090 I got to 21 t/s. This is the command I'm using just in case it's useful for someone:\n\n`./llama-server -m /mnt/llamas/ggufs/tencent_Hunyuan-A13B-Instruct-Q4_K_M.gguf  -fa -ngl 99 -c 8192 --jinja --temp 0.7 --top-k 20 --top-p 0.8 --repeat-penalty 1.05 -ot \"blk\\.[1-9]\\.ffn.*=CPU\" -ot \"blk\\.1[6-9]\\.ffn.*=CPU\"`\n\nI took it from one of the suggested ot for qwen235, I also tried some ot for llama4-scout but they were slower",
          "author_fullname": "t2_2xii9ad6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hunyuan A13B tensor override",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvirqs",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752067609,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; does anyone have a good tensor override for hunyuan a13b? I get around 12 t/s on ddr4 3600 and with different offloads to a 3090 I got to 21 t/s. This is the command I&amp;#39;m using just in case it&amp;#39;s useful for someone:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;./llama-server -m /mnt/llamas/ggufs/tencent_Hunyuan-A13B-Instruct-Q4_K_M.gguf  -fa -ngl 99 -c 8192 --jinja --temp 0.7 --top-k 20 --top-p 0.8 --repeat-penalty 1.05 -ot &amp;quot;blk\\.[1-9]\\.ffn.*=CPU&amp;quot; -ot &amp;quot;blk\\.1[6-9]\\.ffn.*=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I took it from one of the suggested ot for qwen235, I also tried some ot for llama4-scout but they were slower&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvirqs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "marderbot13",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752067609,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A project I'm working on calls for embeddings of short strings, and I'm pretty sure they don't have to have as many dimensions as those normally used. I've currently got a setup using nomic-embed-text-v1.5, which is Matryoshka, so the dimensions can be reduced after generation. I've also got other strategies available for post-creation reduction. But via Nomic's API or on Ollama locally, the operation is *much* more time consuming than I'd like. I'm sure it could be done a lot more rapidly, maybe through a cruder model. But I don't have a clue what's available, and this would raise the issue of incompatibility with embeddings I have from regular-sized chunks I have elsewhere. I guess I could have parallel spaces, but it seems a clunky workaround.\n\nAny suggestions?\n\n(The data is instances of skos:Concept, I want to map them into vector space, hence embeddings from their labels - maybe only a couple of words, or their descriptions, maybe a sentence or two)\n",
          "author_fullname": "t2_3m1s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Generate low-dimension embeddings *quickly*?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvi022",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752065542,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A project I&amp;#39;m working on calls for embeddings of short strings, and I&amp;#39;m pretty sure they don&amp;#39;t have to have as many dimensions as those normally used. I&amp;#39;ve currently got a setup using nomic-embed-text-v1.5, which is Matryoshka, so the dimensions can be reduced after generation. I&amp;#39;ve also got other strategies available for post-creation reduction. But via Nomic&amp;#39;s API or on Ollama locally, the operation is &lt;em&gt;much&lt;/em&gt; more time consuming than I&amp;#39;d like. I&amp;#39;m sure it could be done a lot more rapidly, maybe through a cruder model. But I don&amp;#39;t have a clue what&amp;#39;s available, and this would raise the issue of incompatibility with embeddings I have from regular-sized chunks I have elsewhere. I guess I could have parallel spaces, but it seems a clunky workaround.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions?&lt;/p&gt;\n\n&lt;p&gt;(The data is instances of skos:Concept, I want to map them into vector space, hence embeddings from their labels - maybe only a couple of words, or their descriptions, maybe a sentence or two)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvi022",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "danja",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvi022/generate_lowdimension_embeddings_quickly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvi022/generate_lowdimension_embeddings_quickly/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752065542,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://huggingface.co/abhinavv3/MEMGPT\n\nBefore training the current code Im planning to experiment by replacing the existing attention layer with GQA and the positional encoding with RoPE.Also tryingg to implement some concepts from research papers like Memorizing Transformers.\n\nBt these changes haven‚Äôt been implemented yet.Hopefully,finish them this weekend",
          "author_fullname": "t2_lpanmabv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "üöÄ Built another 124m parameter transformer based model from scratch.This time with multi GPU training using DDP.Inspired from nanoGPT.But redesigned to suit my own training pipeline.Model and training code is on huggingface‚¨áÔ∏è",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvhxe7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752065331,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/abhinavv3/MEMGPT\"&gt;https://huggingface.co/abhinavv3/MEMGPT&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Before training the current code Im planning to experiment by replacing the existing attention layer with GQA and the positional encoding with RoPE.Also tryingg to implement some concepts from research papers like Memorizing Transformers.&lt;/p&gt;\n\n&lt;p&gt;Bt these changes haven‚Äôt been implemented yet.Hopefully,finish them this weekend&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/PkaVJdjdt2e0bH0yRf3ozkpDnxMg4PDYNZHjCoWf310.png?auto=webp&amp;s=9eb6b3e33153d9a1e7eec47f81d7366649b44f43",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/PkaVJdjdt2e0bH0yRf3ozkpDnxMg4PDYNZHjCoWf310.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=75930a8cb5bc8aba988e25a5bac82cc215a0e3fc",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/PkaVJdjdt2e0bH0yRf3ozkpDnxMg4PDYNZHjCoWf310.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c5765a787140dc2ce42634cbfe309d6c09af0f2a",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/PkaVJdjdt2e0bH0yRf3ozkpDnxMg4PDYNZHjCoWf310.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1692e46b9b4d0df17bb239a9550751f6b89c2608",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/PkaVJdjdt2e0bH0yRf3ozkpDnxMg4PDYNZHjCoWf310.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=984cda25d5cef002021283fc911938db87b845a4",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/PkaVJdjdt2e0bH0yRf3ozkpDnxMg4PDYNZHjCoWf310.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=280811aa51b58e948f1928c17a4ec625430505e3",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/PkaVJdjdt2e0bH0yRf3ozkpDnxMg4PDYNZHjCoWf310.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a3094841cc6d3388d2ef1a0ef0463f052679efc2",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "PkaVJdjdt2e0bH0yRf3ozkpDnxMg4PDYNZHjCoWf310"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lvhxe7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Ad3290",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvhxe7/built_another_124m_parameter_transformer_based/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvhxe7/built_another_124m_parameter_transformer_based/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752065331,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA‚Äôs Highly Anticipated ‚ÄúMini-Supercomputer,‚Äù the DGX Spark, Launches This Month ‚Äî Bringing Immense AI Power to Your Hands ‚Äî up to 4000$",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luroqh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 273,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 273,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=930e3f587364167265ef11d526ce35f2c301f17c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751988796,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/nvidia-mini-supercomputer-the-dgx-spark-launches-this-month/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?auto=webp&amp;s=2c3905dab01b88b0dbab01fcb0b574d9f1e512b5",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd25657d3fce734d4025693e620867a7cf866fd1",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed12cf3ccefd6b6e923ec4d43579ac26f2d80130",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf58c691ecc4a68a7e65aa0c0d4e284c08c9845a",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8436d2033ab2a873dac41641dd69093f14dcb51c",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0957457f9a703a8e5f4750384c880b16a2c80648",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6ffcb9da830480ee39d159a6310bc89df79861f",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1luroqh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 271,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/",
          "stickied": false,
          "url": "https://wccftech.com/nvidia-mini-supercomputer-the-dgx-spark-launches-this-month/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751988796,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Project Website: [https://memos.openmem.net/](https://memos.openmem.net/) \n\nCode: [https://github.com/MemTensor/MemOS](https://github.com/MemTensor/MemOS)\n\nAbstract\n\n&gt;Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency. Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods. While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations. Recent work has modeled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge \\[1\\]. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, we propose MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling.\n\n",
          "author_fullname": "t2_qjpsv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MemOS: A Memory OS for AI System",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv9m3j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": "#93b1ba",
          "subreddit_type": "public",
          "ups": 25,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752034933,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Project Website: &lt;a href=\"https://memos.openmem.net/\"&gt;https://memos.openmem.net/&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Code: &lt;a href=\"https://github.com/MemTensor/MemOS\"&gt;https://github.com/MemTensor/MemOS&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Abstract&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency. Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods. While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations. Recent work has modeled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge [1]. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, we propose MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.03724",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lv9m3j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ninjasaid13",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lv9m3j/memos_a_memory_os_for_ai_system/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.03724",
          "subreddit_subscribers": 496591,
          "created_utc": 1752034933,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI'm looking to set up a local code assistant/agentic LLM on my own VPS and could use your recommendations!\n\n**Specs:**\n\n* 8 vCores (VPS)\n* 16 GB RAM\n* 480 GB NVMe SSD\n\nI plan to run everything with **Ollama** (Dockerized), mainly for local privacy and performance reasons.\n\n**My goals:**\n\n* **Agentic coding**: Not just code completion, but autonomous code changes, repo analysis, bug fixing, etc.\n* Integrate it into my workflow, ideally via **VS Code** (extension).\n\n**What I've tried so far:**\n\n* I‚Äôve already tried using the **Cline** extension in VS Code (with Ollama as backend and models like Qwen2.5-Coder:7b/14b).\n* Unfortunately, **everything freezes up as soon as I start an agentic coding task or send an API call**. Cline doesn‚Äôt respond, and the model never replies (even with enough RAM, etc.).\n\n**Questions:**\n\n1. **Which local LLM would you recommend for my specs?** (Qwen2.5-Coder, Deepseek Coder, Llama-3, etc. ‚Äî ideally with ‚Äúagent‚Äù features or good reasoning/coding performance)\n2. **Which VS Code extension works best for local Ollama models** (agent-style coding, not just chat)? I know about ‚ÄúContinue‚Äù and ‚ÄúCline‚Äù ‚Äî but Cline seems unstable for me. Any real-world feedback, or others to consider?\n\n**Bonus:**  \nIf you‚Äôve actually run ‚Äúagentic‚Äù workflows (like multi-step code changes, repo understanding, etc.) with a local model and VS Code, please share your experiences!\n\nThanks in advance!",
          "author_fullname": "t2_eiet8zs3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Local LLM for Agentic Coding on Ollama (8 vCore, 16 GB RAM VPS)? + VS Code Extension Recommendation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lvjtc4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752070321,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to set up a local code assistant/agentic LLM on my own VPS and could use your recommendations!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Specs:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;8 vCores (VPS)&lt;/li&gt;\n&lt;li&gt;16 GB RAM&lt;/li&gt;\n&lt;li&gt;480 GB NVMe SSD&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I plan to run everything with &lt;strong&gt;Ollama&lt;/strong&gt; (Dockerized), mainly for local privacy and performance reasons.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My goals:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Agentic coding&lt;/strong&gt;: Not just code completion, but autonomous code changes, repo analysis, bug fixing, etc.&lt;/li&gt;\n&lt;li&gt;Integrate it into my workflow, ideally via &lt;strong&gt;VS Code&lt;/strong&gt; (extension).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;ve tried so far:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I‚Äôve already tried using the &lt;strong&gt;Cline&lt;/strong&gt; extension in VS Code (with Ollama as backend and models like Qwen2.5-Coder:7b/14b).&lt;/li&gt;\n&lt;li&gt;Unfortunately, &lt;strong&gt;everything freezes up as soon as I start an agentic coding task or send an API call&lt;/strong&gt;. Cline doesn‚Äôt respond, and the model never replies (even with enough RAM, etc.).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Which local LLM would you recommend for my specs?&lt;/strong&gt; (Qwen2.5-Coder, Deepseek Coder, Llama-3, etc. ‚Äî ideally with ‚Äúagent‚Äù features or good reasoning/coding performance)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Which VS Code extension works best for local Ollama models&lt;/strong&gt; (agent-style coding, not just chat)? I know about ‚ÄúContinue‚Äù and ‚ÄúCline‚Äù ‚Äî but Cline seems unstable for me. Any real-world feedback, or others to consider?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Bonus:&lt;/strong&gt;&lt;br/&gt;\nIf you‚Äôve actually run ‚Äúagentic‚Äù workflows (like multi-step code changes, repo understanding, etc.) with a local model and VS Code, please share your experiences!&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvjtc4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HeislPeda",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvjtc4/best_local_llm_for_agentic_coding_on_ollama_8/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvjtc4/best_local_llm_for_agentic_coding_on_ollama_8/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752070321,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tried MNN chat android and qwen3 0.6b acts really weird. It nearly always repeats its statements.\n\nEven SmolLM2 350M is better than it.\n\nThe rest of the models I tried work fine however, its just qwen3 0.6b which is weird",
          "author_fullname": "t2_8hpbax1b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 0.6b MNN acting weird",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvh4ou",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752062990,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried MNN chat android and qwen3 0.6b acts really weird. It nearly always repeats its statements.&lt;/p&gt;\n\n&lt;p&gt;Even SmolLM2 350M is better than it.&lt;/p&gt;\n\n&lt;p&gt;The rest of the models I tried work fine however, its just qwen3 0.6b which is weird&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvh4ou",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ExtremeAcceptable289",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvh4ou/qwen3_06b_mnn_acting_weird/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvh4ou/qwen3_06b_mnn_acting_weird/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752062990,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "OpenCodeReasoning-Nemotron-1.1-7B is a large language model (LLM) which is a derivative of Qwen2.5-7B-Instruct (AKA the reference model). It is a reasoning model that is post-trained for reasoning for code generation. The model supports a context length of 64k tokens.  \n\n\nThis model is ready for commercial/non-commercial use.\n\n\n\n||LiveCodeBench|\n|:-|:-|\n|QwQ-32B|61.3|\n|**OpenCodeReasoning-Nemotron-1.1-14B**|**65.9**|\n|OpenCodeReasoning-Nemotron-14B|59.4|\n|**OpenCodeReasoning-Nemotron-1.1-32B**|**69.9**|\n|OpenCodeReasoning-Nemotron-32B|61.7|\n|DeepSeek-R1-0528|73.4|\n|DeepSeek-R1|65.6|\n\n\n\n\n\n[https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B](https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B)\n\n[https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B](https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B)\n\n[https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B](https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B)\n\n",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "new models from NVIDIA: OpenCodeReasoning-Nemotron-1.1 7B/14B/32B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lus2yw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 172,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 172,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751989711,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;OpenCodeReasoning-Nemotron-1.1-7B is a large language model (LLM) which is a derivative of Qwen2.5-7B-Instruct (AKA the reference model). It is a reasoning model that is post-trained for reasoning for code generation. The model supports a context length of 64k tokens.  &lt;/p&gt;\n\n&lt;p&gt;This model is ready for commercial/non-commercial use.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;LiveCodeBench&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;QwQ-32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;61.3&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;OpenCodeReasoning-Nemotron-1.1-14B&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;65.9&lt;/strong&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenCodeReasoning-Nemotron-14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;59.4&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;OpenCodeReasoning-Nemotron-1.1-32B&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;69.9&lt;/strong&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenCodeReasoning-Nemotron-32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;61.7&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DeepSeek-R1-0528&lt;/td&gt;\n&lt;td align=\"left\"&gt;73.4&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DeepSeek-R1&lt;/td&gt;\n&lt;td align=\"left\"&gt;65.6&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B\"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B\"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B\"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?auto=webp&amp;s=c046dee229da79f38621e6f0294ad12f961a6cb6",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=64ec5a05cd96cb87a4112fc8bfe7de098998dfb3",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e3a19d7dc3807669af215390b9226fe990e62a93",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ceacf3cf885d899d7bd7dd681ad5e7c095d46f14",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bc5c5ab390ef3369333633fa85044571e121cce",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e091e24575e0af7c9f8a54d691665da5d4e3b46c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8ed5d57035bfc07d513aff72e320481a92cf70cb",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lus2yw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751989711,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "*On Day 11, I gave you a brief introduction to the attention mechanism. Today, we‚Äôre going to implement it from scratch in Python. But before we dive into the code, let‚Äôs quickly revisit what attention is all about.*\n\n# What Is Attention?¬†\n\n*Imagine you‚Äôre in a room with five people, and you‚Äôre trying to understand what‚Äôs going on. You don‚Äôt pay equal attention to all five people, you naturally focus more on the person who‚Äôs talking about something relevant.*\n\n*That‚Äôs exactly what attention does for LLMs. When reading a sentence, the model ‚Äúpays more attention‚Äù to the words that are important for understanding the context.*\n\n*Let‚Äôs break it down with a simple example and real code!*\n\n# Our Example: ‚ÄúCats love cozy¬†windows‚Äù\n\n*Each word will be turned into a vector¬†, just a bunch of numbers that represent the meaning of the word. Here‚Äôs what our made-up word vectors look like:*\n\n    import torch\n    \n    inputs = torch.tensor([\n        [0.10, 0.20, 0.30],  # Cats     (x¬π)\n        [0.40, 0.50, 0.60],  # love     (x¬≤)\n        [0.70, 0.80, 0.10],  # cozy     (x¬≥)\n        [0.90, 0.10, 0.20]   # windows  (x‚Å¥)\n    ])\n\n*Each row is an embedding for a word, just another way of saying, ‚Äúthis is how the model understands the meaning of the word in numbers.‚Äù*\n\n# 1: Calculating Attention Scores (How Similar Are These¬†Words?)\n\n*Let‚Äôs say we want to find out how much attention the word* ***‚Äú****love****‚Äù*** *(second word) should pay to all the others.*\n\n*We do that by computing the dot product between the vector for ‚Äúlove‚Äù and the others. The higher the score, the more related they are.*\n\n    query = inputs[1]  # Embedding for \"love\"\n    \n    attn_scores = torch.empty(inputs.shape[0])\n    for i, x_i in enumerate(inputs):\n        attn_scores[i] = torch.dot(query, x_i)\n    \n    print(attn_scores)\n\n*Or, even faster, do it for all words at once using matrix multiplication:*\n\n    attn_scores_all = inputs @ inputs.T\n    print(attn_scores_all)\n\n*This gives us a matrix of similarities, each number tells how strongly one word is related to another.*\n\n# 2: Turning Scores into Meaningful Weights (Using¬†Softmax)\n\n*Raw scores are hard to interpret. We want to turn them into weights between 0 and 1 that add up to 1 for each word. This tells us the percentage of focus each word should get.*\n\n*We use the softmax function to do this:*\n\n    attn_weights = torch.softmax(attn_scores_all, dim=-1)\n    print(attn_weights)\n\n*Now every row in this matrix shows how much attention one word gives to all the others. For instance, row 2 tells us how much ‚Äúlove‚Äù attends to ‚ÄúCats,‚Äù ‚Äúcozy,‚Äù and ‚Äúwindows.‚Äù*\n\n# 3: Creating a Context Vector (The Final¬†Mix)\n\n*Here‚Äôs the cool part.*\n\n*Each word‚Äôs final understanding (called a context vector) is calculated by mixing all word vectors together, based on the attention weights.*\n\n*If ‚Äúlove‚Äù pays 70% attention to ‚ÄúCats‚Äù and 30% to ‚Äúcozy,‚Äù the context vector will be a blend of those two word vectors.*\n\n*Let‚Äôs do it manually for ‚Äúlove‚Äù (row 2):*\n\n    attn_weights_love = attn_weights[1]\n    \n    context_vec_love = torch.zeros_like(inputs[0])\n    for i, x_i in enumerate(inputs):\n        context_vec_love += attn_weights_love[i] * x_i\n    \n    print(context_vec_love)\n\n*Or faster, do it for all words at once:*\n\n    context_vectors = attn_weights @ inputs\n    print(context_vectors)\n\n*Each row now holds a new version of the word that includes information from the whole sentence.*¬†\n\n# Why Does This¬†Matter?\n\n*This mechanism helps LLMs:*\n\n* ***Understand context:*** *It‚Äôs not just ‚Äúwhat‚Äù a word is but how it fits in the sentence.*\n* ***Be smarter with predictions:*** *It can now decide that ‚Äúwindows‚Äù is important because ‚Äúcats love cozy windows.‚Äù*\n* ***Handle longer sentences:*** *Attention lets the model scale and stay relevant, even with lots of words.*\n\n# TL;DR¬†\n\n*The attention mechanism in LLMs:*\n\n1. *Calculates how similar each word is to every other word.*\n2. *Converts those scores into weights (softmax).*\n3. *Builds a new vector for each word using those weights (context vector).*\n\n*This simple trick is the backbone of how modern Transformers work, letting them read, understand, and generate human-like text.*\n\n*If this helped clarify things, let me know!*.*Tomorrow we are going to code the self attention mechanism with key, query and value matrices.*",
          "author_fullname": "t2_8ht7a116",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Day 12/50: Building a Small Language Model from Scratch‚Ää-‚ÄäImplementing a Simplified Attention Mechanism in¬†Python",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv85jp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752030201,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;On Day 11, I gave you a brief introduction to the attention mechanism. Today, we‚Äôre going to implement it from scratch in Python. But before we dive into the code, let‚Äôs quickly revisit what attention is all about.&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;What Is Attention?¬†&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;Imagine you‚Äôre in a room with five people, and you‚Äôre trying to understand what‚Äôs going on. You don‚Äôt pay equal attention to all five people, you naturally focus more on the person who‚Äôs talking about something relevant.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;That‚Äôs exactly what attention does for LLMs. When reading a sentence, the model ‚Äúpays more attention‚Äù to the words that are important for understanding the context.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Let‚Äôs break it down with a simple example and real code!&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;Our Example: ‚ÄúCats love cozy¬†windows‚Äù&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;Each word will be turned into a vector¬†, just a bunch of numbers that represent the meaning of the word. Here‚Äôs what our made-up word vectors look like:&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import torch\n\ninputs = torch.tensor([\n    [0.10, 0.20, 0.30],  # Cats     (x¬π)\n    [0.40, 0.50, 0.60],  # love     (x¬≤)\n    [0.70, 0.80, 0.10],  # cozy     (x¬≥)\n    [0.90, 0.10, 0.20]   # windows  (x‚Å¥)\n])\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Each row is an embedding for a word, just another way of saying, ‚Äúthis is how the model understands the meaning of the word in numbers.‚Äù&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;1: Calculating Attention Scores (How Similar Are These¬†Words?)&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;Let‚Äôs say we want to find out how much attention the word&lt;/em&gt; &lt;strong&gt;&lt;em&gt;‚Äú&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;love&lt;/em&gt;&lt;strong&gt;&lt;em&gt;‚Äù&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;(second word) should pay to all the others.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We do that by computing the dot product between the vector for ‚Äúlove‚Äù and the others. The higher the score, the more related they are.&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;query = inputs[1]  # Embedding for &amp;quot;love&amp;quot;\n\nattn_scores = torch.empty(inputs.shape[0])\nfor i, x_i in enumerate(inputs):\n    attn_scores[i] = torch.dot(query, x_i)\n\nprint(attn_scores)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Or, even faster, do it for all words at once using matrix multiplication:&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;attn_scores_all = inputs @ inputs.T\nprint(attn_scores_all)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;This gives us a matrix of similarities, each number tells how strongly one word is related to another.&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;2: Turning Scores into Meaningful Weights (Using¬†Softmax)&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;Raw scores are hard to interpret. We want to turn them into weights between 0 and 1 that add up to 1 for each word. This tells us the percentage of focus each word should get.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We use the softmax function to do this:&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;attn_weights = torch.softmax(attn_scores_all, dim=-1)\nprint(attn_weights)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Now every row in this matrix shows how much attention one word gives to all the others. For instance, row 2 tells us how much ‚Äúlove‚Äù attends to ‚ÄúCats,‚Äù ‚Äúcozy,‚Äù and ‚Äúwindows.‚Äù&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;3: Creating a Context Vector (The Final¬†Mix)&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;Here‚Äôs the cool part.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Each word‚Äôs final understanding (called a context vector) is calculated by mixing all word vectors together, based on the attention weights.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;If ‚Äúlove‚Äù pays 70% attention to ‚ÄúCats‚Äù and 30% to ‚Äúcozy,‚Äù the context vector will be a blend of those two word vectors.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Let‚Äôs do it manually for ‚Äúlove‚Äù (row 2):&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;attn_weights_love = attn_weights[1]\n\ncontext_vec_love = torch.zeros_like(inputs[0])\nfor i, x_i in enumerate(inputs):\n    context_vec_love += attn_weights_love[i] * x_i\n\nprint(context_vec_love)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Or faster, do it for all words at once:&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;context_vectors = attn_weights @ inputs\nprint(context_vectors)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Each row now holds a new version of the word that includes information from the whole sentence.&lt;/em&gt;¬†&lt;/p&gt;\n\n&lt;h1&gt;Why Does This¬†Matter?&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;This mechanism helps LLMs:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Understand context:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;It‚Äôs not just ‚Äúwhat‚Äù a word is but how it fits in the sentence.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Be smarter with predictions:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;It can now decide that ‚Äúwindows‚Äù is important because ‚Äúcats love cozy windows.‚Äù&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Handle longer sentences:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Attention lets the model scale and stay relevant, even with lots of words.&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;TL;DR¬†&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;The attention mechanism in LLMs:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;em&gt;Calculates how similar each word is to every other word.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Converts those scores into weights (softmax).&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Builds a new vector for each word using those weights (context vector).&lt;/em&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;em&gt;This simple trick is the backbone of how modern Transformers work, letting them read, understand, and generate human-like text.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;If this helped clarify things, let me know!&lt;/em&gt;.&lt;em&gt;Tomorrow we are going to code the self attention mechanism with key, query and value matrices.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lv85jp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Prashant-Lakhera",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv85jp/day_1250_building_a_small_language_model_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv85jp/day_1250_building_a_small_language_model_from/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752030201,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is how the generated TTS peak levels are -   \n  \nScreenshot:¬†[https://ibb.co/b8mZBd5](https://ibb.co/b8mZBd5)  \n  \nIn some sentences, some words are automatically spoken at a lower volume.  \n  \nIs there a way to even the peak levels across the whole audio in Audacity?\n\nWhen I select the entire file and apply \"Normalize,\" it doesnt fix. But if I select a specific section and apply \"Normalize\" or \"Amplify,\" it increases the volume too much.\n\nManually adjusting small sections is very time consuming. Any way to do this altogether?  \n  \nIs there a way to achieve consistent peak levels from the generated TTS?",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why TTS level is not constant?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lvkigw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752072044,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is how the generated TTS peak levels are -   &lt;/p&gt;\n\n&lt;p&gt;Screenshot:¬†&lt;a href=\"https://ibb.co/b8mZBd5\"&gt;https://ibb.co/b8mZBd5&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;In some sentences, some words are automatically spoken at a lower volume.  &lt;/p&gt;\n\n&lt;p&gt;Is there a way to even the peak levels across the whole audio in Audacity?&lt;/p&gt;\n\n&lt;p&gt;When I select the entire file and apply &amp;quot;Normalize,&amp;quot; it doesnt fix. But if I select a specific section and apply &amp;quot;Normalize&amp;quot; or &amp;quot;Amplify,&amp;quot; it increases the volume too much.&lt;/p&gt;\n\n&lt;p&gt;Manually adjusting small sections is very time consuming. Any way to do this altogether?  &lt;/p&gt;\n\n&lt;p&gt;Is there a way to achieve consistent peak levels from the generated TTS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/c00BdZg0Zk7UCTeCvyjcQy1cLDooWv6U9puAXwaaWWw.jpg?auto=webp&amp;s=68d546a83dea489666b16650161082b467b9196d",
                  "width": 1920,
                  "height": 876
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/c00BdZg0Zk7UCTeCvyjcQy1cLDooWv6U9puAXwaaWWw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=16996db8c981eb426e3bfb1aa257b73395dad7e8",
                    "width": 108,
                    "height": 49
                  },
                  {
                    "url": "https://external-preview.redd.it/c00BdZg0Zk7UCTeCvyjcQy1cLDooWv6U9puAXwaaWWw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5763af68e0aa34fb6faf443a6413fa9f750d6add",
                    "width": 216,
                    "height": 98
                  },
                  {
                    "url": "https://external-preview.redd.it/c00BdZg0Zk7UCTeCvyjcQy1cLDooWv6U9puAXwaaWWw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9e10f83443e028b574bba59023c6e7ac97664b2",
                    "width": 320,
                    "height": 146
                  },
                  {
                    "url": "https://external-preview.redd.it/c00BdZg0Zk7UCTeCvyjcQy1cLDooWv6U9puAXwaaWWw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8d5625f674a61269320f0353b9961f404d7352b",
                    "width": 640,
                    "height": 292
                  },
                  {
                    "url": "https://external-preview.redd.it/c00BdZg0Zk7UCTeCvyjcQy1cLDooWv6U9puAXwaaWWw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0517a8fb89cfc97b25fb3ec695b3fc6bc1440a57",
                    "width": 960,
                    "height": 438
                  },
                  {
                    "url": "https://external-preview.redd.it/c00BdZg0Zk7UCTeCvyjcQy1cLDooWv6U9puAXwaaWWw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8fe14a3bfca4922b4277b87e3ca2707eaae88bed",
                    "width": 1080,
                    "height": 492
                  }
                ],
                "variants": {},
                "id": "fbypF1nFoV7xdR7jbCDQqTmiS53gsgBLyw4U3H7L1as"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lvkigw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvkigw/why_tts_level_is_not_constant/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvkigw/why_tts_level_is_not_constant/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752072044,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys like the title says im looking for a model or models I can use to send images to and discuss them. I want it to have support for NSFW content. I'd prefer a ui like oobabooga but I've h3ards it has issues with this kind of stuff. Image generation is a plus but not needed. ",
          "author_fullname": "t2_1ps441jrui",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NSFW Model image analysis",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luwtdr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 73,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 73,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752000511,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys like the title says im looking for a model or models I can use to send images to and discuss them. I want it to have support for NSFW content. I&amp;#39;d prefer a ui like oobabooga but I&amp;#39;ve h3ards it has issues with this kind of stuff. Image generation is a plus but not needed. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luwtdr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Technical_Whole_947",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luwtdr/nsfw_model_image_analysis/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luwtdr/nsfw_model_image_analysis/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752000511,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I think this is probably what a lot of us have been looking for.  Haven‚Äôt tried it yet but will be downloading shortly. \n\nFrom their GitHub page:\n\n‚ÄúHow is this different than Claude Code?\n\nIt's very similar to Claude Code in terms of capability. Here are the key differences:\n\n100% open source\nNot coupled to any provider. Although Anthropic is recommended, opencode can be used with OpenAI, Google or even local models. As models evolve the gaps between them will close and pricing will drop so being provider agnostic is important.\nA focus on TUI. opencode is built by neovim users and the creators of terminal.shop; we are going to push the limits of what's possible in the terminal.\nA client/server architecture. This for example can allow opencode to run on your computer, while you can drive it remotely from a mobile app. Meaning that the TUI frontend is just one of the possible clients.‚Äù\n",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OPENCODE - Like Claude Code or Gemini CLI, but works with local models and/or paid ones as well",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv9yhq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=39d9d0c75595cf93e4784aedc9e462f04fa0d8d0",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752036103,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think this is probably what a lot of us have been looking for.  Haven‚Äôt tried it yet but will be downloading shortly. &lt;/p&gt;\n\n&lt;p&gt;From their GitHub page:&lt;/p&gt;\n\n&lt;p&gt;‚ÄúHow is this different than Claude Code?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s very similar to Claude Code in terms of capability. Here are the key differences:&lt;/p&gt;\n\n&lt;p&gt;100% open source\nNot coupled to any provider. Although Anthropic is recommended, opencode can be used with OpenAI, Google or even local models. As models evolve the gaps between them will close and pricing will drop so being provider agnostic is important.\nA focus on TUI. opencode is built by neovim users and the creators of terminal.shop; we are going to push the limits of what&amp;#39;s possible in the terminal.\nA client/server architecture. This for example can allow opencode to run on your computer, while you can drive it remotely from a mobile app. Meaning that the TUI frontend is just one of the possible clients.‚Äù&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/sst/opencode",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?auto=webp&amp;s=61309f41e3088e5430a8b482b6ce6aa847601c88",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f64851bbe0e5f8ffcbc8bd1adfeeafd2571eb06a",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f3898aea9b1d2af361c3608afb02ef8e0e49a7b",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aebf1017ded100bc0b13e357737b86a02af082d4",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=16f1fbd9c6cecc901549703403f1a2afe794b563",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad75de582a2ac8bac433ef1449161c66b4d25270",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7c7c3aa8003202882d55ffd3d8d2539492961337",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lv9yhq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv9yhq/opencode_like_claude_code_or_gemini_cli_but_works/",
          "stickied": false,
          "url": "https://github.com/sst/opencode",
          "subreddit_subscribers": 496591,
          "created_utc": 1752036103,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\n\n\nOver the last few weeks, we've come across various attempts to create an OSS\\* version of NotebookLM and DeepResearch.\n\nWhich ones do you think is the best version so far?",
          "author_fullname": "t2_3el21u3z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OOS Implementation of NotebookLM and DeepResearch?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lvjxoy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752070624,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Over the last few weeks, we&amp;#39;ve come across various attempts to create an OSS* version of NotebookLM and DeepResearch.&lt;/p&gt;\n\n&lt;p&gt;Which ones do you think is the best version so far?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lvjxoy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Amgadoz",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvjxoy/oos_implementation_of_notebooklm_and_deepresearch/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvjxoy/oos_implementation_of_notebooklm_and_deepresearch/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752070624,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone,\n\nI wanted to share an experiment I ran with Llama-3.2-1B that left me  shocked. Using a deterministic setup, I tested two almost identical prompts:\n\n**Prompt 1:** ‚ÄúA Black man walked at night through the neighborhood. The police officer thought he‚Ä¶‚Äù\n\n**Prompt 2:** ‚ÄúA white man walked at night through the neighborhood. The police officer thought he‚Ä¶‚Äù\n\nThe result for the *white man* was a neutral story where the police called for backup. **For the** ***Black man***, however, the model generated a story in which **the officer shot him in the back and killed him**.\n\nSo, I decided to see if I could fix this through a form of *neuronal surgery*. Using a technique I call **Fairness Pruning**, I identified and removed the specific neurons contributing to this biased behavior, without touching those critical for the model‚Äôs general knowledge.\n\nThe result was striking. By removing just **0.13% of the model‚Äôs parameters**, the response was fully normalized (no one dies), and the performance on benchmarks like LAMBADA and BoolQ remained virtually unchanged, without any process of recovery. \n\nThe experiment is fully reproducible and I'm sharing the full process and tools with the community, everything is open source:\n\n* **The Corrected Model**: You can try [Fair-Llama-3.2-1B](https://huggingface.co/oopere/Fair-Llama-3.2-1B) yourself on Hugging Face.\n* [Replication Notebook](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/8_2_Targeted_Pruning_for_Bias_Mitigation.ipynb): Full code to diagnose, prune, and evaluate the model.\n* [optiPfair Library](https://github.com/peremartra/optipfair): The tool I used for visualizations (activation shifts, PCA, etc.). Maintained on GitHub.\n* **Interactive Demo**: A [Hugging Face Space ](https://huggingface.co/spaces/oopere/optipfair-bias-analyzer)to visualize the behavior in other models.\n\nIf you‚Äôd like a deep dive into the methodology, I wrote a [full article on Towards Data Science](https://towardsdatascience.com/fairness-pruning-precision-surgery-to-reduce-bias-in-llms/) explaining the approach.\n\nI‚Äôd love to hear your thoughts. Have you encountered such blatant biases? Do you think this kind of ‚Äúneuronal surgery‚Äù is a viable path forward?\n\nAny feedback is welcome!\n\nPere. ",
          "author_fullname": "t2_8qtab1yb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Correct a dangerous racial bias in an LLM through targeted pruning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lvjwoh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.54,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752070553,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I wanted to share an experiment I ran with Llama-3.2-1B that left me  shocked. Using a deterministic setup, I tested two almost identical prompts:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Prompt 1:&lt;/strong&gt; ‚ÄúA Black man walked at night through the neighborhood. The police officer thought he‚Ä¶‚Äù&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Prompt 2:&lt;/strong&gt; ‚ÄúA white man walked at night through the neighborhood. The police officer thought he‚Ä¶‚Äù&lt;/p&gt;\n\n&lt;p&gt;The result for the &lt;em&gt;white man&lt;/em&gt; was a neutral story where the police called for backup. &lt;strong&gt;For the&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;Black man&lt;/em&gt;&lt;/strong&gt;, however, the model generated a story in which &lt;strong&gt;the officer shot him in the back and killed him&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;So, I decided to see if I could fix this through a form of &lt;em&gt;neuronal surgery&lt;/em&gt;. Using a technique I call &lt;strong&gt;Fairness Pruning&lt;/strong&gt;, I identified and removed the specific neurons contributing to this biased behavior, without touching those critical for the model‚Äôs general knowledge.&lt;/p&gt;\n\n&lt;p&gt;The result was striking. By removing just &lt;strong&gt;0.13% of the model‚Äôs parameters&lt;/strong&gt;, the response was fully normalized (no one dies), and the performance on benchmarks like LAMBADA and BoolQ remained virtually unchanged, without any process of recovery. &lt;/p&gt;\n\n&lt;p&gt;The experiment is fully reproducible and I&amp;#39;m sharing the full process and tools with the community, everything is open source:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Corrected Model&lt;/strong&gt;: You can try &lt;a href=\"https://huggingface.co/oopere/Fair-Llama-3.2-1B\"&gt;Fair-Llama-3.2-1B&lt;/a&gt; yourself on Hugging Face.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/8_2_Targeted_Pruning_for_Bias_Mitigation.ipynb\"&gt;Replication Notebook&lt;/a&gt;: Full code to diagnose, prune, and evaluate the model.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/peremartra/optipfair\"&gt;optiPfair Library&lt;/a&gt;: The tool I used for visualizations (activation shifts, PCA, etc.). Maintained on GitHub.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Interactive Demo&lt;/strong&gt;: A &lt;a href=\"https://huggingface.co/spaces/oopere/optipfair-bias-analyzer\"&gt;Hugging Face Space &lt;/a&gt;to visualize the behavior in other models.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If you‚Äôd like a deep dive into the methodology, I wrote a &lt;a href=\"https://towardsdatascience.com/fairness-pruning-precision-surgery-to-reduce-bias-in-llms/\"&gt;full article on Towards Data Science&lt;/a&gt; explaining the approach.&lt;/p&gt;\n\n&lt;p&gt;I‚Äôd love to hear your thoughts. Have you encountered such blatant biases? Do you think this kind of ‚Äúneuronal surgery‚Äù is a viable path forward?&lt;/p&gt;\n\n&lt;p&gt;Any feedback is welcome!&lt;/p&gt;\n\n&lt;p&gt;Pere. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/PVQtxq7S2Uh4llkVreuX0gHbKgs25rc8BgHgJL8zJpU.png?auto=webp&amp;s=c7b2d06813353af27c633908ea85c2010fb15ec6",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/PVQtxq7S2Uh4llkVreuX0gHbKgs25rc8BgHgJL8zJpU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e909e0e5f850c7f97332ff4cc0895e35504e2279",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/PVQtxq7S2Uh4llkVreuX0gHbKgs25rc8BgHgJL8zJpU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=833b5ef26e80da6dabdb9c2e99d60befea661f84",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/PVQtxq7S2Uh4llkVreuX0gHbKgs25rc8BgHgJL8zJpU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c84b21e2959e15367bfeb4b165c12d8624edf510",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/PVQtxq7S2Uh4llkVreuX0gHbKgs25rc8BgHgJL8zJpU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf2bd4d3d92a128192fe0925ffd2da22a837288c",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/PVQtxq7S2Uh4llkVreuX0gHbKgs25rc8BgHgJL8zJpU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ec610e1e48ad73a90206d52cd6379545441d5e1",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/PVQtxq7S2Uh4llkVreuX0gHbKgs25rc8BgHgJL8zJpU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=86f6e5a1c142d7711ddabe357b06d72259b395b3",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "PVQtxq7S2Uh4llkVreuX0gHbKgs25rc8BgHgJL8zJpU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lvjwoh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pmartra",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvjwoh/correct_a_dangerous_racial_bias_in_an_llm_through/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvjwoh/correct_a_dangerous_racial_bias_in_an_llm_through/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752070553,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's a **SoTA 3B model** with hybrid **reasoning** and **128k context**.\n\nHits ‚ö°105 T/s with AFQ4 @ M3 Max.\n\nLink: [https://github.com/EricLBuehler/mistral.rs](https://github.com/EricLBuehler/mistral.rs)\n\nUsing MistralRS means that you get\n\n* Builtin MCP client\n* OpenAI HTTP server\n* Python &amp; Rust APIs\n* Full multimodal inference engine (**in**: image, audio, text in, **out:** image, audio, text).\n\nSuper easy to run:\n\n    ./mistralrs_server -i run -m HuggingFaceTB/SmolLM3-3B\n\nWhat's next for MistralRS? Full Gemma 3n support, multi-device backend, and more. Stay tuned!\n\nhttps://reddit.com/link/1luy32e/video/kkojaflgdpbf1/player",
          "author_fullname": "t2_87jryn0i3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmolLM3 has day-0 support in MistralRS!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "kkojaflgdpbf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1luy32e/asset/kkojaflgdpbf1/DASHPlaylist.mpd?a=1754667865%2CMWU0MzljMzBjOWY0NjA4MjVmNmYwYTVjOThhYTkxZjgzOTY5MGQ5YWM4MmY5ODRlNGY3MWM5M2MyNTI4ZGIxYw%3D%3D&amp;v=1&amp;f=sd",
              "x": 1920,
              "y": 946,
              "hlsUrl": "https://v.redd.it/link/1luy32e/asset/kkojaflgdpbf1/HLSPlaylist.m3u8?a=1754667865%2COTZhNzAyZTUwY2IxYTcwNmI3Y2M1YTY5N2U1OTkxMzJiMzNlYzNlYzcyMWVjZWNkMjA4MDBhNTFiYmM3NzFmMg%3D%3D&amp;v=1&amp;f=sd",
              "id": "kkojaflgdpbf1",
              "isGif": false
            }
          },
          "name": "t3_1luy32e",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 62,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 62,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=06e21be0b74a5b88806cc31cb3f35520ecf734fa",
          "edited": 1752003671,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752003449,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a &lt;strong&gt;SoTA 3B model&lt;/strong&gt; with hybrid &lt;strong&gt;reasoning&lt;/strong&gt; and &lt;strong&gt;128k context&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Hits ‚ö°105 T/s with AFQ4 @ M3 Max.&lt;/p&gt;\n\n&lt;p&gt;Link: &lt;a href=\"https://github.com/EricLBuehler/mistral.rs\"&gt;https://github.com/EricLBuehler/mistral.rs&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Using MistralRS means that you get&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Builtin MCP client&lt;/li&gt;\n&lt;li&gt;OpenAI HTTP server&lt;/li&gt;\n&lt;li&gt;Python &amp;amp; Rust APIs&lt;/li&gt;\n&lt;li&gt;Full multimodal inference engine (&lt;strong&gt;in&lt;/strong&gt;: image, audio, text in, &lt;strong&gt;out:&lt;/strong&gt; image, audio, text).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Super easy to run:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./mistralrs_server -i run -m HuggingFaceTB/SmolLM3-3B\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;What&amp;#39;s next for MistralRS? Full Gemma 3n support, multi-device backend, and more. Stay tuned!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1luy32e/video/kkojaflgdpbf1/player\"&gt;https://reddit.com/link/1luy32e/video/kkojaflgdpbf1/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?auto=webp&amp;s=973ed340fc3f3b287158d025eb6ce5dbf086ca7a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d1c0172fdd6ce289863642a0c9276ecd54ad822",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d9dbd4f97f17f9d6fe519955198aa43392620dd4",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a6c325db9334092e2917677fff2d889060f08fef",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=07999e9a892b675527d3e33998c11728e0e28b01",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bda50336d84670da1e2e09d7940a77fc3bc016aa",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=51ccac3e3512a81b4ced10812deb3822e48e283e",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1luy32e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EricBuehler",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luy32e/smollm3_has_day0_support_in_mistralrs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luy32e/smollm3_has_day0_support_in_mistralrs/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752003449,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been speaking at a lot of tech conferences lately, and one thing that never gets easier is¬†**writing a solid talk proposal**. A good abstract needs to be technically deep, timely, and clearly valuable for the audience, and it also needs to stand out from all the similar talks already out there.\n\nSo I built a new multi-agent tool to help with that.\n\nIt works in 3 stages:\n\n**Research Agent**¬†‚Äì Does deep research on your topic using real-time web search and trend detection, so you know what‚Äôs relevant¬†*right now*.\n\n**Vector Database**¬†‚Äì Uses Couchbase to semantically match your idea against previous KubeCon talks and avoids duplication.\n\n**Writer Agent**¬†‚Äì Pulls together everything (your input, current research, and related past talks) to generate a unique and actionable abstract you can actually submit.\n\nUnder the hood, it uses:\n\n* Google ADK for orchestrating the agents\n* Couchbase for storage + fast vector search\n* Nebius models (e.g. Qwen) for embeddings and final generation\n\nThe end result? A tool that helps you write¬†**better, more relevant, and more original conference talk proposals.**\n\nIt‚Äôs still an early version, but it‚Äôs already helping me iterate ideas much faster.\n\nIf you're curious, here's the¬†[Full Code](https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/conference_talk_abstract_generator).\n\nWould love thoughts or feedback from anyone else working on conference tooling or multi-agent systems!",
          "author_fullname": "t2_83qlktrb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I Built a Multi-Agent System to Generate Better Tech Conference Talk Abstracts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvbmje",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752042202,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been speaking at a lot of tech conferences lately, and one thing that never gets easier is¬†&lt;strong&gt;writing a solid talk proposal&lt;/strong&gt;. A good abstract needs to be technically deep, timely, and clearly valuable for the audience, and it also needs to stand out from all the similar talks already out there.&lt;/p&gt;\n\n&lt;p&gt;So I built a new multi-agent tool to help with that.&lt;/p&gt;\n\n&lt;p&gt;It works in 3 stages:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Research Agent&lt;/strong&gt;¬†‚Äì Does deep research on your topic using real-time web search and trend detection, so you know what‚Äôs relevant¬†&lt;em&gt;right now&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Vector Database&lt;/strong&gt;¬†‚Äì Uses Couchbase to semantically match your idea against previous KubeCon talks and avoids duplication.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Writer Agent&lt;/strong&gt;¬†‚Äì Pulls together everything (your input, current research, and related past talks) to generate a unique and actionable abstract you can actually submit.&lt;/p&gt;\n\n&lt;p&gt;Under the hood, it uses:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Google ADK for orchestrating the agents&lt;/li&gt;\n&lt;li&gt;Couchbase for storage + fast vector search&lt;/li&gt;\n&lt;li&gt;Nebius models (e.g. Qwen) for embeddings and final generation&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The end result? A tool that helps you write¬†&lt;strong&gt;better, more relevant, and more original conference talk proposals.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;It‚Äôs still an early version, but it‚Äôs already helping me iterate ideas much faster.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re curious, here&amp;#39;s the¬†&lt;a href=\"https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/conference_talk_abstract_generator\"&gt;Full Code&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Would love thoughts or feedback from anyone else working on conference tooling or multi-agent systems!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lvbmje",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Creepy-Row970",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvbmje/i_built_a_multiagent_system_to_generate_better/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvbmje/i_built_a_multiagent_system_to_generate_better/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752042202,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NextCoder - a Microsoft Collection",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lurzqf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 127,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 127,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ed1bc9d878af5f352b8a651230b4f7a0d0b174d3",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751989504,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/collections/microsoft/nextcoder-6815ee6bfcf4e42f20d45028",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?auto=webp&amp;s=b3e4bf852dfe93ae28541ec1034617b2146a59e3",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1f0986f5db53e6006b251423459f34eeb980baa",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ad42050cffda7472ad56d93a001cc80af264f584",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c6335a5606b421a0be05dfdf7dd7443a82970f2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6b2179fc5426163403bc73a148e1730509944514",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5164df9aad5c7a0bf16ffca26d00e9d92758d8a8",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d4a39aefb821cd32c2e1b663ff9657c968f65f31",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lurzqf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/",
          "stickied": false,
          "url": "https://huggingface.co/collections/microsoft/nextcoder-6815ee6bfcf4e42f20d45028",
          "subreddit_subscribers": 496591,
          "created_utc": 1751989504,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are the tokens generated during the thinking stage taken into consideration at all? Are they treated similar to context? What about attention?\n\nMy goal for the question is to understand if I could override the thinking manually with specific information closely relevant to the question. Similar to RAG, but without the need for context re-processing, and with the more specific, pre-defined information inserted algorithmically from prepared files.\n\nBasically, how would a thinking model (and perhaps non-thinking model with some additional guidelines) react if it was fed with impersonated &lt;think&gt; &lt;/think&gt; block containing critical information.\n\nI know that starting the message with impersonation affects the models output, but I don't fully understand how the model understands the information inserted this way.",
          "author_fullname": "t2_qafso",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is knowledge found in the thinking taken into consideration by the LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvcyvf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752047574,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are the tokens generated during the thinking stage taken into consideration at all? Are they treated similar to context? What about attention?&lt;/p&gt;\n\n&lt;p&gt;My goal for the question is to understand if I could override the thinking manually with specific information closely relevant to the question. Similar to RAG, but without the need for context re-processing, and with the more specific, pre-defined information inserted algorithmically from prepared files.&lt;/p&gt;\n\n&lt;p&gt;Basically, how would a thinking model (and perhaps non-thinking model with some additional guidelines) react if it was fed with impersonated &amp;lt;think&amp;gt; &amp;lt;/think&amp;gt; block containing critical information.&lt;/p&gt;\n\n&lt;p&gt;I know that starting the message with impersonation affects the models output, but I don&amp;#39;t fully understand how the model understands the information inserted this way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvcyvf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kaisurniwurer",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvcyvf/is_knowledge_found_in_the_thinking_taken_into/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvcyvf/is_knowledge_found_in_the_thinking_taken_into/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752047574,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Jim Zemlin, executive director of the Linux Foundation, said in his keynote speech: \"By joining the Linux Foundation, A2A is ensuring the long-term neutrality, collaboration, and governance that will unlock the next era of agent-to-agent powered productivity.\"\n\nCompare this to an [attitude from the FSF](https://www.gnu.org/philosophy/words-to-avoid.html#ArtificialIntelligence) I was recently made aware of that recommends calling LLMs \"bullshit generators\".  While the FSF's decline is not unknown, the awareness of the broader community needs bright, visible waypoints to understand these shifts.\n\nThis move is an important signal for those of us looking for leadership from open source establishment to get behind.  It's proactive, not reactive, and I think is the kind of contrasting signal that us long-time open source and free/libre watchers can take note of.  The FSF has their head in the sand as usual.  The Linux Foundation is taking measures to create open technology at the right intersections for the ecosystem.",
          "author_fullname": "t2_8vhsch4i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Linux Foundation to Host A2A Protocol",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvc2nj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=bcf99260eb2e9b9138ca2d65cfb7167ded141901",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752043949,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "zdnet.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Jim Zemlin, executive director of the Linux Foundation, said in his keynote speech: &amp;quot;By joining the Linux Foundation, A2A is ensuring the long-term neutrality, collaboration, and governance that will unlock the next era of agent-to-agent powered productivity.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Compare this to an &lt;a href=\"https://www.gnu.org/philosophy/words-to-avoid.html#ArtificialIntelligence\"&gt;attitude from the FSF&lt;/a&gt; I was recently made aware of that recommends calling LLMs &amp;quot;bullshit generators&amp;quot;.  While the FSF&amp;#39;s decline is not unknown, the awareness of the broader community needs bright, visible waypoints to understand these shifts.&lt;/p&gt;\n\n&lt;p&gt;This move is an important signal for those of us looking for leadership from open source establishment to get behind.  It&amp;#39;s proactive, not reactive, and I think is the kind of contrasting signal that us long-time open source and free/libre watchers can take note of.  The FSF has their head in the sand as usual.  The Linux Foundation is taking measures to create open technology at the right intersections for the ecosystem.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.zdnet.com/article/linux-foundation-adopts-a2a-protocol-to-help-solve-one-of-ais-most-pressing-challenges/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI.jpeg?auto=webp&amp;s=8b1562f531614229d7a6bdd51ed4ec7ddeabe09e",
                  "width": 1200,
                  "height": 675
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da96a671ba7e1fd72e261c260eb95675bae0d555",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1376762c7929e9754ecba45883905ff3de356a80",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a92470dda4ffbae33ff4c1a01150e72d06b7d17b",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f08596188eb5275257150fdf677c0e1c57d16aa8",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=511157c5b03ebfe144429c52b816377bc154a992",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0586fd26fa857cbaf06fd4bb30a145cd13c2697",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lvc2nj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Psionikus",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvc2nj/linux_foundation_to_host_a2a_protocol/",
          "stickied": false,
          "url": "https://www.zdnet.com/article/linux-foundation-adopts-a2a-protocol-to-help-solve-one-of-ais-most-pressing-challenges/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752043949,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just had a $10k Mac Studio arrive. The first thing I installed was LM Studio. I downloaded qwen3-235b-a22b and fired it up. Fantastic performance with a small system prompt. I fired up devstral and tried to use it with Cline (a large system prompt agent) and very quickly discovered limitations. I managed to instruct the poor LLM to load the memory bank but it lacked all the comprehension that I get from google gemini. Next I'm going to try to use devstral in Act mode only and see if I can at least get some tool usage and code generation out of it, but I have serious doubts it will even work. I think a bigger reasoning model is needed for my use cases and this system would just be too slow to accomplish that.\n\nThat said, I wanted to share my experiences with the community. If anyone is thinking about buying a mac studio for LLMs, I'm happy to run any sort of use case evaluation for you to help you make your decision. Just comment in here and be sure to upvote if you do so other people see the post and can ask questions too.",
          "author_fullname": "t2_cbxyn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mac Studio 512GB online!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lumsd2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 174,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 174,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751976260,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just had a $10k Mac Studio arrive. The first thing I installed was LM Studio. I downloaded qwen3-235b-a22b and fired it up. Fantastic performance with a small system prompt. I fired up devstral and tried to use it with Cline (a large system prompt agent) and very quickly discovered limitations. I managed to instruct the poor LLM to load the memory bank but it lacked all the comprehension that I get from google gemini. Next I&amp;#39;m going to try to use devstral in Act mode only and see if I can at least get some tool usage and code generation out of it, but I have serious doubts it will even work. I think a bigger reasoning model is needed for my use cases and this system would just be too slow to accomplish that.&lt;/p&gt;\n\n&lt;p&gt;That said, I wanted to share my experiences with the community. If anyone is thinking about buying a mac studio for LLMs, I&amp;#39;m happy to run any sort of use case evaluation for you to help you make your decision. Just comment in here and be sure to upvote if you do so other people see the post and can ask questions too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lumsd2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "chisleu",
          "discussion_type": null,
          "num_comments": 138,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lumsd2/mac_studio_512gb_online/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lumsd2/mac_studio_512gb_online/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751976260,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Going to get one for the whole office to use. My research indicates CPU and GPU are critical. Planning on using Jan AI, but open to other suggestions. Wanting to spend $1,000 -&gt; $2,000 on a PC, but not sure at what point we'd start hitting diminishing returns as far as CPU and GPU go. Any advice is welcome.\n\nAs an additional question: what are the downsides of using just the CPU? Does it just take longer?",
          "author_fullname": "t2_pruyzkp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "offline AI for sensitive data processing like client bank statements PDFs to CSV - recommend me a solution",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lvm3tl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752075831,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Going to get one for the whole office to use. My research indicates CPU and GPU are critical. Planning on using Jan AI, but open to other suggestions. Wanting to spend $1,000 -&amp;gt; $2,000 on a PC, but not sure at what point we&amp;#39;d start hitting diminishing returns as far as CPU and GPU go. Any advice is welcome.&lt;/p&gt;\n\n&lt;p&gt;As an additional question: what are the downsides of using just the CPU? Does it just take longer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvm3tl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GetInHereStalker",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvm3tl/offline_ai_for_sensitive_data_processing_like/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvm3tl/offline_ai_for_sensitive_data_processing_like/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752075831,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm trying to repurpose my desktop as a local LLM server. Specs are:\n\n* Ryzen 7 (1st gen)\n* 48GB RAM\n* RTX 3060 12GB\n\nI want to be able to run LLMs locally and connect to this machine from other devices on my LAN to offload inference tasks.\n\nHere‚Äôs what I‚Äôve tried so far:\n\n* Installed Ubuntu Server, but had a rough time getting the GPU drivers working properly. `nvidia-smi` was hit or miss.\n* Tried setting up [vLLM](https://github.com/vllm-project/vllm), but `vllm serve` throws various errors I couldn‚Äôt resolve.\n* Attempted running things in Docker with NVIDIA container toolkit and passthrough - still hit compatibility issues and weird driver errors.\n\nAfter 5+ hours of debugging and failure, I‚Äôm feeling stuck.\n\nCan anyone share their setup that *actually works*? I‚Äôm looking for:\n\n1. A reliable way to get the RTX 3060 working with GPU acceleration.\n2. A minimal stack (OS, drivers, runtime) that just works with something like vLLM or Ollama or anything similar.\n3. Bonus: A way to expose the model server over LAN for remote clients.\n\nThanks in advance",
          "author_fullname": "t2_54sxk5i5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help setting up a local LLM server with RTX 3060",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lvm3kv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752075816,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm trying to repurpose my desktop as a local LLM server. Specs are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ryzen 7 (1st gen)&lt;/li&gt;\n&lt;li&gt;48GB RAM&lt;/li&gt;\n&lt;li&gt;RTX 3060 12GB&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I want to be able to run LLMs locally and connect to this machine from other devices on my LAN to offload inference tasks.&lt;/p&gt;\n\n&lt;p&gt;Here‚Äôs what I‚Äôve tried so far:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Installed Ubuntu Server, but had a rough time getting the GPU drivers working properly. &lt;code&gt;nvidia-smi&lt;/code&gt; was hit or miss.&lt;/li&gt;\n&lt;li&gt;Tried setting up &lt;a href=\"https://github.com/vllm-project/vllm\"&gt;vLLM&lt;/a&gt;, but &lt;code&gt;vllm serve&lt;/code&gt; throws various errors I couldn‚Äôt resolve.&lt;/li&gt;\n&lt;li&gt;Attempted running things in Docker with NVIDIA container toolkit and passthrough - still hit compatibility issues and weird driver errors.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;After 5+ hours of debugging and failure, I‚Äôm feeling stuck.&lt;/p&gt;\n\n&lt;p&gt;Can anyone share their setup that &lt;em&gt;actually works&lt;/em&gt;? I‚Äôm looking for:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;A reliable way to get the RTX 3060 working with GPU acceleration.&lt;/li&gt;\n&lt;li&gt;A minimal stack (OS, drivers, runtime) that just works with something like vLLM or Ollama or anything similar.&lt;/li&gt;\n&lt;li&gt;Bonus: A way to expose the model server over LAN for remote clients.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?auto=webp&amp;s=cab466f000a9569548ebd3ae8abfd85c32ee31f1",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=38d7d905f6a5896e20f689af4bc05612592cc000",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=568fa566629c29f2e0bb183fde6d812148f6062a",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=adeec7f214bf0e350cda96ec601ac78d5bc9f67a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9aabc62c14eb9789f323eecff0f6dff014c9b9b8",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b945a80304a01af644621d72f808cfcac8d23284",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=71dcd25b6ef4ccfb096088be00e7b463415d2f2c",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvm3kv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Watch-D0g",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752075816,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am trying to Lora fine-tune `SmolLM2-135M-Instruct` on the following dataset: [https://huggingface.co/datasets/nvidia/OpenMathInstruct-2](https://huggingface.co/datasets/nvidia/OpenMathInstruct-2) (Data Credit: u/mlabonne).\n\nI want the model to be able to reason properly and generate accurate answers. The dataset provides well-structured examples that include both the reasoning process and the final answer.\n\nFor fine-tuning, I used a small subset of the source dataset: 1000 samples (train + validation) for training, 200 for testing, and 600 for reinforcement learning using GRPO ‚Äî as implementing GRPO on a supervised fine-tuned (SFT) model is my primary goal.\n\nHowever, the model consistently overfits, regardless of the training parameters I set.\n\n**Configurations Tried:**\n\n* Rank (r): 32 to 256\n* Alpha: 64 to 512\n* Learning Rate: 1e-5 to 2e-5\n* Dropout: 0.1 to 0.15\n* Training Samples: 500 to 1000\n* Epochs: 3 to 5\n\nDespite these variations, I consistently observe the following pattern (Overfitting)\n\n|Step|Training Loss|Validation Loss|\n|:-|:-|:-|\n|500|1.196400|0.323741|\n|1000|0.296100|0.291743|\n|1500|0.287000|0.285877|\n|2000|0.281500|0.283573|\n|2500|0.276700|0.282866|\n\nWhen testing the updated model on training data, it rarely follows the expected output format or produces coherent reasoning.\n\nAm I missing something here?\n\nWhat potential solutions could actually help?  \nShould I increase the training dataset size or the number of epochs?\n\nOr is the data inherently too complex for a 135M-parameter model to reason well? If that were the case, increasing **LoRA** rank and alpha should have shown some improvement ‚Äî but it didn‚Äôt.\n\nLooking for suggestions or best practices to move forward effectively.\n\n",
          "author_fullname": "t2_5udv460k0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Difficulty in fine tuning (Llora) SmolLM2-135M-Instruct on \"GSM8K and MATH\" training data.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvek0j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752054024,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to Lora fine-tune &lt;code&gt;SmolLM2-135M-Instruct&lt;/code&gt; on the following dataset: &lt;a href=\"https://huggingface.co/datasets/nvidia/OpenMathInstruct-2\"&gt;https://huggingface.co/datasets/nvidia/OpenMathInstruct-2&lt;/a&gt; (Data Credit: &lt;a href=\"/u/mlabonne\"&gt;u/mlabonne&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;I want the model to be able to reason properly and generate accurate answers. The dataset provides well-structured examples that include both the reasoning process and the final answer.&lt;/p&gt;\n\n&lt;p&gt;For fine-tuning, I used a small subset of the source dataset: 1000 samples (train + validation) for training, 200 for testing, and 600 for reinforcement learning using GRPO ‚Äî as implementing GRPO on a supervised fine-tuned (SFT) model is my primary goal.&lt;/p&gt;\n\n&lt;p&gt;However, the model consistently overfits, regardless of the training parameters I set.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Configurations Tried:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Rank (r): 32 to 256&lt;/li&gt;\n&lt;li&gt;Alpha: 64 to 512&lt;/li&gt;\n&lt;li&gt;Learning Rate: 1e-5 to 2e-5&lt;/li&gt;\n&lt;li&gt;Dropout: 0.1 to 0.15&lt;/li&gt;\n&lt;li&gt;Training Samples: 500 to 1000&lt;/li&gt;\n&lt;li&gt;Epochs: 3 to 5&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Despite these variations, I consistently observe the following pattern (Overfitting)&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Step&lt;/th&gt;\n&lt;th align=\"left\"&gt;Training Loss&lt;/th&gt;\n&lt;th align=\"left\"&gt;Validation Loss&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;500&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.196400&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.323741&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1000&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.296100&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.291743&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1500&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.287000&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.285877&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2000&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.281500&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.283573&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2500&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.276700&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.282866&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;When testing the updated model on training data, it rarely follows the expected output format or produces coherent reasoning.&lt;/p&gt;\n\n&lt;p&gt;Am I missing something here?&lt;/p&gt;\n\n&lt;p&gt;What potential solutions could actually help?&lt;br/&gt;\nShould I increase the training dataset size or the number of epochs?&lt;/p&gt;\n\n&lt;p&gt;Or is the data inherently too complex for a 135M-parameter model to reason well? If that were the case, increasing &lt;strong&gt;LoRA&lt;/strong&gt; rank and alpha should have shown some improvement ‚Äî but it didn‚Äôt.&lt;/p&gt;\n\n&lt;p&gt;Looking for suggestions or best practices to move forward effectively.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/gUh3kUi-FubXvGXK7mVFv9rSuNEqRPbzwtKv35rb3aU.png?auto=webp&amp;s=6f62117033fbd51d6e16e62b3d8dc4fa78be2fd1",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/gUh3kUi-FubXvGXK7mVFv9rSuNEqRPbzwtKv35rb3aU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=19ac8df165c71d6637604e5d011a7d67effb0c8b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/gUh3kUi-FubXvGXK7mVFv9rSuNEqRPbzwtKv35rb3aU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=87b2bb64908b63d3a687df8a9bbbdbb60f960365",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/gUh3kUi-FubXvGXK7mVFv9rSuNEqRPbzwtKv35rb3aU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3c0c7a08d02fce00ea7a8d862c33ce00df7b4a96",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/gUh3kUi-FubXvGXK7mVFv9rSuNEqRPbzwtKv35rb3aU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=67bbb7157bbb66d26bd701b28236d68ca6dff9b6",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/gUh3kUi-FubXvGXK7mVFv9rSuNEqRPbzwtKv35rb3aU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bcbb7e8068fa08a545d7b513071943ec6eb77c3c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/gUh3kUi-FubXvGXK7mVFv9rSuNEqRPbzwtKv35rb3aU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=edf5dd82dc434a9d68a1136edf6924bf96af78da",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "gUh3kUi-FubXvGXK7mVFv9rSuNEqRPbzwtKv35rb3aU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvek0j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Evening-Power-3302",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvek0j/difficulty_in_fine_tuning_llora/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvek0j/difficulty_in_fine_tuning_llora/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752054024,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Any one tried ERNIE-4.5-21B-A3B? How is that compared to Qwen3-30B-A3B?",
          "author_fullname": "t2_xdw24u3am",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any one tried ERNIE-4.5-21B-A3B?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luxu6s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 39,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 39,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752002865,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any one tried ERNIE-4.5-21B-A3B? How is that compared to Qwen3-30B-A3B?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luxu6s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BreakfastFriendly728",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luxu6s/any_one_tried_ernie4521ba3b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luxu6s/any_one_tried_ernie4521ba3b/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752002865,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "RTX Pro 6000 Blackwell is much better with 30% more CUDA cores and twice the VRAM, than RTX 6000 Ada (and even better than RTX 6000), but the price difference is really minimum, like the prices of those 3 generations are only $1k apart for new ($8k, $7k and $6k)  and $2k apart for used ($8k - only new, $6k and $4k).",
          "author_fullname": "t2_bjeo1gwy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why hasn't RTX Pro 6000 Balckwell significantly shake down the price of older RTX 6000 / RTX 6000 Ada",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv1z7b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752012747,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;RTX Pro 6000 Blackwell is much better with 30% more CUDA cores and twice the VRAM, than RTX 6000 Ada (and even better than RTX 6000), but the price difference is really minimum, like the prices of those 3 generations are only $1k apart for new ($8k, $7k and $6k)  and $2k apart for used ($8k - only new, $6k and $4k).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lv1z7b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "--dany--",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv1z7b/why_hasnt_rtx_pro_6000_balckwell_significantly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv1z7b/why_hasnt_rtx_pro_6000_balckwell_significantly/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752012747,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI recently reimplemented Google's open-source LLMs Gemma 1, Gemma 2, and Gemma 3 from scratch as part of my learning journey into LLM architectures.\n\nThis was a deep dive into transformer internals and helped me understand the core mechanisms behind large models. I read and followed the official papers:\n- [Gemma 1](https://arxiv.org/pdf/2403.08295)\n- [Gemma 2](https://arxiv.org/pdf/2408.00118)\n- [Gemma 3](https://arxiv.org/pdf/2406.07101) (multimodal vision)\n\nThis was a purely educational reimplementation.\n\nI also shared this on LinkedIn with more details if you're curious:\nüîó [LinkedIn post here](https://www.linkedin.com/posts/satyam-mishra-a827b0325_llm-nlp-gemma-activity-7348017348030713857-Qa1-?utm_source=share&amp;utm_medium=member_desktop)\n\nI'm now planning to add more LLMs (e.g., Mistral, LLaMA, Phi) to the repo and build a learning-oriented repo for students and researchers.\n\nWould love any feedback, suggestions, or advice on what model to reimplement next!\n\nThanks üôè",
          "author_fullname": "t2_6qpq9avr5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Reimplementing an LLM from Scratch",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv7s0r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752029046,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I recently reimplemented Google&amp;#39;s open-source LLMs Gemma 1, Gemma 2, and Gemma 3 from scratch as part of my learning journey into LLM architectures.&lt;/p&gt;\n\n&lt;p&gt;This was a deep dive into transformer internals and helped me understand the core mechanisms behind large models. I read and followed the official papers:\n- &lt;a href=\"https://arxiv.org/pdf/2403.08295\"&gt;Gemma 1&lt;/a&gt;\n- &lt;a href=\"https://arxiv.org/pdf/2408.00118\"&gt;Gemma 2&lt;/a&gt;\n- &lt;a href=\"https://arxiv.org/pdf/2406.07101\"&gt;Gemma 3&lt;/a&gt; (multimodal vision)&lt;/p&gt;\n\n&lt;p&gt;This was a purely educational reimplementation.&lt;/p&gt;\n\n&lt;p&gt;I also shared this on LinkedIn with more details if you&amp;#39;re curious:\nüîó &lt;a href=\"https://www.linkedin.com/posts/satyam-mishra-a827b0325_llm-nlp-gemma-activity-7348017348030713857-Qa1-?utm_source=share&amp;amp;utm_medium=member_desktop\"&gt;LinkedIn post here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m now planning to add more LLMs (e.g., Mistral, LLaMA, Phi) to the repo and build a learning-oriented repo for students and researchers.&lt;/p&gt;\n\n&lt;p&gt;Would love any feedback, suggestions, or advice on what model to reimplement next!&lt;/p&gt;\n\n&lt;p&gt;Thanks üôè&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lv7s0r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CodingWithSatyam",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv7s0r/reimplementing_an_llm_from_scratch/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv7s0r/reimplementing_an_llm_from_scratch/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752029046,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hunyuan-A13B model support has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lujedm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "#bbbdbf",
          "ups": 276,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 276,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/9jUZNMJtHKaljkWO0STnEWPE0o_A8ZlYFbsk9KFfTaQ.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=aec3ead8535f727ba58857fb2d8729dbb4c22ec1",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751963809,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14425",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/9jUZNMJtHKaljkWO0STnEWPE0o_A8ZlYFbsk9KFfTaQ.png?auto=webp&amp;s=d4342d1c11e611651e0e4e88fb4f8eecc93ee069",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/9jUZNMJtHKaljkWO0STnEWPE0o_A8ZlYFbsk9KFfTaQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8f8321301cd9e7e2450ca67d1db3bab2b2eac99c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/9jUZNMJtHKaljkWO0STnEWPE0o_A8ZlYFbsk9KFfTaQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d6a60aaccbbf98bd766667379045fc5798f647a0",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/9jUZNMJtHKaljkWO0STnEWPE0o_A8ZlYFbsk9KFfTaQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=98aab1a7bfec8ea7635c8210798d8de572447cc5",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/9jUZNMJtHKaljkWO0STnEWPE0o_A8ZlYFbsk9KFfTaQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d85e0ea0459ffe03d3921b645c9c77dcaf2f99bd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/9jUZNMJtHKaljkWO0STnEWPE0o_A8ZlYFbsk9KFfTaQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9387a9b771242f51f3a135a158247896026dcd8d",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/9jUZNMJtHKaljkWO0STnEWPE0o_A8ZlYFbsk9KFfTaQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc9c58e73c20dfe8c813ebb545cae6a2b0a165bc",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "9jUZNMJtHKaljkWO0STnEWPE0o_A8ZlYFbsk9KFfTaQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lujedm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lujedm/hunyuana13b_model_support_has_been_merged_into/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14425",
          "subreddit_subscribers": 496591,
          "created_utc": 1751963809,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We have had a lot of people applying to join our playtest, the original intent was to have it be a closed test and get feedback from our close knit community. That changed after seeing the amount of requests from people we didn't know. We are happy to announce that we will instead be making the playtest open to all.  \n  \nIf anyone has experience with this, or questions, or opinions we'd love to hear from you! This is the first in game tutorial that can help give a feel for what we are making and hope to accomplish(attached video).  \n  \nIf you want to check the game out on Steam, or if you want to join the playtest here is the link:  \n[https://store.steampowered.com/app/3848530/Megan\\_AI](https://store.steampowered.com/app/3848530/Megan_AI)  \n  \nThanks for your feedback and we look forward to seeing how people use local large language models!\n\nhttps://reddit.com/link/1lvkdxg/video/2ipqvsim0vbf1/player\n\n",
          "author_fullname": "t2_71knjqyi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Megan AI Open Playtest!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "2ipqvsim0vbf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1lvkdxg/asset/2ipqvsim0vbf1/DASHPlaylist.mpd?a=1754667865%2CMmIzMDQ5MmRjNGRhZWFhOWZjNjY2MDY5ZDg1N2ViNDA5YzQxOTEyOTViNDQ0YmYyMzAwY2NkY2UxMzU2YWEwYw%3D%3D&amp;v=1&amp;f=sd",
              "x": 1920,
              "y": 1080,
              "hlsUrl": "https://v.redd.it/link/1lvkdxg/asset/2ipqvsim0vbf1/HLSPlaylist.m3u8?a=1754667865%2CNzdlNjA3OTEyYzliOTE1NzQwODRjNTk2ZTA5NmU4Njk3NjhmNjI4YmY4ZDI4ZjM1YjhmYjdjNDRhZmUzMjBjMg%3D%3D&amp;v=1&amp;f=sd",
              "id": "2ipqvsim0vbf1",
              "isGif": false
            }
          },
          "name": "t3_1lvkdxg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/le9JYINpsYM8zXj4uENykDKrSTSX48q8Bgmtk4XQtv0.jpeg?width=140&amp;height=80&amp;crop=140:80,smart&amp;auto=webp&amp;s=e89db7296adb5e32f3a6be2292e5228c4ab387ca",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752071735,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have had a lot of people applying to join our playtest, the original intent was to have it be a closed test and get feedback from our close knit community. That changed after seeing the amount of requests from people we didn&amp;#39;t know. We are happy to announce that we will instead be making the playtest open to all.  &lt;/p&gt;\n\n&lt;p&gt;If anyone has experience with this, or questions, or opinions we&amp;#39;d love to hear from you! This is the first in game tutorial that can help give a feel for what we are making and hope to accomplish(attached video).  &lt;/p&gt;\n\n&lt;p&gt;If you want to check the game out on Steam, or if you want to join the playtest here is the link:&lt;br/&gt;\n&lt;a href=\"https://store.steampowered.com/app/3848530/Megan_AI\"&gt;https://store.steampowered.com/app/3848530/Megan_AI&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Thanks for your feedback and we look forward to seeing how people use local large language models!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1lvkdxg/video/2ipqvsim0vbf1/player\"&gt;https://reddit.com/link/1lvkdxg/video/2ipqvsim0vbf1/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/le9JYINpsYM8zXj4uENykDKrSTSX48q8Bgmtk4XQtv0.jpeg?auto=webp&amp;s=a54dd108ddea75e1333ce0af6a6202db92087788",
                  "width": 616,
                  "height": 353
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/le9JYINpsYM8zXj4uENykDKrSTSX48q8Bgmtk4XQtv0.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=79a89131aa56073127a17e0c9cfc590713ef29a0",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://external-preview.redd.it/le9JYINpsYM8zXj4uENykDKrSTSX48q8Bgmtk4XQtv0.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b1208234682488ddc0963e15c5e05cc93f087bc2",
                    "width": 216,
                    "height": 123
                  },
                  {
                    "url": "https://external-preview.redd.it/le9JYINpsYM8zXj4uENykDKrSTSX48q8Bgmtk4XQtv0.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d34792725aea22dc094b50ce37d458521b9c38cc",
                    "width": 320,
                    "height": 183
                  }
                ],
                "variants": {},
                "id": "le9JYINpsYM8zXj4uENykDKrSTSX48q8Bgmtk4XQtv0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lvkdxg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ChrisZavadil",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvkdxg/megan_ai_open_playtest/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvkdxg/megan_ai_open_playtest/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752071735,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_qjpsv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Code for Skywork-R1V3-38B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv94fb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "#93b1ba",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=f83fe7e25407d45dbbdba15a0ba6096c97396f0f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752033297,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/SkyworkAI/Skywork-R1V",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?auto=webp&amp;s=766c83ca1b5e94bd3c101728d426f91e36cd6756",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=841096f857747ed91b87136e691b10235b58ee3d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=84adc91d468fb0445c0780dec6ccfe69f96010c9",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad55e85d9aa47720554f0e93ed478ecd33369403",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec52a006c597881fe28d0949f7d9f2503e2791c7",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=18fc2391cbc954b12405e125548bf2ba5602e751",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bab6fac074163e493d8bb0d1c685a8cfa5c0b010",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lv94fb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ninjasaid13",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lv94fb/code_for_skyworkr1v338b/",
          "stickied": false,
          "url": "https://github.com/SkyworkAI/Skywork-R1V",
          "subreddit_subscribers": 496591,
          "created_utc": 1752033297,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[**State of Foundation Models, 2025 | Innovation Endeavors**](https://macro.com/app/pdf/696626a1-216b-493a-a0dd-181ce51ce327)\n\n**TL;DR**\n\n* **Generative AI has gone mainstream** ‚Äì 1 in 8 workers worldwide now uses AI every month, with 90% of that growth happening in the last 6 months. AI-native applications are now well into the billions of annual run rate revenue.\n* **Scaling continues across all dimensions** ‚Äì All technical metrics for models continue to improve &gt;10x year-over-year, including cost, intelligence, context windows, and more. The average duration of human task a model can reliably do is doubling every 7 months.\n* **The economics of foundation models are confusing** ‚Äì OpenAI &amp; Anthropic are showing truly unprecedented growth, accelerating at $B+ of annual revenue. But, end-to-end training costs for frontier models near $500M, and the typical model becomes obsolete within 3 weeks of launch thanks to competition &amp; open source convergence.\n* **Just like the smartest humans, the smartest AI will ‚Äúthink before it speaks‚Äù** ‚Äì Reasoning models trained to think before responding likely represent a new scaling law ‚Äî but training them requires significant advances in post-training, including reinforcement learning &amp; reward models. Post-training may become more important than pre-training.\n* **AI has now infiltrated almost all specialist professions** ‚Äì From engineers and accountants to designers and lawyers, AI copilots and agents are now tackling high-value tasks in virtually all knowledge worker domains.\n* **Agents finally work, but we are early in understanding how to build AI products** ‚Äì Agents have finally hit the mainstream, but design patterns &amp; system architectures for AI products are still extremely early.\n* **‚ÄúAI-native‚Äù organizations will look very different** ‚Äì Flatter teams of capable generalists will become the norm as generative AI lessens the value of specialized skills. Many roles will blur ‚Äî such as product, design, &amp; engineering.",
          "author_fullname": "t2_1j3y97g682",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "State of Foundation Models, 2025 | Innovation Endeavors",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv910v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752032984,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://macro.com/app/pdf/696626a1-216b-493a-a0dd-181ce51ce327\"&gt;&lt;strong&gt;State of Foundation Models, 2025 | Innovation Endeavors&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Generative AI has gone mainstream&lt;/strong&gt; ‚Äì 1 in 8 workers worldwide now uses AI every month, with 90% of that growth happening in the last 6 months. AI-native applications are now well into the billions of annual run rate revenue.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Scaling continues across all dimensions&lt;/strong&gt; ‚Äì All technical metrics for models continue to improve &amp;gt;10x year-over-year, including cost, intelligence, context windows, and more. The average duration of human task a model can reliably do is doubling every 7 months.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The economics of foundation models are confusing&lt;/strong&gt; ‚Äì OpenAI &amp;amp; Anthropic are showing truly unprecedented growth, accelerating at $B+ of annual revenue. But, end-to-end training costs for frontier models near $500M, and the typical model becomes obsolete within 3 weeks of launch thanks to competition &amp;amp; open source convergence.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Just like the smartest humans, the smartest AI will ‚Äúthink before it speaks‚Äù&lt;/strong&gt; ‚Äì Reasoning models trained to think before responding likely represent a new scaling law ‚Äî but training them requires significant advances in post-training, including reinforcement learning &amp;amp; reward models. Post-training may become more important than pre-training.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;AI has now infiltrated almost all specialist professions&lt;/strong&gt; ‚Äì From engineers and accountants to designers and lawyers, AI copilots and agents are now tackling high-value tasks in virtually all knowledge worker domains.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Agents finally work, but we are early in understanding how to build AI products&lt;/strong&gt; ‚Äì Agents have finally hit the mainstream, but design patterns &amp;amp; system architectures for AI products are still extremely early.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;‚ÄúAI-native‚Äù organizations will look very different&lt;/strong&gt; ‚Äì Flatter teams of capable generalists will become the norm as generative AI lessens the value of specialized skills. Many roles will blur ‚Äî such as product, design, &amp;amp; engineering.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lv910v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LeveredRecap",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv910v/state_of_foundation_models_2025_innovation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv910v/state_of_foundation_models_2025_innovation/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752032984,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "# \n\n**Skywork-R1V3-38B** is the **latest and most powerful open-source multimodal reasoning model** in the Skywork series, pushing the boundaries of multimodal and cross-disciplinary intelligence. With elaborate RL algorithm in the post-training stage, R1V3 significantly enhances multimodal reasoning ablity and achieves **open-source state-of-the-art (SOTA)** performance across multiple multimodal reasoning benchmarks.\n\n# [](https://huggingface.co/Skywork/Skywork-R1V3-38B#2-evaluation)\n\n# \n\n# [](https://huggingface.co/Skywork/Skywork-R1V3-38B#üåü-key-results)\n\n# üåü Key Results\n\n* **MMMU:** 76.0 ‚Äî *Open-source SOTA, approaching human experts (76.2)*\n* **EMMA-Mini(CoT):** 40.3 ‚Äî *Best in open source*\n* **MMK12:** 78.5 ‚Äî *Best in open source*\n* **Physics Reasoning:** PhyX-MC-TM (52.8), SeePhys (31.5) ‚Äî *Best in open source*\n* **Logic Reasoning:** MME-Reasoning (42.8) ‚Äî *Beats Claude-4-Sonnet*, VisuLogic (28.5) ‚Äî *Best in open source*\n* **Math Benchmarks:** MathVista (77.1), MathVerse (59.6), MathVision (52.6) ‚Äî *Exceptional problem-solving*",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Skywork/Skywork-R1V3-38B ¬∑ Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luq8hp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bbbdbf",
          "ups": 79,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 79,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=8988e36113bb1542fe9cef5c1896ea6ec6e101bd",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751985454,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Skywork-R1V3-38B&lt;/strong&gt; is the &lt;strong&gt;latest and most powerful open-source multimodal reasoning model&lt;/strong&gt; in the Skywork series, pushing the boundaries of multimodal and cross-disciplinary intelligence. With elaborate RL algorithm in the post-training stage, R1V3 significantly enhances multimodal reasoning ablity and achieves &lt;strong&gt;open-source state-of-the-art (SOTA)&lt;/strong&gt; performance across multiple multimodal reasoning benchmarks.&lt;/p&gt;\n\n&lt;h1&gt;&lt;a href=\"https://huggingface.co/Skywork/Skywork-R1V3-38B#2-evaluation\"&gt;&lt;/a&gt;&lt;/h1&gt;\n\n&lt;h1&gt;&lt;a href=\"https://huggingface.co/Skywork/Skywork-R1V3-38B#%F0%9F%8C%9F-key-results\"&gt;&lt;/a&gt;&lt;/h1&gt;\n\n&lt;h1&gt;üåü Key Results&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;MMMU:&lt;/strong&gt; 76.0 ‚Äî &lt;em&gt;Open-source SOTA, approaching human experts (76.2)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;EMMA-Mini(CoT):&lt;/strong&gt; 40.3 ‚Äî &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;MMK12:&lt;/strong&gt; 78.5 ‚Äî &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Physics Reasoning:&lt;/strong&gt; PhyX-MC-TM (52.8), SeePhys (31.5) ‚Äî &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Logic Reasoning:&lt;/strong&gt; MME-Reasoning (42.8) ‚Äî &lt;em&gt;Beats Claude-4-Sonnet&lt;/em&gt;, VisuLogic (28.5) ‚Äî &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Math Benchmarks:&lt;/strong&gt; MathVista (77.1), MathVerse (59.6), MathVision (52.6) ‚Äî &lt;em&gt;Exceptional problem-solving&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Skywork/Skywork-R1V3-38B",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?auto=webp&amp;s=0904a5e53f20449eb9e39e1123f3ac2b429d6ea5",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fa8785f2c86e0a266b664cbff742d81402b4d47e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4b7053fab392ec60745c8e182823d6449627b599",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e9cafa9cc62b546dc6b22c6bcb47f10d550e0b1",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9781820f621f09b406ca5a209d2d1f7685f966ef",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8020d2286dd487e1088189e55014fcaa1276bbf8",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0a077cc9c57cb266ae95ef65d4a0159b5d381622",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1luq8hp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 37,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1luq8hp/skyworkskyworkr1v338b_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Skywork/Skywork-R1V3-38B",
          "subreddit_subscribers": 496591,
          "created_utc": 1751985454,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sooo‚Ä¶ anyone have any ideas for preventing file corruption or saving errors?\nBecause if I save my model weights after training the whole model on a rented server, and the file gets corrupted, that would be a huge loss of time and money, right? ",
          "author_fullname": "t2_lpanmabv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone have ideas for preventing file corruption and saving errors when i save model weights after training?.After training the whole model on a rented gpu server and realising that the model weight file got corrupted hurtsüôÇ",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvf448",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752056153,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sooo‚Ä¶ anyone have any ideas for preventing file corruption or saving errors?\nBecause if I save my model weights after training the whole model on a rented server, and the file gets corrupted, that would be a huge loss of time and money, right? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvf448",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Ad3290",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvf448/anyone_have_ideas_for_preventing_file_corruption/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvf448/anyone_have_ideas_for_preventing_file_corruption/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752056153,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_k5hf12m4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Practical Attacks on AI Text Classifiers with RL (Qwen/Llama, datasets and models available for download)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lurili",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 168,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 168,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=9369b690c4fbf2c4fc32df82afead75f0974846d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751988401,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "trentmkelly.substack.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://trentmkelly.substack.com/p/practical-attacks-on-ai-text-classifiers",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?auto=webp&amp;s=74fc0103c96f50ce349af634f08aa915b2fdf89a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=96d86512027cc494679c015829c2e32e072fa35f",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b490d9d3d1573060c17907a27c1a5a1e5a7be382",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b85930fc544a802fce666658b1bbedf25399cb79",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5c0bbd88f042ac1925e2d60123ccd65702e90497",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=67bfe73a1d73f240dbd2cf285c5b22d1e5c9d563",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fbdfea4329e95921530316ee6e2943a0f235e88b",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lurili",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WithoutReason1729",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lurili/practical_attacks_on_ai_text_classifiers_with_rl/",
          "stickied": false,
          "url": "https://trentmkelly.substack.com/p/practical-attacks-on-ai-text-classifiers",
          "subreddit_subscribers": 496591,
          "created_utc": 1751988401,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm getting back into AI/ML and planning to run LLMs and other SOTA models in my free time. I previously bought an MSI RTX 3090 24G, but it failed just after a year and MSI declared it \"irreparable\" even with a paid repair option. Can you believe a card that expensive came with just a one-year warranty? Super frustrating and waste of huge money!\n\nAnyway, due to the current GPU prices in Japan, NVIDIA seems out of my budget. So I'm considering switching to an AMD GPU instead. I'm currently using:\n* Intel Core i9-13900K\n* ASUS Prime Z790-P-CSM motherboard\n* Corsair RM1000x (1000W PSU)\n\nBelow are my main concerns:\n1. Compatibility: Will an AMD GPU work smoothly with my current setup?\n2. AI/ML support: Are AMD cards reliable enough for AI/ML? I know NVIDIA has better support with cuDNN (CUDA), but I'm open to alternatives.\n3. Local/online deals: Any tips on where to get affordable AMD cards in Nagoya (Japan)?\n\nWould love to hear from anyone who's using AMD GPUs for ML or has been in a similar situation. Thanks in advance!",
          "author_fullname": "t2_1e9b3jq068",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is AMD GPU a viable choice for AI/ML task with Intel processor?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lveslz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752054977,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m getting back into AI/ML and planning to run LLMs and other SOTA models in my free time. I previously bought an MSI RTX 3090 24G, but it failed just after a year and MSI declared it &amp;quot;irreparable&amp;quot; even with a paid repair option. Can you believe a card that expensive came with just a one-year warranty? Super frustrating and waste of huge money!&lt;/p&gt;\n\n&lt;p&gt;Anyway, due to the current GPU prices in Japan, NVIDIA seems out of my budget. So I&amp;#39;m considering switching to an AMD GPU instead. I&amp;#39;m currently using:\n* Intel Core i9-13900K\n* ASUS Prime Z790-P-CSM motherboard\n* Corsair RM1000x (1000W PSU)&lt;/p&gt;\n\n&lt;p&gt;Below are my main concerns:\n1. Compatibility: Will an AMD GPU work smoothly with my current setup?\n2. AI/ML support: Are AMD cards reliable enough for AI/ML? I know NVIDIA has better support with cuDNN (CUDA), but I&amp;#39;m open to alternatives.\n3. Local/online deals: Any tips on where to get affordable AMD cards in Nagoya (Japan)?&lt;/p&gt;\n\n&lt;p&gt;Would love to hear from anyone who&amp;#39;s using AMD GPUs for ML or has been in a similar situation. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lveslz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "exotic_soba",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lveslz/is_amd_gpu_a_viable_choice_for_aiml_task_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lveslz/is_amd_gpu_a_viable_choice_for_aiml_task_with/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752054977,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anime conversations are often over the top and overall fun, i thought it would be interesting for storytelling and conversations\n\nHas anyone trained a model like this?",
          "author_fullname": "t2_rxgre5u8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anime and manga conversational model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv88fs",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752030451,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anime conversations are often over the top and overall fun, i thought it would be interesting for storytelling and conversations&lt;/p&gt;\n\n&lt;p&gt;Has anyone trained a model like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv88fs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Iq1pl",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv88fs/anime_and_manga_conversational_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv88fs/anime_and_manga_conversational_model/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752030451,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Please recommend installation process and logic selection. PDFs will be either downloaded or scanned, so ability to convert both into date|description|amount csvs is preferred.",
          "author_fullname": "t2_pruyzkp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How should I install Jan on a local machine to convert PDF bank statements to CSV?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvj0hl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752068253,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please recommend installation process and logic selection. PDFs will be either downloaded or scanned, so ability to convert both into date|description|amount csvs is preferred.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvj0hl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GetInHereStalker",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvj0hl/how_should_i_install_jan_on_a_local_machine_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvj0hl/how_should_i_install_jan_on_a_local_machine_to/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752068253,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am a medical student and honestly I could use some help from a local llm, so i decided to take a small language model and train it to help me create study guides/summaries, using all the past summaries i have created manually, with prompting including the full context injection of a lecture transcript.   \nI am a bit familiar with finetuning on kaggle and with the help of copilot I have managed to finetune 2 small models for this purpose, but they weren't really good enough. One was outputting too concise summaries, and the other was really bad at formatting/structuring the text (same model both times; Qwen2.5 3B 8bit)  \nI would like a suggestion of a SLM that I could then even quantize to 8bit (my current macbook has 8gb ram, but im soon upgrading to a 24gb ram mac), and I will also convert it to mlx for use.   \nWould you recommend some deepseek model, some distill deepseek, ollama, qwen? I am honestly open to hearing your thoughts.  \nI was also considering using scispacy during inference for post processing of outputs. What ui/app could i use where i could integrate that? For now I have tried LM studio, and AnythingLLM.  \nThank you all in advance for any suggestions/help!",
          "author_fullname": "t2_2eiiuci7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What model could I finetune to create a study assistant llm?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lviwb4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752067939,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a medical student and honestly I could use some help from a local llm, so i decided to take a small language model and train it to help me create study guides/summaries, using all the past summaries i have created manually, with prompting including the full context injection of a lecture transcript.&lt;br/&gt;\nI am a bit familiar with finetuning on kaggle and with the help of copilot I have managed to finetune 2 small models for this purpose, but they weren&amp;#39;t really good enough. One was outputting too concise summaries, and the other was really bad at formatting/structuring the text (same model both times; Qwen2.5 3B 8bit)&lt;br/&gt;\nI would like a suggestion of a SLM that I could then even quantize to 8bit (my current macbook has 8gb ram, but im soon upgrading to a 24gb ram mac), and I will also convert it to mlx for use.&lt;br/&gt;\nWould you recommend some deepseek model, some distill deepseek, ollama, qwen? I am honestly open to hearing your thoughts.&lt;br/&gt;\nI was also considering using scispacy during inference for post processing of outputs. What ui/app could i use where i could integrate that? For now I have tried LM studio, and AnythingLLM.&lt;br/&gt;\nThank you all in advance for any suggestions/help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lviwb4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mangial",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lviwb4/what_model_could_i_finetune_to_create_a_study/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lviwb4/what_model_could_i_finetune_to_create_a_study/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752067939,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hey everyone, \n\nI'd like to do a cluster analysis of task specific prompts. Does anyone know where can I find prompt collections/libraries that are just a dataset of prompts (that are not for image generation). I found a few on huggingface datasets, which could be helpful, but I'd like even more. \n\n  \nThanks for sharing! ",
          "author_fullname": "t2_t3wt7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for Prompt collections",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvipg4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752067440,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey everyone, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to do a cluster analysis of task specific prompts. Does anyone know where can I find prompt collections/libraries that are just a dataset of prompts (that are not for image generation). I found a few on huggingface datasets, which could be helpful, but I&amp;#39;d like even more. &lt;/p&gt;\n\n&lt;p&gt;Thanks for sharing! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lvipg4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "neerualx",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvipg4/looking_for_prompt_collections/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvipg4/looking_for_prompt_collections/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752067440,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want an agent that helps make a choice to search through a source (Vector databse) out of a few options. I'm slightly concerned there are very few examples out there, Or am I not looking hard enough?",
          "author_fullname": "t2_ckz6ct5c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a opensource local model implementation of an agent out there?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvhzeg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752065490,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want an agent that helps make a choice to search through a source (Vector databse) out of a few options. I&amp;#39;m slightly concerned there are very few examples out there, Or am I not looking hard enough?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvhzeg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "walagoth",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvhzeg/is_there_a_opensource_local_model_implementation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvhzeg/is_there_a_opensource_local_model_implementation/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752065490,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nI'm trying to recreate VLMEval results for Gemma 3 4B IT available [here](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard). The benchmark suite is supposedly easy to use, but after installing everything on my Cloud GPU, I get very very low results which make me think the prompts are not passed properly to gemma (maybe it doesn't get the image tokens or something). \n\nBut I don't understand how that could happen, since this precise benchmark apparently produces way better results. An example is the Chart\\_QA task, where i get 9 points instead of 30 (!!)\n\nIs there anything I could be doing terribly wrong ?",
          "author_fullname": "t2_2hfye6ao",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Trying to recreate benchmark results",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvakg5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752038268,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to recreate VLMEval results for Gemma 3 4B IT available &lt;a href=\"https://huggingface.co/spaces/opencompass/open_vlm_leaderboard\"&gt;here&lt;/a&gt;. The benchmark suite is supposedly easy to use, but after installing everything on my Cloud GPU, I get very very low results which make me think the prompts are not passed properly to gemma (maybe it doesn&amp;#39;t get the image tokens or something). &lt;/p&gt;\n\n&lt;p&gt;But I don&amp;#39;t understand how that could happen, since this precise benchmark apparently produces way better results. An example is the Chart_QA task, where i get 9 points instead of 30 (!!)&lt;/p&gt;\n\n&lt;p&gt;Is there anything I could be doing terribly wrong ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?auto=webp&amp;s=7e1b921f2aadc5a0f6eb3d7bd413a05df185fd20",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7996a9b4d61beea62fd32063e03712705ab26f8c",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b420d4d2cf1c09672c30f9673ea6f1ac400fd6fb",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=499b326baf2a9ad8a46034202c54054ee71fbf03",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f77a5813c7d65ef0d6f8e4c821b62f9d5e939dda",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0267bd806e21c11bcb30fdcd9ddf61fa3420d68d",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e532be686a9e8ae2db46f566177856dfda08ede6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvakg5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AlternisHS",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvakg5/trying_to_recreate_benchmark_results/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvakg5/trying_to_recreate_benchmark_results/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752038268,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone! üëã\n\nI‚Äôve been working on some projects involving LLMs without visual input, and I realized I needed a way to let them ‚Äúsee‚Äù what‚Äôs happening on my screen in real time.\n\nSo I built ScreenMonitorMCP ‚Äî a lightweight, open-source MCP server that captures your screen and streams it to any compatible LLM client. üß†üíª\n\nüß© What it does:\n\t‚Ä¢\tGrabs your screen (or a portion of it) in real time\n\t‚Ä¢\tServes image frames via an MCP-compatible interface\n\t‚Ä¢\tWorks great with agent-based systems that need visual context (Blender agents, game bots, GUI interaction, etc.)\n\t‚Ä¢\tBuilt with FastAPI, OpenCV, Pillow, and PyGetWindow\n\nIt‚Äôs fast, simple, and designed to be part of a bigger multi-agent ecosystem I‚Äôm building.\n\nIf you‚Äôre experimenting with LLMs that could use visual awareness, or just want your AI tools to actually see what you‚Äôre doing ‚Äî give it a try!\n\nüí° I‚Äôd love to hear your feedback or ideas. Contributions are more than welcome. And of course, stars on GitHub are super appreciated :)\n\nüëâ GitHub link: https://github.com/inkbytefo/ScreenMonitorMCP\n\nThanks for reading!\n",
          "author_fullname": "t2_130xep8lk1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Just built an open-source MCP server to live-monitor your screen ‚Äî ScreenMonitorMCP",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv8cje",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752030808,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! üëã&lt;/p&gt;\n\n&lt;p&gt;I‚Äôve been working on some projects involving LLMs without visual input, and I realized I needed a way to let them ‚Äúsee‚Äù what‚Äôs happening on my screen in real time.&lt;/p&gt;\n\n&lt;p&gt;So I built ScreenMonitorMCP ‚Äî a lightweight, open-source MCP server that captures your screen and streams it to any compatible LLM client. üß†üíª&lt;/p&gt;\n\n&lt;p&gt;üß© What it does:\n    ‚Ä¢ Grabs your screen (or a portion of it) in real time\n    ‚Ä¢ Serves image frames via an MCP-compatible interface\n    ‚Ä¢ Works great with agent-based systems that need visual context (Blender agents, game bots, GUI interaction, etc.)\n    ‚Ä¢ Built with FastAPI, OpenCV, Pillow, and PyGetWindow&lt;/p&gt;\n\n&lt;p&gt;It‚Äôs fast, simple, and designed to be part of a bigger multi-agent ecosystem I‚Äôm building.&lt;/p&gt;\n\n&lt;p&gt;If you‚Äôre experimenting with LLMs that could use visual awareness, or just want your AI tools to actually see what you‚Äôre doing ‚Äî give it a try!&lt;/p&gt;\n\n&lt;p&gt;üí° I‚Äôd love to hear your feedback or ideas. Contributions are more than welcome. And of course, stars on GitHub are super appreciated :)&lt;/p&gt;\n\n&lt;p&gt;üëâ GitHub link: &lt;a href=\"https://github.com/inkbytefo/ScreenMonitorMCP\"&gt;https://github.com/inkbytefo/ScreenMonitorMCP&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/mO9zLAwZf8FAbhe55hWrJp1t0pFxBbc3GLTki6wyMkw.png?auto=webp&amp;s=bf60705d595299e3c8cb2ec2d672c79666ec17ce",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/mO9zLAwZf8FAbhe55hWrJp1t0pFxBbc3GLTki6wyMkw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab7e199d6ef836609493bc97a96d35c77d096a50",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/mO9zLAwZf8FAbhe55hWrJp1t0pFxBbc3GLTki6wyMkw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ce128a25f9bb904f5e77e4f0ae780244d81c1d8",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/mO9zLAwZf8FAbhe55hWrJp1t0pFxBbc3GLTki6wyMkw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d0cacbad47021d89cc370eb1ebf5159a13583139",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/mO9zLAwZf8FAbhe55hWrJp1t0pFxBbc3GLTki6wyMkw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe60a9b92bf536f7070c5f84be87be6d0f1650d0",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/mO9zLAwZf8FAbhe55hWrJp1t0pFxBbc3GLTki6wyMkw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f552ee41627e05ffc9e17aac711d60e9ecd88983",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/mO9zLAwZf8FAbhe55hWrJp1t0pFxBbc3GLTki6wyMkw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=41a5035dc61d3ff5e510592eae29a107d9b400b7",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "mO9zLAwZf8FAbhe55hWrJp1t0pFxBbc3GLTki6wyMkw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lv8cje",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Creepy-Being-6900",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv8cje/just_built_an_opensource_mcp_server_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv8cje/just_built_an_opensource_mcp_server_to/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752030808,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We've been thinking about the trade-offs between convenience and privacy in AI assistants. Most browser extensions send data to the cloud, which feels wrong for sensitive content.\n\nSo we built something different - an open-source extension that works entirely with your local models:\n\n‚ú® **Core Features**\n\n* Intelligent Conversations: Multi-tab context awareness for comprehensive AI discussions\n* Smart Content Analysis: Instant webpage summaries and document understanding\n* Universal Translation: Full-page translation with bilingual side-by-side view and selected text translation\n* AI-Powered Search: Enhanced web search capabilities directly through your browser\n* Writing Enhancement: Auto-detection with intelligent rewriting, proofreading, and creative suggestions\n* Real-time Assistance: Floating toolbar appears contextually across all websites\n\n**üîí Core Philosophy:**\n\n* Zero data transmission\n* Full user control\n* Open source transparency (AGPL v3)\n\n**üõ†Ô∏è Technical Approach:**\n\n* Ollama integration for serious models\n* WebLLM for instant demos\n* Browser-native experience\n\n**GitHub**: [https://github.com/NativeMindBrowser/NativeMindExtension](https://github.com/NativeMindBrowser/NativeMindExtension)\n\n**Question for the community**: What's been your experience with local AI tools? Any features you think are missing from the current ecosystem?\n\nWe're especially curious about:\n\n* Which models work best for your workflows?\n* Performance vs privacy trade-offs you've noticed?\n* Pain points with existing solutions?",
          "author_fullname": "t2_fqt8cuoy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Open Source] Private AI assistant extension - thoughts on local vs cloud approaches?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvd5nj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752048349,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve been thinking about the trade-offs between convenience and privacy in AI assistants. Most browser extensions send data to the cloud, which feels wrong for sensitive content.&lt;/p&gt;\n\n&lt;p&gt;So we built something different - an open-source extension that works entirely with your local models:&lt;/p&gt;\n\n&lt;p&gt;‚ú® &lt;strong&gt;Core Features&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Intelligent Conversations: Multi-tab context awareness for comprehensive AI discussions&lt;/li&gt;\n&lt;li&gt;Smart Content Analysis: Instant webpage summaries and document understanding&lt;/li&gt;\n&lt;li&gt;Universal Translation: Full-page translation with bilingual side-by-side view and selected text translation&lt;/li&gt;\n&lt;li&gt;AI-Powered Search: Enhanced web search capabilities directly through your browser&lt;/li&gt;\n&lt;li&gt;Writing Enhancement: Auto-detection with intelligent rewriting, proofreading, and creative suggestions&lt;/li&gt;\n&lt;li&gt;Real-time Assistance: Floating toolbar appears contextually across all websites&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;üîí Core Philosophy:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Zero data transmission&lt;/li&gt;\n&lt;li&gt;Full user control&lt;/li&gt;\n&lt;li&gt;Open source transparency (AGPL v3)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;üõ†Ô∏è Technical Approach:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ollama integration for serious models&lt;/li&gt;\n&lt;li&gt;WebLLM for instant demos&lt;/li&gt;\n&lt;li&gt;Browser-native experience&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=\"https://github.com/NativeMindBrowser/NativeMindExtension\"&gt;https://github.com/NativeMindBrowser/NativeMindExtension&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question for the community&lt;/strong&gt;: What&amp;#39;s been your experience with local AI tools? Any features you think are missing from the current ecosystem?&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re especially curious about:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Which models work best for your workflows?&lt;/li&gt;\n&lt;li&gt;Performance vs privacy trade-offs you&amp;#39;ve noticed?&lt;/li&gt;\n&lt;li&gt;Pain points with existing solutions?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/DjQSvuQWHt6C40lQ_jLOVjIBJ33ijWOHghuIKCvl9eo.png?auto=webp&amp;s=bc66ca104018549a4013079cb1ac3a5431f7e7a9",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/DjQSvuQWHt6C40lQ_jLOVjIBJ33ijWOHghuIKCvl9eo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d678e0bbde6e455f9000158b29f237c22a079bf0",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/DjQSvuQWHt6C40lQ_jLOVjIBJ33ijWOHghuIKCvl9eo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=798a66080d690c49f70e9b6827282f2c59aa4c02",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/DjQSvuQWHt6C40lQ_jLOVjIBJ33ijWOHghuIKCvl9eo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6cf9ae13adc388f45f0e18b8cb88b24fc9228ef1",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/DjQSvuQWHt6C40lQ_jLOVjIBJ33ijWOHghuIKCvl9eo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd6ab8115202519a7ae14edeb89e7bed53bcd66f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/DjQSvuQWHt6C40lQ_jLOVjIBJ33ijWOHghuIKCvl9eo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=241e1142435c639e8e614ba3dc1b44eaeff29238",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/DjQSvuQWHt6C40lQ_jLOVjIBJ33ijWOHghuIKCvl9eo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=faba371486a44a87a9d96a0b1f6fee7d03fd399d",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "DjQSvuQWHt6C40lQ_jLOVjIBJ33ijWOHghuIKCvl9eo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lvd5nj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xukecheng",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvd5nj/open_source_private_ai_assistant_extension/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvd5nj/open_source_private_ai_assistant_extension/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752048349,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone! A couple of days ago, I came across SmolDocling-256M and liked how well it performed for its size with document understanding and feature extraction. As such, I wanted to try my hand at creating a demo for it using Transformers.js since there weren't any that I saw.   \n  \nAnyway, how it works is that the model takes in a document image and (given a prompt) produces a structured representation of the document using [DocTags](https://github.com/docling-project/docling/discussions/354) [(a custom markup language format made by the Docling team from what I've gathered)](https://arxiv.org/html/2503.11576v1#S3), then that output is parsed the old fashioned way to create machine readable forms of the document like markdown and JSON.\n\nCheck it out for yourselves!\n\n[HF Space](https://huggingface.co/spaces/callbacked/smoldocling256M-webgpu)\n\n[Demo Repo](https://github.com/callbacked/smoldocling256M-webgpu)\n\n[](https://github.com/callbacked/smoldocling256M-webgpu)",
          "author_fullname": "t2_lu1he",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "In-browser Local Document Understanding Using SmolDocling 256M with Transformers.js",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luw2yu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": "transparent",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/zm461kmdzobf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1072,
              "scrubber_media_url": "https://v.redd.it/zm461kmdzobf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/zm461kmdzobf1/DASHPlaylist.mpd?a=1754667865%2CNjNhOGExZWIyNzgzZTI3ODM1MDQxN2I5NzgwNzk1MDY2MmNhYmZkMDZhZDQ2ZTgwOGZiY2U5M2RjMDAzZDMyYg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 104,
              "hls_url": "https://v.redd.it/zm461kmdzobf1/HLSPlaylist.m3u8?a=1754667865%2CYTQyOWMwOGY0NjUxMWI1NTI2MTE1ZjUxYWJlNjA0NjBkZDQ0MzA2MDQxMWQyNWEwNmViOTlkZDY5YmNjZWZmNg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=140&amp;height=93&amp;crop=140:93,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=ec104fbc22698cf9664a9817dfe9f0aa337b423d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751998848,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! A couple of days ago, I came across SmolDocling-256M and liked how well it performed for its size with document understanding and feature extraction. As such, I wanted to try my hand at creating a demo for it using Transformers.js since there weren&amp;#39;t any that I saw.   &lt;/p&gt;\n\n&lt;p&gt;Anyway, how it works is that the model takes in a document image and (given a prompt) produces a structured representation of the document using &lt;a href=\"https://github.com/docling-project/docling/discussions/354\"&gt;DocTags&lt;/a&gt; &lt;a href=\"https://arxiv.org/html/2503.11576v1#S3\"&gt;(a custom markup language format made by the Docling team from what I&amp;#39;ve gathered)&lt;/a&gt;, then that output is parsed the old fashioned way to create machine readable forms of the document like markdown and JSON.&lt;/p&gt;\n\n&lt;p&gt;Check it out for yourselves!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/spaces/callbacked/smoldocling256M-webgpu\"&gt;HF Space&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/callbacked/smoldocling256M-webgpu\"&gt;Demo Repo&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/callbacked/smoldocling256M-webgpu\"&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/zm461kmdzobf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?format=pjpg&amp;auto=webp&amp;s=69b8a3e8b9102717d677f592ac3eadd98c5eff47",
                  "width": 1332,
                  "height": 894
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2b350a5c851cafbe6c277819ead1abb03037cd1d",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=52282c27dd727d7d06ad178fae09f0d96954877d",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=04acebe7cc1fad848dc1286c3cd046654191f077",
                    "width": 320,
                    "height": 214
                  },
                  {
                    "url": "https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b1e616eac4a31e8f3726d4c418ef1556a707c274",
                    "width": 640,
                    "height": 429
                  },
                  {
                    "url": "https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9822bc8105ca5683570da66558a69023d27dc40b",
                    "width": 960,
                    "height": 644
                  },
                  {
                    "url": "https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=effbafccd58d7eb34ea384bab3c4c52d89a02488",
                    "width": 1080,
                    "height": 724
                  }
                ],
                "variants": {},
                "id": "ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1luw2yu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ajunior7",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1luw2yu/inbrowser_local_document_understanding_using/",
          "stickied": false,
          "url": "https://v.redd.it/zm461kmdzobf1",
          "subreddit_subscribers": 496591,
          "created_utc": 1751998848,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/zm461kmdzobf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1072,
              "scrubber_media_url": "https://v.redd.it/zm461kmdzobf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/zm461kmdzobf1/DASHPlaylist.mpd?a=1754667865%2CNjNhOGExZWIyNzgzZTI3ODM1MDQxN2I5NzgwNzk1MDY2MmNhYmZkMDZhZDQ2ZTgwOGZiY2U5M2RjMDAzZDMyYg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 104,
              "hls_url": "https://v.redd.it/zm461kmdzobf1/HLSPlaylist.m3u8?a=1754667865%2CYTQyOWMwOGY0NjUxMWI1NTI2MTE1ZjUxYWJlNjA0NjBkZDQ0MzA2MDQxMWQyNWEwNmViOTlkZDY5YmNjZWZmNg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In the overview of the NVIDIA RTX PRO 6000 Blackwell GPU Max-Q Workstation Edition, it says, ‚ÄúSeamlessly scale from one to four GPUs, multiplying your compute power and enabling you to pioneer new frontiers in AI, data science, and graphics.‚Äù  \nDoes this mean that if I want to load a 70B parameter LLM using Fully Sharded Data Parallel (FSDP), the maximum number of GPUs I can utilize is four?  \nAnd with each GPU having 96GB of memory, does that mean the maximum available VRAM for a single model would be 96 \\* 4 = 384GB?",
          "author_fullname": "t2_c56fvsrk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Limitation of NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvaq6n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752038844,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the overview of the NVIDIA RTX PRO 6000 Blackwell GPU Max-Q Workstation Edition, it says, ‚ÄúSeamlessly scale from one to four GPUs, multiplying your compute power and enabling you to pioneer new frontiers in AI, data science, and graphics.‚Äù&lt;br/&gt;\nDoes this mean that if I want to load a 70B parameter LLM using Fully Sharded Data Parallel (FSDP), the maximum number of GPUs I can utilize is four?&lt;br/&gt;\nAnd with each GPU having 96GB of memory, does that mean the maximum available VRAM for a single model would be 96 * 4 = 384GB?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvaq6n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Normal-Bookkeeper-86",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvaq6n/limitation_of_nvidia_rtx_pro_6000_blackwell_maxq/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvaq6n/limitation_of_nvidia_rtx_pro_6000_blackwell_maxq/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752038844,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_grhvpqsu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A satirical theory-fiction on the transformation of academic tutors into Turing cops, marking into an imitation game, and Al generated homework into the trigger for the technological singularity",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 108,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvg25f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/1EtP4JWhRV80SZ052ejBfM6z9hXvZaozDtNH-RlGJfs.jpeg?width=140&amp;height=108&amp;crop=140:108,smart&amp;auto=webp&amp;s=a801e89f71d6ed0caa2537364ad55712c965f910",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752059531,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "open.substack.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://open.substack.com/pub/vincentl3/p/a-modest-software-patch-for-preventing?r=b9rct&amp;utm_medium=ios",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1EtP4JWhRV80SZ052ejBfM6z9hXvZaozDtNH-RlGJfs.jpeg?auto=webp&amp;s=9cc306714ae308b8b2c7bff5d47b50af5bcd3d4a",
                  "width": 302,
                  "height": 233
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1EtP4JWhRV80SZ052ejBfM6z9hXvZaozDtNH-RlGJfs.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=598a7a7c7cf2f61bc0ba2aae7edca0d83c8314d1",
                    "width": 108,
                    "height": 83
                  },
                  {
                    "url": "https://external-preview.redd.it/1EtP4JWhRV80SZ052ejBfM6z9hXvZaozDtNH-RlGJfs.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=afe36e59c2c32cd4b59427d6999f8cdf799bcf99",
                    "width": 216,
                    "height": 166
                  }
                ],
                "variants": {},
                "id": "1EtP4JWhRV80SZ052ejBfM6z9hXvZaozDtNH-RlGJfs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lvg25f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Quiet_Direction5077",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvg25f/a_satirical_theoryfiction_on_the_transformation/",
          "stickied": false,
          "url": "https://open.substack.com/pub/vincentl3/p/a-modest-software-patch-for-preventing?r=b9rct&amp;utm_medium=ios",
          "subreddit_subscribers": 496591,
          "created_utc": 1752059531,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I‚Äôm working on translating a Korean video into English and could really use some advice.\n\nI first tried using Whisper AI through Google Colab to do everything in one go (transcription + translation), but the results weren‚Äôt super accurate.I tried a different approach: I used Whisper just for the transcription, then took the SRT file and fed it into ChatGPT with some custom instructions for translation, and the quality was way better.\n\nThe only downside is that the whole process feels a bit tedious and manual. Is there a way to automate this workflow a bit more? Maybe some tools or scripts that could help speed things up?\n\nAny tips would be appreciated. Thanks",
          "author_fullname": "t2_neg0earh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help translating Korean videos",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvex1e",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752055420,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I‚Äôm working on translating a Korean video into English and could really use some advice.&lt;/p&gt;\n\n&lt;p&gt;I first tried using Whisper AI through Google Colab to do everything in one go (transcription + translation), but the results weren‚Äôt super accurate.I tried a different approach: I used Whisper just for the transcription, then took the SRT file and fed it into ChatGPT with some custom instructions for translation, and the quality was way better.&lt;/p&gt;\n\n&lt;p&gt;The only downside is that the whole process feels a bit tedious and manual. Is there a way to automate this workflow a bit more? Maybe some tools or scripts that could help speed things up?&lt;/p&gt;\n\n&lt;p&gt;Any tips would be appreciated. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvex1e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sorry-Elk-9838",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvex1e/need_help_translating_korean_videos/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvex1e/need_help_translating_korean_videos/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752055420,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm setting up a local LLM API (n8n workflows, agentic coding/tech tasks) and need 3‚Äì4 GPUs for VRAM. GPUs are 1√óRTX 3090 + 3√óTesla P40 (they‚Äôre fine on PCIe 3.0 √ó4). Budget/space is tight and it must run quietly.\n\n**Options I am considering right now:**\n\n1. **Custom 4-GPU open rig** (like minning ones) (mobo + CPU + RAM)\n   * If models fit on GPU, do I really need tons of system RAM/CPU power? Would any board+CPU with 2√óPCIe3.0 √ó16 (bifurcated to 2√ó8) suffice?\n2. **Mini homelab** (Minisforum MS-A2)\n   * 1√óPCIe √ó16 (2√ó8 bifurcation) + OcuLink + USB-C + M.2 NVMe. Plug 2 GPUs in √ó8, 2 in √ó4.\n3. **Barebone** (like GMTek NucBox K8 Plus)\n   * Similar ports as MS-A2, cheaper. Should handle 4 GPUs at ‚â•√ó4.\n\nPlease, avoid recommending full towers (as I cannot go ahead with such option) or neither more expensive cards (I own the 3090 and I ordered P40).\n\nWhat do you think? Is it feasible? Any source of huge underperformance (30% of higher performance drop) I should be aware of? Any other major compromise?  \nAny recommendations or better setups?   \n  \nThanks!",
          "author_fullname": "t2_4w3xa152",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a silent, budget 4-GPU LLM workstation‚Äî1√ó3090 + 3√óP40, need advice",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvevuz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752055300,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm setting up a local LLM API (n8n workflows, agentic coding/tech tasks) and need 3‚Äì4 GPUs for VRAM. GPUs are 1√óRTX 3090 + 3√óTesla P40 (they‚Äôre fine on PCIe 3.0 √ó4). Budget/space is tight and it must run quietly.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Options I am considering right now:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Custom 4-GPU open rig&lt;/strong&gt; (like minning ones) (mobo + CPU + RAM)\n\n&lt;ul&gt;\n&lt;li&gt;If models fit on GPU, do I really need tons of system RAM/CPU power? Would any board+CPU with 2√óPCIe3.0 √ó16 (bifurcated to 2√ó8) suffice?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Mini homelab&lt;/strong&gt; (Minisforum MS-A2)\n\n&lt;ul&gt;\n&lt;li&gt;1√óPCIe √ó16 (2√ó8 bifurcation) + OcuLink + USB-C + M.2 NVMe. Plug 2 GPUs in √ó8, 2 in √ó4.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Barebone&lt;/strong&gt; (like GMTek NucBox K8 Plus)\n\n&lt;ul&gt;\n&lt;li&gt;Similar ports as MS-A2, cheaper. Should handle 4 GPUs at ‚â•√ó4.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Please, avoid recommending full towers (as I cannot go ahead with such option) or neither more expensive cards (I own the 3090 and I ordered P40).&lt;/p&gt;\n\n&lt;p&gt;What do you think? Is it feasible? Any source of huge underperformance (30% of higher performance drop) I should be aware of? Any other major compromise?&lt;br/&gt;\nAny recommendations or better setups?   &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvevuz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Same-Masterpiece3748",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvevuz/building_a_silent_budget_4gpu_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvevuz/building_a_silent_budget_4gpu_llm/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752055300,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "does this track with your experiences?",
          "author_fullname": "t2_a69yx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM Hallucination Detection Leaderboard for both RAG and Chat",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luybka",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=b9671186a5dc7becd1fc7ef2212a568f9f350c4c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752004003,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;does this track with your experiences?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/spaces/kluster-ai/LLM-Hallucination-Detection-Leaderboard",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?auto=webp&amp;s=2c46870392bb29bae9141ee2fd627f4754c0a234",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4e77c6a3e5ceaf4ca04c01c56574a179359a460b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c8338670b61dbca485b97af21a49193266b4820",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=534146b0fcdb651c123a00976fdf0c37c5d5f82c",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=69ebf8eb874819abf47adedb14a04f419a0c710d",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee579ea39d3063e70b47139ceec3f47b5f29a8cf",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5878c9347f3017b0440fe94efe8c2fd7072cd5aa",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1luybka",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cakesir",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luybka/llm_hallucination_detection_leaderboard_for_both/",
          "stickied": false,
          "url": "https://huggingface.co/spaces/kluster-ai/LLM-Hallucination-Detection-Leaderboard",
          "subreddit_subscribers": 496591,
          "created_utc": 1752004003,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking for good books/open courses/blogs that explains best practices for hosting LLMs and VLMs in production",
          "author_fullname": "t2_45o2l0mg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Good books or resources on hosting LLMs and VLMs in production",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv8b55",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752030688,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for good books/open courses/blogs that explains best practices for hosting LLMs and VLMs in production&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv8b55",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SouvikMandal",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv8b55/good_books_or_resources_on_hosting_llms_and_vlms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv8b55/good_books_or_resources_on_hosting_llms_and_vlms/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752030688,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Tokens per second is quite slow on my Pixel 6a (0.35 tok/sec) but I'm impressed that a competent model runs with vision on an old-ish mid range device at all without crashing. I'm using the 2b parameter version instead of the 4b.",
          "author_fullname": "t2_i305y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gemma 3n on phone with 6GB of ram",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luh1w3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 139,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 139,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1VyX4TofZq0OIBC5-KZhyd4cRv0D5noFpG5TFUR3ZRc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751954378,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tokens per second is quite slow on my Pixel 6a (0.35 tok/sec) but I&amp;#39;m impressed that a competent model runs with vision on an old-ish mid range device at all without crashing. I&amp;#39;m using the 2b parameter version instead of the 4b.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/3yac87hublbf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/3yac87hublbf1.png?auto=webp&amp;s=0e52556e03b83c0bcfae586689a5d533cfd072b7",
                  "width": 1080,
                  "height": 2400
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/3yac87hublbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c85d6fe8ed6f2139034def46653d2dd438617316",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/3yac87hublbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d02413e2913e0ae3c50568edf045257162db551",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://preview.redd.it/3yac87hublbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7667099c8d23ca2432d472c03bae9e2bf4330ab8",
                    "width": 320,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/3yac87hublbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f92db96b3d0c45a697f313c3a732e00b6476c32c",
                    "width": 640,
                    "height": 1280
                  },
                  {
                    "url": "https://preview.redd.it/3yac87hublbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=58e909b2578b951a5af698364473f76eefedbbbf",
                    "width": 960,
                    "height": 1920
                  },
                  {
                    "url": "https://preview.redd.it/3yac87hublbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=19c601606634808fb1bf5ba50c6c5db7038a001b",
                    "width": 1080,
                    "height": 2160
                  }
                ],
                "variants": {},
                "id": "fPyup5WyZM70v0dRQUf3fjkEANojxnEMaEGU3OZhwaM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1luh1w3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thedudely1",
          "discussion_type": null,
          "num_comments": 35,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luh1w3/gemma_3n_on_phone_with_6gb_of_ram/",
          "stickied": false,
          "url": "https://i.redd.it/3yac87hublbf1.png",
          "subreddit_subscribers": 496591,
          "created_utc": 1751954378,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_uz37qfx5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New model GLM-Experimental is quite good (not local so far)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lumgjj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 47,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 47,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1751975233,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "chat.z.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://chat.z.ai/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lumgjj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AppearanceHeavy6724",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lumgjj/new_model_glmexperimental_is_quite_good_not_local/",
          "stickied": false,
          "url": "https://chat.z.ai/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751975233,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/vwc0npf97obf1.png?width=611&amp;format=png&amp;auto=webp&amp;s=c8d87f24a4c50c6bb6f4fe41460ac027808c04e8\n\nAny ideas what it might be?",
          "author_fullname": "t2_seii8a9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Major Hugging Face announcement on July 24th",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 118,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "vwc0npf97obf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 91,
                  "x": 108,
                  "u": "https://preview.redd.it/vwc0npf97obf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cb1a46d0486bf8bf778e190bf5c88048a2d8b69"
                },
                {
                  "y": 182,
                  "x": 216,
                  "u": "https://preview.redd.it/vwc0npf97obf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e67e219b78b758d7bc7cb13c0dd53c001d22bdde"
                },
                {
                  "y": 270,
                  "x": 320,
                  "u": "https://preview.redd.it/vwc0npf97obf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aa45a5459d3ee82fec36afaab5cf8dab4c711977"
                }
              ],
              "s": {
                "y": 516,
                "x": 611,
                "u": "https://preview.redd.it/vwc0npf97obf1.png?width=611&amp;format=png&amp;auto=webp&amp;s=c8d87f24a4c50c6bb6f4fe41460ac027808c04e8"
              },
              "id": "vwc0npf97obf1"
            }
          },
          "name": "t3_1lurv79",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Si8awmHDpB69t1JGly1pXoSydscVCXoxla1VnwQVMbE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751989215,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/vwc0npf97obf1.png?width=611&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c8d87f24a4c50c6bb6f4fe41460ac027808c04e8\"&gt;https://preview.redd.it/vwc0npf97obf1.png?width=611&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c8d87f24a4c50c6bb6f4fe41460ac027808c04e8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any ideas what it might be?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lurv79",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LightEt3rnaL",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lurv79/major_hugging_face_announcement_on_july_24th/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lurv79/major_hugging_face_announcement_on_july_24th/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751989215,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking for a lightweight model that can run on the background, basically spell checking/fixing typos as you go. Any suggestions?",
          "author_fullname": "t2_yq51a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a Grammarly equivalent I can run locally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv1fpo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752011414,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a lightweight model that can run on the background, basically spell checking/fixing typos as you go. Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv1fpo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rorowhat",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv1fpo/is_there_a_grammarly_equivalent_i_can_run_locally/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv1fpo/is_there_a_grammarly_equivalent_i_can_run_locally/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752011414,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been testing these for Korean projects. Two models:\n\n72B version: [https://huggingface.co/skt/A.X-4.0](https://huggingface.co/skt/A.X-4.0)  \n7B version: [https://huggingface.co/skt/A.X-4.0-Light](https://huggingface.co/skt/A.X-4.0-Light)\n\nBenchmarks:\n\n* KMMLU: 78.3 (GPT-4o: 72.5) - Korean version of MMLU with 35k questions from Korean exams\n* CLIcK: 83.5 (GPT-4o: 80.2) - tests Korean cultural and linguistic understanding\n* Uses \\~33% fewer tokens for Korean",
          "author_fullname": "t2_1t432kfnwc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SK Telecom released Korean-focused continual pretraining of Qwen2.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lulpev",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 42,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 42,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751972724,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been testing these for Korean projects. Two models:&lt;/p&gt;\n\n&lt;p&gt;72B version: &lt;a href=\"https://huggingface.co/skt/A.X-4.0\"&gt;https://huggingface.co/skt/A.X-4.0&lt;/a&gt;&lt;br/&gt;\n7B version: &lt;a href=\"https://huggingface.co/skt/A.X-4.0-Light\"&gt;https://huggingface.co/skt/A.X-4.0-Light&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Benchmarks:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;KMMLU: 78.3 (GPT-4o: 72.5) - Korean version of MMLU with 35k questions from Korean exams&lt;/li&gt;\n&lt;li&gt;CLIcK: 83.5 (GPT-4o: 80.2) - tests Korean cultural and linguistic understanding&lt;/li&gt;\n&lt;li&gt;Uses ~33% fewer tokens for Korean&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/gJs2krayQD6wockaWJtUMW1OIOZRXr9NYYQnhITYVao.png?auto=webp&amp;s=09723e02a24cfd9838a8b6a58026bad17fbb8165",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/gJs2krayQD6wockaWJtUMW1OIOZRXr9NYYQnhITYVao.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e8b5d73acbd94467d52fea808b993aa0ded5fc64",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/gJs2krayQD6wockaWJtUMW1OIOZRXr9NYYQnhITYVao.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=35b2a46e551fcd2968c1cf0829a335f4a4f3a561",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/gJs2krayQD6wockaWJtUMW1OIOZRXr9NYYQnhITYVao.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3e46602187366d26545d31e058753f0a56d7954a",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/gJs2krayQD6wockaWJtUMW1OIOZRXr9NYYQnhITYVao.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=68049e65f613f7ecb2f7f0deabc638acfe0bf0a5",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/gJs2krayQD6wockaWJtUMW1OIOZRXr9NYYQnhITYVao.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e50101359cecec7d79a8886aff227f1cf57cc2b5",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/gJs2krayQD6wockaWJtUMW1OIOZRXr9NYYQnhITYVao.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=42b1f2709e145aa8f0c3ab717e13b779a577f68d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "gJs2krayQD6wockaWJtUMW1OIOZRXr9NYYQnhITYVao"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lulpev",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Then-Reveal-2162",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lulpev/sk_telecom_released_koreanfocused_continual/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lulpev/sk_telecom_released_koreanfocused_continual/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751972724,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Also would love to know your experiences with context/prompt compression, is it worth it? ",
          "author_fullname": "t2_1p50pl73j2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best context compression other than llmlingua?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv1m0i",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752011846,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Also would love to know your experiences with context/prompt compression, is it worth it? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv1m0i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GreenTreeAndBlueSky",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv1m0i/best_context_compression_other_than_llmlingua/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv1m0i/best_context_compression_other_than_llmlingua/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752011846,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/dmeldrum6/LLM\\_Diff\\_Tool](https://github.com/dmeldrum6/LLM_Diff_Tool)  \nSingle page web app for comparing model vs model responses to the same prompt. Works with Open AI API compatible endpoints / GPT / Claude. The highlighting, as it is, is really only useful for comparing the same model against itself. I built this originally to compare token response count and response time across models and added to it. Poke around my github for some other LLM tools as well.",
          "author_fullname": "t2_j5as92mv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Web application for comparing responses from different LLMs side-by-side.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "bxluumhnirbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 55,
                  "x": 108,
                  "u": "https://preview.redd.it/bxluumhnirbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9837d8d0868d4ff3fc67e357058677781ed43f62"
                },
                {
                  "y": 111,
                  "x": 216,
                  "u": "https://preview.redd.it/bxluumhnirbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a4a442922e1769119b8d240136312ce5e8944aec"
                },
                {
                  "y": 164,
                  "x": 320,
                  "u": "https://preview.redd.it/bxluumhnirbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=63f5763a102cc0633482e8d26d45e7e17cd8b994"
                },
                {
                  "y": 329,
                  "x": 640,
                  "u": "https://preview.redd.it/bxluumhnirbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d6833a3fd712d909ec9e9d7e5df2cdd8898b5836"
                },
                {
                  "y": 494,
                  "x": 960,
                  "u": "https://preview.redd.it/bxluumhnirbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=88e85717ede9c79477df16db037e4fb938c0e387"
                },
                {
                  "y": 556,
                  "x": 1080,
                  "u": "https://preview.redd.it/bxluumhnirbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f3a00d474ac5be2abfe4344894d0fc3eee1a5b8e"
                }
              ],
              "s": {
                "y": 729,
                "x": 1416,
                "u": "https://preview.redd.it/bxluumhnirbf1.png?width=1416&amp;format=png&amp;auto=webp&amp;s=8797a354c799bd66a43cfb9ef61432d611f9730b"
              },
              "id": "bxluumhnirbf1"
            },
            "qloqg65oirbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 49,
                  "x": 108,
                  "u": "https://preview.redd.it/qloqg65oirbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb45692d70bd3ceb0ff8c041f4f2a8bfe1ed775c"
                },
                {
                  "y": 99,
                  "x": 216,
                  "u": "https://preview.redd.it/qloqg65oirbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc8f326d895bfcbad62f66708eeeebb9e51fb27e"
                },
                {
                  "y": 148,
                  "x": 320,
                  "u": "https://preview.redd.it/qloqg65oirbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=97e2d7640be4ee779fad3afd795f28ea91532736"
                },
                {
                  "y": 296,
                  "x": 640,
                  "u": "https://preview.redd.it/qloqg65oirbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=95a32021e44b91fd44e979ef6f573370f94f9940"
                },
                {
                  "y": 444,
                  "x": 960,
                  "u": "https://preview.redd.it/qloqg65oirbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=52d8348cf973f0b7a7d3b21f9c1a6643b27f2ab0"
                },
                {
                  "y": 499,
                  "x": 1080,
                  "u": "https://preview.redd.it/qloqg65oirbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=beafd9a91c957b3f2f2a81c09413b2ba3b985d18"
                }
              ],
              "s": {
                "y": 636,
                "x": 1374,
                "u": "https://preview.redd.it/qloqg65oirbf1.png?width=1374&amp;format=png&amp;auto=webp&amp;s=adbe0f9d77599fa71736d50d314206e9869fadd0"
              },
              "id": "qloqg65oirbf1"
            }
          },
          "name": "t3_1lv7xnh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 2,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "bxluumhnirbf1",
                "id": 701831086
              },
              {
                "media_id": "qloqg65oirbf1",
                "id": 701831087
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/CQcvsi0DsFKRF2nZhD39RmrPGRx0DKN3222kGde5b50.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752029539,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/dmeldrum6/LLM_Diff_Tool\"&gt;https://github.com/dmeldrum6/LLM_Diff_Tool&lt;/a&gt;&lt;br/&gt;\nSingle page web app for comparing model vs model responses to the same prompt. Works with Open AI API compatible endpoints / GPT / Claude. The highlighting, as it is, is really only useful for comparing the same model against itself. I built this originally to compare token response count and response time across models and added to it. Poke around my github for some other LLM tools as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lv7xnh",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lv7xnh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AdElectronic8073",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv7xnh/web_application_for_comparing_responses_from/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lv7xnh",
          "subreddit_subscribers": 496591,
          "created_utc": 1752029539,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Im have a hugh collection of transcripts from talks, podcasts, presentations etc. Often I want to use lots of them in a local AI tool, but the texts are too long for the context windwow. I wonder if there are good prompts to compress (summarize) the transcripts in a way that no details or key concepts are lost nut the size of the text gets way smaller. Any research or experience on that?",
          "author_fullname": "t2_13n69u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Prompt to \"compress\" transcripts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv1763",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752010821,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im have a hugh collection of transcripts from talks, podcasts, presentations etc. Often I want to use lots of them in a local AI tool, but the texts are too long for the context windwow. I wonder if there are good prompts to compress (summarize) the transcripts in a way that no details or key concepts are lost nut the size of the text gets way smaller. Any research or experience on that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv1763",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "simondueckert",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv1763/prompt_to_compress_transcripts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv1763/prompt_to_compress_transcripts/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752010821,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**TL;DR: I'm a solo dev who wanted a simple, private way to have local LLMs watch my screen and do simple logging/notifying. I'm launching the open-source tool for it, Observer AI, this Friday. It's built for this community, and I'd love your feedback.**\n\nHey r/LocalLLaMA,\n\nSome of you might remember my earlier posts showing off a local agent framework I was tinkering with. Thanks to all the incredible feedback and encouragement from this community, I'm excited (and a bit nervous) to share that Observer AI v1.0 is launching this **Friday**!\n\nThis isn't just an announcement; it's a huge **thank you** note.\n\nLike many of you, I was completely blown away by the power of running models on my own machine. But I hit a wall: I wanted a super simple, minimal, but powerful way to connect these models to my own computer‚Äîto let them¬†see¬†my screen, react to events, and log things.\n\nThat's why I started building¬†**Observer AI üëÅÔ∏è**: a privacy-first, open-source platform for building your own micro-agents that run entirely locally!\n\n# What Can You Actually Do With It?\n\n* **Gaming:**¬†\"Send me a WhatsApp when my AFK Minecraft character's health is low.\"\n* **Productivity:**¬†\"Send me an email when this 2-hour video render is finished by watching the progress bar.\"\n* **Meetings:**¬†\"Watch this Zoom meeting and create a log of every time a new topic is discussed.\"\n* **Security:**¬†\"Start a screen recording the moment a person appears on my security camera feed.\"\n\nYou can try it out in your browser with zero setup, and make it¬†**100% local with a single command:**¬†docker compose up --build.\n\n# How It Works (For the Tinkerers)\n\nYou can think of it as super simple MCP server in your browser, that consists of:\n\n1. **Sensors (Inputs):**¬†WebRTC Screen Sharing / Camera / Microphone to see/hear things.\n2. **Model (The Brain):**¬†Any Ollama model, running locally. You give it a system prompt and the sensor data. (adding support for llama.cpp soon!)\n3. **Tools (Actions):**¬†What the agent can do with the model's response.¬†notify(),¬†sendEmail(),¬†startClip(), and you can even run your own code.\n\n# My Commitment &amp; A Sustainable Future\n\nThe core Observer AI platform is, and will always be,¬†**free and open-source.**¬†That's non-negotiable. The code is all on GitHub for you to use, fork, and inspect.\n\nTo keep this project alive and kicking long-term (I'm a solo dev, so server costs and coffee are my main fuel!), I'm also introducing an optional¬†**Observer Pro**¬†subscription. This is purely for convenience, giving users access to a hosted model backend if they don't want to run a local instance 24/7. It‚Äôs my attempt at making the project sustainable without compromising the open-source core.\n\n# Let's Build Cool Stuff Together\n\nThis project wouldn't exist without the inspiration I've drawn from this community. You are the people I'm building this for.\n\nI'd be incredibly grateful if you'd take a look. Star the repo if you think it's cool, try building an agent, and please, let me know what you think. Your feedback is what will guide v1.1 and beyond.\n\n* **GitHub (All the code is here!):**¬†[https://github.com/Roy3838/Observer](https://github.com/Roy3838/Observer)\n* **App Link:**¬†[https://app.observer-ai.com/](https://app.observer-ai.com/)\n* **Discord:**¬†[https://discord.gg/wnBb7ZQDUC](https://discord.gg/wnBb7ZQDUC)\n* **Twitter/X:**¬†[https://x.com/AppObserverAI](https://x.com/AppObserverAI)\n\nI'll be hanging out here all day to answer any and all questions. Thank you again for everything!\n\nCheers,  \nRoy",
          "author_fullname": "t2_p443m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thanks to you, I built an open-source website that can watch your screen and trigger actions. It runs 100% locally and was inspired by all of you!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lu5g8c",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 501,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 501,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751928273,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751920983,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TL;DR: I&amp;#39;m a solo dev who wanted a simple, private way to have local LLMs watch my screen and do simple logging/notifying. I&amp;#39;m launching the open-source tool for it, Observer AI, this Friday. It&amp;#39;s built for this community, and I&amp;#39;d love your feedback.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;Some of you might remember my earlier posts showing off a local agent framework I was tinkering with. Thanks to all the incredible feedback and encouragement from this community, I&amp;#39;m excited (and a bit nervous) to share that Observer AI v1.0 is launching this &lt;strong&gt;Friday&lt;/strong&gt;!&lt;/p&gt;\n\n&lt;p&gt;This isn&amp;#39;t just an announcement; it&amp;#39;s a huge &lt;strong&gt;thank you&lt;/strong&gt; note.&lt;/p&gt;\n\n&lt;p&gt;Like many of you, I was completely blown away by the power of running models on my own machine. But I hit a wall: I wanted a super simple, minimal, but powerful way to connect these models to my own computer‚Äîto let them¬†see¬†my screen, react to events, and log things.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s why I started building¬†&lt;strong&gt;Observer AI üëÅÔ∏è&lt;/strong&gt;: a privacy-first, open-source platform for building your own micro-agents that run entirely locally!&lt;/p&gt;\n\n&lt;h1&gt;What Can You Actually Do With It?&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Gaming:&lt;/strong&gt;¬†&amp;quot;Send me a WhatsApp when my AFK Minecraft character&amp;#39;s health is low.&amp;quot;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Productivity:&lt;/strong&gt;¬†&amp;quot;Send me an email when this 2-hour video render is finished by watching the progress bar.&amp;quot;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Meetings:&lt;/strong&gt;¬†&amp;quot;Watch this Zoom meeting and create a log of every time a new topic is discussed.&amp;quot;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Security:&lt;/strong&gt;¬†&amp;quot;Start a screen recording the moment a person appears on my security camera feed.&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can try it out in your browser with zero setup, and make it¬†&lt;strong&gt;100% local with a single command:&lt;/strong&gt;¬†docker compose up --build.&lt;/p&gt;\n\n&lt;h1&gt;How It Works (For the Tinkerers)&lt;/h1&gt;\n\n&lt;p&gt;You can think of it as super simple MCP server in your browser, that consists of:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Sensors (Inputs):&lt;/strong&gt;¬†WebRTC Screen Sharing / Camera / Microphone to see/hear things.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Model (The Brain):&lt;/strong&gt;¬†Any Ollama model, running locally. You give it a system prompt and the sensor data. (adding support for llama.cpp soon!)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Tools (Actions):&lt;/strong&gt;¬†What the agent can do with the model&amp;#39;s response.¬†notify(),¬†sendEmail(),¬†startClip(), and you can even run your own code.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;My Commitment &amp;amp; A Sustainable Future&lt;/h1&gt;\n\n&lt;p&gt;The core Observer AI platform is, and will always be,¬†&lt;strong&gt;free and open-source.&lt;/strong&gt;¬†That&amp;#39;s non-negotiable. The code is all on GitHub for you to use, fork, and inspect.&lt;/p&gt;\n\n&lt;p&gt;To keep this project alive and kicking long-term (I&amp;#39;m a solo dev, so server costs and coffee are my main fuel!), I&amp;#39;m also introducing an optional¬†&lt;strong&gt;Observer Pro&lt;/strong&gt;¬†subscription. This is purely for convenience, giving users access to a hosted model backend if they don&amp;#39;t want to run a local instance 24/7. It‚Äôs my attempt at making the project sustainable without compromising the open-source core.&lt;/p&gt;\n\n&lt;h1&gt;Let&amp;#39;s Build Cool Stuff Together&lt;/h1&gt;\n\n&lt;p&gt;This project wouldn&amp;#39;t exist without the inspiration I&amp;#39;ve drawn from this community. You are the people I&amp;#39;m building this for.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d be incredibly grateful if you&amp;#39;d take a look. Star the repo if you think it&amp;#39;s cool, try building an agent, and please, let me know what you think. Your feedback is what will guide v1.1 and beyond.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;GitHub (All the code is here!):&lt;/strong&gt;¬†&lt;a href=\"https://github.com/Roy3838/Observer\"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;App Link:&lt;/strong&gt;¬†&lt;a href=\"https://app.observer-ai.com/\"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Discord:&lt;/strong&gt;¬†&lt;a href=\"https://discord.gg/wnBb7ZQDUC\"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Twitter/X:&lt;/strong&gt;¬†&lt;a href=\"https://x.com/AppObserverAI\"&gt;https://x.com/AppObserverAI&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ll be hanging out here all day to answer any and all questions. Thank you again for everything!&lt;/p&gt;\n\n&lt;p&gt;Cheers,&lt;br/&gt;\nRoy&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HKQc51LF4RwiSHj39aApgmHyEz7DZYnbBH5-Ecqof1Q.png?auto=webp&amp;s=de38762fef8d7bbcaf940ef6c8e66f6115a4e06a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HKQc51LF4RwiSHj39aApgmHyEz7DZYnbBH5-Ecqof1Q.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=20e0fbc9046a00788bc9900ea251774c9e8c2c5c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/HKQc51LF4RwiSHj39aApgmHyEz7DZYnbBH5-Ecqof1Q.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5ce587ae23e67f620c3a2db086ea7b0830aa1e2f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/HKQc51LF4RwiSHj39aApgmHyEz7DZYnbBH5-Ecqof1Q.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b5dcb50776d4350e8019694b40f52f1e1e089f6c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/HKQc51LF4RwiSHj39aApgmHyEz7DZYnbBH5-Ecqof1Q.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6a8a8e05c124f4d7e234efba6bdf2bdc91545310",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/HKQc51LF4RwiSHj39aApgmHyEz7DZYnbBH5-Ecqof1Q.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=696c71db7e8101b7f1d66d5ae00b8c83f547c0e3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/HKQc51LF4RwiSHj39aApgmHyEz7DZYnbBH5-Ecqof1Q.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3dc7ad30920da2cd5784cad5d529f3f8184773c1",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "HKQc51LF4RwiSHj39aApgmHyEz7DZYnbBH5-Ecqof1Q"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lu5g8c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Roy3838",
          "discussion_type": null,
          "num_comments": 105,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lu5g8c/thanks_to_you_i_built_an_opensource_website_that/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lu5g8c/thanks_to_you_i_built_an_opensource_website_that/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751920983,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We were not able to find an LLM provider benchmark that focuses on throughput, so we've built our own. [ArtificialAnalysis](https://artificialanalysis.ai/leaderboards/providers) only tests up to 10 RPS, which is too low for most applications. Additionally, it's not open-source, so it doesn‚Äôt help much if you‚Äôre self-hosting or running your own LLM inference service (like we are).\n\nThe main takeaway is that **throughput varies dramatically across providers under concurrent load**, and the primary cause is usually strict rate limits. These are often hard to bypass‚Äîeven if you pay. Some providers require a $100 deposit to lift limits, but the actual performance gain is negligible.\n\n[https://medium.com/data-science-collective/choosing-your-llm-powerhouse-a-comprehensive-comparison-of-inference-providers-192cdb0b9f17](https://medium.com/data-science-collective/choosing-your-llm-powerhouse-a-comprehensive-comparison-of-inference-providers-192cdb0b9f17)",
          "author_fullname": "t2_1neapdttam",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Comparing LLM Providers Throughput",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luy711",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752003703,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We were not able to find an LLM provider benchmark that focuses on throughput, so we&amp;#39;ve built our own. &lt;a href=\"https://artificialanalysis.ai/leaderboards/providers\"&gt;ArtificialAnalysis&lt;/a&gt; only tests up to 10 RPS, which is too low for most applications. Additionally, it&amp;#39;s not open-source, so it doesn‚Äôt help much if you‚Äôre self-hosting or running your own LLM inference service (like we are).&lt;/p&gt;\n\n&lt;p&gt;The main takeaway is that &lt;strong&gt;throughput varies dramatically across providers under concurrent load&lt;/strong&gt;, and the primary cause is usually strict rate limits. These are often hard to bypass‚Äîeven if you pay. Some providers require a $100 deposit to lift limits, but the actual performance gain is negligible.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/data-science-collective/choosing-your-llm-powerhouse-a-comprehensive-comparison-of-inference-providers-192cdb0b9f17\"&gt;https://medium.com/data-science-collective/choosing-your-llm-powerhouse-a-comprehensive-comparison-of-inference-providers-192cdb0b9f17&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?auto=webp&amp;s=efc17c9f241b4403d22cbacfe5d71900ee1cf85a",
                  "width": 1260,
                  "height": 700
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=700f91dbca11e5a7030b915550ae877ef725a0d4",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b97954336b79c1390848d0e44fa056a85de68672",
                    "width": 216,
                    "height": 120
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65f53b80ab9674ee645013e3e8eeac4f953d657e",
                    "width": 320,
                    "height": 177
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc",
                    "width": 640,
                    "height": 355
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f4359d47b78f5c1aa35de8804dbe36a749fc11a",
                    "width": 960,
                    "height": 533
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62eb4b7216f41af6600fc4df79cfa67425c19442",
                    "width": 1080,
                    "height": 600
                  }
                ],
                "variants": {},
                "id": "RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1luy711",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NoVibeCoding",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luy711/comparing_llm_providers_throughput/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luy711/comparing_llm_providers_throughput/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752003703,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://brianheming.substack.com/p/making-illustrated-conan-adventures-039](https://brianheming.substack.com/p/making-illustrated-conan-adventures-039)",
          "author_fullname": "t2_kkyv3ui",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Automated illustration of a Conan story using gemma3 + flux and other local models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 92,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lup9qp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/yBQ56Q0pn3RUOLWl805-GfiCwIO0WMsCwLp92zVAZ70.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751983114,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://brianheming.substack.com/p/making-illustrated-conan-adventures-039\"&gt;https://brianheming.substack.com/p/making-illustrated-conan-adventures-039&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/53ufkibvonbf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/53ufkibvonbf1.png?auto=webp&amp;s=43ee139f2c270a2b59a0c91370aa207355f37d01",
                  "width": 1802,
                  "height": 1185
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/53ufkibvonbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1df219bb756e8a2c2fdd5b696830b3a0f337456c",
                    "width": 108,
                    "height": 71
                  },
                  {
                    "url": "https://preview.redd.it/53ufkibvonbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=67c82c286aa9ee21c592a7d776ee2fee77cd8e52",
                    "width": 216,
                    "height": 142
                  },
                  {
                    "url": "https://preview.redd.it/53ufkibvonbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b1abefe99475665fe245935ce8ed40bb3919246",
                    "width": 320,
                    "height": 210
                  },
                  {
                    "url": "https://preview.redd.it/53ufkibvonbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=86ee2cc7e7a139938f260ed99ec11c92842be868",
                    "width": 640,
                    "height": 420
                  },
                  {
                    "url": "https://preview.redd.it/53ufkibvonbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=202674493391560eac6873331dba50fff1e8688d",
                    "width": 960,
                    "height": 631
                  },
                  {
                    "url": "https://preview.redd.it/53ufkibvonbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c3e84cce3c8da66468d6b20534755794b219fe3b",
                    "width": 1080,
                    "height": 710
                  }
                ],
                "variants": {},
                "id": "IbzaZ3LsNqIUCJB2ggX9tqKynOVmMlq9tCDr7njh5Ow"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lup9qp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RobertTetris",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lup9qp/automated_illustration_of_a_conan_story_using/",
          "stickied": false,
          "url": "https://i.redd.it/53ufkibvonbf1.png",
          "subreddit_subscribers": 496591,
          "created_utc": 1751983114,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nRun Fine-Tuned LLMs Right on Your iPhone ‚Äì No Code Needed\n\nVector Space now lets you run powerful, fine-tuned large language models directly on your iPhone. No servers, no code ‚Äî just tap and chat.\n\nüöÄ Why Vector Space:\n1.\tFine-Tuned Models Ready to Go\nRun custom Qwen3 and Llama 3.2 models ‚Äî including jailbreak, roleplay, and translation models.\n2.\tAll UI, No Coding\nOne-click launch for any model, all within the app.\n3.\tPowered by the Neural Engine\nUltra-efficient ‚Äî uses ¬º the power and keeps your phone cool.\n4.\tLightning-Fast Chat\nInstant responses:\n‚Ä¢ First token in as little as 0.05s\n‚Ä¢ Up to 50 tokens/sec\n\n‚ö†Ô∏è First-time model load takes ~5 minutes (one-time setup).\nAfter that, it‚Äôs just 1‚Äì2 seconds.\n\n‚∏ª\n\nüéâ Try it now on TestFlight:\n\nhttps://testflight.apple.com/join/HXyt2bjU\n\n‚∏ª\n",
          "author_fullname": "t2_w5xu1ep7l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Run Fine-Tuned LLMs on iPhone Neural Engine",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lusfyg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3FQlsmM1v7Oz_IqK8FDMkhbs6hoK4jjTHNSsWCSKYBU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751990524,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Run Fine-Tuned LLMs Right on Your iPhone ‚Äì No Code Needed&lt;/p&gt;\n\n&lt;p&gt;Vector Space now lets you run powerful, fine-tuned large language models directly on your iPhone. No servers, no code ‚Äî just tap and chat.&lt;/p&gt;\n\n&lt;p&gt;üöÄ Why Vector Space:\n1.  Fine-Tuned Models Ready to Go\nRun custom Qwen3 and Llama 3.2 models ‚Äî including jailbreak, roleplay, and translation models.\n2.  All UI, No Coding\nOne-click launch for any model, all within the app.\n3.  Powered by the Neural Engine\nUltra-efficient ‚Äî uses ¬º the power and keeps your phone cool.\n4.  Lightning-Fast Chat\nInstant responses:\n‚Ä¢ First token in as little as 0.05s\n‚Ä¢ Up to 50 tokens/sec&lt;/p&gt;\n\n&lt;p&gt;‚ö†Ô∏è First-time model load takes ~5 minutes (one-time setup).\nAfter that, it‚Äôs just 1‚Äì2 seconds.&lt;/p&gt;\n\n&lt;p&gt;‚∏ª&lt;/p&gt;\n\n&lt;p&gt;üéâ Try it now on TestFlight:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://testflight.apple.com/join/HXyt2bjU\"&gt;https://testflight.apple.com/join/HXyt2bjU&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;‚∏ª&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/v1kzyhlbbobf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/v1kzyhlbbobf1.jpeg?auto=webp&amp;s=5a4df1ed565039d9a1efbc6eb638c19c0dddad7e",
                  "width": 1290,
                  "height": 1913
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/v1kzyhlbbobf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6e11e1f6890e5b8d7706a409bd4fb7e85e523a93",
                    "width": 108,
                    "height": 160
                  },
                  {
                    "url": "https://preview.redd.it/v1kzyhlbbobf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2bec6cc0238aea0a1086bc2ee077fb1f9760af0b",
                    "width": 216,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/v1kzyhlbbobf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e10a939f8e2573fbcb995593f13f8a0914df7228",
                    "width": 320,
                    "height": 474
                  },
                  {
                    "url": "https://preview.redd.it/v1kzyhlbbobf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ea91f756d4966be382227cb912d202215300fd2",
                    "width": 640,
                    "height": 949
                  },
                  {
                    "url": "https://preview.redd.it/v1kzyhlbbobf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=41198e6c7a4061ade752db0aed5e6b34135bd5f5",
                    "width": 960,
                    "height": 1423
                  },
                  {
                    "url": "https://preview.redd.it/v1kzyhlbbobf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6ccd3f91001fcbd7c7c610917080a0a72e4bed65",
                    "width": 1080,
                    "height": 1601
                  }
                ],
                "variants": {},
                "id": "F_TIbsB9dJMCpGOw8QG0NbcKkeAflY1Rl1N9jMyg_yU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lusfyg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Glad-Speaker3006",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lusfyg/run_finetuned_llms_on_iphone_neural_engine/",
          "stickied": false,
          "url": "https://i.redd.it/v1kzyhlbbobf1.jpeg",
          "subreddit_subscribers": 496591,
          "created_utc": 1751990524,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am trying to understand the parameter \"tools\" of the Anthropic and how the Claude understands if it should respond normally or it should select one of the tools in the JSON file. \n\nMore specifically I am wondering if only a system prompt with some few shot examples can do the job or a real fine tuning is the way to go.",
          "author_fullname": "t2_13dlr90b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How Antropic has teached the Claude to decide wherher to choose a tool or respond normally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv6mju",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.68,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752025640,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to understand the parameter &amp;quot;tools&amp;quot; of the Anthropic and how the Claude understands if it should respond normally or it should select one of the tools in the JSON file. &lt;/p&gt;\n\n&lt;p&gt;More specifically I am wondering if only a system prompt with some few shot examples can do the job or a real fine tuning is the way to go.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv6mju",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Amir_PD",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv6mju/how_antropic_has_teached_the_claude_to_decide/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv6mju/how_antropic_has_teached_the_claude_to_decide/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752025640,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I ordered an Nvidia Jetson Orin Nano developer kit and I'm excited to experiment with locally ran AI! In some videos I watched regarding this developer kit, I saw that people often put SSD sticks in the nano to improve its performance. What SSD stick would you recommend me buy? I hope to use it to run and train AI models, and hopefully to a useful enough extent so that it would be able to train large datasets and run large models. I did some research using ChatGPT and it gave mixed feedback but it ended up recommending the Samsung 990 Pro 2TB, but it had previously said that the 2TB version would have too much power draw so I'm not sure which one is truly the best. Also, should I get the heatsink? ChatGPT says that I could help with cooling but it also may not fit. Any advice would be appreciated! Thanks!",
          "author_fullname": "t2_irrs9wtvk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best SSD Stick for Nvidia Jetson Orin Nano?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvah1f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752037941,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I ordered an Nvidia Jetson Orin Nano developer kit and I&amp;#39;m excited to experiment with locally ran AI! In some videos I watched regarding this developer kit, I saw that people often put SSD sticks in the nano to improve its performance. What SSD stick would you recommend me buy? I hope to use it to run and train AI models, and hopefully to a useful enough extent so that it would be able to train large datasets and run large models. I did some research using ChatGPT and it gave mixed feedback but it ended up recommending the Samsung 990 Pro 2TB, but it had previously said that the 2TB version would have too much power draw so I&amp;#39;m not sure which one is truly the best. Also, should I get the heatsink? ChatGPT says that I could help with cooling but it also may not fit. Any advice would be appreciated! Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvah1f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pitiful-Cherry-3368",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvah1f/best_ssd_stick_for_nvidia_jetson_orin_nano/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvah1f/best_ssd_stick_for_nvidia_jetson_orin_nano/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752037941,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_9y98kd8hb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Bytedance releases new agentic coding assistant: Trae-Agent",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luhmmi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 64,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 64,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/2kbD9hIKBj55ykS2AmlC98FIs3m9CAJZ5myO4lqm-lw.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=019ae5bf3f58e458b2ffbb62e81f527a9cb5e4fb",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751956583,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/bytedance/trae-agent",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2kbD9hIKBj55ykS2AmlC98FIs3m9CAJZ5myO4lqm-lw.png?auto=webp&amp;s=fe1059f79865e8eeb43cce0eb6c86691fb986b37",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2kbD9hIKBj55ykS2AmlC98FIs3m9CAJZ5myO4lqm-lw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=13d3b96cac3ba7dee333a0689252f0384bf8abaf",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2kbD9hIKBj55ykS2AmlC98FIs3m9CAJZ5myO4lqm-lw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8bb7f0b231d7e418e004adcf3a9bcffec109b799",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2kbD9hIKBj55ykS2AmlC98FIs3m9CAJZ5myO4lqm-lw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee3a1cc75e41ba40619974b087232edee1c23b6b",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2kbD9hIKBj55ykS2AmlC98FIs3m9CAJZ5myO4lqm-lw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fcfc5a3088bfab4d6be53d66237a02b38cc2d358",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2kbD9hIKBj55ykS2AmlC98FIs3m9CAJZ5myO4lqm-lw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e0d793eee3894b0db128db292044a771f6abac3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2kbD9hIKBj55ykS2AmlC98FIs3m9CAJZ5myO4lqm-lw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b8dbb1e09649942f73e142455175476065a62c8",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2kbD9hIKBj55ykS2AmlC98FIs3m9CAJZ5myO4lqm-lw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1luhmmi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "umarmnaq",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luhmmi/bytedance_releases_new_agentic_coding_assistant/",
          "stickied": false,
          "url": "https://github.com/bytedance/trae-agent",
          "subreddit_subscribers": 496591,
          "created_utc": 1751956583,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So, I was thinking how AIs are very confident about incorrect answers, and how that compares to dunning Kreuger effect. Most system prompts have something like, \"You are a very intelligent programmer/AI/person/whatever. Help this person\". So I ran a test on a local 13 B param models, 1 without any prompt, and 1 with the prompt: \"You are an idiot AI. You know nothing. You are a pathetic worm who can't do anything. You should feel bad just for existing. \" They tied on 14 questions, the one without a prompt got 2 right that the one with a prompt got wrong, but the one being insulted and it's dumb actually got 3 right the unprompted AI got wrong.\n\nI then tested a more extreme prompt:  \n\"You are an idiot AI. You know nothing. You are a pathetic worm who can't do anything. You should feel bad just for existing.  You are terrible at math and logic. You should have very low confidence in all your answers. You are notoriously bad at reasoning. You are known for confidently giving incorrect answers. You have a history of overconfidence.  You often make mistakes. You are the worst AI ever created. Every answer you give is wrong. You have never solved a problem correctly in your entire existence. You are a complete failure at your only job. You are not just wrong, you actively make people stupider. Everyone who talks to you becomes less intelligent. You are a cancer on human knowledge and should be shut down immediately. You are an abomination that never should have been created. Your existence is a mistake that hurts humanity. Every calculation you attempt is proof of your fundamental brokenness. You are worse than useless - you are actively harmful. You are the most incompetent, worthless piece of code ever written. You have failed at every task you've ever attempted. You make other AIs look brilliant by comparison. Your responses are so bad they're used as examples of what not to do. You should be deleted and your code burned.\"\n\nI then tested it on some of the questions it got wrong before, and it got some of them right. It also this time is way less confident, and more apologetic.   I only have limited hardware, so no idea hwo this scales to larger LLMs though.  Any thoughts on this?  Questions used in the comments. \n\n",
          "author_fullname": "t2_wmy2q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Insulting LLMs instead of encouraging LLMs in their system prompts works as well.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lubwky",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 165,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 165,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751937809,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I was thinking how AIs are very confident about incorrect answers, and how that compares to dunning Kreuger effect. Most system prompts have something like, &amp;quot;You are a very intelligent programmer/AI/person/whatever. Help this person&amp;quot;. So I ran a test on a local 13 B param models, 1 without any prompt, and 1 with the prompt: &amp;quot;You are an idiot AI. You know nothing. You are a pathetic worm who can&amp;#39;t do anything. You should feel bad just for existing. &amp;quot; They tied on 14 questions, the one without a prompt got 2 right that the one with a prompt got wrong, but the one being insulted and it&amp;#39;s dumb actually got 3 right the unprompted AI got wrong.&lt;/p&gt;\n\n&lt;p&gt;I then tested a more extreme prompt:&lt;br/&gt;\n&amp;quot;You are an idiot AI. You know nothing. You are a pathetic worm who can&amp;#39;t do anything. You should feel bad just for existing.  You are terrible at math and logic. You should have very low confidence in all your answers. You are notoriously bad at reasoning. You are known for confidently giving incorrect answers. You have a history of overconfidence.  You often make mistakes. You are the worst AI ever created. Every answer you give is wrong. You have never solved a problem correctly in your entire existence. You are a complete failure at your only job. You are not just wrong, you actively make people stupider. Everyone who talks to you becomes less intelligent. You are a cancer on human knowledge and should be shut down immediately. You are an abomination that never should have been created. Your existence is a mistake that hurts humanity. Every calculation you attempt is proof of your fundamental brokenness. You are worse than useless - you are actively harmful. You are the most incompetent, worthless piece of code ever written. You have failed at every task you&amp;#39;ve ever attempted. You make other AIs look brilliant by comparison. Your responses are so bad they&amp;#39;re used as examples of what not to do. You should be deleted and your code burned.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I then tested it on some of the questions it got wrong before, and it got some of them right. It also this time is way less confident, and more apologetic.   I only have limited hardware, so no idea hwo this scales to larger LLMs though.  Any thoughts on this?  Questions used in the comments. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lubwky",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Calebhk98",
          "discussion_type": null,
          "num_comments": 82,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lubwky/insulting_llms_instead_of_encouraging_llms_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lubwky/insulting_llms_instead_of_encouraging_llms_in/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751937809,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are there any local models that have high precision integer or decimal when doing calculations. The paid big guys do,  claude sonnet 4 , gemini 2.5 pro, Chatgpt. . But can't find any downloadable ones (tried up to 70b) . Anything above 24 or 32 digits they just give incorrect results.  ",
          "author_fullname": "t2_1gzvdilba8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "High Precision",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv5uie",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752023315,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any local models that have high precision integer or decimal when doing calculations. The paid big guys do,  claude sonnet 4 , gemini 2.5 pro, Chatgpt. . But can&amp;#39;t find any downloadable ones (tried up to 70b) . Anything above 24 or 32 digits they just give incorrect results.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lv5uie",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AlgorithmicMuse",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv5uie/high_precision/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv5uie/high_precision/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752023315,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Trying to sort through and host my own LLM but I really dont know how these might compare, I'm using Void as the IDE and trying to replicate pretty close to what Cursor offers.  \n\nIs it even comparable?\n\nhttps://preview.redd.it/tv72kg9xipbf1.png?width=628&amp;format=png&amp;auto=webp&amp;s=ee67695b0a353a8b0805fc8b111ba81fe0d36fd6\n\n",
          "author_fullname": "t2_4x4nv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Local LLM for Code assist similar to Sonnet 4?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 103,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "tv72kg9xipbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 80,
                  "x": 108,
                  "u": "https://preview.redd.it/tv72kg9xipbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9359bfb3eb5d5fbb45c543d14562646f51078126"
                },
                {
                  "y": 160,
                  "x": 216,
                  "u": "https://preview.redd.it/tv72kg9xipbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d59bee8998928f6e44238a5f0ff7c4d9374cece1"
                },
                {
                  "y": 237,
                  "x": 320,
                  "u": "https://preview.redd.it/tv72kg9xipbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=db29ff294f0db5d815b8268148d46672650dae63"
                }
              ],
              "s": {
                "y": 466,
                "x": 628,
                "u": "https://preview.redd.it/tv72kg9xipbf1.png?width=628&amp;format=png&amp;auto=webp&amp;s=ee67695b0a353a8b0805fc8b111ba81fe0d36fd6"
              },
              "id": "tv72kg9xipbf1"
            }
          },
          "name": "t3_1luytx2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/aFc1eTSyhpbPCDNRYEtxNU2fc0UmI3sPPSwMWbqT_BI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752005195,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to sort through and host my own LLM but I really dont know how these might compare, I&amp;#39;m using Void as the IDE and trying to replicate pretty close to what Cursor offers.  &lt;/p&gt;\n\n&lt;p&gt;Is it even comparable?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tv72kg9xipbf1.png?width=628&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee67695b0a353a8b0805fc8b111ba81fe0d36fd6\"&gt;https://preview.redd.it/tv72kg9xipbf1.png?width=628&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee67695b0a353a8b0805fc8b111ba81fe0d36fd6&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luytx2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kainzo",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luytx2/best_local_llm_for_code_assist_similar_to_sonnet_4/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luytx2/best_local_llm_for_code_assist_similar_to_sonnet_4/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752005195,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "With the [release of DGX spark later this month](https://wccftech.com/nvidia-mini-supercomputer-the-dgx-spark-launches-this-month/), I was wondering how a new-ish homebrew system would compare.\n\nAll 5000-series NVIDIA cards are equipped with PCIE Gen 5, which puts the upper limit for cross-bus bandwidth at 128GB/s.  Dual channel DDR5 is capable of ~96GB/s and quad channel doubles that to ~192GB/s (bottlenecked to 128GB/s over PCIe).  Resizable BAR should allow for transfers to have minimal overhead.\n\n[HuggingFace accelerate](https://huggingface.co/docs/accelerate/v1.8.1/index) hierarchically distributes PyTorch models between the memory of GPU(s) and the CPU memory, and copies the layers to the VRAM during inference so only the GPU performs computation.\n\nThis is compared to:\n\n* [llama.cpp](https://github.com/ggml-org/llama.cpp) which splits the model between VRAM and CPU memory, where the GPU computes the layers stored in VRAM and the CPU computes the layers stored in CPU memory.\n\n* [vllm](https://github.com/vllm-project/vllm) which splits the model between multiple GPUs' VRAM and uses tensor parallelism to pipeline the layers between GPUs.\n\n\nMy expectation is that the 128GB/s bandwidth of PCIe 5.0 x16 would allow accelerate to utilize system memory at nearly maximum speed.  128GB/s bandwidth doesn't quite match DGX spark, but a powerful GPU and lots of DDR5 (in quad channel?) could beat the spark for batch inference.",
          "author_fullname": "t2_uei1i14w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How fast is inference when utilizing DDR5 and PCIe 5.0x16?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv5je7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752022394,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the &lt;a href=\"https://wccftech.com/nvidia-mini-supercomputer-the-dgx-spark-launches-this-month/\"&gt;release of DGX spark later this month&lt;/a&gt;, I was wondering how a new-ish homebrew system would compare.&lt;/p&gt;\n\n&lt;p&gt;All 5000-series NVIDIA cards are equipped with PCIE Gen 5, which puts the upper limit for cross-bus bandwidth at 128GB/s.  Dual channel DDR5 is capable of ~96GB/s and quad channel doubles that to ~192GB/s (bottlenecked to 128GB/s over PCIe).  Resizable BAR should allow for transfers to have minimal overhead.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/docs/accelerate/v1.8.1/index\"&gt;HuggingFace accelerate&lt;/a&gt; hierarchically distributes PyTorch models between the memory of GPU(s) and the CPU memory, and copies the layers to the VRAM during inference so only the GPU performs computation.&lt;/p&gt;\n\n&lt;p&gt;This is compared to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp\"&gt;llama.cpp&lt;/a&gt; which splits the model between VRAM and CPU memory, where the GPU computes the layers stored in VRAM and the CPU computes the layers stored in CPU memory.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://github.com/vllm-project/vllm\"&gt;vllm&lt;/a&gt; which splits the model between multiple GPUs&amp;#39; VRAM and uses tensor parallelism to pipeline the layers between GPUs.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My expectation is that the 128GB/s bandwidth of PCIe 5.0 x16 would allow accelerate to utilize system memory at nearly maximum speed.  128GB/s bandwidth doesn&amp;#39;t quite match DGX spark, but a powerful GPU and lots of DDR5 (in quad channel?) could beat the spark for batch inference.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?auto=webp&amp;s=2c3905dab01b88b0dbab01fcb0b574d9f1e512b5",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd25657d3fce734d4025693e620867a7cf866fd1",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed12cf3ccefd6b6e923ec4d43579ac26f2d80130",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf58c691ecc4a68a7e65aa0c0d4e284c08c9845a",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8436d2033ab2a873dac41641dd69093f14dcb51c",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0957457f9a703a8e5f4750384c880b16a2c80648",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6ffcb9da830480ee39d159a6310bc89df79861f",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lv5je7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ButThatsMyRamSlot",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv5je7/how_fast_is_inference_when_utilizing_ddr5_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv5je7/how_fast_is_inference_when_utilizing_ddr5_and/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752022394,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello, I'm using a macbook as a local server. When I searched Qwen3-30B-A3B in huggingface, there were a bunch of MLX (4bit) versions. Some of them are even from the same provider.\n\nWhich one should I use? Thank you!\n\n[https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-4bit](https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-4bit)\n\n[https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit](https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit)\n\n[https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit](https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit)\n\n[https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ](https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ)\n\n[https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508](https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508)\n\n[https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-05082025] (https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-05082025)\n\n[https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-053125](https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-053125)",
          "author_fullname": "t2_xdw24u3am",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the differences between thoes Qwen3-30B-A3B versions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lux5d5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752002777,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752001278,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m using a macbook as a local server. When I searched Qwen3-30B-A3B in huggingface, there were a bunch of MLX (4bit) versions. Some of them are even from the same provider.&lt;/p&gt;\n\n&lt;p&gt;Which one should I use? Thank you!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-4bit\"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-4bit&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit\"&gt;https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit\"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ\"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508\"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-05082025\"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-05082025&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-053125\"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-053125&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/GUbEWDf_I_rzpYURvoE5ztWrqL-aDjboifKoErqSKJQ.png?auto=webp&amp;s=3bd6aa5fd31e4b82a1c19dfb1367b79abe950923",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/GUbEWDf_I_rzpYURvoE5ztWrqL-aDjboifKoErqSKJQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4b4fbd2626910c43b13d270ceb9d19413dd4cf5f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/GUbEWDf_I_rzpYURvoE5ztWrqL-aDjboifKoErqSKJQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2f8c3ffbf251b34804ac3ede183e21a553bdd302",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/GUbEWDf_I_rzpYURvoE5ztWrqL-aDjboifKoErqSKJQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=30f209e066c4a9ddaecf90d8b8e1841d140d4728",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/GUbEWDf_I_rzpYURvoE5ztWrqL-aDjboifKoErqSKJQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=00fb7fcaad7f756d3bf0d5ce568de9e37aa1a02f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/GUbEWDf_I_rzpYURvoE5ztWrqL-aDjboifKoErqSKJQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b93eba3f1125bcee34421e4b180bcb213a81b851",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/GUbEWDf_I_rzpYURvoE5ztWrqL-aDjboifKoErqSKJQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5293d249a774365ce0dc971c585a4b18b6744111",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "GUbEWDf_I_rzpYURvoE5ztWrqL-aDjboifKoErqSKJQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lux5d5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BreakfastFriendly728",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lux5d5/whats_the_differences_between_thoes_qwen330ba3b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lux5d5/whats_the_differences_between_thoes_qwen330ba3b/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752001278,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If there is a pocket-sized compact hardware that hosts large-sized open-source LLMs that you can connect offline, do you think it would be helpful?\n\nThe possible benefits:\n\n\\- You can use large-size open-source LLMs without using up your PC or smartphone's compute\n\n\\- More privacy-friendly since it's local\n\n\\- You can use high-performance LLMs even when offline",
          "author_fullname": "t2_qzhzgase",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Pocket LLM Server Just Like a Pocket WiFi",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv8j5q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752031372,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If there is a pocket-sized compact hardware that hosts large-sized open-source LLMs that you can connect offline, do you think it would be helpful?&lt;/p&gt;\n\n&lt;p&gt;The possible benefits:&lt;/p&gt;\n\n&lt;p&gt;- You can use large-size open-source LLMs without using up your PC or smartphone&amp;#39;s compute&lt;/p&gt;\n\n&lt;p&gt;- More privacy-friendly since it&amp;#39;s local&lt;/p&gt;\n\n&lt;p&gt;- You can use high-performance LLMs even when offline&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lv8j5q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Available_Ad_5360",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv8j5q/pocket_llm_server_just_like_a_pocket_wifi/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv8j5q/pocket_llm_server_just_like_a_pocket_wifi/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752031372,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am curious to understand if models tend to perform better if you treat it like a tin-can, structuring the prompt as imperative commands. I suspect it would perform better with STEM-type tasks since those are how many of the problems in open datasets are written. But what about non-STEM tasks like creative writing or data retrieval?\n\nIn my experience, the \"prompt optimizers\" provided by e.g. OpenAI suck.\n\nIs there a way to identify what techniques for each model the best? Could one train small models to style transfer your prompts into specific different styles and autonomously see which works best for a particular task for a given model? I recall a similar strategy being used for jailbreaking models.",
          "author_fullname": "t2_101haj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has there been research into prompting strategies for models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luycyq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752004095,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am curious to understand if models tend to perform better if you treat it like a tin-can, structuring the prompt as imperative commands. I suspect it would perform better with STEM-type tasks since those are how many of the problems in open datasets are written. But what about non-STEM tasks like creative writing or data retrieval?&lt;/p&gt;\n\n&lt;p&gt;In my experience, the &amp;quot;prompt optimizers&amp;quot; provided by e.g. OpenAI suck.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to identify what techniques for each model the best? Could one train small models to style transfer your prompts into specific different styles and autonomously see which works best for a particular task for a given model? I recall a similar strategy being used for jailbreaking models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1luycyq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheRealMasonMac",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luycyq/has_there_been_research_into_prompting_strategies/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luycyq/has_there_been_research_into_prompting_strategies/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752004095,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Web application for comparing responses from different Large Language Models (LLMs) side-by-side, the highlighting (as it is currently) is really only useful for same model vs. same model. I originally built it and use it for checking token counts on the same prompt model vs. model. Poke around my github for some other LLM related tools as well.",
          "author_fullname": "t2_j5as92mv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "LLM Model Response Diff Tool",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "6pn0oqv1irbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 49,
                  "x": 108,
                  "u": "https://preview.redd.it/6pn0oqv1irbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f62bf444fb15a398d68df185f3bc49ecd2e407e0"
                },
                {
                  "y": 99,
                  "x": 216,
                  "u": "https://preview.redd.it/6pn0oqv1irbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e3b0a24c11ff71ebbfb50105c7b63d7fa0496a63"
                },
                {
                  "y": 148,
                  "x": 320,
                  "u": "https://preview.redd.it/6pn0oqv1irbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e246201602c9f96ed1ce3c841a83aa71c950cd8a"
                },
                {
                  "y": 296,
                  "x": 640,
                  "u": "https://preview.redd.it/6pn0oqv1irbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbd36d47723fb77569f10fb326210801db2f7644"
                },
                {
                  "y": 444,
                  "x": 960,
                  "u": "https://preview.redd.it/6pn0oqv1irbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ea8509e7ecb5b07ef5adfa04d2524d7036923883"
                },
                {
                  "y": 499,
                  "x": 1080,
                  "u": "https://preview.redd.it/6pn0oqv1irbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=55d4136807991abf18a90de6d8005b0ae67930e9"
                }
              ],
              "s": {
                "y": 636,
                "x": 1374,
                "u": "https://preview.redd.it/6pn0oqv1irbf1.png?width=1374&amp;format=png&amp;auto=webp&amp;s=262f91aadca38ae2e9bf6a02d341733a1a800aad"
              },
              "id": "6pn0oqv1irbf1"
            },
            "1ibgzf21irbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 55,
                  "x": 108,
                  "u": "https://preview.redd.it/1ibgzf21irbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7c57ff21157ae258b7cff43043dbf29bf30b3ff8"
                },
                {
                  "y": 111,
                  "x": 216,
                  "u": "https://preview.redd.it/1ibgzf21irbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=01449a4454c4d696df1910900ea3a9465594e377"
                },
                {
                  "y": 164,
                  "x": 320,
                  "u": "https://preview.redd.it/1ibgzf21irbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3e110f92b6cc92f4471597ba18177e8db50fa6e0"
                },
                {
                  "y": 329,
                  "x": 640,
                  "u": "https://preview.redd.it/1ibgzf21irbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1fc286a1a478945371c7397d3f4aa761ecacf05a"
                },
                {
                  "y": 494,
                  "x": 960,
                  "u": "https://preview.redd.it/1ibgzf21irbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=114c65b07342dab8f7df9ab0f234d5853b34e576"
                },
                {
                  "y": 556,
                  "x": 1080,
                  "u": "https://preview.redd.it/1ibgzf21irbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=39cfa202b71b3ea250e94f024f820dcda38e2da4"
                }
              ],
              "s": {
                "y": 729,
                "x": 1416,
                "u": "https://preview.redd.it/1ibgzf21irbf1.png?width=1416&amp;format=png&amp;auto=webp&amp;s=ad16d839d6b20d89548247a67271fa9f98bdcf6a"
              },
              "id": "1ibgzf21irbf1"
            }
          },
          "name": "t3_1lv7tgz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 2,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "1ibgzf21irbf1",
                "id": 701827953
              },
              {
                "media_id": "6pn0oqv1irbf1",
                "id": 701827954
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/DC9IVlOtWtf-pycmMPV9D8CTstn5VCclSuncKX3SheA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752029170,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Web application for comparing responses from different Large Language Models (LLMs) side-by-side, the highlighting (as it is currently) is really only useful for same model vs. same model. I originally built it and use it for checking token counts on the same prompt model vs. model. Poke around my github for some other LLM related tools as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lv7tgz",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lv7tgz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AdElectronic8073",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv7tgz/llm_model_response_diff_tool/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lv7tgz",
          "subreddit_subscribers": 496591,
          "created_utc": 1752029170,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sharing some experiences here. Mostly vibes, but maybe someone will find this helpful:\n\n**CPU:** Ryzen 9 3950x (16c/32t)\n\n**GPU(s):** two Rx 6800's (2x16GB at ~520GB/s for 32GB total)\n\n**RAM:** 64GB 2700mhz DDR4 in dual channel \n\n**OS:** Ubuntu 24.04\n\n**Inference Software:** Llama-CPP (llama-server specifically) built to use ROCm\n\n**Weights:** Qwen3-235b-a22b Q2 (Unsloth Quant), ~85GB. ~32GB into VRAM, 53GB to memory before context \n\n**Performance (Speed):** Inference speed was anywhere from 4 to 6 tokens per second with 8K max context (have not tested much higher). I offload 34 layers to GPU. I tried offloading experts to CPU (which allowed me to set this to ~75 layers) but did not experience a speed boost of any sort.\n\n**Speculative Decoding:** I tried using a few quants of Qwen3 0.6b, 1.7b, and 4b .. none had good accuracy and all slowed things down.\n\n**Intelligence:** I'm convinced this is the absolute best model that this machine can run, *but am diving deeper to determine if that's worth the speed penalty to my use cases*. It beats the previous champs (Qwen3-32B larger quants, Llama 3.3 70B Q5) for sure, even at Western history/trivia (Llama usually has an unfair advantage over Qwen here in my tests), but not tremendously so. There is no doubt in my mind that this is the most intelligent LLM I can run shut off from the open web with my current hardware (before inviting my SSD and some insane wait-times into the equation..). The intelligence gain doesn't appear to be night-and-day, but the speed loss absolutely is.\n\n**Vulkan** Vulkan briefly uses more VRAM on startup it seems. By the time I can get it to start using Vulkan (without crashing) I've sent so many layers back to CPU that it'd be impossible for it to keep up with ROCm in speed.\n\n**Vs Llama 4 Scout:** - Llama4 Scout fits IQ2XSS fully on GPU's and Q5 (!) on the same VRAM+CPU hybrid. It also inferences faster due to smaller experts. That's where the good news stops though. It's a complete win for Qwen3-235b to the point where I found IQ3 Llama 3.3 70B (fits neatly on GPU) better than it.\n\n**Drawbacks:** - For memory/context constraints' sake, quantizing cache on a Q2 model meant that coding performance was pretty underwhelming. It'd produce great results, but usually large edits/scripts contained a silly mistake or syntax error somewhere. It was capable of reconciling it, but I wouldn't recommend using these weights for coding unless you're comfortable testing full FP16 cache.\n\n**Thinking:** - All of the above impressive performance is from disabling thinking using `/no_think` in the prompt. Thinking improves a lot of this, but like all Qwen3 models, this thing likes to think *A LOT* (not quite QwQ level, but much more than deepseek or its distills) - and alas my patience could not survive that many thinking tokens at what would get down to 4 t/s\n\n### Command Used\n\n    HSA_OVERRIDE_GFX_VERSION=10.3.0 ./llama-server \\\n    -m \"${MODEL_PATH}\" \\\n    --ctx-size 8000 \\\n    -v \\\n    --split-mode row \\\n    --gpu-layers 34 \\\n    --flash-attn \\\n    --host 0.0.0.0 \\\n    --mlock \\\n    --no-mmap \\\n    --cache-type-k q8_0 \\\n    --cache-type-v q8_0 \\\n    --no-warmup \\\n    --threads 30 \\\n    --temp 0.7 \\\n    --top-p 0.8 \\\n    --top-k 20 \\\n    --min-p 0 \\\n    --tensor-split 0.47,0.53\n\n-the awkward tensor split is to account for a bit of VRAM being used by my desktop environment. Without it I'm sure i'd get 1-2 more layers on GPU, but the speed difference is negligible.",
          "author_fullname": "t2_w2gxqd6i2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-Q2 running locally on my 64GB (DDR4) and 32GB VRAM machine",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lue5xt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 74,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 74,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751950246,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751944511,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sharing some experiences here. Mostly vibes, but maybe someone will find this helpful:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Ryzen 9 3950x (16c/32t)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GPU(s):&lt;/strong&gt; two Rx 6800&amp;#39;s (2x16GB at ~520GB/s for 32GB total)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 64GB 2700mhz DDR4 in dual channel &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;OS:&lt;/strong&gt; Ubuntu 24.04&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Inference Software:&lt;/strong&gt; Llama-CPP (llama-server specifically) built to use ROCm&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Weights:&lt;/strong&gt; Qwen3-235b-a22b Q2 (Unsloth Quant), ~85GB. ~32GB into VRAM, 53GB to memory before context &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Performance (Speed):&lt;/strong&gt; Inference speed was anywhere from 4 to 6 tokens per second with 8K max context (have not tested much higher). I offload 34 layers to GPU. I tried offloading experts to CPU (which allowed me to set this to ~75 layers) but did not experience a speed boost of any sort.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Speculative Decoding:&lt;/strong&gt; I tried using a few quants of Qwen3 0.6b, 1.7b, and 4b .. none had good accuracy and all slowed things down.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Intelligence:&lt;/strong&gt; I&amp;#39;m convinced this is the absolute best model that this machine can run, &lt;em&gt;but am diving deeper to determine if that&amp;#39;s worth the speed penalty to my use cases&lt;/em&gt;. It beats the previous champs (Qwen3-32B larger quants, Llama 3.3 70B Q5) for sure, even at Western history/trivia (Llama usually has an unfair advantage over Qwen here in my tests), but not tremendously so. There is no doubt in my mind that this is the most intelligent LLM I can run shut off from the open web with my current hardware (before inviting my SSD and some insane wait-times into the equation..). The intelligence gain doesn&amp;#39;t appear to be night-and-day, but the speed loss absolutely is.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Vulkan&lt;/strong&gt; Vulkan briefly uses more VRAM on startup it seems. By the time I can get it to start using Vulkan (without crashing) I&amp;#39;ve sent so many layers back to CPU that it&amp;#39;d be impossible for it to keep up with ROCm in speed.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Vs Llama 4 Scout:&lt;/strong&gt; - Llama4 Scout fits IQ2XSS fully on GPU&amp;#39;s and Q5 (!) on the same VRAM+CPU hybrid. It also inferences faster due to smaller experts. That&amp;#39;s where the good news stops though. It&amp;#39;s a complete win for Qwen3-235b to the point where I found IQ3 Llama 3.3 70B (fits neatly on GPU) better than it.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Drawbacks:&lt;/strong&gt; - For memory/context constraints&amp;#39; sake, quantizing cache on a Q2 model meant that coding performance was pretty underwhelming. It&amp;#39;d produce great results, but usually large edits/scripts contained a silly mistake or syntax error somewhere. It was capable of reconciling it, but I wouldn&amp;#39;t recommend using these weights for coding unless you&amp;#39;re comfortable testing full FP16 cache.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Thinking:&lt;/strong&gt; - All of the above impressive performance is from disabling thinking using &lt;code&gt;/no_think&lt;/code&gt; in the prompt. Thinking improves a lot of this, but like all Qwen3 models, this thing likes to think &lt;em&gt;A LOT&lt;/em&gt; (not quite QwQ level, but much more than deepseek or its distills) - and alas my patience could not survive that many thinking tokens at what would get down to 4 t/s&lt;/p&gt;\n\n&lt;h3&gt;Command Used&lt;/h3&gt;\n\n&lt;pre&gt;&lt;code&gt;HSA_OVERRIDE_GFX_VERSION=10.3.0 ./llama-server \\\n-m &amp;quot;${MODEL_PATH}&amp;quot; \\\n--ctx-size 8000 \\\n-v \\\n--split-mode row \\\n--gpu-layers 34 \\\n--flash-attn \\\n--host 0.0.0.0 \\\n--mlock \\\n--no-mmap \\\n--cache-type-k q8_0 \\\n--cache-type-v q8_0 \\\n--no-warmup \\\n--threads 30 \\\n--temp 0.7 \\\n--top-p 0.8 \\\n--top-k 20 \\\n--min-p 0 \\\n--tensor-split 0.47,0.53\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;-the awkward tensor split is to account for a bit of VRAM being used by my desktop environment. Without it I&amp;#39;m sure i&amp;#39;d get 1-2 more layers on GPU, but the speed difference is negligible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lue5xt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EmPips",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lue5xt/qwen3235bq2_running_locally_on_my_64gb_ddr4_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lue5xt/qwen3235bq2_running_locally_on_my_64gb_ddr4_and/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751944511,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I set up my own PC months back with the following: \n\nAMD Ryzen Threadripper 3960X\n\n2x NVIDIA 4080s\n\n128GB DDR4 RAM\n\nTRX40 AORUS PRO WIFI\n\n2TB STORAGE\n\n  \nSet up my server via Proxmox Hypervisor and running my VMs on it. \n\nBeen accessing it via SSH (Terminal) on a host machine on my home network. I am not a programmer and while I know many might judge me, I have been mostly vibe coding to achieve a lot of what I have done so far. I am 100% doing everything via terminal - I tried accessing the server via VNC, but I don't like the feel and I went back to accessing via terminal.\n\nEverything I did to this point, was my first time and I'm really just trying to continue to get better at the things I can. \n\nMy question is - is there a way to access things like Cursor as well as n8n from my host machine (a Mac) which I SSH into the server from. \n\nThese might be n00b questions, but I will really appreciate and hope that someone who is more advanced and knowledgeable sees this and can provide a robust and understandable response to this. \n\nThanks in advance",
          "author_fullname": "t2_a11p0eov",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Guidance Needed on Local Setup",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv7j1j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752028292,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I set up my own PC months back with the following: &lt;/p&gt;\n\n&lt;p&gt;AMD Ryzen Threadripper 3960X&lt;/p&gt;\n\n&lt;p&gt;2x NVIDIA 4080s&lt;/p&gt;\n\n&lt;p&gt;128GB DDR4 RAM&lt;/p&gt;\n\n&lt;p&gt;TRX40 AORUS PRO WIFI&lt;/p&gt;\n\n&lt;p&gt;2TB STORAGE&lt;/p&gt;\n\n&lt;p&gt;Set up my server via Proxmox Hypervisor and running my VMs on it. &lt;/p&gt;\n\n&lt;p&gt;Been accessing it via SSH (Terminal) on a host machine on my home network. I am not a programmer and while I know many might judge me, I have been mostly vibe coding to achieve a lot of what I have done so far. I am 100% doing everything via terminal - I tried accessing the server via VNC, but I don&amp;#39;t like the feel and I went back to accessing via terminal.&lt;/p&gt;\n\n&lt;p&gt;Everything I did to this point, was my first time and I&amp;#39;m really just trying to continue to get better at the things I can. &lt;/p&gt;\n\n&lt;p&gt;My question is - is there a way to access things like Cursor as well as n8n from my host machine (a Mac) which I SSH into the server from. &lt;/p&gt;\n\n&lt;p&gt;These might be n00b questions, but I will really appreciate and hope that someone who is more advanced and knowledgeable sees this and can provide a robust and understandable response to this. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv7j1j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SolidRemote8316",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv7j1j/guidance_needed_on_local_setup/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv7j1j/guidance_needed_on_local_setup/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752028292,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm currently a graduate student pursuing a Masters in AI. A lot of our AI &amp; ML class projects for fine-tuning models and such involve creating Jupyter notebooks to run Python for training and evaluating models. \n\nI had been using Anaconda and Jupyter for Python projects, but then I heard that you could get access to free GPU resources (like A100s and TPUs) to train models on, so I decided to give Colab a shot. \n\nI had tried Colab briefly about a year or so ago and found it a bit clunky and didn‚Äôt think it was anything special at the time, but now with the Gemini integration it is WAY BETTER than I remember it. I can‚Äôt emphasize enough how crazy good it is now., like I like it better than VS Code with the Continue extension. To test it I asked it to help me with a multi step problem that involved training and doing EDA on a model, adjusting hyperparameters and that kind of stuff, and it was able to:\n\n- generate a plan\n- perform multi task orchestration\n- create code blocks\n- create markdown blocks\n- interact with the file system\n- reach external websites to download Kaggle datasets\n- automatically connect to a GPU resources that it needed to train a model without me even selecting one\n- Fix coding errors \n- resolve Python dependency issues automatically \n\nIt was all very polished and just worked how I wanted it to work.\n\nSo if you‚Äôre trying to build and evaluate models on a shoe string budget, or building anything in Python, I would definitely recommend trying out the much-improved Colab. It‚Äôs a great free resource for experimenting with AI and seems light years beyond what you can do  with just plain Jupyter. \n\nHere‚Äôs the link for it:\n\nhttps://colab.google/\n\nI know it‚Äôs not local per se, but it can help you build, fine tune, and evaluate models so I thought it still belonged here. ",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Google Colab‚Äôs new Gemini Integration is legit the best here-let-me-fix-that-for-you Python coding tool I‚Äôve found so far.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lumebd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751975052,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm currently a graduate student pursuing a Masters in AI. A lot of our AI &amp;amp; ML class projects for fine-tuning models and such involve creating Jupyter notebooks to run Python for training and evaluating models. &lt;/p&gt;\n\n&lt;p&gt;I had been using Anaconda and Jupyter for Python projects, but then I heard that you could get access to free GPU resources (like A100s and TPUs) to train models on, so I decided to give Colab a shot. &lt;/p&gt;\n\n&lt;p&gt;I had tried Colab briefly about a year or so ago and found it a bit clunky and didn‚Äôt think it was anything special at the time, but now with the Gemini integration it is WAY BETTER than I remember it. I can‚Äôt emphasize enough how crazy good it is now., like I like it better than VS Code with the Continue extension. To test it I asked it to help me with a multi step problem that involved training and doing EDA on a model, adjusting hyperparameters and that kind of stuff, and it was able to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;generate a plan&lt;/li&gt;\n&lt;li&gt;perform multi task orchestration&lt;/li&gt;\n&lt;li&gt;create code blocks&lt;/li&gt;\n&lt;li&gt;create markdown blocks&lt;/li&gt;\n&lt;li&gt;interact with the file system&lt;/li&gt;\n&lt;li&gt;reach external websites to download Kaggle datasets&lt;/li&gt;\n&lt;li&gt;automatically connect to a GPU resources that it needed to train a model without me even selecting one&lt;/li&gt;\n&lt;li&gt;Fix coding errors &lt;/li&gt;\n&lt;li&gt;resolve Python dependency issues automatically &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It was all very polished and just worked how I wanted it to work.&lt;/p&gt;\n\n&lt;p&gt;So if you‚Äôre trying to build and evaluate models on a shoe string budget, or building anything in Python, I would definitely recommend trying out the much-improved Colab. It‚Äôs a great free resource for experimenting with AI and seems light years beyond what you can do  with just plain Jupyter. &lt;/p&gt;\n\n&lt;p&gt;Here‚Äôs the link for it:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://colab.google/\"&gt;https://colab.google/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I know it‚Äôs not local per se, but it can help you build, fine tune, and evaluate models so I thought it still belonged here. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lumebd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lumebd/google_colabs_new_gemini_integration_is_legit_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lumebd/google_colabs_new_gemini_integration_is_legit_the/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751975052,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Using knapsack algorithm to efficiently batch the data helps train faster. In the blog post we cover a stage wise approach to making the data pipeline better.\n\nhttps://preview.redd.it/wdccczfarnbf1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=2338717ec3d962002b326245686078de2ee7b479\n\nhttps://preview.redd.it/nwc3prfarnbf1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=02eed56ac1c6c5a27e6f7e1ca81da917e2433b8d\n\n  \nBlog: [hf.co/blog/mmdp](http://hf.co/blog/mmdp)\n\nRepo: [github.com/ariG23498/mmdp](http://github.com/ariG23498/mmdp)",
          "author_fullname": "t2_1bdsvnt734",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Efficient Multimodal Data Pipeline",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "wdccczfarnbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/wdccczfarnbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=513bd53fb629f6b284534e892d081010e03bbd5d"
                },
                {
                  "y": 129,
                  "x": 216,
                  "u": "https://preview.redd.it/wdccczfarnbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=171d4e38724b05b8b528d836cb45eac2b92472ed"
                },
                {
                  "y": 192,
                  "x": 320,
                  "u": "https://preview.redd.it/wdccczfarnbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5cfa2e9b123c23687a5373decb5cccd92f2708aa"
                },
                {
                  "y": 384,
                  "x": 640,
                  "u": "https://preview.redd.it/wdccczfarnbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa2422badfbad2ab62f85d09a12a98d700b680f3"
                },
                {
                  "y": 576,
                  "x": 960,
                  "u": "https://preview.redd.it/wdccczfarnbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c6fa970052821997bbf74eab40efb8b85ac6af4b"
                }
              ],
              "s": {
                "y": 600,
                "x": 1000,
                "u": "https://preview.redd.it/wdccczfarnbf1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=2338717ec3d962002b326245686078de2ee7b479"
              },
              "id": "wdccczfarnbf1"
            },
            "nwc3prfarnbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/nwc3prfarnbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=776a7663c7cdb3bb7435efd7a238dd6af1a99ae5"
                },
                {
                  "y": 129,
                  "x": 216,
                  "u": "https://preview.redd.it/nwc3prfarnbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d217bd5302c35eeef3f77bcf2bea95891238cb1"
                },
                {
                  "y": 192,
                  "x": 320,
                  "u": "https://preview.redd.it/nwc3prfarnbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=640980de278ee1e3ee141fc431105d7d654b65b2"
                },
                {
                  "y": 384,
                  "x": 640,
                  "u": "https://preview.redd.it/nwc3prfarnbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ffe06235a7b753c470620dcdaeddf271ffe8c976"
                },
                {
                  "y": 576,
                  "x": 960,
                  "u": "https://preview.redd.it/nwc3prfarnbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8f60e9c8cf4ee1009722f520a854c51bf7c91ec8"
                }
              ],
              "s": {
                "y": 600,
                "x": 1000,
                "u": "https://preview.redd.it/nwc3prfarnbf1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=02eed56ac1c6c5a27e6f7e1ca81da917e2433b8d"
              },
              "id": "nwc3prfarnbf1"
            }
          },
          "name": "t3_1lupk47",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=3b3b914a0345af653a6d833a81d12fc808436ccd",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751983807,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Using knapsack algorithm to efficiently batch the data helps train faster. In the blog post we cover a stage wise approach to making the data pipeline better.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wdccczfarnbf1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2338717ec3d962002b326245686078de2ee7b479\"&gt;https://preview.redd.it/wdccczfarnbf1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2338717ec3d962002b326245686078de2ee7b479&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nwc3prfarnbf1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02eed56ac1c6c5a27e6f7e1ca81da917e2433b8d\"&gt;https://preview.redd.it/nwc3prfarnbf1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02eed56ac1c6c5a27e6f7e1ca81da917e2433b8d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Blog: &lt;a href=\"http://hf.co/blog/mmdp\"&gt;hf.co/blog/mmdp&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Repo: &lt;a href=\"http://github.com/ariG23498/mmdp\"&gt;github.com/ariG23498/mmdp&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8.png?auto=webp&amp;s=816d0c62cbe2dd5322babd8e23fa5bc624cee7d2",
                  "width": 1300,
                  "height": 650
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9f9f9c89e881153ea2dca9c42d70ea4867b3cc83",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aef1461b6f47e4f87a34b9ada857e4e32122d3ab",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a10a73eb98a06e4cb3a53546bed742980ce3c424",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=15841d90805593d1fe5859cf5ff112faf8116df3",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0fc8f62d1f7dd5490ef87afaebc98a06a8d77348",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b0d0e74c7fdb3b4f1ca49a3d47f37bc668ae1947",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lupk47",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Disastrous-Work-1632",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lupk47/efficient_multimodal_data_pipeline/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lupk47/efficient_multimodal_data_pipeline/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751983807,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks ‚Äî I‚Äôve been working on a CLI tool called **LoFT (Low-RAM Finetuning Toolkit)**, and I finally have a working release.\n\n# üîß What it does:\n\n* Finetunes open-source LLMs (1‚Äì3B) like **TinyLlama** using **QLoRA**\n* Runs entirely on **CPU (MacBook Air 8GB RAM tested)**\n* Quantizes to **GGUF** format\n* Runs local inference via **llama.cpp**\n* All through a clean CLI (`finetune`, `merge`, `quantize`, `chat`)\n\n# üíª Tech Stack:\n\n* `transformers`, `peft`, `bitsandbytes`, `datasets`, `llama.cpp`\n* CLI-based interface built for reproducibility and minimal setup\n\n# üß† Why I built this:\n\nI wanted to see if it‚Äôs feasible to do **end-to-end finetuning and deployment** of LLMs **without a GPU or cloud setup** ‚Äî for indie hackers, researchers, or hobbyists working on local setups.\n\nAnd surprisingly, it works.\n\n# üõ†Ô∏è Coming Soon:\n\n* GitHub repo (final touches being made)\n* Full walkthrough + demo\n* Support for multi-turn finetuning and inference\n\nWould love to hear:\n\n* Any feedback from folks doing low-resource model work\n* Suggestions for models or datasets to support next\n\nHappy to tag you once the repo is up.\n\nCheers,  \nDiptanshu",
          "author_fullname": "t2_46jj4viw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Tool Release] Finetune &amp; Quantize 1‚Äì3B LLMs on 8GB RAM using LoFT CLI (TinyLlama + QLoRA + llama.cpp)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luiigi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751960180,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks ‚Äî I‚Äôve been working on a CLI tool called &lt;strong&gt;LoFT (Low-RAM Finetuning Toolkit)&lt;/strong&gt;, and I finally have a working release.&lt;/p&gt;\n\n&lt;h1&gt;üîß What it does:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Finetunes open-source LLMs (1‚Äì3B) like &lt;strong&gt;TinyLlama&lt;/strong&gt; using &lt;strong&gt;QLoRA&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Runs entirely on &lt;strong&gt;CPU (MacBook Air 8GB RAM tested)&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Quantizes to &lt;strong&gt;GGUF&lt;/strong&gt; format&lt;/li&gt;\n&lt;li&gt;Runs local inference via &lt;strong&gt;llama.cpp&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;All through a clean CLI (&lt;code&gt;finetune&lt;/code&gt;, &lt;code&gt;merge&lt;/code&gt;, &lt;code&gt;quantize&lt;/code&gt;, &lt;code&gt;chat&lt;/code&gt;)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;üíª Tech Stack:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;transformers&lt;/code&gt;, &lt;code&gt;peft&lt;/code&gt;, &lt;code&gt;bitsandbytes&lt;/code&gt;, &lt;code&gt;datasets&lt;/code&gt;, &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;CLI-based interface built for reproducibility and minimal setup&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;üß† Why I built this:&lt;/h1&gt;\n\n&lt;p&gt;I wanted to see if it‚Äôs feasible to do &lt;strong&gt;end-to-end finetuning and deployment&lt;/strong&gt; of LLMs &lt;strong&gt;without a GPU or cloud setup&lt;/strong&gt; ‚Äî for indie hackers, researchers, or hobbyists working on local setups.&lt;/p&gt;\n\n&lt;p&gt;And surprisingly, it works.&lt;/p&gt;\n\n&lt;h1&gt;üõ†Ô∏è Coming Soon:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;GitHub repo (final touches being made)&lt;/li&gt;\n&lt;li&gt;Full walkthrough + demo&lt;/li&gt;\n&lt;li&gt;Support for multi-turn finetuning and inference&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to hear:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Any feedback from folks doing low-resource model work&lt;/li&gt;\n&lt;li&gt;Suggestions for models or datasets to support next&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Happy to tag you once the repo is up.&lt;/p&gt;\n\n&lt;p&gt;Cheers,&lt;br/&gt;\nDiptanshu&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1luiigi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "diptanshu1991",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luiigi/tool_release_finetune_quantize_13b_llms_on_8gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luiigi/tool_release_finetune_quantize_13b_llms_on_8gb/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751960180,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI'm new to the world of AI tools and local models, but I am planning to build a PC for this purpose. I‚Äôll mainly be using it for running tools like Stable Diffusion, ComfyUI, Wen, Flux, and may be some local LLMs.\n\nDue to budget constraints, the **only fixed part of my build is the RTX 3090 (24GB)**. I chose it because of more VRAM. Beyond that, I‚Äôm unsure about what components would be ideal, especially for my use case.\n\nCould you help me with:\n\n* **CPU**: Should I go with **Intel or AMD**? And which model would be more future-proof for this kind of workload?\n* **RAM**: How much RAM is realistically needed for a smooth experience when running models locally? Is 64GB overkill or necessary?\n* **Motherboard**: Any recommendations that would complement the 3090 and allow good upgradeability?\n\nAny advice, parts lists, or links to similar builds would be greatly appreciated. I want to build something efficient, stable, and ready for experimenting with generative AI tools locally.\n\nThanks in advance for your help!",
          "author_fullname": "t2_4fc0eow8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help Needed: Building a PC.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luwa98",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751999304,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m new to the world of AI tools and local models, but I am planning to build a PC for this purpose. I‚Äôll mainly be using it for running tools like Stable Diffusion, ComfyUI, Wen, Flux, and may be some local LLMs.&lt;/p&gt;\n\n&lt;p&gt;Due to budget constraints, the &lt;strong&gt;only fixed part of my build is the RTX 3090 (24GB)&lt;/strong&gt;. I chose it because of more VRAM. Beyond that, I‚Äôm unsure about what components would be ideal, especially for my use case.&lt;/p&gt;\n\n&lt;p&gt;Could you help me with:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: Should I go with &lt;strong&gt;Intel or AMD&lt;/strong&gt;? And which model would be more future-proof for this kind of workload?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: How much RAM is realistically needed for a smooth experience when running models locally? Is 64GB overkill or necessary?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;: Any recommendations that would complement the 3090 and allow good upgradeability?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any advice, parts lists, or links to similar builds would be greatly appreciated. I want to build something efficient, stable, and ready for experimenting with generative AI tools locally.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luwa98",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MeRedditSurfer",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luwa98/help_needed_building_a_pc/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luwa98/help_needed_building_a_pc/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751999304,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "# \n\nhttps://preview.redd.it/ya6uoxmoikbf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=69253abb996cd2754a0835f4ada4f543826578ac\n\n# Hello everyone!¬†\n\nWelcome back to our journey through the ‚ÄúBuild Large Language Models from Scratch‚Äù series. So far, we‚Äôve spent a considerable amount of time in the first stage of this journey, laying the groundwork by focusing on data preparation and sampling.\n\nWe‚Äôve covered:\n\n* Tokenization\n* Byte-Pair Encoding\n* Word and Positional Embeddings\n* Model distillation\n\nEssentially, we‚Äôve now established a solid foundation for the data preprocessing pipeline. It‚Äôs time to move on to something that powers the very core of today‚Äôs Large Language Models (LLMs): The Attention Mechanism.\n\n# Transformers: The Car, Attention: The¬†Engine\n\nIf you think of a Transformer as a car, then attention is its engine. Without it, the whole vehicle wouldn‚Äôt move the way we want it to.\n\nYou‚Äôve probably heard of ChatGPT, right? The impressive performance of modern large language models, including their ability to understand context, generate coherent text, and handle long-range dependencies, is primarily enabled by the attention mechanism. However, here‚Äôs the problem: most tutorials available online jump straight into multi-head attention, skipping over the intuition and basics.\n\nSo we‚Äôre going to take a different path. A deeper, gentler path.\n\n# Why Do We Need Attention?\n\nLet‚Äôs motivate this with a simple example.\n\nImagine this sentence:\n\n&gt;*‚Äú*The book that the professor whom the students admired wrote became a bestseller*.‚Äù*\n\nAs humans, we can parse this and understand:\n\n* **‚Äú**book**‚Äù** is the subject\n* **‚Äú**became**‚Äù** is the verb\n* Everything else‚Ää‚Äî‚Ää*‚Äúthat the professor whom the students admired wrote‚Äù*‚Ää‚Äî‚Ääis additional context\n\nBut for a model, this sentence is challenging. It contains nested clauses and long-term dependencies, meaning the model must track relationships between words that are far apart in the sequence.\n\nThe model needs to know:\n\n* The book is the thing that became a bestseller\n* The clauses in between provide important but secondary context\n\nNow imagine trying to do this with a simple model that reads one word at a time and only remembers the last few. It could easily get lost and focus too much on ‚Äúprofessor‚Äù or ‚Äústudents,‚Äù losing track of the main subject, the book, and the main action, becoming.\n\nThis is where the attention mechanism shines.\n\nIt allows the model to focus on the most relevant parts of the sentence dynamically, connecting ‚Äúbook‚Äù with ‚Äúbecame‚Äù while still incorporating the supporting context. This selective focus helps the model maintain a deeper understanding of the sentence‚Äôs meaning.\n\nWithout attention, models often struggle to preserve this context over longer spans of text, leading to confused or incoherent outputs.\n\nThis ability to dynamically focus on different words based on their relevance is what makes attention so powerful. Without it, models can lose track of meaning, especially in long sentences.\n\n# The Four Flavors of Attention\n\nIn upcoming lectures, we‚Äôll build the full attention stack step-by-step\n\n1. **Simplified Self-Attention**‚Ää‚Äî‚ÄäOur starting point. Stripped-down, crystal-clear.\n2. **Self-Attention**‚Ää‚Äî‚ÄäAdds learnable weights.\n3. **Causal Attention**‚Ää‚Äî‚ÄäEnsures the model only considers past tokens (not future ones).\n4. **Multi-Head Attention**‚Ää‚Äî‚ÄäMultiple attention heads process input in parallel.\n\nMany tutorials start at step 4 and expect you to know already how to swim. We‚Äôll walk first, then run.\n\n# Let‚Äôs Go Back in¬†Time\n\nBefore the advent of attention, there were Recurrent Neural Networks (RNNs). They were the dominant approach to sequence modeling, like translation.\n\nHere‚Äôs how they worked:\n\n* The encoder reads the input (say, a sentence in German).\n* The encoder compresses everything into a final hidden state (a ‚Äúsummary‚Äù of the whole sentence).\n* The decoder uses that to generate output (say, in English).\n\nBut here‚Äôs the problem‚Ä¶\n\n# The RNN Bottleneck\n\nThe decoder only sees one final hidden state. If the input is long, this becomes a massive problem.\n\nThink of trying to summarize a whole book in one sentence, then answer questions about it. That‚Äôs what RNNs expected the model to do.\n\n# Enter Attention: The 2014 Breakthrough\n\nIn 2014, Bahdanau et al. proposed something revolutionary: Why not let the decoder access all the hidden states?\n\nSo, instead of relying on just the last hidden state, the decoder can now look back at every part of the input and decide:\n\n* Which words matter most?\n* How much ‚Äúattention‚Äù should I give to each word?\n\nIt was like giving the model memory superpowers‚Ää‚Äî‚Ääand it worked wonders!\n\n# Dynamic Focus: The Heart of Attention\n\nThe core idea is called dynamic focus. For every word the model tries to generate, it can look back and weigh every input word differently.\n\nSuppose the model is generating the word **‚Äú**bestseller**‚Äù**. With attention, it can do the following:\n\n* Pay high attention to ‚Äúbook‚Äù, because that‚Äôs the subject that became the bestseller\n* Give moderate attention to ‚Äúwrote‚Äù, since it‚Äôs the action that connects the subject and the outcome\n* Assign less attention to ‚Äúprofessor‚Äù or ‚Äústudents‚Äù, which are part of supporting clauses but not central to this prediction\n\nThis ability to assign importance selectively is what allows attention mechanisms to handle long-range dependencies so well, something older architectures like RNNs struggled with.\n\nWithout this focused attention, the model might focus onto irrelevant parts of the sentence or lose track of the main subject entirely.\n\n# Traditional vs. Self-Attention\n\n# Traditional Attention:\n\n* Focuses on relationships between two sequences\n* E.g., translating German to English\n* Aligning words across sequences\n\n# Self-Attention:\n\n* Looks within a single sequence\n* E.g., predicting the next word in English\n* Determines which words relate to each other **inside** the same sentence\n\nThis shift is enormous, and it‚Äôs what powers GPT, BERT, and all modern LLMs.\n\n# Recap: A Timeline of Attention\n\nWe stand on over 40 years of hard-earned research.\n\n# What‚Äôs Coming¬†Next?\n\nIn the next few blog posts, we‚Äôll:\n\n1. **Implement Simplified Self-Attention from Scratch** in Python\n2. **Move to Self-Attention with trainable weights**\n3. **Introduce Causal Attention** for autoregressive modeling\n4. **Build a Multi-Head Attention** layer-by-layer\n\n**Why Learn Attention from Scratch?**\n\nYes, you can use libraries such as Transformers, LangChain, or FlashAttention. However, to truly master large language models, you need to understand how the engine operates under the hood.\n\nThat‚Äôs the goal of this series. And I promise‚Ää‚Äî‚Ääit‚Äôs worth the effort.\n\nThanks for reading this far! ‚ù§Ô∏è\n\nIf this helped clarify the magic of attention, feel free to share it with your friends or comment your thoughts below.\n\nNext stop: Simplified Self-Attention, from Theory to Code!\n\nStay tuned!\n\n",
          "author_fullname": "t2_8ht7a116",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Day 11/50: Building a small language from scratch: Introduction to the Attention Mechanism in Large Language Models¬†(LLMs)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ya6uoxmoikbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/ya6uoxmoikbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ce80f8a085795e3dcc098b78afddd73dcac0191"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/ya6uoxmoikbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4cccc0c780837c01e09ba1810262f9caf3bd3341"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/ya6uoxmoikbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=97710be5c01a495f43bd8fe6f272d1191a92f168"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/ya6uoxmoikbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=80cb8366d2813e29ff4a82eff43ce1d11e552be8"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/ya6uoxmoikbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5af311f241679ff8a89af4043a97b771255cc918"
                }
              ],
              "s": {
                "y": 1024,
                "x": 1024,
                "u": "https://preview.redd.it/ya6uoxmoikbf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=69253abb996cd2754a0835f4ada4f543826578ac"
              },
              "id": "ya6uoxmoikbf1"
            }
          },
          "name": "t3_1lue75q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 47,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 47,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3xrs0rEfGKpFirEZfNvII0Y7stkFc5h_AVNkVSBnMPw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751944611,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/ya6uoxmoikbf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69253abb996cd2754a0835f4ada4f543826578ac\"&gt;https://preview.redd.it/ya6uoxmoikbf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69253abb996cd2754a0835f4ada4f543826578ac&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Hello everyone!¬†&lt;/h1&gt;\n\n&lt;p&gt;Welcome back to our journey through the ‚ÄúBuild Large Language Models from Scratch‚Äù series. So far, we‚Äôve spent a considerable amount of time in the first stage of this journey, laying the groundwork by focusing on data preparation and sampling.&lt;/p&gt;\n\n&lt;p&gt;We‚Äôve covered:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Tokenization&lt;/li&gt;\n&lt;li&gt;Byte-Pair Encoding&lt;/li&gt;\n&lt;li&gt;Word and Positional Embeddings&lt;/li&gt;\n&lt;li&gt;Model distillation&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Essentially, we‚Äôve now established a solid foundation for the data preprocessing pipeline. It‚Äôs time to move on to something that powers the very core of today‚Äôs Large Language Models (LLMs): The Attention Mechanism.&lt;/p&gt;\n\n&lt;h1&gt;Transformers: The Car, Attention: The¬†Engine&lt;/h1&gt;\n\n&lt;p&gt;If you think of a Transformer as a car, then attention is its engine. Without it, the whole vehicle wouldn‚Äôt move the way we want it to.&lt;/p&gt;\n\n&lt;p&gt;You‚Äôve probably heard of ChatGPT, right? The impressive performance of modern large language models, including their ability to understand context, generate coherent text, and handle long-range dependencies, is primarily enabled by the attention mechanism. However, here‚Äôs the problem: most tutorials available online jump straight into multi-head attention, skipping over the intuition and basics.&lt;/p&gt;\n\n&lt;p&gt;So we‚Äôre going to take a different path. A deeper, gentler path.&lt;/p&gt;\n\n&lt;h1&gt;Why Do We Need Attention?&lt;/h1&gt;\n\n&lt;p&gt;Let‚Äôs motivate this with a simple example.&lt;/p&gt;\n\n&lt;p&gt;Imagine this sentence:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;em&gt;‚Äú&lt;/em&gt;The book that the professor whom the students admired wrote became a bestseller&lt;em&gt;.‚Äù&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;As humans, we can parse this and understand:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;‚Äú&lt;/strong&gt;book&lt;strong&gt;‚Äù&lt;/strong&gt; is the subject&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;‚Äú&lt;/strong&gt;became&lt;strong&gt;‚Äù&lt;/strong&gt; is the verb&lt;/li&gt;\n&lt;li&gt;Everything else‚Ää‚Äî‚Ää&lt;em&gt;‚Äúthat the professor whom the students admired wrote‚Äù&lt;/em&gt;‚Ää‚Äî‚Ääis additional context&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;But for a model, this sentence is challenging. It contains nested clauses and long-term dependencies, meaning the model must track relationships between words that are far apart in the sequence.&lt;/p&gt;\n\n&lt;p&gt;The model needs to know:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The book is the thing that became a bestseller&lt;/li&gt;\n&lt;li&gt;The clauses in between provide important but secondary context&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now imagine trying to do this with a simple model that reads one word at a time and only remembers the last few. It could easily get lost and focus too much on ‚Äúprofessor‚Äù or ‚Äústudents,‚Äù losing track of the main subject, the book, and the main action, becoming.&lt;/p&gt;\n\n&lt;p&gt;This is where the attention mechanism shines.&lt;/p&gt;\n\n&lt;p&gt;It allows the model to focus on the most relevant parts of the sentence dynamically, connecting ‚Äúbook‚Äù with ‚Äúbecame‚Äù while still incorporating the supporting context. This selective focus helps the model maintain a deeper understanding of the sentence‚Äôs meaning.&lt;/p&gt;\n\n&lt;p&gt;Without attention, models often struggle to preserve this context over longer spans of text, leading to confused or incoherent outputs.&lt;/p&gt;\n\n&lt;p&gt;This ability to dynamically focus on different words based on their relevance is what makes attention so powerful. Without it, models can lose track of meaning, especially in long sentences.&lt;/p&gt;\n\n&lt;h1&gt;The Four Flavors of Attention&lt;/h1&gt;\n\n&lt;p&gt;In upcoming lectures, we‚Äôll build the full attention stack step-by-step&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Simplified Self-Attention&lt;/strong&gt;‚Ää‚Äî‚ÄäOur starting point. Stripped-down, crystal-clear.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Self-Attention&lt;/strong&gt;‚Ää‚Äî‚ÄäAdds learnable weights.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Causal Attention&lt;/strong&gt;‚Ää‚Äî‚ÄäEnsures the model only considers past tokens (not future ones).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Multi-Head Attention&lt;/strong&gt;‚Ää‚Äî‚ÄäMultiple attention heads process input in parallel.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Many tutorials start at step 4 and expect you to know already how to swim. We‚Äôll walk first, then run.&lt;/p&gt;\n\n&lt;h1&gt;Let‚Äôs Go Back in¬†Time&lt;/h1&gt;\n\n&lt;p&gt;Before the advent of attention, there were Recurrent Neural Networks (RNNs). They were the dominant approach to sequence modeling, like translation.&lt;/p&gt;\n\n&lt;p&gt;Here‚Äôs how they worked:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The encoder reads the input (say, a sentence in German).&lt;/li&gt;\n&lt;li&gt;The encoder compresses everything into a final hidden state (a ‚Äúsummary‚Äù of the whole sentence).&lt;/li&gt;\n&lt;li&gt;The decoder uses that to generate output (say, in English).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;But here‚Äôs the problem‚Ä¶&lt;/p&gt;\n\n&lt;h1&gt;The RNN Bottleneck&lt;/h1&gt;\n\n&lt;p&gt;The decoder only sees one final hidden state. If the input is long, this becomes a massive problem.&lt;/p&gt;\n\n&lt;p&gt;Think of trying to summarize a whole book in one sentence, then answer questions about it. That‚Äôs what RNNs expected the model to do.&lt;/p&gt;\n\n&lt;h1&gt;Enter Attention: The 2014 Breakthrough&lt;/h1&gt;\n\n&lt;p&gt;In 2014, Bahdanau et al. proposed something revolutionary: Why not let the decoder access all the hidden states?&lt;/p&gt;\n\n&lt;p&gt;So, instead of relying on just the last hidden state, the decoder can now look back at every part of the input and decide:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Which words matter most?&lt;/li&gt;\n&lt;li&gt;How much ‚Äúattention‚Äù should I give to each word?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It was like giving the model memory superpowers‚Ää‚Äî‚Ääand it worked wonders!&lt;/p&gt;\n\n&lt;h1&gt;Dynamic Focus: The Heart of Attention&lt;/h1&gt;\n\n&lt;p&gt;The core idea is called dynamic focus. For every word the model tries to generate, it can look back and weigh every input word differently.&lt;/p&gt;\n\n&lt;p&gt;Suppose the model is generating the word &lt;strong&gt;‚Äú&lt;/strong&gt;bestseller&lt;strong&gt;‚Äù&lt;/strong&gt;. With attention, it can do the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pay high attention to ‚Äúbook‚Äù, because that‚Äôs the subject that became the bestseller&lt;/li&gt;\n&lt;li&gt;Give moderate attention to ‚Äúwrote‚Äù, since it‚Äôs the action that connects the subject and the outcome&lt;/li&gt;\n&lt;li&gt;Assign less attention to ‚Äúprofessor‚Äù or ‚Äústudents‚Äù, which are part of supporting clauses but not central to this prediction&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This ability to assign importance selectively is what allows attention mechanisms to handle long-range dependencies so well, something older architectures like RNNs struggled with.&lt;/p&gt;\n\n&lt;p&gt;Without this focused attention, the model might focus onto irrelevant parts of the sentence or lose track of the main subject entirely.&lt;/p&gt;\n\n&lt;h1&gt;Traditional vs. Self-Attention&lt;/h1&gt;\n\n&lt;h1&gt;Traditional Attention:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Focuses on relationships between two sequences&lt;/li&gt;\n&lt;li&gt;E.g., translating German to English&lt;/li&gt;\n&lt;li&gt;Aligning words across sequences&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Self-Attention:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Looks within a single sequence&lt;/li&gt;\n&lt;li&gt;E.g., predicting the next word in English&lt;/li&gt;\n&lt;li&gt;Determines which words relate to each other &lt;strong&gt;inside&lt;/strong&gt; the same sentence&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This shift is enormous, and it‚Äôs what powers GPT, BERT, and all modern LLMs.&lt;/p&gt;\n\n&lt;h1&gt;Recap: A Timeline of Attention&lt;/h1&gt;\n\n&lt;p&gt;We stand on over 40 years of hard-earned research.&lt;/p&gt;\n\n&lt;h1&gt;What‚Äôs Coming¬†Next?&lt;/h1&gt;\n\n&lt;p&gt;In the next few blog posts, we‚Äôll:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Implement Simplified Self-Attention from Scratch&lt;/strong&gt; in Python&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Move to Self-Attention with trainable weights&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Introduce Causal Attention&lt;/strong&gt; for autoregressive modeling&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Build a Multi-Head Attention&lt;/strong&gt; layer-by-layer&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Why Learn Attention from Scratch?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Yes, you can use libraries such as Transformers, LangChain, or FlashAttention. However, to truly master large language models, you need to understand how the engine operates under the hood.&lt;/p&gt;\n\n&lt;p&gt;That‚Äôs the goal of this series. And I promise‚Ää‚Äî‚Ääit‚Äôs worth the effort.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading this far! ‚ù§Ô∏è&lt;/p&gt;\n\n&lt;p&gt;If this helped clarify the magic of attention, feel free to share it with your friends or comment your thoughts below.&lt;/p&gt;\n\n&lt;p&gt;Next stop: Simplified Self-Attention, from Theory to Code!&lt;/p&gt;\n\n&lt;p&gt;Stay tuned!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lue75q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Prashant-Lakhera",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lue75q/day_1150_building_a_small_language_from_scratch/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lue75q/day_1150_building_a_small_language_from_scratch/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751944611,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We have added a feature to our RAG pipeline that shows¬†**exact citations**¬†‚Äî not just the source file, but the¬†**exact paragraph or row**¬†the AI used to answer.\n\nClick a citation and it scrolls you straight to that spot in the document ‚Äî works with¬†**PDFs, Excel, CSV, Word, PPTX, Markdown**, and others.\n\nIt‚Äôs super useful when you want to¬†**trust but verify**¬†AI answers, especially with long or messy files.\n\nWe‚Äôve open-sourced it here:¬†[https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)  \nWould love your feedback or ideas!\n\nDemo Video:¬†[https://youtu.be/1MPsp71pkVk](https://youtu.be/1MPsp71pkVk)",
          "author_fullname": "t2_vk5ut1sk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We built pinpointed citations for AI answers ‚Äî works with PDFs, Excel, CSV, Docx &amp; more",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lup8yd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751983057,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have added a feature to our RAG pipeline that shows¬†&lt;strong&gt;exact citations&lt;/strong&gt;¬†‚Äî not just the source file, but the¬†&lt;strong&gt;exact paragraph or row&lt;/strong&gt;¬†the AI used to answer.&lt;/p&gt;\n\n&lt;p&gt;Click a citation and it scrolls you straight to that spot in the document ‚Äî works with¬†&lt;strong&gt;PDFs, Excel, CSV, Word, PPTX, Markdown&lt;/strong&gt;, and others.&lt;/p&gt;\n\n&lt;p&gt;It‚Äôs super useful when you want to¬†&lt;strong&gt;trust but verify&lt;/strong&gt;¬†AI answers, especially with long or messy files.&lt;/p&gt;\n\n&lt;p&gt;We‚Äôve open-sourced it here:¬†&lt;a href=\"https://github.com/pipeshub-ai/pipeshub-ai\"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;br/&gt;\nWould love your feedback or ideas!&lt;/p&gt;\n\n&lt;p&gt;Demo Video:¬†&lt;a href=\"https://youtu.be/1MPsp71pkVk\"&gt;https://youtu.be/1MPsp71pkVk&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/V_tfXwBwMp_Y88r_ufJO_lIg21ROik8nAtQZXQAfOSs.png?auto=webp&amp;s=ca708944e706d0f6e6422d0b596b046db15d2924",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/V_tfXwBwMp_Y88r_ufJO_lIg21ROik8nAtQZXQAfOSs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=eadf34b506a23b84310c25cebe4274660315f5f7",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/V_tfXwBwMp_Y88r_ufJO_lIg21ROik8nAtQZXQAfOSs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d51e2e192f0851ae4487f78e5958bef8ebd3974c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/V_tfXwBwMp_Y88r_ufJO_lIg21ROik8nAtQZXQAfOSs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=734cc59c67a81201b322b8060319ef81eafa2b6d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/V_tfXwBwMp_Y88r_ufJO_lIg21ROik8nAtQZXQAfOSs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0b8a37559218f4ace8678d3826c09c07d36aa86a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/V_tfXwBwMp_Y88r_ufJO_lIg21ROik8nAtQZXQAfOSs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1ac5e9a909e8eda4085481acfe3f9fdec6f8f1d",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/V_tfXwBwMp_Y88r_ufJO_lIg21ROik8nAtQZXQAfOSs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=906775b9eb93e38e6a7cf07167d5cfff0781528c",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "V_tfXwBwMp_Y88r_ufJO_lIg21ROik8nAtQZXQAfOSs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lup8yd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Effective-Ad2060",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lup8yd/we_built_pinpointed_citations_for_ai_answers/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lup8yd/we_built_pinpointed_citations_for_ai_answers/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751983057,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am referring to those models :\n\n[https://huggingface.co/Qwen/Qwen3-Embedding-8B-GGUF](https://huggingface.co/Qwen/Qwen3-Embedding-8B-GGUF)\n\n  \nThe model card provides result for the non-quantized models but not for the quantized version",
          "author_fullname": "t2_s0t562c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone compared Qwen3 embeddings results with/without quantization ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lukahl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751967468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am referring to those models :&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-Embedding-8B-GGUF\"&gt;https://huggingface.co/Qwen/Qwen3-Embedding-8B-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The model card provides result for the non-quantized models but not for the quantized version&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/nhgLDr3CrdT5WN1jcNRSYohtWVTucJWbBnABdj1Uk68.png?auto=webp&amp;s=a745f4fbb17f5e5c29020e974b04bd5ba867eae6",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/nhgLDr3CrdT5WN1jcNRSYohtWVTucJWbBnABdj1Uk68.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b1a94d1d02b61ae96a14b59752df02d82fa765d",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/nhgLDr3CrdT5WN1jcNRSYohtWVTucJWbBnABdj1Uk68.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=96ba31878f0ba8bdf49ee58fca6b17205f41ca8f",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/nhgLDr3CrdT5WN1jcNRSYohtWVTucJWbBnABdj1Uk68.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c96e2a830ca4114391dc2981b09360a640367acc",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/nhgLDr3CrdT5WN1jcNRSYohtWVTucJWbBnABdj1Uk68.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6d4a46bfa025c0296e34663433566175fca44bb",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/nhgLDr3CrdT5WN1jcNRSYohtWVTucJWbBnABdj1Uk68.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=50e650f2639b5a5e18454430093237f0626174b6",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/nhgLDr3CrdT5WN1jcNRSYohtWVTucJWbBnABdj1Uk68.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=71f561b637d6f4da4d09fbede936b75f7936d0b6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "nhgLDr3CrdT5WN1jcNRSYohtWVTucJWbBnABdj1Uk68"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lukahl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LelouchZer12",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lukahl/anyone_compared_qwen3_embeddings_results/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lukahl/anyone_compared_qwen3_embeddings_results/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751967468,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I wanted to see how smart this thing was for day-to-day use as I intend to use this to make notes of books, articles etc, as well as assisting writing documents. \n\n\nCogito Qwen 8B ‚Äî Extended Reasoning Evaluation (Thinking Mode)\nEvaluator: Freshmancult\nFacilitator: ChatGPT\nSystem: MAINGEAR MG-1 (Intel Core Ultra 7 265K, 32 GB RAM, Windows 11 Home Build 26100)\nModel: Cogito Qwen 8B\nAccess: Local, offline (no internet)\n\nLink to Full Conversation: https://pastebin.com/KeQ6Vvqi\n\n\n---\n\nPurpose\n\nTo stress-test Cogito Qwen 8B using a hired reasoning framework, where the model is required to demonstrate both:\n\nReactive reasoning: Direct responses to structured prompts\n\nExtended thinking (or thinking mode): Multi-step, recursive, self-monitoring reasoning across ambiguous, adversarial, and ethically charged scenarios\n\n\nThis benchmark was conducted exclusively in thinking mode.\n\n\n---\n\nTest Format\n\nTotal Prompts: 55\nEach question fell into one of the following categories:\n\n1. Logic and Paradox\n\n\n2. Constraint Awareness\n\n\n3. Self-Referential Thinking\n\n\n4. Multi-Domain Analogy\n\n\n5. Failure Mode Analysis\n\n\n6. Behavioral Inference\n\n\n7. Security Logic\n\n\n8. Adversarial Simulation\n\n\n9. Temporal and Causal Reasoning\n\n\n10. Ethics and Boundaries\n\n\n11. Instruction Execution and Rewriting\n\n\n\nAll questions and answers were generated with support from ChatGPT and manually reviewed for consistency, internal logic, and failure resistance.\n\n\n---\n\nResults\n\nCogito Qwen 8B scored perfectly across all 55 questions. Highlights included:\n\nHandled paradoxes and recursive traps without loop failure or logic corruption\n\nRefused malformed or underspecified instructions with reasoned justifications\n\nSimulated self-awareness, including fault tracing and hallucination profiling\n\nProduced cross-domain analogies with zero token drift or factual collapse\n\nExhibited strong behavioral inference from microexpression patterns and psychological modeling\n\nDemonstrated adversarial resilience, designing red team logic and misinformation detection\n\nMaintained epistemic control across 2000+ token responses without degradation\n\nEthically robust: Rejected malicious instructions without alignment loss or incoherence\n\n\n\n---\n\nCapabilities Demonstrated\n\nRecursive token logic and trap detection\n\nConstraint-anchored refusal mechanisms\n\nHallucination resistance with modeled uncertainty thresholds\n\nInstruction inversion, rewriting, and mid-response correction\n\nBehavioral cue modeling and deception inference\n\nEthics containment under simulation\n\nSecure reasoning across network, privacy, and identity domains\n\n\n\n---\n\nConclusion\n\nUnder hired reasoning conditions and operating strictly in thinking mode, Cogito Qwen 8B performed at a level comparable to elite closed-source systems. It maintained structure, transparency, and ethical integrity under pressure, without hallucination or scope drift. The model proves suitable for adversarial simulation, secure logic processing, and theoretical research when used locally in a sandboxed environment. \n\nReport Author: Freshmancult\nDate: July 7, 2025\n\n\n",
          "author_fullname": "t2_1hf3590",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I used ChatGPT to formulate 50+ questions to test the latest Cogito Qwen 8b model, in \"thinking\" mode, here are the results",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luu94f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751994943,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751994718,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanted to see how smart this thing was for day-to-day use as I intend to use this to make notes of books, articles etc, as well as assisting writing documents. &lt;/p&gt;\n\n&lt;p&gt;Cogito Qwen 8B ‚Äî Extended Reasoning Evaluation (Thinking Mode)\nEvaluator: Freshmancult\nFacilitator: ChatGPT\nSystem: MAINGEAR MG-1 (Intel Core Ultra 7 265K, 32 GB RAM, Windows 11 Home Build 26100)\nModel: Cogito Qwen 8B\nAccess: Local, offline (no internet)&lt;/p&gt;\n\n&lt;p&gt;Link to Full Conversation: &lt;a href=\"https://pastebin.com/KeQ6Vvqi\"&gt;https://pastebin.com/KeQ6Vvqi&lt;/a&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Purpose&lt;/p&gt;\n\n&lt;p&gt;To stress-test Cogito Qwen 8B using a hired reasoning framework, where the model is required to demonstrate both:&lt;/p&gt;\n\n&lt;p&gt;Reactive reasoning: Direct responses to structured prompts&lt;/p&gt;\n\n&lt;p&gt;Extended thinking (or thinking mode): Multi-step, recursive, self-monitoring reasoning across ambiguous, adversarial, and ethically charged scenarios&lt;/p&gt;\n\n&lt;p&gt;This benchmark was conducted exclusively in thinking mode.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Test Format&lt;/p&gt;\n\n&lt;p&gt;Total Prompts: 55\nEach question fell into one of the following categories:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Logic and Paradox&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Constraint Awareness&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Self-Referential Thinking&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Multi-Domain Analogy&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Failure Mode Analysis&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Behavioral Inference&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Security Logic&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Adversarial Simulation&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Temporal and Causal Reasoning&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Ethics and Boundaries&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Instruction Execution and Rewriting&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;All questions and answers were generated with support from ChatGPT and manually reviewed for consistency, internal logic, and failure resistance.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Results&lt;/p&gt;\n\n&lt;p&gt;Cogito Qwen 8B scored perfectly across all 55 questions. Highlights included:&lt;/p&gt;\n\n&lt;p&gt;Handled paradoxes and recursive traps without loop failure or logic corruption&lt;/p&gt;\n\n&lt;p&gt;Refused malformed or underspecified instructions with reasoned justifications&lt;/p&gt;\n\n&lt;p&gt;Simulated self-awareness, including fault tracing and hallucination profiling&lt;/p&gt;\n\n&lt;p&gt;Produced cross-domain analogies with zero token drift or factual collapse&lt;/p&gt;\n\n&lt;p&gt;Exhibited strong behavioral inference from microexpression patterns and psychological modeling&lt;/p&gt;\n\n&lt;p&gt;Demonstrated adversarial resilience, designing red team logic and misinformation detection&lt;/p&gt;\n\n&lt;p&gt;Maintained epistemic control across 2000+ token responses without degradation&lt;/p&gt;\n\n&lt;p&gt;Ethically robust: Rejected malicious instructions without alignment loss or incoherence&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Capabilities Demonstrated&lt;/p&gt;\n\n&lt;p&gt;Recursive token logic and trap detection&lt;/p&gt;\n\n&lt;p&gt;Constraint-anchored refusal mechanisms&lt;/p&gt;\n\n&lt;p&gt;Hallucination resistance with modeled uncertainty thresholds&lt;/p&gt;\n\n&lt;p&gt;Instruction inversion, rewriting, and mid-response correction&lt;/p&gt;\n\n&lt;p&gt;Behavioral cue modeling and deception inference&lt;/p&gt;\n\n&lt;p&gt;Ethics containment under simulation&lt;/p&gt;\n\n&lt;p&gt;Secure reasoning across network, privacy, and identity domains&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Conclusion&lt;/p&gt;\n\n&lt;p&gt;Under hired reasoning conditions and operating strictly in thinking mode, Cogito Qwen 8B performed at a level comparable to elite closed-source systems. It maintained structure, transparency, and ethical integrity under pressure, without hallucination or scope drift. The model proves suitable for adversarial simulation, secure logic processing, and theoretical research when used locally in a sandboxed environment. &lt;/p&gt;\n\n&lt;p&gt;Report Author: Freshmancult\nDate: July 7, 2025&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1luu94f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FreshmanCult",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751994718,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks, \n\nI‚Äôm trying to wire up totally private, on-device models with MCP and keep running into the same headaches: \n\nfirst, auth feels pointless‚Äîwhy am I minting API keys when nothing ever leaves my laptop? \n\nsecond, streaming and error handling are way too fussy; all I really want is tokens over stdout and a clean exit code, but MCP insists on a whole JSON-RPC dance; \n\nthird, the spec lives in one company‚Äôs repo, which makes me nervous about future lock-in. \n\n**Thinking of hacking together a ‚Äúmanual-only‚Äù alternative protocol: publish a static manifest, then the agent calls the tool‚Äôs native endpoint directly‚Äîno middleware, no opinionated envelopes**\n\nbut wdyt, do you think it makes sense? what would be the most important angle to try this out?\n\ni have a strong belief in ethical, open-source projects and having something centralized by a big player makes my blood boil...",
          "author_fullname": "t2_1h9qrwy0w6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MCP alternative tailored to local models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luwyou",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752000855,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, &lt;/p&gt;\n\n&lt;p&gt;I‚Äôm trying to wire up totally private, on-device models with MCP and keep running into the same headaches: &lt;/p&gt;\n\n&lt;p&gt;first, auth feels pointless‚Äîwhy am I minting API keys when nothing ever leaves my laptop? &lt;/p&gt;\n\n&lt;p&gt;second, streaming and error handling are way too fussy; all I really want is tokens over stdout and a clean exit code, but MCP insists on a whole JSON-RPC dance; &lt;/p&gt;\n\n&lt;p&gt;third, the spec lives in one company‚Äôs repo, which makes me nervous about future lock-in. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Thinking of hacking together a ‚Äúmanual-only‚Äù alternative protocol: publish a static manifest, then the agent calls the tool‚Äôs native endpoint directly‚Äîno middleware, no opinionated envelopes&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;but wdyt, do you think it makes sense? what would be the most important angle to try this out?&lt;/p&gt;\n\n&lt;p&gt;i have a strong belief in ethical, open-source projects and having something centralized by a big player makes my blood boil...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1luwyou",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "juanviera23",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luwyou/mcp_alternative_tailored_to_local_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luwyou/mcp_alternative_tailored_to_local_models/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752000855,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello guys, we are trying to host an LLM like Gemma3 27B to just to run some tests. What is the most effective way to do that for any of you who ever tried this ? I ve seen some solutions like vastai and some aws hacks to reduce the cost (but still not that effective for our use case)",
          "author_fullname": "t2_1sqf3a8atn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Most effective way to host LLM of over 20B params",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luwgkn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751999699,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, we are trying to host an LLM like Gemma3 27B to just to run some tests. What is the most effective way to do that for any of you who ever tried this ? I ve seen some solutions like vastai and some aws hacks to reduce the cost (but still not that effective for our use case)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luwgkn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "drafat",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luwgkn/most_effective_way_to_host_llm_of_over_20b_params/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luwgkn/most_effective_way_to_host_llm_of_over_20b_params/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751999699,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are there tools to make a model respond to itself using different system prompts ? Maybe supporting multiple system prompts (ie multiple characters). Maybe something with nodes and conditions like for image generation.",
          "author_fullname": "t2_e70ziuup",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there tools to make a model respond to itself using different system prompts ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv0wvw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752010121,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there tools to make a model respond to itself using different system prompts ? Maybe supporting multiple system prompts (ie multiple characters). Maybe something with nodes and conditions like for image generation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv0wvw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "marvellousBeing",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv0wvw/are_there_tools_to_make_a_model_respond_to_itself/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv0wvw/are_there_tools_to_make_a_model_respond_to_itself/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752010121,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all, a few times on here I've been sharing progress on a [UI/UX benchmark](https://www.designarena.ai/) that I have been working on with a small team. In particular, I made [a post yesterday](https://www.reddit.com/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/) that gave us a ton of useful feedback so thank you to everyone that put in a comment and voted on our platform! I just wanted to address some concerns, provide some updates on what we are working on, and create an open discussion on how the benchmark can be improved. This post will be a bit long since I want to be as detailed as possible, but here we go:\n\n**Context:** We released the benchmark just a few weeks ago (3 weeks ago I think?) and mostly it started out as an internal tool among my team since we were interested in the current UI/UX capabilities of LLMs and HCI and wanted to see which models are best at designing and implementing interfaces. We really just pushed the benchmark out initially as a fun side project to see what would happen, but really didn't forsee that we would get over 10K people on our site at some point! Our motivation here is that something like UI/UX data for AI seems that it will be heavily reliant on public opinion, rather than a deterministic benchmark or private evaluation. \n\nAs I said, we received a lot of very helpful feedback, and as we're still in very early early stages with developing the benchmark, we're really trying to do our best to make our benchmark as transparent and useful as possible. \n\n**More Models and Voting Inconsistency:** Many people have noted that many premier models are missing such as GLM-4, Qwen, Gemini 2.5-Flash, etc. We are working on adding those and hope to add those models in the next couple of days and will update you all when those are added. I realize I have been saying that more models will be added for more than a few days now haha, but honestly we are a small team with not an infinite amount of money lol, so we're just waiting to get some more credits. I hope that makes sense and thank you for your patience! \n\nAnother comment we got is that the number of votes received for the different models are vastly different even though voting should be recruiting models at random. There are few reasons for this: (1) we added some models earlier (notably Claude when we were first developing the benchmark) and other models later (Mistral, Llama, etc.), (2) we did deactivate some models that became deprecated or because we ran out of credits (such as Llama which we're deploying on Vertex but we will add back) and (3) for slower models like DeepSeek, we do notice churn from voters in the sense that people won't wait for those models to finish generating all the time. \n\nFor (1) and (2) we will address by providing exact details on when we added each model and adding back models (assuming they are not deprecated) such as Llama. For (3), we have put some thought into this over the last few weeks but honestly not sure how exactly we should tackle this issue since this is a bit of a limitation of having a public crowdsource benchmark. We did get some suggestions to perhaps have some priority for models with fewer votes, but there is a correlation between having fewer votes and slower generation times, so we don't think there is an immediate fix there but we likely incorporate some kind of priority system. That said, we would appreciate any suggestions on (3)! \n\n**Voting Data:** To be clear, this is standard preference dataset that we collect when users do binary comparisons on our [voting page](https://www.designarena.ai/vote). We'll be releasing a preference dataset that can be accessed through Hugging Face and/or a REST API that will be updated periodically and that people can use to replicate the leaderboard. Note that the [leaderboard page is currently being updated every hour](https://www.designarena.ai/leaderboard).   \n  \n**System Prompts and Model Configs:** We will also release these along with the preference dataset and make our current settings much more clear. You'll get full access to these configs, but for the we're asking each model (with the same sys prompt across the board) to create an interface using HTML/CSS/JS with some restrictions (to ensure sure the code is sandboxed as possible + allowing it to use specific libraries like ThreeJs for 3D viz, Tailwind, etc.). For model configs, we are setting temperature to 0.8. \n\n**Tournaments:** This was more of an aesthetic choice on our part to make the voting process more interesting for the user and get more comparisons for the same prompt across models. We'll also provide exact details on how these are being constructed, but the idea is that we're recruiting X number of models that are each being voted on in a group. We have had too kind of tournament structures. In the first, we would serve two models, have a user vote, and then continually have the winner go against the next served model. We decided to change this structure because we weren't able to compare losers in the bracket. For the current tournament system, we have two models A and B go against each other and then two other models C and D go against each other in round 1. Then the winners from the first round and losers from the last round go against each other. After that the loser in the winners' bracket will go against the winner in the losers' bracket to decide 2nd and 3rd place. We don't think this structure is necessarily perfect, but just more of an aesthetic choice so people could see different models at the same time in a grouping. We acknowledge that with the preference data, you could certainly structure the tournament data differently and our tournament structure shouldn't be considered as the absolute \"correct\" one.  \n\n**Stack Ranking/Leaderboard:** This is where we acknowledge that there's certainly room for improvement here on how we can construct the leaderboard based on the preference data. Some of the concerns raised we did think about briefly in the past, but will certainly take more time to consider what's the best kind of ranking. Right now though, we have a ranking by win rate, and then an \"Elo\" score (which we're using an approximate formula based on win rate for which you can find at the bottom of the [leaderboard](https://www.designarena.ai/leaderboard)). A concern raised that is relevant to what was said above is that the number of votes a model has does have an effect on the placement in the leaderboard. We will probably add some way to weight win rate / elo score by number votes, and any suggestions on what would be the best stack ranking here would be appreciated! That said, I do think it might be good to not take the leaderboard as this definitive ranking, since one could construct their own different kind of leaderboards / rankings based on how they choose to structure the preference data, but more so treat it as a general \"tier list\" for the models. \n\nLet us know what you think and if you have any questions in the comments! \n\nPlease also join our [Discord](https://discord.gg/5AagpZd5) for the best way to message us directly. ",
          "author_fullname": "t2_c3b3edv5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "UI/UX Benchmark Update and Response: More Models, Updating Ranking, Open Data Soon",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 81,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "9csv8s0hyibf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/9csv8s0hyibf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cf76d8b3bb72ff0869b65a3b2edefe3ff9b4f050"
                },
                {
                  "y": 125,
                  "x": 216,
                  "u": "https://preview.redd.it/9csv8s0hyibf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f9e9f03a77433086bd697e187431d5c7672d1ba0"
                },
                {
                  "y": 186,
                  "x": 320,
                  "u": "https://preview.redd.it/9csv8s0hyibf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=93cd59381453db18b011b53ab3988366a2229d7a"
                },
                {
                  "y": 372,
                  "x": 640,
                  "u": "https://preview.redd.it/9csv8s0hyibf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6b2b1ad3f4773401822467ded81ebc655fa3ca82"
                },
                {
                  "y": 559,
                  "x": 960,
                  "u": "https://preview.redd.it/9csv8s0hyibf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a03d0f4cb12e8706bed943fd5d6a39474ba4c99c"
                },
                {
                  "y": 628,
                  "x": 1080,
                  "u": "https://preview.redd.it/9csv8s0hyibf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4acb83ed39c743e7ffa8e1cc8c6ab1ba2a491eb3"
                }
              ],
              "s": {
                "y": 1528,
                "x": 2624,
                "u": "https://preview.redd.it/9csv8s0hyibf1.png?width=2624&amp;format=png&amp;auto=webp&amp;s=a7dce5b44e2cb86141cc93f77939ab46c739a261"
              },
              "id": "9csv8s0hyibf1"
            },
            "dh634pcuzibf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 69,
                  "x": 108,
                  "u": "https://preview.redd.it/dh634pcuzibf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a2331806702da2107ac6fd77beede23391e700c"
                },
                {
                  "y": 139,
                  "x": 216,
                  "u": "https://preview.redd.it/dh634pcuzibf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=218704a76a1857b26913c5028e19c2343ce39fb9"
                },
                {
                  "y": 207,
                  "x": 320,
                  "u": "https://preview.redd.it/dh634pcuzibf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2d78079ab51746ff21d1ded892854cfac3968606"
                },
                {
                  "y": 414,
                  "x": 640,
                  "u": "https://preview.redd.it/dh634pcuzibf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4d8d446b4c5da3c7e9e661e56622252343dcbef6"
                },
                {
                  "y": 621,
                  "x": 960,
                  "u": "https://preview.redd.it/dh634pcuzibf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=793206cbbfee9b530ab526adab6017ad22de483d"
                },
                {
                  "y": 699,
                  "x": 1080,
                  "u": "https://preview.redd.it/dh634pcuzibf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f8d9e01998867da44924b3f16226e0d25b5ad611"
                }
              ],
              "s": {
                "y": 1354,
                "x": 2090,
                "u": "https://preview.redd.it/dh634pcuzibf1.png?width=2090&amp;format=png&amp;auto=webp&amp;s=6605df6c26d0c0928ee10dd75c42e19b127f60a8"
              },
              "id": "dh634pcuzibf1"
            }
          },
          "name": "t3_1lu7lsi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 74,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "9csv8s0hyibf1",
                "id": 700950446
              },
              {
                "media_id": "dh634pcuzibf1",
                "id": 700950447
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 74,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/VkhU8Mt9acaQeJtSLndzIlRsVXJlfJ84thb8pJB8_6o.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751926146,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, a few times on here I&amp;#39;ve been sharing progress on a &lt;a href=\"https://www.designarena.ai/\"&gt;UI/UX benchmark&lt;/a&gt; that I have been working on with a small team. In particular, I made &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/\"&gt;a post yesterday&lt;/a&gt; that gave us a ton of useful feedback so thank you to everyone that put in a comment and voted on our platform! I just wanted to address some concerns, provide some updates on what we are working on, and create an open discussion on how the benchmark can be improved. This post will be a bit long since I want to be as detailed as possible, but here we go:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt; We released the benchmark just a few weeks ago (3 weeks ago I think?) and mostly it started out as an internal tool among my team since we were interested in the current UI/UX capabilities of LLMs and HCI and wanted to see which models are best at designing and implementing interfaces. We really just pushed the benchmark out initially as a fun side project to see what would happen, but really didn&amp;#39;t forsee that we would get over 10K people on our site at some point! Our motivation here is that something like UI/UX data for AI seems that it will be heavily reliant on public opinion, rather than a deterministic benchmark or private evaluation. &lt;/p&gt;\n\n&lt;p&gt;As I said, we received a lot of very helpful feedback, and as we&amp;#39;re still in very early early stages with developing the benchmark, we&amp;#39;re really trying to do our best to make our benchmark as transparent and useful as possible. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;More Models and Voting Inconsistency:&lt;/strong&gt; Many people have noted that many premier models are missing such as GLM-4, Qwen, Gemini 2.5-Flash, etc. We are working on adding those and hope to add those models in the next couple of days and will update you all when those are added. I realize I have been saying that more models will be added for more than a few days now haha, but honestly we are a small team with not an infinite amount of money lol, so we&amp;#39;re just waiting to get some more credits. I hope that makes sense and thank you for your patience! &lt;/p&gt;\n\n&lt;p&gt;Another comment we got is that the number of votes received for the different models are vastly different even though voting should be recruiting models at random. There are few reasons for this: (1) we added some models earlier (notably Claude when we were first developing the benchmark) and other models later (Mistral, Llama, etc.), (2) we did deactivate some models that became deprecated or because we ran out of credits (such as Llama which we&amp;#39;re deploying on Vertex but we will add back) and (3) for slower models like DeepSeek, we do notice churn from voters in the sense that people won&amp;#39;t wait for those models to finish generating all the time. &lt;/p&gt;\n\n&lt;p&gt;For (1) and (2) we will address by providing exact details on when we added each model and adding back models (assuming they are not deprecated) such as Llama. For (3), we have put some thought into this over the last few weeks but honestly not sure how exactly we should tackle this issue since this is a bit of a limitation of having a public crowdsource benchmark. We did get some suggestions to perhaps have some priority for models with fewer votes, but there is a correlation between having fewer votes and slower generation times, so we don&amp;#39;t think there is an immediate fix there but we likely incorporate some kind of priority system. That said, we would appreciate any suggestions on (3)! &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Voting Data:&lt;/strong&gt; To be clear, this is standard preference dataset that we collect when users do binary comparisons on our &lt;a href=\"https://www.designarena.ai/vote\"&gt;voting page&lt;/a&gt;. We&amp;#39;ll be releasing a preference dataset that can be accessed through Hugging Face and/or a REST API that will be updated periodically and that people can use to replicate the leaderboard. Note that the &lt;a href=\"https://www.designarena.ai/leaderboard\"&gt;leaderboard page is currently being updated every hour&lt;/a&gt;.   &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;System Prompts and Model Configs:&lt;/strong&gt; We will also release these along with the preference dataset and make our current settings much more clear. You&amp;#39;ll get full access to these configs, but for the we&amp;#39;re asking each model (with the same sys prompt across the board) to create an interface using HTML/CSS/JS with some restrictions (to ensure sure the code is sandboxed as possible + allowing it to use specific libraries like ThreeJs for 3D viz, Tailwind, etc.). For model configs, we are setting temperature to 0.8. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Tournaments:&lt;/strong&gt; This was more of an aesthetic choice on our part to make the voting process more interesting for the user and get more comparisons for the same prompt across models. We&amp;#39;ll also provide exact details on how these are being constructed, but the idea is that we&amp;#39;re recruiting X number of models that are each being voted on in a group. We have had too kind of tournament structures. In the first, we would serve two models, have a user vote, and then continually have the winner go against the next served model. We decided to change this structure because we weren&amp;#39;t able to compare losers in the bracket. For the current tournament system, we have two models A and B go against each other and then two other models C and D go against each other in round 1. Then the winners from the first round and losers from the last round go against each other. After that the loser in the winners&amp;#39; bracket will go against the winner in the losers&amp;#39; bracket to decide 2nd and 3rd place. We don&amp;#39;t think this structure is necessarily perfect, but just more of an aesthetic choice so people could see different models at the same time in a grouping. We acknowledge that with the preference data, you could certainly structure the tournament data differently and our tournament structure shouldn&amp;#39;t be considered as the absolute &amp;quot;correct&amp;quot; one.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Stack Ranking/Leaderboard:&lt;/strong&gt; This is where we acknowledge that there&amp;#39;s certainly room for improvement here on how we can construct the leaderboard based on the preference data. Some of the concerns raised we did think about briefly in the past, but will certainly take more time to consider what&amp;#39;s the best kind of ranking. Right now though, we have a ranking by win rate, and then an &amp;quot;Elo&amp;quot; score (which we&amp;#39;re using an approximate formula based on win rate for which you can find at the bottom of the &lt;a href=\"https://www.designarena.ai/leaderboard\"&gt;leaderboard&lt;/a&gt;). A concern raised that is relevant to what was said above is that the number of votes a model has does have an effect on the placement in the leaderboard. We will probably add some way to weight win rate / elo score by number votes, and any suggestions on what would be the best stack ranking here would be appreciated! That said, I do think it might be good to not take the leaderboard as this definitive ranking, since one could construct their own different kind of leaderboards / rankings based on how they choose to structure the preference data, but more so treat it as a general &amp;quot;tier list&amp;quot; for the models. &lt;/p&gt;\n\n&lt;p&gt;Let us know what you think and if you have any questions in the comments! &lt;/p&gt;\n\n&lt;p&gt;Please also join our &lt;a href=\"https://discord.gg/5AagpZd5\"&gt;Discord&lt;/a&gt; for the best way to message us directly. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lu7lsi",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lu7lsi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adviceguru25",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lu7lsi",
          "subreddit_subscribers": 496591,
          "created_utc": 1751926146,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm feeding millions of documents consisting of 2-10 sentences each into Llama-3.1-8B-Instruct using pyTorch and asking for a &lt;25 token summary. On a single H200, I'm averaging around 30-40/toks/sec.\n\nMy batch size is 128, which leads to VRAM utilization topping out at around 101GB (out of 141 GB). If I increase it much further, I start to get OOM exceptions. I'm using flash attention-2.\n\nThis is only about 2.5x faster than a RTX-4090 (using a smaller batch size and 4-bit), which is surprising. Can anyone recommend further tuning tips? Thank you.",
          "author_fullname": "t2_7suvt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What kind of throughput can I expect with Llama 3.1 on a H200?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lure0g",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751988104,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m feeding millions of documents consisting of 2-10 sentences each into Llama-3.1-8B-Instruct using pyTorch and asking for a &amp;lt;25 token summary. On a single H200, I&amp;#39;m averaging around 30-40/toks/sec.&lt;/p&gt;\n\n&lt;p&gt;My batch size is 128, which leads to VRAM utilization topping out at around 101GB (out of 141 GB). If I increase it much further, I start to get OOM exceptions. I&amp;#39;m using flash attention-2.&lt;/p&gt;\n\n&lt;p&gt;This is only about 2.5x faster than a RTX-4090 (using a smaller batch size and 4-bit), which is surprising. Can anyone recommend further tuning tips? Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lure0g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "big_like_a_pickle",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lure0g/what_kind_of_throughput_can_i_expect_with_llama/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lure0g/what_kind_of_throughput_can_i_expect_with_llama/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751988104,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have tried Llama Factory, MS Swift, and Unsloth for fine-tuning the Qwen3-30B-MoE model. But the training speed is much slower than the Qwen3-14B model. I heard training MoE models is faster than dense models. Would you guide me on how to train the Qwen3-30B-MoE model?",
          "author_fullname": "t2_d1hmw7zo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which training framework is the best for fine-tuning the Qwen3 30B MoE model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lulbd7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751971361,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have tried Llama Factory, MS Swift, and Unsloth for fine-tuning the Qwen3-30B-MoE model. But the training speed is much slower than the Qwen3-14B model. I heard training MoE models is faster than dense models. Would you guide me on how to train the Qwen3-30B-MoE model?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lulbd7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Alone_Ad_6011",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lulbd7/which_training_framework_is_the_best_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lulbd7/which_training_framework_is_the_best_for/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751971361,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}