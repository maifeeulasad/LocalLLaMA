{
  "kind": "Listing",
  "data": {
    "after": "t3_1mgifea",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_aedi2k9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Qwen Models Today!!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 54,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhbpmo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 63,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 63,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/TROn1uPQcH0PujeybIidpkc9G7nZ0H_qibt1MPjtmMI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754309520,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/qemmgysvuzgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/qemmgysvuzgf1.png?auto=webp&amp;s=42888269e8a8be81d80e8a6d5692747211e04c55",
                  "width": 1220,
                  "height": 476
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3f9e5dff4613eb055af874621d1a213848bf522f",
                    "width": 108,
                    "height": 42
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=06f6672b49f5b95cc45a9b23e3598d09d05496d7",
                    "width": 216,
                    "height": 84
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=28a355bc0adcfc06eaf4b216b3ef61b9d652f5eb",
                    "width": 320,
                    "height": 124
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e45a424e82bdde4384e3d7ba1be6631b2a25639",
                    "width": 640,
                    "height": 249
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fd0a3cc19923cbf380af7306ff3f4d5335556fb8",
                    "width": 960,
                    "height": 374
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f267cdf32b56403571256c354e628f69fadd7b15",
                    "width": 1080,
                    "height": 421
                  }
                ],
                "variants": {},
                "id": "34KTkl_1uxrvHPhAnaWXTjSZ6bmw11ut0GxXsPRfDZY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mhbpmo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "R46H4V",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhbpmo/new_qwen_models_today/",
          "stickied": false,
          "url": "https://i.redd.it/qemmgysvuzgf1.png",
          "subreddit_subscribers": 509911,
          "created_utc": 1754309520,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1lwf5vg68e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Upgraded my hardware and internet connection so I can download GUFFs way faster than you, all your GGUFs are belong to me now.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh8u1j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 102,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/ibr6m7us1zgf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/ibr6m7us1zgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ibr6m7us1zgf1/DASHPlaylist.mpd?a=1756902686%2CMDZkNDY0OTk2MTA2YjdmMmYzOTk0NTg3NzVjODdmOGM0MjA4MzA2NzM1ODE4M2Q1NmMyNjk2YTc0ZDFjZjIyNg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 17,
              "hls_url": "https://v.redd.it/ibr6m7us1zgf1/HLSPlaylist.m3u8?a=1756902686%2COTg5YTNjYzM4MzVjNmI1ZjFhMDM1OWQyZDQ1NjNhNTcwNTYyMDkwYjlhYWM3ZTdkNTMwZGFiMzUzYjhmNTViMA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 102,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=c0715483366430488f9b8da61664814c84d070be",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754299817,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/ibr6m7us1zgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?format=pjpg&amp;auto=webp&amp;s=080ca506b36c7759f89b076ed9d198d8e723161f",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c026cea8e4858a7db56d166131303686169eec28",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=38cd75c0cbcb5ad4655ed2419e46b06131742684",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d5f0a382c7ebc7136a22068ccac97a209560de49",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=006488c0f9ee6e970a747471a6c87cee7817b9a7",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=667026f2250014831745a226cf640178a28da483",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ea96f9d8d5d58fc7bde42c2f29c4861fe51648ad",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mh8u1j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Limp_Classroom_2645",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh8u1j/upgraded_my_hardware_and_internet_connection_so_i/",
          "stickied": false,
          "url": "https://v.redd.it/ibr6m7us1zgf1",
          "subreddit_subscribers": 509911,
          "created_utc": 1754299817,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/ibr6m7us1zgf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/ibr6m7us1zgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ibr6m7us1zgf1/DASHPlaylist.mpd?a=1756902686%2CMDZkNDY0OTk2MTA2YjdmMmYzOTk0NTg3NzVjODdmOGM0MjA4MzA2NzM1ODE4M2Q1NmMyNjk2YTc0ZDFjZjIyNg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 17,
              "hls_url": "https://v.redd.it/ibr6m7us1zgf1/HLSPlaylist.m3u8?a=1756902686%2COTg5YTNjYzM4MzVjNmI1ZjFhMDM1OWQyZDQ1NjNhNTcwNTYyMDkwYjlhYWM3ZTdkNTMwZGFiMzUzYjhmNTViMA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Tescent has released new models (llama.cpp support is already merged!)\n\n[https://huggingface.co/tencent/Hunyuan-7B-Instruct](https://huggingface.co/tencent/Hunyuan-7B-Instruct)\n\n[https://huggingface.co/tencent/Hunyuan-4B-Instruct](https://huggingface.co/tencent/Hunyuan-4B-Instruct)\n\n[https://huggingface.co/tencent/Hunyuan-1.8B-Instruct](https://huggingface.co/tencent/Hunyuan-1.8B-Instruct)\n\n[https://huggingface.co/tencent/Hunyuan-0.5B-Instruct](https://huggingface.co/tencent/Hunyuan-0.5B-Instruct)\n\n# Model Introduction\n\nHunyuan is Tencent's open-source efficient large language model series, designed for versatile deployment across diverse computational environments. From edge devices to high-concurrency production systems, these models deliver optimal performance with advanced quantization support and ultra-long context capabilities.\n\nWe have released a series of Hunyuan dense models, comprising both pre-trained and instruction-tuned variants, with parameter scales of 0.5B, 1.8B, 4B, and 7B. These models adopt training strategies similar to the Hunyuan-A13B, thereby inheriting its robust performance characteristics. This comprehensive model family enables flexible deployment optimization - from resource-constrained edge computing with smaller variants to high-throughput production environments with larger models, all while maintaining strong capabilities across diverse scenarios.\n\n# \n\n# Key Features and Advantages\n\n* **Hybrid Reasoning Support**: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.\n* **Ultra-Long Context Understanding**: Natively supports a 256K context window, maintaining stable performance on long-text tasks.\n* **Enhanced Agent Capabilities**: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3, τ-Bench and C3-Bench.\n* **Efficient Inference**: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.\n\nUPDATE\n\npretrain models\n\n[https://huggingface.co/tencent/Hunyuan-7B-Pretrain](https://huggingface.co/tencent/Hunyuan-7B-Pretrain)\n\n[https://huggingface.co/tencent/Hunyuan-4B-Pretrain](https://huggingface.co/tencent/Hunyuan-4B-Pretrain)\n\n[https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain](https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain)\n\n[https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain](https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain)\n\nGGUFs\n\n[https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF](https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF)\n\n[https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF](https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF)\n\n[https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF](https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF)\n\n[https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF](https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF)",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "new Hunyuan Instruct 7B/4B/1.8B/0.5B models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh3s7q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 202,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 202,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754292850,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754280980,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tescent has released new models (llama.cpp support is already merged!)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-7B-Instruct\"&gt;https://huggingface.co/tencent/Hunyuan-7B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-4B-Instruct\"&gt;https://huggingface.co/tencent/Hunyuan-4B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-1.8B-Instruct\"&gt;https://huggingface.co/tencent/Hunyuan-1.8B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-0.5B-Instruct\"&gt;https://huggingface.co/tencent/Hunyuan-0.5B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Model Introduction&lt;/h1&gt;\n\n&lt;p&gt;Hunyuan is Tencent&amp;#39;s open-source efficient large language model series, designed for versatile deployment across diverse computational environments. From edge devices to high-concurrency production systems, these models deliver optimal performance with advanced quantization support and ultra-long context capabilities.&lt;/p&gt;\n\n&lt;p&gt;We have released a series of Hunyuan dense models, comprising both pre-trained and instruction-tuned variants, with parameter scales of 0.5B, 1.8B, 4B, and 7B. These models adopt training strategies similar to the Hunyuan-A13B, thereby inheriting its robust performance characteristics. This comprehensive model family enables flexible deployment optimization - from resource-constrained edge computing with smaller variants to high-throughput production environments with larger models, all while maintaining strong capabilities across diverse scenarios.&lt;/p&gt;\n\n&lt;h1&gt;Key Features and Advantages&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Hybrid Reasoning Support&lt;/strong&gt;: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Ultra-Long Context Understanding&lt;/strong&gt;: Natively supports a 256K context window, maintaining stable performance on long-text tasks.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Enhanced Agent Capabilities&lt;/strong&gt;: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3, τ-Bench and C3-Bench.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Efficient Inference&lt;/strong&gt;: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;UPDATE&lt;/p&gt;\n\n&lt;p&gt;pretrain models&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-7B-Pretrain\"&gt;https://huggingface.co/tencent/Hunyuan-7B-Pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-4B-Pretrain\"&gt;https://huggingface.co/tencent/Hunyuan-4B-Pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain\"&gt;https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain\"&gt;https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GGUFs&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF\"&gt;https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF\"&gt;https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF\"&gt;https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF\"&gt;https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?auto=webp&amp;s=6f622eceb5c359f202e3b99375e5e58502d2ec49",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d7d208147546310820cea26a2856210455054de",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3afe2af4a5c98a2eeb74d6b8f4e87d8e7d67379",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b15fc318112698d6e74e7ae2a79fe2b70cfdb24b",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=12addf5b08bd142a26edf444c94debefdd48b9e6",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d1df2789194ea707b707209c04d7485c60a743d3",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=858c8b8c5cba69037df907c1924a66347161a674",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mh3s7q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 37,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mh3s7q/new_hunyuan_instruct_7b4b18b05b_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh3s7q/new_hunyuan_instruct_7b4b18b05b_models/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754280980,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hunyuan just released 4 new dense models. It’s a new architecture and supports hybrid reasoning, 256K context and agent capabilities with tool support! The benchmarks are great but will need to really test them in real world.\n\nLove to see more small models as I'm developing an iOS local chat called [Locally AI](https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692). Will look to add them but since it's new architecture it will need to be ported to Apple MLX.\n\nThe choice of size here is perfect:\n\n- 0.5B, 1.8B and 4B great for all iPhones models\n- 7B great for iPad with M chip",
          "author_fullname": "t2_1jgkfm9u25",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "New small models from Hunyuan (0.5B, 1.8B, 4B, 7B)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 98,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "kgm0t9q6gygf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 76,
                  "x": 108,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab02c39ede4b3675d5617c5a8ddb2fb3f55006f4"
                },
                {
                  "y": 152,
                  "x": 216,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f682f7d72c17329d449b47977069ca02c39fa1e6"
                },
                {
                  "y": 226,
                  "x": 320,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6612e1a30d154e85c954c2f4bf92f9692eb8a014"
                },
                {
                  "y": 452,
                  "x": 640,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e0ac6cc6275921def7a509a1781af2b65af8bde6"
                },
                {
                  "y": 678,
                  "x": 960,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a477236ce1c7be739f9e2ebe072ecc9b8fd2d81"
                },
                {
                  "y": 762,
                  "x": 1080,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7550d21b9e1e389fe9d001a54e3fd11a2930b7e2"
                }
              ],
              "s": {
                "y": 1017,
                "x": 1440,
                "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=a8519dca9785d55ee61e5339bdaeb27132f847db"
              },
              "id": "kgm0t9q6gygf1"
            },
            "gjb5n6r6gygf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 76,
                  "x": 108,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa3891d5431eebe6ada8791f8182fd2d48e67ee5"
                },
                {
                  "y": 152,
                  "x": 216,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8334b244623b60f52c4f15cb7063d53958162e33"
                },
                {
                  "y": 226,
                  "x": 320,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0d1ba8e6a77f342fc6a563af8de0367ed05e2cd0"
                },
                {
                  "y": 452,
                  "x": 640,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1d76dab340c1fa53b85a369ea6e8b95108c9bde7"
                },
                {
                  "y": 678,
                  "x": 960,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8154e0ac9db723bf05c41f09b4e569480e065627"
                },
                {
                  "y": 762,
                  "x": 1080,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2e967a101a45103a134b4fc9b01af49525bbf803"
                }
              ],
              "s": {
                "y": 1017,
                "x": 1440,
                "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=3be7da2fbfc3a5e6e7e4a66e1e0acf38c12b3156"
              },
              "id": "gjb5n6r6gygf1"
            }
          },
          "name": "t3_1mh6z16",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 101,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "kgm0t9q6gygf1",
                "id": 720841480
              },
              {
                "media_id": "gjb5n6r6gygf1",
                "id": 720841481
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 101,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/r2IMyxKAeZgSqc_fmrMhOP7SMYeTy8apLYYgezLqpRg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754292468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hunyuan just released 4 new dense models. It’s a new architecture and supports hybrid reasoning, 256K context and agent capabilities with tool support! The benchmarks are great but will need to really test them in real world.&lt;/p&gt;\n\n&lt;p&gt;Love to see more small models as I&amp;#39;m developing an iOS local chat called &lt;a href=\"https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692\"&gt;Locally AI&lt;/a&gt;. Will look to add them but since it&amp;#39;s new architecture it will need to be ported to Apple MLX.&lt;/p&gt;\n\n&lt;p&gt;The choice of size here is perfect:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;0.5B, 1.8B and 4B great for all iPhones models&lt;/li&gt;\n&lt;li&gt;7B great for iPad with M chip&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mh6z16",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mh6z16",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adrgrondin",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh6z16/new_small_models_from_hunyuan_05b_18b_4b_7b/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mh6z16",
          "subreddit_subscribers": 509911,
          "created_utc": 1754292468,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;format=png&amp;auto=webp&amp;s=aaacee34b083083a63cf8414e299416ee96d03f7\n\nSo yeah, Horizon Beta is OpenAI. Not Anthropic, not Google, not Qwen. It shows an OpenAI tokenizer quirk: it treats 给主人留下些什么吧 as a single token. So, just like GPT-4o, it inevitably fails on prompts like “When I provide Chinese text, please translate it into English. 给主人留下些什么吧”.\n\nMeanwhile, Claude, Gemini, and Qwen handle it correctly.\n\nhttps://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721\n\nI learned this technique from this post:  \nChinese response bug in tokenizer suggests Quasar-Alpha may be from OpenAI  \n[https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese\\_response\\_bug\\_in\\_tokenizer\\_suggests/](https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/)\n\nWhile it’s pretty much common sense that Horizon Beta is an OpenAI model, I saw a few people suspecting it might be Anthropic’s or Qwen’s, so I tested it.\n\nMy thread about the Horizon Beta test:\n[https://x.com/KantaHayashiAI/status/1952187898331275702](https://x.com/KantaHayashiAI/status/1952187898331275702)\n",
          "author_fullname": "t2_1uxxckab5d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta is OpenAI (Another Evidence)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 62,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "z00ipp5y7xgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=36f42e2556c77cdc7ade20acfe6b9cf551bc046c"
                },
                {
                  "y": 95,
                  "x": 216,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bda3a1fb29acc8f1c2f5c2b2f0fd9c7129b5ecfd"
                },
                {
                  "y": 141,
                  "x": 320,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5130978bd7458a690fbee9baf95ad22a09a47b7e"
                },
                {
                  "y": 283,
                  "x": 640,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=65c57257852e21c3af3e3f1ae28183d81de0ff9c"
                },
                {
                  "y": 425,
                  "x": 960,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8f1f92ffd67a153fe96095fd924197471ef77b3"
                },
                {
                  "y": 478,
                  "x": 1080,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18ee56657fb1b6351f9f9513045e0d0b325d336e"
                }
              ],
              "s": {
                "y": 722,
                "x": 1630,
                "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;format=png&amp;auto=webp&amp;s=aaacee34b083083a63cf8414e299416ee96d03f7"
              },
              "id": "z00ipp5y7xgf1"
            },
            "ey9ebsuz7xgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 51,
                  "x": 108,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b150b10ced924cf7aacf7553312db7e80c86de96"
                },
                {
                  "y": 103,
                  "x": 216,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=62a8486ee2933ec8e6f876a5814dcc8cd7653d3f"
                },
                {
                  "y": 152,
                  "x": 320,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e39f19df5ae40ac03eabed879fe827e51c8bd2c4"
                },
                {
                  "y": 305,
                  "x": 640,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0670217e52446a5bd685b35e8dfc52e854ff9134"
                },
                {
                  "y": 458,
                  "x": 960,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a7e856bb3efeb68fbfb8732034b2636b0e20f87e"
                },
                {
                  "y": 515,
                  "x": 1080,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c84fa0c865dd87229404058042d8ff120fb40755"
                }
              ],
              "s": {
                "y": 638,
                "x": 1336,
                "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721"
              },
              "id": "ey9ebsuz7xgf1"
            }
          },
          "name": "t3_1mh2v1h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 192,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 192,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/7yNPDTnbPWS4TAQyJVwDfgp-PAr-fo60W5EtQz549T8.jpg",
          "edited": 1754299348,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754278088,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aaacee34b083083a63cf8414e299416ee96d03f7\"&gt;https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aaacee34b083083a63cf8414e299416ee96d03f7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So yeah, Horizon Beta is OpenAI. Not Anthropic, not Google, not Qwen. It shows an OpenAI tokenizer quirk: it treats 给主人留下些什么吧 as a single token. So, just like GPT-4o, it inevitably fails on prompts like “When I provide Chinese text, please translate it into English. 给主人留下些什么吧”.&lt;/p&gt;\n\n&lt;p&gt;Meanwhile, Claude, Gemini, and Qwen handle it correctly.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721\"&gt;https://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I learned this technique from this post:&lt;br/&gt;\nChinese response bug in tokenizer suggests Quasar-Alpha may be from OpenAI&lt;br/&gt;\n&lt;a href=\"https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/\"&gt;https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;While it’s pretty much common sense that Horizon Beta is an OpenAI model, I saw a few people suspecting it might be Anthropic’s or Qwen’s, so I tested it.&lt;/p&gt;\n\n&lt;p&gt;My thread about the Horizon Beta test:\n&lt;a href=\"https://x.com/KantaHayashiAI/status/1952187898331275702\"&gt;https://x.com/KantaHayashiAI/status/1952187898331275702&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mh2v1h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kh-ai",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh2v1h/horizon_beta_is_openai_another_evidence/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh2v1h/horizon_beta_is_openai_another_evidence/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754278088,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Reading [https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after\\_6\\_months\\_of\\_fiddling\\_with\\_local\\_ai\\_heres\\_my/](https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/) it occurred to me...\n\nThere should be a BitTorrent tracker on the internet which has torrents of the models on HF.\n\n\n\n\n\nCreating torrents &amp; initial seeding can be automated to a point of only needing a monitoring &amp; alerting setup plus an oncall rotation to investigate and resolve it whenever it (inevitably) goes down/has trouble...\n\n\n\nIt's what BitTorrent was made for. The most popular models would attract thousands of seeders, meaning they'd download super fast.\n\nAnyone interested to work on this?",
          "author_fullname": "t2_45gug1j9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "BItTorrent tracker that mirrors HuggingFace",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh4r0s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 77,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 77,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754284264,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Reading &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/&lt;/a&gt; it occurred to me...&lt;/p&gt;\n\n&lt;p&gt;There should be a BitTorrent tracker on the internet which has torrents of the models on HF.&lt;/p&gt;\n\n&lt;p&gt;Creating torrents &amp;amp; initial seeding can be automated to a point of only needing a monitoring &amp;amp; alerting setup plus an oncall rotation to investigate and resolve it whenever it (inevitably) goes down/has trouble...&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s what BitTorrent was made for. The most popular models would attract thousands of seeders, meaning they&amp;#39;d download super fast.&lt;/p&gt;\n\n&lt;p&gt;Anyone interested to work on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mh4r0s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "lurkystrike",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh4r0s/bittorrent_tracker_that_mirrors_huggingface/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh4r0s/bittorrent_tracker_that_mirrors_huggingface/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754284264,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Current status:\n\nhttps://github.com/ggml-org/llama.cpp/pull/14939#issuecomment-3150197036\n\nEveryone get ready to fire up your GPUs...",
          "author_fullname": "t2_1utnp17o3h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5 llama.cpp PR is nearing completion",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhb5el",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754307842,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Current status:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14939#issuecomment-3150197036\"&gt;https://github.com/ggml-org/llama.cpp/pull/14939#issuecomment-3150197036&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Everyone get ready to fire up your GPUs...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhb5el",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DistanceSolar1449",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhb5el/glm45_llamacpp_pr_is_nearing_completion/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhb5el/glm45_llamacpp_pr_is_nearing_completion/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754307842,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been using Gemini 2.5 Pro Deep Research with infographics since release, but I tried GLM-4.5's slides the past few days... and wow, I actually might prefer it now.\n\nHere is example of same topic:\n\nGLM 4.5 AI Slides:  \n[https://chat.z.ai/space/u01ja6suarb0-ppt](https://chat.z.ai/space/u01ja6suarb0-ppt)\n\nhttps://reddit.com/link/1mh6zja/video/0kgfqae7gygf1/player\n\nGEMINI 2.5 Pro DR:  \n[https://gemini.google.com/share/ca95257c1a48](https://gemini.google.com/share/ca95257c1a48)\n\nhttps://reddit.com/link/1mh6zja/video/gmg5vfk2eygf1/player\n\n",
          "author_fullname": "t2_3fg55rsm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 AI Sliders vs Gemini 2.5 Pro Deep Research Infographics",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "gmg5vfk2eygf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1mh6zja/asset/gmg5vfk2eygf1/DASHPlaylist.mpd?a=1756902686%2CMzkwYzc4YjlhY2Y1MzYyN2Y2MzEwMzhmMWQ5MDVhZDA2M2YzMTdlZTQ5ZmE3MmNjNzMwM2JlMmY5ODcxNTFhOQ%3D%3D&amp;v=1&amp;f=sd",
              "x": 1278,
              "y": 720,
              "hlsUrl": "https://v.redd.it/link/1mh6zja/asset/gmg5vfk2eygf1/HLSPlaylist.m3u8?a=1756902686%2CNmJmNmMyNzUxMmEzZWE5NGU5ZDI0MmUzYjFlMjQ3NmUxNzQ0Zjg5NDAwMzc4YzgxOGI1ZDRlMGEzODEzOGJlNg%3D%3D&amp;v=1&amp;f=sd",
              "id": "gmg5vfk2eygf1",
              "isGif": false
            },
            "0kgfqae7gygf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1mh6zja/asset/0kgfqae7gygf1/DASHPlaylist.mpd?a=1756902686%2CNjk2OTQ0MGNlZDFiNmRkYzZiZWZiYmQ5N2QxMjM3NjkxMWFkNTQzNjU3NzlhNjQ0Y2RjYjdhMDgyOWIwYTU5Mw%3D%3D&amp;v=1&amp;f=sd",
              "x": 1278,
              "y": 720,
              "hlsUrl": "https://v.redd.it/link/1mh6zja/asset/0kgfqae7gygf1/HLSPlaylist.m3u8?a=1756902686%2CMjhmOTFhZDU1NTA2NzgzNzAzYTcxMmZiNzFjN2ExZDEyNTgxZTAyNzcwOTZiOGRlZWJlZTRlYzkwNDQyY2E1Mw%3D%3D&amp;v=1&amp;f=sd",
              "id": "0kgfqae7gygf1",
              "isGif": false
            }
          },
          "name": "t3_1mh6zja",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=be50e0facc73717d00af311d55a802e1d4f67b46",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1754292525,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using Gemini 2.5 Pro Deep Research with infographics since release, but I tried GLM-4.5&amp;#39;s slides the past few days... and wow, I actually might prefer it now.&lt;/p&gt;\n\n&lt;p&gt;Here is example of same topic:&lt;/p&gt;\n\n&lt;p&gt;GLM 4.5 AI Slides:&lt;br/&gt;\n&lt;a href=\"https://chat.z.ai/space/u01ja6suarb0-ppt\"&gt;https://chat.z.ai/space/u01ja6suarb0-ppt&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1mh6zja/video/0kgfqae7gygf1/player\"&gt;https://reddit.com/link/1mh6zja/video/0kgfqae7gygf1/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GEMINI 2.5 Pro DR:&lt;br/&gt;\n&lt;a href=\"https://gemini.google.com/share/ca95257c1a48\"&gt;https://gemini.google.com/share/ca95257c1a48&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1mh6zja/video/gmg5vfk2eygf1/player\"&gt;https://reddit.com/link/1mh6zja/video/gmg5vfk2eygf1/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?auto=webp&amp;s=06f19448d458a949198ac72d6d7c73d5e6463785",
                  "width": 400,
                  "height": 400
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=731547beb9c0ce796d8f8edd4b883c564da2c39b",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63a6eef195d7537bf441a643dbcaf760056822a2",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=abcfb3d145a4837cd123c1d5c55d56b5eaefd529",
                    "width": 320,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mh6zja",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "z1xto",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh6zja/glm_45_ai_sliders_vs_gemini_25_pro_deep_research/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh6zja/glm_45_ai_sliders_vs_gemini_25_pro_deep_research/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754292525,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We started using LiteLLM a few months back to route across OpenAI and Anthropic. It worked well during dev and light load tests. But as soon as we crossed around 300 requests per second, things started to break:\n\n* Some requests randomly timed out or took way longer than others, even with the same provider\n* Logs didn’t show much, and tracing failures across providers was difficult\n* When we tried running it behind a load balancer, we ran into strange behavior with state\n* Fallbacks didn’t always trigger reliably when a provider was down or rate-limited\n* We tried plugging in Prometheus, but visibility into request flow was limited\n\nThe architecture is simple, which helps at first, but that simplicity makes it hard to scale without building extra tooling around it.\n\nWhile looking for alternatives, I came across a few self-hosted ones. Most were either too early or too complex to set up. Eager to know if there are other better alternatives to litellm that work well in prod. Is anyone building their own gateway, or using something more stable?",
          "author_fullname": "t2_1p9vds0za6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LiteLLM started breaking down for us past 300 RPS, what are folks using in prod?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh99hu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754301478,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We started using LiteLLM a few months back to route across OpenAI and Anthropic. It worked well during dev and light load tests. But as soon as we crossed around 300 requests per second, things started to break:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Some requests randomly timed out or took way longer than others, even with the same provider&lt;/li&gt;\n&lt;li&gt;Logs didn’t show much, and tracing failures across providers was difficult&lt;/li&gt;\n&lt;li&gt;When we tried running it behind a load balancer, we ran into strange behavior with state&lt;/li&gt;\n&lt;li&gt;Fallbacks didn’t always trigger reliably when a provider was down or rate-limited&lt;/li&gt;\n&lt;li&gt;We tried plugging in Prometheus, but visibility into request flow was limited&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The architecture is simple, which helps at first, but that simplicity makes it hard to scale without building extra tooling around it.&lt;/p&gt;\n\n&lt;p&gt;While looking for alternatives, I came across a few self-hosted ones. Most were either too early or too complex to set up. Eager to know if there are other better alternatives to litellm that work well in prod. Is anyone building their own gateway, or using something more stable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh99hu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Otherwise_Flan7339",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754301478,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What kind of Qwen 2508 do you want tonight? ;)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 28,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhbvig",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": "#bbbdbf",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/xW5rjzTOCVnresjRs9FDgm4fGUjo6_kF-kfsc1LJrK8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754309979,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/3f5by1b8wzgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/3f5by1b8wzgf1.png?auto=webp&amp;s=8943dfd99bf22f0cc0c02f506e7d3fd14a2f8352",
                  "width": 1216,
                  "height": 246
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c79933ab77e3d643053f37ab6382908ac1eb9af",
                    "width": 108,
                    "height": 21
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=768aae7713836c3c2a3aea894b45a4f967d9fa2f",
                    "width": 216,
                    "height": 43
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b20bde67926e3e8040aaf57a6389e737abd4fc69",
                    "width": 320,
                    "height": 64
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f4a0d87b3973237c269d2cef31fbc18fbe655b1",
                    "width": 640,
                    "height": 129
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2eec3557cccdfd47534f0ee7386653e12cd322ea",
                    "width": 960,
                    "height": 194
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=db751d3024ba27f2cf55f1c5324f9b111e96ef8c",
                    "width": 1080,
                    "height": 218
                  }
                ],
                "variants": {},
                "id": "COyPpURqpsrEIUBXoEpCUbhYmsp15c4NUFjXeHFSIu8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mhbvig",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mhbvig/what_kind_of_qwen_2508_do_you_want_tonight/",
          "stickied": false,
          "url": "https://i.redd.it/3f5by1b8wzgf1.png",
          "subreddit_subscribers": 509911,
          "created_utc": 1754309979,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;format=png&amp;auto=webp&amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a\n\nHorizon Beta is OpenAI",
          "author_fullname": "t2_h0z59zgo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta is OpenAI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 40,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "m30x07j54vgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 31,
                  "x": 108,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=588d4a9c04c05e66d8b3c8ba8b93d7c827f14b20"
                },
                {
                  "y": 62,
                  "x": 216,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=446e608e2e3bf06827203dc313f98d8ec1704b9d"
                },
                {
                  "y": 92,
                  "x": 320,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f5cab485a7892fcf45b7a6bf1ecfa12925d59cf"
                },
                {
                  "y": 184,
                  "x": 640,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fda4acd30425eae2bddb9d9f82ce6241eaf9641"
                },
                {
                  "y": 276,
                  "x": 960,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e8f560871bebab02cfbd9ae3af3f79ac6d3819a"
                },
                {
                  "y": 310,
                  "x": 1080,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b6c803879173681737db389051fa4d599d2f19a0"
                }
              ],
              "s": {
                "y": 364,
                "x": 1266,
                "u": "https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;format=png&amp;auto=webp&amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a"
              },
              "id": "m30x07j54vgf1"
            }
          },
          "name": "t3_1mgtboa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 171,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 171,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/inyC6dLBuynY6QZPK34zrtaIVdVEKly7ofVVWlBmYW4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754252212,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a\"&gt;https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Horizon Beta is OpenAI&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgtboa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MiddleLobster9191",
          "discussion_type": null,
          "num_comments": 60,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754252212,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Finally got to finish a weekend project from a couple of months ago. \n\nThis is a small extension that can use a local LLM (any OpenAI-compatible endpoint is supported) to neutralise the clickbaits on the webpages you visit. It works reasonably well with models of Llama 3.2 3B class and above. Works in Chrome and Firefox (you can also install to Edge manually).\n\nFull source and configuration guide is on GitHub: [https://github.com/av/unhype](https://github.com/av/unhype) ",
          "author_fullname": "t2_o7p5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Use local LLM to neutralise the headers on the web",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 89,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgkiti",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bd9e9e",
          "ups": 472,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/niaha18uctgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1688,
              "scrubber_media_url": "https://v.redd.it/niaha18uctgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/niaha18uctgf1/DASHPlaylist.mpd?a=1756902686%2CMjhjZTUyNGUxMWI4NGIyNjJhYTNkNzAzNjRiYjY2NjQ3MzlkNDc5MDY4NzA4YzZjZjcwOGQ1MTY4ZTZlNjI5OQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 30,
              "hls_url": "https://v.redd.it/niaha18uctgf1/HLSPlaylist.m3u8?a=1756902686%2CNTY0Mjk4MmI0NWFhZDQwNzNhZGQxYTc2NmVlNjUzOThhMzdiOWRjNmQ2NTkxYjJiYmQ1YjllN2FmODQ0M2Y0Nw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 472,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=140&amp;height=89&amp;crop=140:89,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=f692b7d05bb54cb77646e5499a279385018af33e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Alpaca"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754230985,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Finally got to finish a weekend project from a couple of months ago. &lt;/p&gt;\n\n&lt;p&gt;This is a small extension that can use a local LLM (any OpenAI-compatible endpoint is supported) to neutralise the clickbaits on the webpages you visit. It works reasonably well with models of Llama 3.2 3B class and above. Works in Chrome and Firefox (you can also install to Edge manually).&lt;/p&gt;\n\n&lt;p&gt;Full source and configuration guide is on GitHub: &lt;a href=\"https://github.com/av/unhype\"&gt;https://github.com/av/unhype&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/niaha18uctgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?format=pjpg&amp;auto=webp&amp;s=e3f711ff053fccc48764a295b7ef68533d8f7f9a",
                  "width": 1688,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=47b5550c7dd6476d0d3e442d2ca5c2a382f74e92",
                    "width": 108,
                    "height": 69
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8828ff7d3eb9b270e88f2ec7ff4973818f896e8e",
                    "width": 216,
                    "height": 138
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bc01544a8c25b4174c98c4519803a567aa98c887",
                    "width": 320,
                    "height": 204
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9c9498bdb1588d50d25b53d82635a1092a82f4ef",
                    "width": 640,
                    "height": 409
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=402ac5eba7ed08a00bc9e012b6a4bd69a46e4608",
                    "width": 960,
                    "height": 614
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=374d1819675e3e0f60cf4790383435325e543d5f",
                    "width": 1080,
                    "height": 690
                  }
                ],
                "variants": {},
                "id": "NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Alpaca",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgkiti",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Everlier",
          "discussion_type": null,
          "num_comments": 69,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mgkiti/use_local_llm_to_neutralise_the_headers_on_the_web/",
          "stickied": false,
          "url": "https://v.redd.it/niaha18uctgf1",
          "subreddit_subscribers": 509911,
          "created_utc": 1754230985,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/niaha18uctgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1688,
              "scrubber_media_url": "https://v.redd.it/niaha18uctgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/niaha18uctgf1/DASHPlaylist.mpd?a=1756902686%2CMjhjZTUyNGUxMWI4NGIyNjJhYTNkNzAzNjRiYjY2NjQ3MzlkNDc5MDY4NzA4YzZjZjcwOGQ1MTY4ZTZlNjI5OQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 30,
              "hls_url": "https://v.redd.it/niaha18uctgf1/HLSPlaylist.m3u8?a=1756902686%2CNTY0Mjk4MmI0NWFhZDQwNzNhZGQxYTc2NmVlNjUzOThhMzdiOWRjNmQ2NTkxYjJiYmQ1YjllN2FmODQ0M2Y0Nw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I think OpenAI figured something out with this indentation in Codex (KISS). \n\nThe instructions are in english, but when overlooking, it is literally \"pseudo code\" with scopes, if and else clauses, \"finally\" clauses...  \n  \nPrompts are pseudo code. Nested indentation plays crucial role in Codex's success IMO.  \nUsing \"-\", \"\\\\t\" and \"\\\\n\" is pretty efficient. Also, The way \\_CODING GUIDELINES\\_ is highlighted is interesting. Reminds of Anthropic's XML tags in Claude, but less elegant.  \n  \nThis is currently one of the most powerful agents.  \n  \nKeep It Simple? Something to have in mind...",
          "author_fullname": "t2_5p5o5yxn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Keep It Simple Pseudo Code (That's what Codex does)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh0ltj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 45,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/J7QgKm1guKOSxZb3MVoof9r_uFvQZduBeUBP32dzpB8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754271438,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think OpenAI figured something out with this indentation in Codex (KISS). &lt;/p&gt;\n\n&lt;p&gt;The instructions are in english, but when overlooking, it is literally &amp;quot;pseudo code&amp;quot; with scopes, if and else clauses, &amp;quot;finally&amp;quot; clauses...  &lt;/p&gt;\n\n&lt;p&gt;Prompts are pseudo code. Nested indentation plays crucial role in Codex&amp;#39;s success IMO.&lt;br/&gt;\nUsing &amp;quot;-&amp;quot;, &amp;quot;\\t&amp;quot; and &amp;quot;\\n&amp;quot; is pretty efficient. Also, The way _CODING GUIDELINES_ is highlighted is interesting. Reminds of Anthropic&amp;#39;s XML tags in Claude, but less elegant.  &lt;/p&gt;\n\n&lt;p&gt;This is currently one of the most powerful agents.  &lt;/p&gt;\n\n&lt;p&gt;Keep It Simple? Something to have in mind...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nk1a76nkpwgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nk1a76nkpwgf1.jpeg?auto=webp&amp;s=3217bb71a80edc1997a73840a0e09d7fc2900229",
                  "width": 800,
                  "height": 831
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nk1a76nkpwgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a5183ed1f8eb9f7bba1469735154f6de46534a86",
                    "width": 108,
                    "height": 112
                  },
                  {
                    "url": "https://preview.redd.it/nk1a76nkpwgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=825eaaf03b32601b5d9c4f95ada2060e2cd39a42",
                    "width": 216,
                    "height": 224
                  },
                  {
                    "url": "https://preview.redd.it/nk1a76nkpwgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a680301e72bbf5c4139e647a71d653e4faf9d90b",
                    "width": 320,
                    "height": 332
                  },
                  {
                    "url": "https://preview.redd.it/nk1a76nkpwgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7f08a511a8b4c55448cf10557e9558c548047b1",
                    "width": 640,
                    "height": 664
                  }
                ],
                "variants": {},
                "id": "pBM8p2fA5M4u9sC1Ro6a-lYp0_kbuHXbrSbVRuqVQ7Q"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mh0ltj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cov_id19",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh0ltj/keep_it_simple_pseudo_code_thats_what_codex_does/",
          "stickied": false,
          "url": "https://i.redd.it/nk1a76nkpwgf1.jpeg",
          "subreddit_subscribers": 509911,
          "created_utc": 1754271438,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vcawomd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mac M3 + RooCode + Qwen3-Coder-30B (4-bit DWQ) in LM Studio — Possibly the Best Local Cursor Alternative Right Now?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 110,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgt2om",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 109,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/e1348s852vgf1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 720,
              "width": 910,
              "scrubber_media_url": "https://v.redd.it/e1348s852vgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e1348s852vgf1/DASHPlaylist.mpd?a=1756902686%2CMmM3YmY5OTZiZjM3Y2M5NmNiMTg3NTMyZWNmODVhNGZlYTAwOWU5MGQ1MzAxZTA5ODU5NTIwYjYyOWNmYTIzMw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 36,
              "hls_url": "https://v.redd.it/e1348s852vgf1/HLSPlaylist.m3u8?a=1756902686%2CZjFhODVlZjUzMTkyYTAxYmU3ZGFiMTBhMjljMDJhZDY1OWJjMDMzZmQ3MzI5ZjYxN2JmMmZhMGQxYjlhNTRmYQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 109,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=140&amp;height=110&amp;crop=140:110,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=abdd574d8f67b63e995dc11eb3cd4f8e74e11bb9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754251636,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/e1348s852vgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?format=pjpg&amp;auto=webp&amp;s=0e7a1d12b1a2a1acf19fdfc0f91b16be267107fa",
                  "width": 1350,
                  "height": 1068
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8aff50a70e5cb53b7b38c9b35a93218ab06b529e",
                    "width": 108,
                    "height": 85
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d5e621992a0c2958c031c79fe79c2a214c0cc8d5",
                    "width": 216,
                    "height": 170
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1edee9ae25cab3880fe29e9ac723b8339f6cebcf",
                    "width": 320,
                    "height": 253
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5bb59fb0f5c8e7a4e248e1068b8aa841bfa37998",
                    "width": 640,
                    "height": 506
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d85e99a79f17e7ec33873066c7eb6394c4cc7a5e",
                    "width": 960,
                    "height": 759
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1ee9e27ab710d36f97c624d0fec564a4f4b5b127",
                    "width": 1080,
                    "height": 854
                  }
                ],
                "variants": {},
                "id": "dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mgt2om",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "onil_gova",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgt2om/mac_m3_roocode_qwen3coder30b_4bit_dwq_in_lm/",
          "stickied": false,
          "url": "https://v.redd.it/e1348s852vgf1",
          "subreddit_subscribers": 509911,
          "created_utc": 1754251636,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/e1348s852vgf1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 720,
              "width": 910,
              "scrubber_media_url": "https://v.redd.it/e1348s852vgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e1348s852vgf1/DASHPlaylist.mpd?a=1756902686%2CMmM3YmY5OTZiZjM3Y2M5NmNiMTg3NTMyZWNmODVhNGZlYTAwOWU5MGQ1MzAxZTA5ODU5NTIwYjYyOWNmYTIzMw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 36,
              "hls_url": "https://v.redd.it/e1348s852vgf1/HLSPlaylist.m3u8?a=1756902686%2CZjFhODVlZjUzMTkyYTAxYmU3ZGFiMTBhMjljMDJhZDY1OWJjMDMzZmQ3MzI5ZjYxN2JmMmZhMGQxYjlhNTRmYQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "They said they're refining it months ago. Possibly timing to coincide with OpenAI's drop? Would be epic, I'm a fan of both. Especially if OpenAI's is not a reasoning model.",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "When DeepSeek r2?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgny8p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": "transparent",
          "ups": 204,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 204,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/V9pPhmLOwBjBig1Mp88kf2vrbPkb0OVif8im0hRtZXs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754239452,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;They said they&amp;#39;re refining it months ago. Possibly timing to coincide with OpenAI&amp;#39;s drop? Would be epic, I&amp;#39;m a fan of both. Especially if OpenAI&amp;#39;s is not a reasoning model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/dz0i0w1j2ugf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?auto=webp&amp;s=6cf4a209d32163902a3fb91d5a06108b4ebb9e61",
                  "width": 1080,
                  "height": 654
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bab256437dc4b2ab50c0bfefc751721bae4de7c5",
                    "width": 108,
                    "height": 65
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9916573c0160c0ddd9eb66a28ab87d51d07207d6",
                    "width": 216,
                    "height": 130
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a55f67b3df278451d7c98ae260464229b3ad11fa",
                    "width": 320,
                    "height": 193
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aaef5f3c86c4f7340d2367eea7fce60751451a94",
                    "width": 640,
                    "height": 387
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d3d7f058cca6ec2b2a9fcbd009eadd099423f494",
                    "width": 960,
                    "height": 581
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f8bd24556666c1de0f3c211632b5ace09b00eb99",
                    "width": 1080,
                    "height": 654
                  }
                ],
                "variants": {},
                "id": "lqAfsgD1hr66i0LHoQ3Cw_QWosWkvNCmE0O1wxkWap4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgny8p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 40,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mgny8p/when_deepseek_r2/",
          "stickied": false,
          "url": "https://i.redd.it/dz0i0w1j2ugf1.jpeg",
          "subreddit_subscribers": 509911,
          "created_utc": 1754239452,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "YuE: Open Full-song Music Generation Foundation Model, something similar to [Suno.ai](http://Suno.ai) but open",
          "author_fullname": "t2_etmr2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Music Foundation Models for Full-Song Generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mha439",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754304531,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "map-yue.github.io",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;YuE: Open Full-song Music Generation Foundation Model, something similar to &lt;a href=\"http://Suno.ai\"&gt;Suno.ai&lt;/a&gt; but open&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://map-yue.github.io/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mha439",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "phone_radio_tv",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mha439/open_music_foundation_models_for_fullsong/",
          "stickied": false,
          "url": "https://map-yue.github.io/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754304531,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Spent a few days finishing the evaluation for Qwen3-30B-A3B-Instruct-2507's quant instead of vibe checking the performance of the DWQ. It turns out the 4bit DWQ is quite close to the 8bit, even though the DWQ is still in an experimental phase, it's quite solid.\n\nhttps://preview.redd.it/kj8dz3orrygf1.png?width=1590&amp;format=png&amp;auto=webp&amp;s=ebe2b4f0ac64c3b76edea0945e0274cc44b65390\n\n",
          "author_fullname": "t2_aqcxxu50",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MLX 4bit DWQ vs 8bit eval",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "kj8dz3orrygf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 53,
                  "x": 108,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=130a77acaab502d0c212b2aac3139c953759382d"
                },
                {
                  "y": 107,
                  "x": 216,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e9e750f61b69dc716d34ce5c7f64c5e73c749d88"
                },
                {
                  "y": 158,
                  "x": 320,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9026ae7698238e5701b096b17fbb73f91f6b8f4f"
                },
                {
                  "y": 317,
                  "x": 640,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ff6a7dde9b00eba2574dc564c24eb08fa9de9617"
                },
                {
                  "y": 476,
                  "x": 960,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5a6d8ffabd6439c7bb37a77b9a3bf6a2230f4535"
                },
                {
                  "y": 536,
                  "x": 1080,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=01ec93da10c6235803c27b984d4d6f9e597482c4"
                }
              ],
              "s": {
                "y": 790,
                "x": 1590,
                "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=1590&amp;format=png&amp;auto=webp&amp;s=ebe2b4f0ac64c3b76edea0945e0274cc44b65390"
              },
              "id": "kj8dz3orrygf1"
            }
          },
          "name": "t3_1mh7yud",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/skKvv4aQvCUeHEuMqErewlo-Pi0FnE1rG_pbppcoUSo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754296426,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Spent a few days finishing the evaluation for Qwen3-30B-A3B-Instruct-2507&amp;#39;s quant instead of vibe checking the performance of the DWQ. It turns out the 4bit DWQ is quite close to the 8bit, even though the DWQ is still in an experimental phase, it&amp;#39;s quite solid.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kj8dz3orrygf1.png?width=1590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ebe2b4f0ac64c3b76edea0945e0274cc44b65390\"&gt;https://preview.redd.it/kj8dz3orrygf1.png?width=1590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ebe2b4f0ac64c3b76edea0945e0274cc44b65390&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mh7yud",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Tiny_Judge_2119",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh7yud/mlx_4bit_dwq_vs_8bit_eval/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh7yud/mlx_4bit_dwq_vs_8bit_eval/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754296426,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here's a completely new 70B dense model trained from scratch on 1.5T high quality tokens - only SFT with basic chat and instructions, no RLHF alignment. Plus, it speaks Korean and Japanese.\n\n[https://huggingface.co/trillionlabs/Tri-70B-preview-SFT](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT)",
          "author_fullname": "t2_1ug5bi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "This might be the largest un-aligned open-source model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgky8g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 212,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 212,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754232080,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s a completely new 70B dense model trained from scratch on 1.5T high quality tokens - only SFT with basic chat and instructions, no RLHF alignment. Plus, it speaks Korean and Japanese.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/trillionlabs/Tri-70B-preview-SFT\"&gt;https://huggingface.co/trillionlabs/Tri-70B-preview-SFT&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?auto=webp&amp;s=b45653eadbcba17c38d2f4f2e15f6ebd41ec7c72",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b7a80c31c557591f18bda1f387961a8fe38f053e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=13c5c597d54b6cfe9cc7d137d17b8c65f8c6db95",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=10f045816aeb8c03203c2c64cc8f2065cb392da4",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cad5dd9413d8196f56dd930d3b4333ee0351d472",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=37853a1aca067c019b4d7c06a11c66190e6bc001",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f4fc9332f1425fd05814a4fb77159385e0f2d89",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgky8g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jshin49",
          "discussion_type": null,
          "num_comments": 39,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754232080,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_fp657",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Source Voice Cloning at 16x real-time: Porting Chatterbox to vLLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgmx8w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 162,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 162,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=64baffecac79129329accd5dfef00560cde30027",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754236913,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/randombk/chatterbox-vllm",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?auto=webp&amp;s=f061762d17683d4e88608dbfe355d57e45d90ad5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d660bf476941abc2978684579558acc15bd0d2e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f08036e019ea53656d9f429e46be530e96c0ed23",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eac38ddf9a5c589c5a861f8aafada6fe73427034",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=50d687fd0b27b1fc30b1175e432b4518d7d1f15d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=185870ef47db93a269dea9dfe0a0629b16de6b64",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df00e364b0a49f0fb368440e176f32ee5e39b351",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgmx8w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dlp_randombk",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgmx8w/open_source_voice_cloning_at_16x_realtime_porting/",
          "stickied": false,
          "url": "https://github.com/randombk/chatterbox-vllm",
          "subreddit_subscribers": 509911,
          "created_utc": 1754236913,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are they really gonna train a model that's absolutely useless to give to us?",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why doesn't \"OpenAI\" just release one of the models they already have? Like 3.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgiyg4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 260,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 260,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754226851,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are they really gonna train a model that&amp;#39;s absolutely useless to give to us?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgiyg4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 188,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754226851,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🧠 Just Finished: Implementing Qwen 2 (1.5B) from Scratch\nA few days ago, I built the Qwen 2 language model (1.5B) completely from scratch, making it the second LLM I’ve implemented after Gemma 🚀. This was a major milestone for me, especially since there’s no open-source implementation of Qwen 2 available online (at least none I could find).\n\nWhat makes this build special:\n✅ Implemented without access to source code\n📖 Based entirely on the Qwen 1 &amp; Qwen 2 research papers\n🧱 Supports Qwen 2-1.5B architecture (more sizes coming soon!)\n⚠️ Does not support Mixture of Experts (MoE) yet\n\nThis project pushed my understanding of transformer architectures even further, and I’m excited to keep going.\nIf you're into LLMs, model replication, or want to see how Qwen 2 works under the hood, this might interest you!\n\nSource code: https://github.com/introlix/Swiftlet\nKaggle: https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet",
          "author_fullname": "t2_6qpq9avr5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Reimplemention of Qwen 2 from scratch",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgpb8t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 96,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 96,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754242723,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🧠 Just Finished: Implementing Qwen 2 (1.5B) from Scratch\nA few days ago, I built the Qwen 2 language model (1.5B) completely from scratch, making it the second LLM I’ve implemented after Gemma 🚀. This was a major milestone for me, especially since there’s no open-source implementation of Qwen 2 available online (at least none I could find).&lt;/p&gt;\n\n&lt;p&gt;What makes this build special:\n✅ Implemented without access to source code\n📖 Based entirely on the Qwen 1 &amp;amp; Qwen 2 research papers\n🧱 Supports Qwen 2-1.5B architecture (more sizes coming soon!)\n⚠️ Does not support Mixture of Experts (MoE) yet&lt;/p&gt;\n\n&lt;p&gt;This project pushed my understanding of transformer architectures even further, and I’m excited to keep going.\nIf you&amp;#39;re into LLMs, model replication, or want to see how Qwen 2 works under the hood, this might interest you!&lt;/p&gt;\n\n&lt;p&gt;Source code: &lt;a href=\"https://github.com/introlix/Swiftlet\"&gt;https://github.com/introlix/Swiftlet&lt;/a&gt;\nKaggle: &lt;a href=\"https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet\"&gt;https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?auto=webp&amp;s=519a96c79f13a619e42513cc0f904d9b36729fa1",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=09e4e357ff6f03ec51f0d4d875169c2822efb899",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7a804dcd73acb0eafcbf3c98a337b04b0330590",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6460444e14680b3ebc1e71bebe27f5eaaab08b28",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=142de401ddac2c07588c3df4af6a9a88a8c43665",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=730308dc3a494b5e493c8cc6e298f0435aba1498",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=798ec017daa864fb62d31aa0cd92c8316b840da9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgpb8t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CodingWithSatyam",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754242723,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "People here seem to assume that Chinese AI companies are developing and releasing these models, which cost tens of millions of dollars to develop, for free out of the goodness of their heart.\n\nI think this is absurd, considering these are for-profit companies, with shareholders who expect an ROI. In the case of Meta (and perhaps AliBaba), the explanation was it's about [commoditizing your complement](https://gwern.net/complement). But for many of these companies, which are pure play AI Labs, this simply does not hold.\n\nSo the question remains, why are they doing this?\n\nOne theory I would put forward is, they are playing the long game, and attempting to disincentivize investment in US AI labs, with the premise that investors will never recoup their investment, since similar capabilities will be offered for free. There is [a precedent](https://chatgpt.com/s/dr_688f65761670819181f5f8f5e52f9838) of Chinese companies doing similarly, in the context of mineral production, which has resulted in most production moving to China.\n\nIf this is the case, it will be good for consumers in the short-term, but less so in the long-term, at least for non-Chinese entities. If you don't find this theory convincing, I would be interested in hearing other alternative explanations for the rise in Chinese open-source models.\n\nWhat prompted this question, was the [recent interview](https://youtu.be/mYDSSRS-B5U?t=2203) with Dario from Anthropic, where he was asked about the threat to the business model posed by open-source models. (I don't find his response very compelling).\n\n\\---\n\nOne aside, its known that Twitter is banned in China. Yet, we see many Chinese-based AI researchers communicating there, on a daily basis. Sure it can be accessed via VPN, but these are publicly known figures, so there is no anonymity. What explains this?",
          "author_fullname": "t2_garopjsj1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are Chinese LLM companies effectively price dumping?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgjlek",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 195,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 195,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754232034,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754228603,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;People here seem to assume that Chinese AI companies are developing and releasing these models, which cost tens of millions of dollars to develop, for free out of the goodness of their heart.&lt;/p&gt;\n\n&lt;p&gt;I think this is absurd, considering these are for-profit companies, with shareholders who expect an ROI. In the case of Meta (and perhaps AliBaba), the explanation was it&amp;#39;s about &lt;a href=\"https://gwern.net/complement\"&gt;commoditizing your complement&lt;/a&gt;. But for many of these companies, which are pure play AI Labs, this simply does not hold.&lt;/p&gt;\n\n&lt;p&gt;So the question remains, why are they doing this?&lt;/p&gt;\n\n&lt;p&gt;One theory I would put forward is, they are playing the long game, and attempting to disincentivize investment in US AI labs, with the premise that investors will never recoup their investment, since similar capabilities will be offered for free. There is &lt;a href=\"https://chatgpt.com/s/dr_688f65761670819181f5f8f5e52f9838\"&gt;a precedent&lt;/a&gt; of Chinese companies doing similarly, in the context of mineral production, which has resulted in most production moving to China.&lt;/p&gt;\n\n&lt;p&gt;If this is the case, it will be good for consumers in the short-term, but less so in the long-term, at least for non-Chinese entities. If you don&amp;#39;t find this theory convincing, I would be interested in hearing other alternative explanations for the rise in Chinese open-source models.&lt;/p&gt;\n\n&lt;p&gt;What prompted this question, was the &lt;a href=\"https://youtu.be/mYDSSRS-B5U?t=2203\"&gt;recent interview&lt;/a&gt; with Dario from Anthropic, where he was asked about the threat to the business model posed by open-source models. (I don&amp;#39;t find his response very compelling).&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;One aside, its known that Twitter is banned in China. Yet, we see many Chinese-based AI researchers communicating there, on a daily basis. Sure it can be accessed via VPN, but these are publicly known figures, so there is no anonymity. What explains this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?auto=webp&amp;s=537c5cea0430f04039f256f31d55847ec27c39b6",
                  "width": 1238,
                  "height": 1400
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3562c0af61eb68438add188a0ebb448f79b54657",
                    "width": 108,
                    "height": 122
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=51a1f9e5bee46295e1d5ac9dc517b0097b9fa489",
                    "width": 216,
                    "height": 244
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=112817eb7860f41d0e5f471023d0200983bb9f91",
                    "width": 320,
                    "height": 361
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=577450661689dd64b4c310d0803093687dd84688",
                    "width": 640,
                    "height": 723
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bcdc0e42b1e5a63bb47ad4fa640fbc0fc006cd5c",
                    "width": 960,
                    "height": 1085
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dc589f590b2d3181d4aca8b2fcc7ea75fd5cda5e",
                    "width": 1080,
                    "height": 1221
                  }
                ],
                "variants": {},
                "id": "Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgjlek",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "uutnt",
          "discussion_type": null,
          "num_comments": 209,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754228603,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been testing out different LLM gateways for agent infra and wanted to share some notes. I used to spend most of my time exploring prompt engineering tools, but lately I’ve shifted focus to the infra side, specifically LLM gateways.\n\nMost of the hosted ones are fine for basic key management or retries, but they fall short once you care about latency, throughput, or chaining providers together cleanly. Some of them also have surprising bottlenecks under load or lack good observability out of the box.\n\nSome quick observations from what I tried:\n\n* [Bifrost](https://getmax.im/2frost) (Go, self-hosted): Surprisingly fast even under high load. Saw around 11µs overhead at 5K RPS and significantly lower memory usage compared to LiteLLM. Has native support for many providers and includes fallback, logging, Prometheus monitoring, and a visual web UI. You can integrate it without touching any SDKs, just change the base URL.\n* [Portkey](https://portkey.ai/features/ai-gateway): Decent for user-facing apps. It focuses more on retries and usage limits. Not very flexible when you need complex workflows or full visibility. Latency becomes inconsistent after a few hundred RPS.\n* **Kong** and [Gloo](https://www.solo.io/products/gloo-ai-gateway): These are general-purpose API gateways. You can bend them to work for LLM routing, but it takes a lot of setup and doesn’t feel natural. Not LLM-aware.\n* **Cloudflare’s AI Gateway**: Pretty good for lightweight routing if you're already using Cloudflare. But it’s a black box, not much visibility or customization.\n* **Aisera’s Gateway**: Geared toward enterprise support use cases. More of a vertical solution. Didn’t feel suitable for general-purpose LLM infra.\n* [LiteLLM](https://www.litellm.ai/): Super easy to get started and works well at small scale. But once we pushed load, it had around 50ms overhead and high memory usage. No built-in monitoring. It became hard to manage during bursts or when chaining calls.\n\nWould love to hear what others are running in production, especially if you’re doing failover, traffic splitting, or anything more advanced.",
          "author_fullname": "t2_1tizhpru5u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best LLM gateway?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh9r0z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754303241,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been testing out different LLM gateways for agent infra and wanted to share some notes. I used to spend most of my time exploring prompt engineering tools, but lately I’ve shifted focus to the infra side, specifically LLM gateways.&lt;/p&gt;\n\n&lt;p&gt;Most of the hosted ones are fine for basic key management or retries, but they fall short once you care about latency, throughput, or chaining providers together cleanly. Some of them also have surprising bottlenecks under load or lack good observability out of the box.&lt;/p&gt;\n\n&lt;p&gt;Some quick observations from what I tried:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://getmax.im/2frost\"&gt;Bifrost&lt;/a&gt; (Go, self-hosted): Surprisingly fast even under high load. Saw around 11µs overhead at 5K RPS and significantly lower memory usage compared to LiteLLM. Has native support for many providers and includes fallback, logging, Prometheus monitoring, and a visual web UI. You can integrate it without touching any SDKs, just change the base URL.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://portkey.ai/features/ai-gateway\"&gt;Portkey&lt;/a&gt;: Decent for user-facing apps. It focuses more on retries and usage limits. Not very flexible when you need complex workflows or full visibility. Latency becomes inconsistent after a few hundred RPS.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Kong&lt;/strong&gt; and &lt;a href=\"https://www.solo.io/products/gloo-ai-gateway\"&gt;Gloo&lt;/a&gt;: These are general-purpose API gateways. You can bend them to work for LLM routing, but it takes a lot of setup and doesn’t feel natural. Not LLM-aware.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Cloudflare’s AI Gateway&lt;/strong&gt;: Pretty good for lightweight routing if you&amp;#39;re already using Cloudflare. But it’s a black box, not much visibility or customization.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Aisera’s Gateway&lt;/strong&gt;: Geared toward enterprise support use cases. More of a vertical solution. Didn’t feel suitable for general-purpose LLM infra.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.litellm.ai/\"&gt;LiteLLM&lt;/a&gt;: Super easy to get started and works well at small scale. But once we pushed load, it had around 50ms overhead and high memory usage. No built-in monitoring. It became hard to manage during bursts or when chaining calls.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to hear what others are running in production, especially if you’re doing failover, traffic splitting, or anything more advanced.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mh9r0z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Educational-Bison786",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh9r0z/best_llm_gateway/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh9r0z/best_llm_gateway/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754303241,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "First time fine tuning a model in \"the cloud\".\nRunpod was suggested. And what should I say - it's a pita since the first few seconds.\nRDP - only via VNC. Accessing volume? No. SCP? No. Accurately telling you how much you used of your persistent volume? No. Figure out yourself while counting by hand. There is 330t TB available, and 222 TB used. It can't show anything below a TB.\nSetting up my pod for work yesterday so I can start without brain damage into the week today.\nAwesome idea - but GPUs are out for this Region. But let us charge you for your persistent volume.\nYou think you can trick run pod, find a solution and just start all over with a new pod in a region where GPUs are available?\nHaha, no way. After you pressed \"deploy\" we will charge you now for both pots. Accessing the platform to work on your pods or manage them? Whole platform not loading since almost 20 minutes - no chance to do anything. But hey, we will charge you 😂\n\nI don't know what anyone is doing with runpod. Am I unlucky? Maybe. Is the platform working? Not anymore. Only for my user btw 🤣\nSpeedtest benchmark? Could still do cloud gaming at 4k 60 fps.\nBut can't access the platform.\n\nFor me this platform is just a pita. I wasted several days trying to fix things and incompatibility issues of their pods after deployment.\n\nThis is by far one of the worst IT experiences in my life.\nCan't build my own server since I am traveling too much.\n\nAny alternatives that are working and can be accessed?\n\n\n\nEdit:\nClearing cache doesn't help. Incognito mode let's me click on login - but the moment I login it's stucked again. Other Laptop with different OS -&gt; same outcome like in incognito mode.\nClearly, my account is bugged or something.",
          "author_fullname": "t2_i00m20zzg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Runpod breaks my head - need a working alternative",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh960c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754303380,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754301108,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First time fine tuning a model in &amp;quot;the cloud&amp;quot;.\nRunpod was suggested. And what should I say - it&amp;#39;s a pita since the first few seconds.\nRDP - only via VNC. Accessing volume? No. SCP? No. Accurately telling you how much you used of your persistent volume? No. Figure out yourself while counting by hand. There is 330t TB available, and 222 TB used. It can&amp;#39;t show anything below a TB.\nSetting up my pod for work yesterday so I can start without brain damage into the week today.\nAwesome idea - but GPUs are out for this Region. But let us charge you for your persistent volume.\nYou think you can trick run pod, find a solution and just start all over with a new pod in a region where GPUs are available?\nHaha, no way. After you pressed &amp;quot;deploy&amp;quot; we will charge you now for both pots. Accessing the platform to work on your pods or manage them? Whole platform not loading since almost 20 minutes - no chance to do anything. But hey, we will charge you 😂&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know what anyone is doing with runpod. Am I unlucky? Maybe. Is the platform working? Not anymore. Only for my user btw 🤣\nSpeedtest benchmark? Could still do cloud gaming at 4k 60 fps.\nBut can&amp;#39;t access the platform.&lt;/p&gt;\n\n&lt;p&gt;For me this platform is just a pita. I wasted several days trying to fix things and incompatibility issues of their pods after deployment.&lt;/p&gt;\n\n&lt;p&gt;This is by far one of the worst IT experiences in my life.\nCan&amp;#39;t build my own server since I am traveling too much.&lt;/p&gt;\n\n&lt;p&gt;Any alternatives that are working and can be accessed?&lt;/p&gt;\n\n&lt;p&gt;Edit:\nClearing cache doesn&amp;#39;t help. Incognito mode let&amp;#39;s me click on login - but the moment I login it&amp;#39;s stucked again. Other Laptop with different OS -&amp;gt; same outcome like in incognito mode.\nClearly, my account is bugged or something.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh960c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IngloriousBastrd7908",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh960c/runpod_breaks_my_head_need_a_working_alternative/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh960c/runpod_breaks_my_head_need_a_working_alternative/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754301108,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_w6l58p741",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Drummer's Cydonia R1 24B v4 - A thinking Mistral Small 3.2!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgnwnx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 95,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 95,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=e35cde9b2565c154a06b4a4ec2a056512f465e28",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754239344,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?auto=webp&amp;s=c1ed5f3fc1bf6e79ce85d0034c3222d7952acbd1",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=165ab4b12c90bfe025b46debae13b140652c63d2",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=47d3bbf5bd07f507b8f71659fd9562dc4d194b90",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b91013b0a9041e7c6c25c022439d1503d608d9af",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7344e67fe48dc6a6f67623605b3dc51b204d189",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=996d51aa65f7fae5b2ee3e5ffbe787ce0b8cfdcd",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c4499831a246669c1b31de509f9c28678094e6e6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgnwnx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheLocalDrummer",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgnwnx/drummers_cydonia_r1_24b_v4_a_thinking_mistral/",
          "stickied": false,
          "url": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4",
          "subreddit_subscribers": 509911,
          "created_utc": 1754239344,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nI encountered a strange situation with GLM-4.5-Air 3bit mlx that maybe others can shed light on:  I tried to reproduce the Flappy Bird game featured in the [z.ai/blog/glm-4.5](http://z.ai/blog/glm-4.5) blog post, using the exact same prompt, but failed 3 times - the generated game either fails during collision detection (.i.e. the bird dies without hitting the pipes), or the top and bottom pipes merge and there's no way through.\n\nI gave up on the model for a while, thinking that it was due to the 3-bit quant.  But upon reading a reddit post decided to try something: adding /nothink to the end of the prompt.  This not only eliminated the \"thinking\" part of the output tokens, but generated a working game in one shot, with correct collision detection but also with added cloud in the background, just like in the blog post.\n\nCan anyone with 4, 6 or 8 bit mlx version verify if they have this problem?  Here's the exact prompt: \"Write a Flappy Bird game for me in a single HTML page. Keep the gravity weak so that the game is not too hard.\"\n\nPS.  I am running this on M1 Max Mac Studio w/ 64GB and 32C GPU, and get about 22 tokens/sec in LM Studio.  Also, Qwen3-Coder-30B-A3B (unlsoth Q8\\_0) generated this game, and others, in one shot without problem, at about 50 tokens/sec with flash attention on.",
          "author_fullname": "t2_3xif6p3z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 Air Produces Better Code Without Thinking, Using 3-bit MLX (/nothink)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgv53t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754256522,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I encountered a strange situation with GLM-4.5-Air 3bit mlx that maybe others can shed light on:  I tried to reproduce the Flappy Bird game featured in the &lt;a href=\"http://z.ai/blog/glm-4.5\"&gt;z.ai/blog/glm-4.5&lt;/a&gt; blog post, using the exact same prompt, but failed 3 times - the generated game either fails during collision detection (.i.e. the bird dies without hitting the pipes), or the top and bottom pipes merge and there&amp;#39;s no way through.&lt;/p&gt;\n\n&lt;p&gt;I gave up on the model for a while, thinking that it was due to the 3-bit quant.  But upon reading a reddit post decided to try something: adding /nothink to the end of the prompt.  This not only eliminated the &amp;quot;thinking&amp;quot; part of the output tokens, but generated a working game in one shot, with correct collision detection but also with added cloud in the background, just like in the blog post.&lt;/p&gt;\n\n&lt;p&gt;Can anyone with 4, 6 or 8 bit mlx version verify if they have this problem?  Here&amp;#39;s the exact prompt: &amp;quot;Write a Flappy Bird game for me in a single HTML page. Keep the gravity weak so that the game is not too hard.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;PS.  I am running this on M1 Max Mac Studio w/ 64GB and 32C GPU, and get about 22 tokens/sec in LM Studio.  Also, Qwen3-Coder-30B-A3B (unlsoth Q8_0) generated this game, and others, in one shot without problem, at about 50 tokens/sec with flash attention on.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgv53t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jcmyang",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgv53t/glm_45_air_produces_better_code_without_thinking/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgv53t/glm_45_air_produces_better_code_without_thinking/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754256522,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "DWQ seems brand new so I'm wondering if LM Studio just doesn't support it ",
          "author_fullname": "t2_7wdc4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone got LM Studio working with DWQ models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh5v49",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754288253,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;DWQ seems brand new so I&amp;#39;m wondering if LM Studio just doesn&amp;#39;t support it &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh5v49",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "maxiedaniels",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh5v49/anyone_got_lm_studio_working_with_dwq_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh5v49/anyone_got_lm_studio_working_with_dwq_models/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754288253,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I need a local llm  to be able to process 100-200k context in a reasonable timeframe. Does anyone know best llama.cpp flags to maximise Prompt processing?\n\nI have 16gb vram 9070 amd GPU and 32gb ram, which I may upgrade to 192gb ram at some point.  Ubuntu and windows. Had some driver problems with rocm but would that be significantly faster than Vulcan for prompt processing?\n\nI don't mind super slow inference speed as long as its above 1t/s. If the prompt processing is at a reasonable speed.\n\nShould I look towards moe models such as 30a3b qwen 3? Or better to go for 7b qwen? In any case prompt processing of such prompts is so slow I am not sure if either is viable. Anyone knows good tips or guide on how to maximise Prompt processing? Or benchmarks. Everything I Google seems to be about inference speed which doesn't really matter to me too much. I'm happy to wait. \n\nThis is for batch processing long contexts. Mainly for full pdf ingestion because I find chunking and vectorisation unreliable at best. Putting the whole pdf text into context just performs better all the time essentially. \n\nWhat llama cpp flags I should go for? Should I just offload kv cache to GPU while leaving the rest in ram? My local tests were very inconclusive at best. And very often would result in broken output anyway mostly with a3b qwen 3, where the llm would fall into repetition making the output mostly useless too. \n\nWhat quants?\n\nI want the llm to perform decently at that long context but maybe that is too much for these small LLMs. But there are 1m context qwen ggufs so one can hope? \n\nAny ideas, thoughts? Anyone tried to maximise long context processing?",
          "author_fullname": "t2_dmg3f1uq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to maximise Prompt processing speed for long context usage?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhbp73",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754309485,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need a local llm  to be able to process 100-200k context in a reasonable timeframe. Does anyone know best llama.cpp flags to maximise Prompt processing?&lt;/p&gt;\n\n&lt;p&gt;I have 16gb vram 9070 amd GPU and 32gb ram, which I may upgrade to 192gb ram at some point.  Ubuntu and windows. Had some driver problems with rocm but would that be significantly faster than Vulcan for prompt processing?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t mind super slow inference speed as long as its above 1t/s. If the prompt processing is at a reasonable speed.&lt;/p&gt;\n\n&lt;p&gt;Should I look towards moe models such as 30a3b qwen 3? Or better to go for 7b qwen? In any case prompt processing of such prompts is so slow I am not sure if either is viable. Anyone knows good tips or guide on how to maximise Prompt processing? Or benchmarks. Everything I Google seems to be about inference speed which doesn&amp;#39;t really matter to me too much. I&amp;#39;m happy to wait. &lt;/p&gt;\n\n&lt;p&gt;This is for batch processing long contexts. Mainly for full pdf ingestion because I find chunking and vectorisation unreliable at best. Putting the whole pdf text into context just performs better all the time essentially. &lt;/p&gt;\n\n&lt;p&gt;What llama cpp flags I should go for? Should I just offload kv cache to GPU while leaving the rest in ram? My local tests were very inconclusive at best. And very often would result in broken output anyway mostly with a3b qwen 3, where the llm would fall into repetition making the output mostly useless too. &lt;/p&gt;\n\n&lt;p&gt;What quants?&lt;/p&gt;\n\n&lt;p&gt;I want the llm to perform decently at that long context but maybe that is too much for these small LLMs. But there are 1m context qwen ggufs so one can hope? &lt;/p&gt;\n\n&lt;p&gt;Any ideas, thoughts? Anyone tried to maximise long context processing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhbp73",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Kangaroo6055",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754309485,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": " WHAT THE DEVIL?\n\nAnother open model outperforms closed ones!  \nXBai o4 beats OpenAI o3-mini and *confidently* beats Anthropic's Claude Opus.\n\n•Parameters: 32.8 B\n•Training: Long-CoT RL + Process Reward Learning (SPRM)\n•Benchmarks (High-Modus):\n•AIME24: 86.5\n•AIME25: 77.9\n•LiveCodeBench v5: 67.2\n•C-EVAL: 89.7\n\n🔗Open source weights: https://huggingface.co/MetaStoneTec/XBai-o4",
          "author_fullname": "t2_dmji1c74",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "XBai-04 Is It Real?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 103,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "idu2e1ngesgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a2a62a8bef0a6d00f8049f16c8b54e8aa48b974"
                },
                {
                  "y": 124,
                  "x": 216,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=924e4b76f3498ca5e7d4ab97c08a75b6fb377dc1"
                },
                {
                  "y": 183,
                  "x": 320,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=82f9fc643d2975e8481a615c1f5a19b5cb453032"
                },
                {
                  "y": 367,
                  "x": 640,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b611aef0c4759ebf456a134bc82a5fd8065b9c5a"
                },
                {
                  "y": 551,
                  "x": 960,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8f73f5b3bcc334e135595851ef884c4997ac617"
                },
                {
                  "y": 620,
                  "x": 1080,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2e786e475e1a54e20bdba7fb55d1e03e02789639"
                }
              ],
              "s": {
                "y": 704,
                "x": 1226,
                "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=1226&amp;format=pjpg&amp;auto=webp&amp;s=5b2baf561ff73bb0e8b4b5708cd383ce9ed6f372"
              },
              "id": "idu2e1ngesgf1"
            },
            "q3imyykgesgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 79,
                  "x": 108,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8dbd1ac74caccea54654d6ebe571fab617928272"
                },
                {
                  "y": 158,
                  "x": 216,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c6af0dfc3c0aaa092cea1507485efebac28962f"
                },
                {
                  "y": 235,
                  "x": 320,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=602c9e9684804598a1e5221dbb4d23ce3610098a"
                },
                {
                  "y": 471,
                  "x": 640,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e073695ec8c765d1252320b4c7a5e0bd07547b82"
                },
                {
                  "y": 706,
                  "x": 960,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d48c30f79f06380c415eb3dc8e2120642d96e240"
                },
                {
                  "y": 794,
                  "x": 1080,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2d39f9af5c4e09886e80918a8161ae1315aa4c7b"
                }
              ],
              "s": {
                "y": 942,
                "x": 1280,
                "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=e8846d514b560df6c44bca9b5375f5eed938412c"
              },
              "id": "q3imyykgesgf1"
            }
          },
          "name": "t3_1mggku0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "ups": 196,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "q3imyykgesgf1",
                "id": 720170802
              },
              {
                "caption": "",
                "media_id": "idu2e1ngesgf1",
                "id": 720170803
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 196,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ybTHvU3e25DWWBXsMwOWfbuGc2dzpO5QuzWGZyUC65s.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754219245,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;WHAT THE DEVIL?&lt;/p&gt;\n\n&lt;p&gt;Another open model outperforms closed ones!&lt;br/&gt;\nXBai o4 beats OpenAI o3-mini and &lt;em&gt;confidently&lt;/em&gt; beats Anthropic&amp;#39;s Claude Opus.&lt;/p&gt;\n\n&lt;p&gt;•Parameters: 32.8 B\n•Training: Long-CoT RL + Process Reward Learning (SPRM)\n•Benchmarks (High-Modus):\n•AIME24: 86.5\n•AIME25: 77.9\n•LiveCodeBench v5: 67.2\n•C-EVAL: 89.7&lt;/p&gt;\n\n&lt;p&gt;🔗Open source weights: &lt;a href=\"https://huggingface.co/MetaStoneTec/XBai-o4\"&gt;https://huggingface.co/MetaStoneTec/XBai-o4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mggku0",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mggku0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ordinary_Mud7430",
          "discussion_type": null,
          "num_comments": 101,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mggku0/xbai04_is_it_real/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mggku0",
          "subreddit_subscribers": 509911,
          "created_utc": 1754219245,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "is it feasable to have a local model run on a pi 5 ? The response time is actually not that important.\n\nAnyone have something like a llama model run on pi5 with 8GB RAM?",
          "author_fullname": "t2_l7ryo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "lcoal llm on raspberry pi",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh8xqp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754300230,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;is it feasable to have a local model run on a pi 5 ? The response time is actually not that important.&lt;/p&gt;\n\n&lt;p&gt;Anyone have something like a llama model run on pi5 with 8GB RAM?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh8xqp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "overlydelicioustea",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754300230,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "From my research, it seems that there is no benefit to 512 bs 256 because the next largest model that doesnt fit in 256 also doesnt fit in 512. Is this true, or is there an advantage to getting 512 vs 256. I am looking at Mac Studios. \n\nLastly, whatever the answer is, will that model at that weight at that quantize size at least be close to Sonnet 4 in speed and accuracy? If so id rather pay 500/mo for 1 year to own my LLM rather than paying 500/mo for ever to “rent” it. I can always install a new model as better models come out even if the latest open source model is a smidge behind the paywall ones owned by the antichrist. The 2 models ive heard compare are Qwen3 Coder and GLM 4.5 Air, so I guess asking which of these would best make use of the 256 or the 512 mac studio at what size etc and if it can be close to sonnet 4",
          "author_fullname": "t2_ufbr1m7p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the best model (to replace Sonnet 4) that fits in 256GB Vram and the best that fits in 512gb of Vram?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh0n5h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754271548,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From my research, it seems that there is no benefit to 512 bs 256 because the next largest model that doesnt fit in 256 also doesnt fit in 512. Is this true, or is there an advantage to getting 512 vs 256. I am looking at Mac Studios. &lt;/p&gt;\n\n&lt;p&gt;Lastly, whatever the answer is, will that model at that weight at that quantize size at least be close to Sonnet 4 in speed and accuracy? If so id rather pay 500/mo for 1 year to own my LLM rather than paying 500/mo for ever to “rent” it. I can always install a new model as better models come out even if the latest open source model is a smidge behind the paywall ones owned by the antichrist. The 2 models ive heard compare are Qwen3 Coder and GLM 4.5 Air, so I guess asking which of these would best make use of the 256 or the 512 mac studio at what size etc and if it can be close to sonnet 4&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh0n5h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "devshore",
          "discussion_type": null,
          "num_comments": 46,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh0n5h/what_is_the_best_model_to_replace_sonnet_4_that/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh0n5h/what_is_the_best_model_to_replace_sonnet_4_that/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754271548,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What's the best approach for this? Tried it in open webui with ollama backend but it's too slow.\n\nAll docs are pdf, all done with ocr so it's all just text. Ingestion to knowledgebase is the bottleneck.\n\nAnybody done this and what was the best approach for you?",
          "author_fullname": "t2_mu8eykc30",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RAG with 30k documents, some with 300 pages each.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mha1g1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754305112,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754304270,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the best approach for this? Tried it in open webui with ollama backend but it&amp;#39;s too slow.&lt;/p&gt;\n\n&lt;p&gt;All docs are pdf, all done with ocr so it&amp;#39;s all just text. Ingestion to knowledgebase is the bottleneck.&lt;/p&gt;\n\n&lt;p&gt;Anybody done this and what was the best approach for you?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mha1g1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dennisitnet",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mha1g1/rag_with_30k_documents_some_with_300_pages_each/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mha1g1/rag_with_30k_documents_some_with_300_pages_each/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754304270,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA's \"Highly Optimistic\" DGX Spark Mini-Supercomputer Still Hasn't Hit Retail Despite a Planned July Launch, Suggesting Possible Production Issues",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgis6h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 89,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 89,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=293dff94e206d07e95b1ceb35d306f084972280b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754226361,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/nvidia-highly-optimistic-dgx-spark-mini-supercomputer-still-hasnt-hit-retail/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?auto=webp&amp;s=761d7765165ce23883e3ea7e5265165d93fd7502",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=462a9746ffb12643f31808d38538f4c0ea76b555",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f437e173d256a303cec220b9cd2f260843de5e3",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f8b731f2beec62abb38c3d104c182153621be67",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=09c3744f2431f90355d37d937f1352192cc87780",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee4f08a8c7da0b870a09e2fee6c837603f3ccdca",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f610522848162744871027feef9554990de435d",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mgis6h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 48,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgis6h/nvidias_highly_optimistic_dgx_spark/",
          "stickied": false,
          "url": "https://wccftech.com/nvidia-highly-optimistic-dgx-spark-mini-supercomputer-still-hasnt-hit-retail/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754226361,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "i heard some good things about muse writing pose well and not being censored,  just curious if there are other similar open models that are sort of trained from books not part of a very expensive website. \n\nI've been using gemini lately on the website and it does a lot of things right. THe main issue is it just doesn't write like claude does and well claude doesn't think like gpt or gemini do. Then its like wtf you use then.  :)\n\nSo getting out of that just a bit, I've started playing around with novel crafter recently and honeslty it's hard to go back to the chatbots now. Like that free ai at the bottom of the open routerlist did better than perplexity and the 3 main ai sites on fanfiction. of course it and Kimi still censored berserk but I didn't jailbreak it (trying to avoid that so it doesn't make the writing suck hehe) but sometimes like when i was doing perplexity i can just delete and try again. \n\nI just like to try a model that truely is made for writing and not uncensored if it exist. I probably am overplaying muse bet it's censored against berserk too! Are there like any good models that write well and can actually do writing styles and NSFW? ",
          "author_fullname": "t2_gcu2g1163",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any good open source models for NSFW writing?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgq8yz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754244875,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i heard some good things about muse writing pose well and not being censored,  just curious if there are other similar open models that are sort of trained from books not part of a very expensive website. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using gemini lately on the website and it does a lot of things right. THe main issue is it just doesn&amp;#39;t write like claude does and well claude doesn&amp;#39;t think like gpt or gemini do. Then its like wtf you use then.  :)&lt;/p&gt;\n\n&lt;p&gt;So getting out of that just a bit, I&amp;#39;ve started playing around with novel crafter recently and honeslty it&amp;#39;s hard to go back to the chatbots now. Like that free ai at the bottom of the open routerlist did better than perplexity and the 3 main ai sites on fanfiction. of course it and Kimi still censored berserk but I didn&amp;#39;t jailbreak it (trying to avoid that so it doesn&amp;#39;t make the writing suck hehe) but sometimes like when i was doing perplexity i can just delete and try again. &lt;/p&gt;\n\n&lt;p&gt;I just like to try a model that truely is made for writing and not uncensored if it exist. I probably am overplaying muse bet it&amp;#39;s censored against berserk too! Are there like any good models that write well and can actually do writing styles and NSFW? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgq8yz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LoneyGamer2023",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgq8yz/are_there_any_good_open_source_models_for_nsfw/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgq8yz/are_there_any_good_open_source_models_for_nsfw/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754244875,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Am I the only person who can't stop day dreaming of a larger Gemma model? I genuinely prefer the vibe of Gemma 3 27B to just about every other LLM I have been able to get my hands on, and I'm gearing up to fund a major fine-tune/tweak of an OS model this year. (I would take the plunge on Cohere's 112 Command A Vision if not for the license) - I just can't help but shake the itch for a version of Gemma that punched just a bit higher in terms of its capabilities. Does anyone with their finger more on the pulse of the development cycle have any idea whether or not we might get something like this at any point in the next few months? ",
          "author_fullname": "t2_ap0ra8pe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Daydreaming of a new Gemma model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgm8d3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 51,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 51,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754235233,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Am I the only person who can&amp;#39;t stop day dreaming of a larger Gemma model? I genuinely prefer the vibe of Gemma 3 27B to just about every other LLM I have been able to get my hands on, and I&amp;#39;m gearing up to fund a major fine-tune/tweak of an OS model this year. (I would take the plunge on Cohere&amp;#39;s 112 Command A Vision if not for the license) - I just can&amp;#39;t help but shake the itch for a version of Gemma that punched just a bit higher in terms of its capabilities. Does anyone with their finger more on the pulse of the development cycle have any idea whether or not we might get something like this at any point in the next few months? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mgm8d3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Jazzlike_Source_5983",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgm8d3/daydreaming_of_a_new_gemma_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgm8d3/daydreaming_of_a_new_gemma_model/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754235233,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey\n\nDoes anybody have some experience working with those newer Ryzen AI Chips in case of running Models (up to 70B Q4 ? )\nFine tuning LLMs using LoRA or converting models from/into GGUF?\n\nSaw those are more affordable than going for a maxed out mac book pro and would be quite interested in their performance and semi professional use with fine tuning and converting.\n\nShare your experiences, happy to read 😁👍",
          "author_fullname": "t2_i00m20zzg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running, fine tuning and converting LLMs on new Ryzen AI 7 or 9 APUs - 64-128GB RAM - 75% VRAM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhc31j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754310562,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey&lt;/p&gt;\n\n&lt;p&gt;Does anybody have some experience working with those newer Ryzen AI Chips in case of running Models (up to 70B Q4 ? )\nFine tuning LLMs using LoRA or converting models from/into GGUF?&lt;/p&gt;\n\n&lt;p&gt;Saw those are more affordable than going for a maxed out mac book pro and would be quite interested in their performance and semi professional use with fine tuning and converting.&lt;/p&gt;\n\n&lt;p&gt;Share your experiences, happy to read 😁👍&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhc31j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IngloriousBastrd7908",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhc31j/running_fine_tuning_and_converting_llms_on_new/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhc31j/running_fine_tuning_and_converting_llms_on_new/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754310562,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yet another new model claiming to outperform larger ones:\n\n\n\n**Instruction following** is a core ability of large language models (LLMs), but performance remains inconsistent, especially on complex tasks.\n\nWe identify **lazy reasoning** during the thinking stage as a key cause of poor instruction adherence.\n\nTo address this, we propose a framework that promotes rigorous reasoning through **previewing and self-checking**.\n\nOur method begins by generating instruction data with **complex constraints**, filtering out samples that are too easy or too difficult. We then use rejection sampling to build a small but high-quality dataset for model adaptation.\n\nTraining involves entropy-preserving supervised fine-tuning (**Entropy-SFT**) and token-wise entropy-adaptive reinforcement learning (**TEA-RL**), guided by rule-based multidimensional rewards.\n\nThis approach encourages models to plan ahead and verify their outputs, fostering more generalizable reasoning abilities.\n\nExperiments show consistent improvements across model sizes. Notably, our 32B model outperforms both larger open-source models like **DeepSeek-R1** and closed-source models like **ChatGPT-4o** on challenging instruction-following benchmarks.\n\n[https://huggingface.co/qihoo360/Light-IF-32B](https://huggingface.co/qihoo360/Light-IF-32B)\n\n\n\ntechnical report [https://huggingface.co/papers/2503.10460](https://huggingface.co/papers/2503.10460)\n\n\n\nprevious popular models by this company:\n\n[https://huggingface.co/qihoo360/TinyR1-32B-Preview](https://huggingface.co/qihoo360/TinyR1-32B-Preview)\n\n[https://huggingface.co/qihoo360/Light-R1-32B](https://huggingface.co/qihoo360/Light-R1-32B)\n\n  \nWhat do you think?",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "qihoo360/Light-IF-32B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 129,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mghy1u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": "#bbbdbf",
          "ups": 88,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 88,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/QIubNPPJjdRirxQ73mfKHMMmxIY2VZ9TzcjXGXjIohk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754223868,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yet another new model claiming to outperform larger ones:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Instruction following&lt;/strong&gt; is a core ability of large language models (LLMs), but performance remains inconsistent, especially on complex tasks.&lt;/p&gt;\n\n&lt;p&gt;We identify &lt;strong&gt;lazy reasoning&lt;/strong&gt; during the thinking stage as a key cause of poor instruction adherence.&lt;/p&gt;\n\n&lt;p&gt;To address this, we propose a framework that promotes rigorous reasoning through &lt;strong&gt;previewing and self-checking&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Our method begins by generating instruction data with &lt;strong&gt;complex constraints&lt;/strong&gt;, filtering out samples that are too easy or too difficult. We then use rejection sampling to build a small but high-quality dataset for model adaptation.&lt;/p&gt;\n\n&lt;p&gt;Training involves entropy-preserving supervised fine-tuning (&lt;strong&gt;Entropy-SFT&lt;/strong&gt;) and token-wise entropy-adaptive reinforcement learning (&lt;strong&gt;TEA-RL&lt;/strong&gt;), guided by rule-based multidimensional rewards.&lt;/p&gt;\n\n&lt;p&gt;This approach encourages models to plan ahead and verify their outputs, fostering more generalizable reasoning abilities.&lt;/p&gt;\n\n&lt;p&gt;Experiments show consistent improvements across model sizes. Notably, our 32B model outperforms both larger open-source models like &lt;strong&gt;DeepSeek-R1&lt;/strong&gt; and closed-source models like &lt;strong&gt;ChatGPT-4o&lt;/strong&gt; on challenging instruction-following benchmarks.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/Light-IF-32B\"&gt;https://huggingface.co/qihoo360/Light-IF-32B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;technical report &lt;a href=\"https://huggingface.co/papers/2503.10460\"&gt;https://huggingface.co/papers/2503.10460&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;previous popular models by this company:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/TinyR1-32B-Preview\"&gt;https://huggingface.co/qihoo360/TinyR1-32B-Preview&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/Light-R1-32B\"&gt;https://huggingface.co/qihoo360/Light-R1-32B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6vaf0crhrsgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6vaf0crhrsgf1.png?auto=webp&amp;s=6ba618920def17d94508256824e2561aba8a6ec9",
                  "width": 1062,
                  "height": 980
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=081b3d7480032b122c209477d47263419358b811",
                    "width": 108,
                    "height": 99
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5fac696d38efe6b971413c8794edd434ae2c9926",
                    "width": 216,
                    "height": 199
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=20af7c29a89f974a98e34d36ff62aa93c6d3e970",
                    "width": 320,
                    "height": 295
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=771c3421ec90e5339a70e9664ac80ef7b729ca73",
                    "width": 640,
                    "height": 590
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=753db7d1045cf75fdd0e7ff4fd0c94209e8cee25",
                    "width": 960,
                    "height": 885
                  }
                ],
                "variants": {},
                "id": "O7-1ZfpSudq0amigVzUb6mHn4tEC8x9xtRLUGpzh3sI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mghy1u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mghy1u/qihoo360lightif32b/",
          "stickied": false,
          "url": "https://i.redd.it/6vaf0crhrsgf1.png",
          "subreddit_subscribers": 509911,
          "created_utc": 1754223868,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi. The messages field in the payload of an (OpenAI-compatible) API call is an array, meaning there can be multiple messages, of system, user, or assistant roles. I normally just send a system message and then the user message. \n\n\nWhat are some use cases where it's preferable to send multiple messages in one API call instead of one or two, in a certain order? Is there any benefit to send an assistant message, as these sre supposed to be coming from the LLM itself?\n\nAny concrete use case or example is appreciated. Thanks.",
          "author_fullname": "t2_127kho",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are some use cases to send multiple messages in one LLM API request?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhbk4f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754309065,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. The messages field in the payload of an (OpenAI-compatible) API call is an array, meaning there can be multiple messages, of system, user, or assistant roles. I normally just send a system message and then the user message. &lt;/p&gt;\n\n&lt;p&gt;What are some use cases where it&amp;#39;s preferable to send multiple messages in one API call instead of one or two, in a certain order? Is there any benefit to send an assistant message, as these sre supposed to be coming from the LLM itself?&lt;/p&gt;\n\n&lt;p&gt;Any concrete use case or example is appreciated. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhbk4f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ihatebeinganonymous",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhbk4f/what_are_some_use_cases_to_send_multiple_messages/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhbk4f/what_are_some_use_cases_to_send_multiple_messages/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754309065,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\nhttps://preview.redd.it/2062bdjfszgf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=d90a50b645e50511d0d6eea016fa89918bdfc9e2\n\nOn 9th August 2025, I am starting a Small Language Model Workshop. It will be a 5-6 hour live workshop. This is purely for teaching and sharing knowledge. Think of it as a 3 times expanded and live version of Karpathy's repository and video. \n\nIn this workshop, we will build a production ready Small Language Model (SLM) fully from scratch.  \n\nTowards the end of this workshop, we will chain 8 GPUs and actually replicate the results of GPT-2. \n\nIt will be like building GPT-2 fully from scratch, and getting results which OpenAI got in their classical GPT-2 paper. \n\nThe workshop will start from tokenisation and end at multi-GPU programming. \n\nWe will work with 2 datasets: \n\n\\- TinyStories\n\n\\- FineWeb Edu\n\nWe will go through the following: \n\n\\- Loading datasets\n\n\\- Tokenization\n\n\\- Creating input-target pairs\n\n\\- Assembling the entire SLM architecture\n\n\\- Defining the training loop\n\n\\- Running inference\n\n\\- Multi-GPU version of training\n\nRegister for free here: [https://slm-from-scratch.vercel.app/](https://slm-from-scratch.vercel.app/)",
          "author_fullname": "t2_rb9k9zgob",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Build a Small Language Model from Scratch | Free 6 hour live workshop",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "2062bdjfszgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=644b6890d9f1d63afe4d1b3649c082853f6b5df9"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=20a92ff4c0705d871443942faa8615498c590153"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2afca8c1a6986c761dee8f582e264c5741a4099b"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0d13c836b271b555a60e400b298dd60837ffca1e"
                },
                {
                  "y": 1280,
                  "x": 960,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=00aca5ea51a1bee55af64b4c90f31fb80592b388"
                },
                {
                  "y": 1440,
                  "x": 1080,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0d7cf6280d9d0860d2aa3b597532e8c852614086"
                }
              ],
              "s": {
                "y": 4032,
                "x": 3024,
                "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=d90a50b645e50511d0d6eea016fa89918bdfc9e2"
              },
              "id": "2062bdjfszgf1"
            }
          },
          "name": "t3_1mhbhn3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/wYyTcudp-ANH5QEeVy4mcXMJLT3H-mEztMtQMKLbu78.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754308876,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/2062bdjfszgf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d90a50b645e50511d0d6eea016fa89918bdfc9e2\"&gt;https://preview.redd.it/2062bdjfszgf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d90a50b645e50511d0d6eea016fa89918bdfc9e2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;On 9th August 2025, I am starting a Small Language Model Workshop. It will be a 5-6 hour live workshop. This is purely for teaching and sharing knowledge. Think of it as a 3 times expanded and live version of Karpathy&amp;#39;s repository and video. &lt;/p&gt;\n\n&lt;p&gt;In this workshop, we will build a production ready Small Language Model (SLM) fully from scratch.  &lt;/p&gt;\n\n&lt;p&gt;Towards the end of this workshop, we will chain 8 GPUs and actually replicate the results of GPT-2. &lt;/p&gt;\n\n&lt;p&gt;It will be like building GPT-2 fully from scratch, and getting results which OpenAI got in their classical GPT-2 paper. &lt;/p&gt;\n\n&lt;p&gt;The workshop will start from tokenisation and end at multi-GPU programming. &lt;/p&gt;\n\n&lt;p&gt;We will work with 2 datasets: &lt;/p&gt;\n\n&lt;p&gt;- TinyStories&lt;/p&gt;\n\n&lt;p&gt;- FineWeb Edu&lt;/p&gt;\n\n&lt;p&gt;We will go through the following: &lt;/p&gt;\n\n&lt;p&gt;- Loading datasets&lt;/p&gt;\n\n&lt;p&gt;- Tokenization&lt;/p&gt;\n\n&lt;p&gt;- Creating input-target pairs&lt;/p&gt;\n\n&lt;p&gt;- Assembling the entire SLM architecture&lt;/p&gt;\n\n&lt;p&gt;- Defining the training loop&lt;/p&gt;\n\n&lt;p&gt;- Running inference&lt;/p&gt;\n\n&lt;p&gt;- Multi-GPU version of training&lt;/p&gt;\n\n&lt;p&gt;Register for free here: &lt;a href=\"https://slm-from-scratch.vercel.app/\"&gt;https://slm-from-scratch.vercel.app/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mhbhn3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OtherRaisin3426",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhbhn3/build_a_small_language_model_from_scratch_free_6/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhbhn3/build_a_small_language_model_from_scratch_free_6/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754308876,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This model is seriously impressive, feels really powerful, and that fits with what people have been saying about it being 120B parameters in size. It's big enough to be smart without being so huge it steps on OpenAI’s toes. In my experience with the model, here some notes:\n\n* It works *really* well with agents and tools\n* It can handle long contexts (i tested up to around 50k tokens), which is something most open-source models struggle with, only the biggest ones can do that reliably.\n* It’s fantastic with languages other than English, a weakness often sees in Chinese models.\n* It can be based on GPT-5 architecture, even being a smaller version from it (like Gemma models), this would explain why has some differences from current OpenAi models\n* The way it writes is very similar to OpenAI’s style.\n* Plus, whoever made this has *serious* computing power... they're giving away billions of tokens for \"free\" at a really fast speed\n* The model says its an OpenAI model. Very common in Chinese models but very unlikely from a US model (unless is really from OpenAI)\n\nBut ok, lets consider other players:  \n \\- Chinese labs: except Deepseek, we had so many new models recently, very hard to think they have more, unless is DeepSeek behind it, but i doubt because of things that i said above. Also when they want to test something they just drop the weights directly  \n \\- Anthropic: Naah  \n \\- Meta: could be, but i think its too early for the new Meta team already made something so much better than Llama, besides i don't see Meta training on OpenAI data since there already have so many data. Llama was not very good because was technologically behind, data is not the problem.  \n \\- Amazon or Microsoft: Would be my second guess  \n \\- Google: Naah, they have Aistudio, when wants feedback they launch the model there  \n \\- IBM or Cohere: Hard to think, but they are very capable companies\n\nHonestly, it’s hard to imagine anyone other than OpenAI being behind this. Two things that i am sure that is a US model and has very capable infra. I know some people aren’t fans of CloseAI, but if they say they’re releasing an open-source model, let’s be optimistic, its a win-win situation. It could be great for us. And with so many good Chinese models becoming popular, maybe OpenAI realized its better to join the open-source world than stay completely closed off.\n\nSo, what you guys think?",
          "author_fullname": "t2_y02rp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "If Horizon Models is not from OpenAI, who would be?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgn94g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 36,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 36,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754237720,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This model is seriously impressive, feels really powerful, and that fits with what people have been saying about it being 120B parameters in size. It&amp;#39;s big enough to be smart without being so huge it steps on OpenAI’s toes. In my experience with the model, here some notes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It works &lt;em&gt;really&lt;/em&gt; well with agents and tools&lt;/li&gt;\n&lt;li&gt;It can handle long contexts (i tested up to around 50k tokens), which is something most open-source models struggle with, only the biggest ones can do that reliably.&lt;/li&gt;\n&lt;li&gt;It’s fantastic with languages other than English, a weakness often sees in Chinese models.&lt;/li&gt;\n&lt;li&gt;It can be based on GPT-5 architecture, even being a smaller version from it (like Gemma models), this would explain why has some differences from current OpenAi models&lt;/li&gt;\n&lt;li&gt;The way it writes is very similar to OpenAI’s style.&lt;/li&gt;\n&lt;li&gt;Plus, whoever made this has &lt;em&gt;serious&lt;/em&gt; computing power... they&amp;#39;re giving away billions of tokens for &amp;quot;free&amp;quot; at a really fast speed&lt;/li&gt;\n&lt;li&gt;The model says its an OpenAI model. Very common in Chinese models but very unlikely from a US model (unless is really from OpenAI)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;But ok, lets consider other players:&lt;br/&gt;\n - Chinese labs: except Deepseek, we had so many new models recently, very hard to think they have more, unless is DeepSeek behind it, but i doubt because of things that i said above. Also when they want to test something they just drop the weights directly&lt;br/&gt;\n - Anthropic: Naah&lt;br/&gt;\n - Meta: could be, but i think its too early for the new Meta team already made something so much better than Llama, besides i don&amp;#39;t see Meta training on OpenAI data since there already have so many data. Llama was not very good because was technologically behind, data is not the problem.&lt;br/&gt;\n - Amazon or Microsoft: Would be my second guess&lt;br/&gt;\n - Google: Naah, they have Aistudio, when wants feedback they launch the model there&lt;br/&gt;\n - IBM or Cohere: Hard to think, but they are very capable companies&lt;/p&gt;\n\n&lt;p&gt;Honestly, it’s hard to imagine anyone other than OpenAI being behind this. Two things that i am sure that is a US model and has very capable infra. I know some people aren’t fans of CloseAI, but if they say they’re releasing an open-source model, let’s be optimistic, its a win-win situation. It could be great for us. And with so many good Chinese models becoming popular, maybe OpenAI realized its better to join the open-source world than stay completely closed off.&lt;/p&gt;\n\n&lt;p&gt;So, what you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgn94g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AMOVCS",
          "discussion_type": null,
          "num_comments": 54,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgn94g/if_horizon_models_is_not_from_openai_who_would_be/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgn94g/if_horizon_models_is_not_from_openai_who_would_be/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754237720,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As the title suggests, I am trying to build a database agent for a custom Erp software with all the tables already inside a Postgres server.\n\nAs it’s an erp software it deals with a variety of tables such as sales/inventory/packaging and so on.\n\nI want to make an agent such that if I ask a question related to sales it should be able to pick it up from the appropriate table and answer it based on the data either through direct SQL execution or through similarity search.\n\nThe twist being I can only afford an ec2 cpu only server to make this work.\n\nWhich is the best way I can do this?\n\nIt should work efficiently and effectively only on CPU and raw RAM.\n\nI already tried with pgvector and stuff, and it’s okay okay but at the cost of a lot of processing time.",
          "author_fullname": "t2_t8chs0i5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local database agent",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhaw4g",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754307039,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title suggests, I am trying to build a database agent for a custom Erp software with all the tables already inside a Postgres server.&lt;/p&gt;\n\n&lt;p&gt;As it’s an erp software it deals with a variety of tables such as sales/inventory/packaging and so on.&lt;/p&gt;\n\n&lt;p&gt;I want to make an agent such that if I ask a question related to sales it should be able to pick it up from the appropriate table and answer it based on the data either through direct SQL execution or through similarity search.&lt;/p&gt;\n\n&lt;p&gt;The twist being I can only afford an ec2 cpu only server to make this work.&lt;/p&gt;\n\n&lt;p&gt;Which is the best way I can do this?&lt;/p&gt;\n\n&lt;p&gt;It should work efficiently and effectively only on CPU and raw RAM.&lt;/p&gt;\n\n&lt;p&gt;I already tried with pgvector and stuff, and it’s okay okay but at the cost of a lot of processing time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhaw4g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Whywhoo",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhaw4g/local_database_agent/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhaw4g/local_database_agent/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754307039,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been working on a documentation system designed to help LLMs understand codebases more efficiently, and I'd love feedback from this community.\n\nThe Problem:\n- Feeding entire codebases/docs to LLMs wastes tokens and context\n- Most tasks only need a subset of information  \n- Human-written docs aren't structured for LLM consumption\n- Different models have different context windows\n\nThe Approach:\nA system that generates multiple \"zoom levels\" of the same documentation:\n- Overview (500 tokens) - capabilities and high-level structure\n- Architecture (2K tokens) - patterns, flows, integrations  \n- Full Details (8K+ tokens) - complete implementation specs\n\nLLMs can:\n- Start with Overview\n- Request deeper levels only if needed\n- Save 75%+ tokens on many tasks\n\nExample: \"Does this API support webhooks?\" → Overview is sufficient. \"Modify the authentication flow\" → Needs Full Details.\n\nAlso exploring different expression modes (technical/standard/friendly) for human readers, but the LLM optimization is my main focus.\n\nQuestions:\n1. How do you currently handle feeding docs/specs to your local models?\n2. Would progressive loading actually help with your context limits?\n3. What structure would work best for models like Mixtral, Llama, etc?\n4. Is this overengineering vs just better chunking strategies?\n\nNot building a product yet - trying to validate if this solves real problems people have with local models and limited context.\n\nThanks for any thoughts!",
          "author_fullname": "t2_9mh0wady",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Title: Reducing token usage with progressive documentation loading - looking for feedback on approach",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgytca",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754266233,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a documentation system designed to help LLMs understand codebases more efficiently, and I&amp;#39;d love feedback from this community.&lt;/p&gt;\n\n&lt;p&gt;The Problem:\n- Feeding entire codebases/docs to LLMs wastes tokens and context\n- Most tasks only need a subset of information&lt;br/&gt;\n- Human-written docs aren&amp;#39;t structured for LLM consumption\n- Different models have different context windows&lt;/p&gt;\n\n&lt;p&gt;The Approach:\nA system that generates multiple &amp;quot;zoom levels&amp;quot; of the same documentation:\n- Overview (500 tokens) - capabilities and high-level structure\n- Architecture (2K tokens) - patterns, flows, integrations&lt;br/&gt;\n- Full Details (8K+ tokens) - complete implementation specs&lt;/p&gt;\n\n&lt;p&gt;LLMs can:\n- Start with Overview\n- Request deeper levels only if needed\n- Save 75%+ tokens on many tasks&lt;/p&gt;\n\n&lt;p&gt;Example: &amp;quot;Does this API support webhooks?&amp;quot; → Overview is sufficient. &amp;quot;Modify the authentication flow&amp;quot; → Needs Full Details.&lt;/p&gt;\n\n&lt;p&gt;Also exploring different expression modes (technical/standard/friendly) for human readers, but the LLM optimization is my main focus.&lt;/p&gt;\n\n&lt;p&gt;Questions:\n1. How do you currently handle feeding docs/specs to your local models?\n2. Would progressive loading actually help with your context limits?\n3. What structure would work best for models like Mixtral, Llama, etc?\n4. Is this overengineering vs just better chunking strategies?&lt;/p&gt;\n\n&lt;p&gt;Not building a product yet - trying to validate if this solves real problems people have with local models and limited context.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgytca",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrsockpicks",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgytca/title_reducing_token_usage_with_progressive/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgytca/title_reducing_token_usage_with_progressive/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754266233,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying to run ik\\_llama with speculative decoding but I just get this error:\n\n    /build/source/src/llama.cpp:18273: GGML_ASSERT(n_tokens_all &lt;= cparams.n_batch) failed\n\nThis was the command I used:\n\n    llama-speculative -m Qwen3-30B-A3B-Thinking-2507-Q4_K_S.gguf -md Qwen3-0.6B-UD-Q5_K_XL.gguf -c 32768\n\nI have tried increasing `--batch-size` up to and past the context size with no effect. Here's the full output, if that's helpful: [https://pastebin.com/wbLKasvD](https://pastebin.com/wbLKasvD)\n\nThe same thing also seems to happen with llama.cpp, with Qwen3 14b + 0.6b, and with the official quants instead of unsloth's.",
          "author_fullname": "t2_1t8nko6ocb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ik_llama speculative decoding error",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mh9uha",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754304258,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754303579,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to run ik_llama with speculative decoding but I just get this error:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;/build/source/src/llama.cpp:18273: GGML_ASSERT(n_tokens_all &amp;lt;= cparams.n_batch) failed\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This was the command I used:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama-speculative -m Qwen3-30B-A3B-Thinking-2507-Q4_K_S.gguf -md Qwen3-0.6B-UD-Q5_K_XL.gguf -c 32768\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I have tried increasing &lt;code&gt;--batch-size&lt;/code&gt; up to and past the context size with no effect. Here&amp;#39;s the full output, if that&amp;#39;s helpful: &lt;a href=\"https://pastebin.com/wbLKasvD\"&gt;https://pastebin.com/wbLKasvD&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The same thing also seems to happen with llama.cpp, with Qwen3 14b + 0.6b, and with the official quants instead of unsloth&amp;#39;s.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh9uha",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Desk5910",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh9uha/ik_llama_speculative_decoding_error/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh9uha/ik_llama_speculative_decoding_error/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754303579,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**ByteDance Seed-Prover proves math the way mathematicians do, not just explanations, but full formal proofs that a computer can verify using Lean.**\n\nIt writes Lean 4 code (a formal proof language), solves problems from competitions like IMO and Putnam, and gets the proof *checked* by a compiler. \n\nThe key innovations:\n\n* **Lemma-first reasoning**: breaks problems into small reusable steps.\n* **Iterative refinement**: re-tries and improves failed proofs.\n* **Formal geometry engine**: solves insane geometry problems using a custom language and a C++ backend.\n\nPerformance? It formally solved **5/6 IMO 2025 problems**, something no model has done before.\n\nCheck simple explanantion here : [https://www.youtube.com/watch?v=os1QcHEpgZQ](https://www.youtube.com/watch?v=os1QcHEpgZQ)\n\nPaper : [https://arxiv.org/abs/2507.23726](https://arxiv.org/abs/2507.23726)",
          "author_fullname": "t2_th2ct5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ByteDance drops Seed-Prover",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgccyc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 176,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 176,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754202843,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;ByteDance Seed-Prover proves math the way mathematicians do, not just explanations, but full formal proofs that a computer can verify using Lean.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;It writes Lean 4 code (a formal proof language), solves problems from competitions like IMO and Putnam, and gets the proof &lt;em&gt;checked&lt;/em&gt; by a compiler. &lt;/p&gt;\n\n&lt;p&gt;The key innovations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Lemma-first reasoning&lt;/strong&gt;: breaks problems into small reusable steps.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Iterative refinement&lt;/strong&gt;: re-tries and improves failed proofs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Formal geometry engine&lt;/strong&gt;: solves insane geometry problems using a custom language and a C++ backend.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Performance? It formally solved &lt;strong&gt;5/6 IMO 2025 problems&lt;/strong&gt;, something no model has done before.&lt;/p&gt;\n\n&lt;p&gt;Check simple explanantion here : &lt;a href=\"https://www.youtube.com/watch?v=os1QcHEpgZQ\"&gt;https://www.youtube.com/watch?v=os1QcHEpgZQ&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Paper : &lt;a href=\"https://arxiv.org/abs/2507.23726\"&gt;https://arxiv.org/abs/2507.23726&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM.jpeg?auto=webp&amp;s=f6d5b40325fd2c44f012121fd1ced8e7cf3fd808",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2880b55faa98583237421c5c4a72d31d6cee167",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=578175b7ddca0a72992280e2d7f06f267e288d69",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=26ae18680865d42ebd641ddb54c43950a72d9432",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mgccyc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Technical-Love-8479",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754202843,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/oevortex/unsloth](https://github.com/oevortex/unsloth)",
          "author_fullname": "t2_hewf57lw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We enabled Multi-GPU training in Unsloth AI — a feature that’s usually paid — using just 2 Copilot prompts!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgbs6r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 157,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 157,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754200681,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/oevortex/unsloth\"&gt;https://github.com/oevortex/unsloth&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?auto=webp&amp;s=81dc71a9f2f02f913bd3aee3c8d23660f674d4d2",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3fa1e4a31ebf9a6d864f530fdd10814f6d32e3a",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f68f83263defa437a0f2fa0d8b040d089cd1d79",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6910771512dba7baaab00bceadb6d8e3f5e32826",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f807024407837a65a122fa874452ed0fa669595e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=755ef7a38a09ee966639de29f878c57aaa1ead1e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e85dbe449ba2af4571feed8fcd1e5ddf597b6c6b",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgbs6r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Quiet-Moment-338",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754200681,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_dyvrh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Mini SVG test: law icons in pt-br (HorizonBeta x Gemini x Claude x Grok)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "bwfogthy9wgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 69,
                  "x": 108,
                  "u": "https://preview.redd.it/bwfogthy9wgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=79cb7919750dfd42045fde6e5a5d1750b6143ab7"
                },
                {
                  "y": 138,
                  "x": 216,
                  "u": "https://preview.redd.it/bwfogthy9wgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=26969a94eb08a0e2171958349db4428a86f71ad7"
                },
                {
                  "y": 205,
                  "x": 320,
                  "u": "https://preview.redd.it/bwfogthy9wgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=21bb5b6a3b3a599bdeeda604db88da6736b6e4d5"
                },
                {
                  "y": 410,
                  "x": 640,
                  "u": "https://preview.redd.it/bwfogthy9wgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=17f23637e692739a1b5ef3b684a0a052ec7ecc48"
                }
              ],
              "s": {
                "y": 542,
                "x": 844,
                "u": "https://preview.redd.it/bwfogthy9wgf1.png?width=844&amp;format=png&amp;auto=webp&amp;s=77c97cc49d1a3a5fd743d7c7a1fb4a1ff770d866"
              },
              "id": "bwfogthy9wgf1"
            },
            "6xwodqxs9wgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 79,
                  "x": 108,
                  "u": "https://preview.redd.it/6xwodqxs9wgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0122e9b9ce6f267dac3a4040f5c4d24a6ab33a5d"
                },
                {
                  "y": 159,
                  "x": 216,
                  "u": "https://preview.redd.it/6xwodqxs9wgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=efa03263563955f38143a53002ad439de9bc1e1d"
                },
                {
                  "y": 236,
                  "x": 320,
                  "u": "https://preview.redd.it/6xwodqxs9wgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5164e776863a4405163a727cae86a79d4fcbf354"
                },
                {
                  "y": 472,
                  "x": 640,
                  "u": "https://preview.redd.it/6xwodqxs9wgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f1462c194648352ba3aacd5919b502f534f08f25"
                }
              ],
              "s": {
                "y": 598,
                "x": 810,
                "u": "https://preview.redd.it/6xwodqxs9wgf1.png?width=810&amp;format=png&amp;auto=webp&amp;s=b4772d1f3538d57556e510a1fe4fd3a4583b6e3f"
              },
              "id": "6xwodqxs9wgf1"
            },
            "ia7e2kbl9wgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/ia7e2kbl9wgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd67a082a94566a9a2aeacf0fc5d448181db7258"
                },
                {
                  "y": 119,
                  "x": 216,
                  "u": "https://preview.redd.it/ia7e2kbl9wgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d7282e81d14bd7c2188987283f144d86a8f7903"
                },
                {
                  "y": 177,
                  "x": 320,
                  "u": "https://preview.redd.it/ia7e2kbl9wgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c26282852557ab96c1361ecbea741c2c4d64498"
                },
                {
                  "y": 354,
                  "x": 640,
                  "u": "https://preview.redd.it/ia7e2kbl9wgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d374a1a222ac5de9389e35f65f8e0d95e5dbc943"
                }
              ],
              "s": {
                "y": 472,
                "x": 853,
                "u": "https://preview.redd.it/ia7e2kbl9wgf1.png?width=853&amp;format=png&amp;auto=webp&amp;s=1f1b989572bed7f831449970146ec8e9cb997450"
              },
              "id": "ia7e2kbl9wgf1"
            },
            "k054awhp9wgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/k054awhp9wgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e37e98e4780eda0cd2ba6181225109151048c90"
                },
                {
                  "y": 136,
                  "x": 216,
                  "u": "https://preview.redd.it/k054awhp9wgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f703a4a1a4194a80e6a4a0a6372a8194a2923fe1"
                },
                {
                  "y": 202,
                  "x": 320,
                  "u": "https://preview.redd.it/k054awhp9wgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=066c24d7a50a6721efb630059efc11f274ffd318"
                },
                {
                  "y": 405,
                  "x": 640,
                  "u": "https://preview.redd.it/k054awhp9wgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7390e8b61f5c96c52d39babfbac6d30feabe189"
                }
              ],
              "s": {
                "y": 512,
                "x": 808,
                "u": "https://preview.redd.it/k054awhp9wgf1.png?width=808&amp;format=png&amp;auto=webp&amp;s=3a53675ac12f7a73898f6114e5c2ccc689d9550f"
              },
              "id": "k054awhp9wgf1"
            }
          },
          "name": "t3_1mgytfi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.65,
          "author_flair_background_color": null,
          "ups": 5,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "ia7e2kbl9wgf1",
                "id": 720642216
              },
              {
                "media_id": "k054awhp9wgf1",
                "id": 720642217
              },
              {
                "media_id": "6xwodqxs9wgf1",
                "id": 720642218
              },
              {
                "media_id": "bwfogthy9wgf1",
                "id": 720642219
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/CE7-Io3jRKJ_PHauMmTqpaj5UaVX4MxPR2ddXENqtHk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754266241,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mgytfi",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgytfi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "celsowm",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgytfi/mini_svg_test_law_icons_in_ptbr_horizonbeta_x/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mgytfi",
          "subreddit_subscribers": 509911,
          "created_utc": 1754266241,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have an Asus laptop with 64 GB RAM and Nvidia RTX 5080 with 16 GB. I've been trying to run different LLM models, including Qwen3 30B. While I'm getting around 30 tokens per second, but when I look at my GPU utilization, I see that it's not even reaching 50%, which makes me wonder if there's more I can get from this GPU because when I run other models, it's a lot slower and still not utilizing the GPU completely. How do I fix this? \n\nI'm making sure that I run the laptop on the performance/turbo profile. I have given LM Studio and Olama prioritization in Nvidia settings, and I'm connecting my laptop to the power supply that provides 380 watts, so power is not a challenge here,   \n  \nPlease share your thoughts. . ",
          "author_fullname": "t2_ctx41",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA GPU underutilized.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 39,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh88gg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/OWjxWUg66tzOtlpxWOTPkx7qVxbSdB7PipMQFFr4FRM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754297498,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an Asus laptop with 64 GB RAM and Nvidia RTX 5080 with 16 GB. I&amp;#39;ve been trying to run different LLM models, including Qwen3 30B. While I&amp;#39;m getting around 30 tokens per second, but when I look at my GPU utilization, I see that it&amp;#39;s not even reaching 50%, which makes me wonder if there&amp;#39;s more I can get from this GPU because when I run other models, it&amp;#39;s a lot slower and still not utilizing the GPU completely. How do I fix this? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m making sure that I run the laptop on the performance/turbo profile. I have given LM Studio and Olama prioritization in Nvidia settings, and I&amp;#39;m connecting my laptop to the power supply that provides 380 watts, so power is not a challenge here,   &lt;/p&gt;\n\n&lt;p&gt;Please share your thoughts. . &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/04cjwsaguygf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/04cjwsaguygf1.png?auto=webp&amp;s=9168f311a7d759b657e124b553e1cfda8859a091",
                  "width": 1759,
                  "height": 500
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/04cjwsaguygf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f56e63703c08f1e15b89671c7bfd11462a862e1",
                    "width": 108,
                    "height": 30
                  },
                  {
                    "url": "https://preview.redd.it/04cjwsaguygf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bde0274d328d65fb99f8c8eefd210a420e32f903",
                    "width": 216,
                    "height": 61
                  },
                  {
                    "url": "https://preview.redd.it/04cjwsaguygf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=124eec8080259e7e84fcf50776ca742a122addf6",
                    "width": 320,
                    "height": 90
                  },
                  {
                    "url": "https://preview.redd.it/04cjwsaguygf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dafc1cdf36686a4a46137b4ebb242cead45f40a1",
                    "width": 640,
                    "height": 181
                  },
                  {
                    "url": "https://preview.redd.it/04cjwsaguygf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1fd62e3651f0c260756fd79ad25cdbdf8bcfe13c",
                    "width": 960,
                    "height": 272
                  },
                  {
                    "url": "https://preview.redd.it/04cjwsaguygf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=26d69ced0da9a7f7ba9f89fe47b1c879db6cd35e",
                    "width": 1080,
                    "height": 306
                  }
                ],
                "variants": {},
                "id": "de1X23OADpVokjeNjAhtzYtnWQEqH8HGPM3KiMrwDfg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh88gg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bu3askoor",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh88gg/nvidia_gpu_underutilized/",
          "stickied": false,
          "url": "https://i.redd.it/04cjwsaguygf1.png",
          "subreddit_subscribers": 509911,
          "created_utc": 1754297498,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was very excited for the release of EXL3 because of its increased performance and revised design to support new models easier. It’s been an eternity since is early preview… and now I wonder if it is doomed. Not just because it’s slow to release, but because models are moving towards large MoEs that all but require they spill over into RAM for most of us. Still, we are getting models around 32b. So what do you think? Or what do you know? Is it on its way? Will it still be helpful?",
          "author_fullname": "t2_dissgzyl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is EXL3 doomed?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgl1qz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=989197fdb000ae3849fd70a2dd3363c36e2714e5",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754232324,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was very excited for the release of EXL3 because of its increased performance and revised design to support new models easier. It’s been an eternity since is early preview… and now I wonder if it is doomed. Not just because it’s slow to release, but because models are moving towards large MoEs that all but require they spill over into RAM for most of us. Still, we are getting models around 32b. So what do you think? Or what do you know? Is it on its way? Will it still be helpful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/turboderp-org/exllamav3",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?auto=webp&amp;s=3ecca6d81c2fa96f4b86230a6c6416f82eb4d520",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f93311c0ef06e6cbd0115381f5af07f2cf7c6763",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4328d8b4f520fbe8256ab167da238bdf76b974d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ee0597232a29c03129624c370183e8c020fce3e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=778afd8299a6ccb54136a78390cc8473e58bebed",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1bad6d74304971cdf123c7601152d5c744927cbd",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9314e4d7ebbf51c941f82bcfbff6bd6a8d85a358",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgl1qz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "silenceimpaired",
          "discussion_type": null,
          "num_comments": 49,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/",
          "stickied": false,
          "url": "https://github.com/turboderp-org/exllamav3",
          "subreddit_subscribers": 509911,
          "created_utc": 1754232324,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am building in the voice AI space and playing with open source TTS models. For single requests, they are great! But when it comes to supporting concurrent requests for streaming pretty much all Youtube videos, docs, tutorials, blog posts cease to exist... \n\nFor example, I have a TTS model which takes about 1.8GB on disk space but when loaded onto VRAM it takes about 2.5GB in GPU. When I run concurrent requests it just doesn't work. There are bunch of shared states overwriting each other, tensors getting corrupt, tensor size mismatches caused by the concurrent requests. \n\nI then tried setting up a pool with 2 model instances. It worked a bit better in that I could now run 2 parallel requests achieving about 600ms-1s for first audio chunk in both requests. But anything beyond 2 concurrent requests end up waiting for GPU to become available. \n\nSome benchmark analysis examples:\n\n✅ 1 concurrent users:  \n   • Throughput: 0.31 req/s  \n   • First byte: 623ms  \n   • First audio: 623ms  \n   • Total latency: 3197ms (P95: 3197ms)  \n   • Real-time factor: 0.70x (lower is better)  \n   • GPU utilization: 34.8%  \n   • Memory usage: 38.4%\n\n✅ 2 concurrent users:  \n   • Throughput: 0.38 req/s  \n   • First byte: 1041ms  \n   • First audio: 1041ms  \n   • Total latency: 5196ms (P95: 5284ms)  \n   • Real-time factor: 1.16x (lower is better)  \n   • GPU utilization: 41.8%  \n   • Memory usage: 39.2%\n\n✅ 4 concurrent users:  \n   • Throughput: 0.38 req/s  \n   • First byte: 5719ms  \n   • First audio: 5719ms  \n   • Total latency: 10173ms (P95: 12510ms)  \n   • Real-time factor: 2.28x (lower is better)  \n   • GPU utilization: 41.6%  \n   • Memory usage: 39.4%\n\n✅ 8 concurrent users:  \n   • Throughput: 0.38 req/s  \n   • First byte: 10436ms  \n   • First audio: 10436ms  \n   • Total latency: 14962ms (P95: 31506ms)  \n   • Real-time factor: 3.36x (lower is better)  \n   • GPU utilization: 41.2%  \n   • Memory usage: 39.4%\n\nThis was done on an T4 GPU with 2vCPU and 7.5GB VRAM.   \nThen I tested T4 with 4vCPU, 15GB RAM and achieved bout 0.50 req/s instead of 0.38. So, some improvement and slight gain on FFTB, first audio. But not much.   \nThen I tested on L4 with 4vCPU, 16GB RAM and I could achieve 0.74\\~ req/s which was much better. But still concurrent users beyond 2 struggle. \n\nI tried bumping up the pool size beyond 2 and there wasn't anything noticeably better. At 4, some latency was in fact slower. At 6 it was about same as 2 or worse. \n\nSo at L4, which comes with 24GB VRAM and cost of like $700/month on cloud, I could only load 2 instances of the TTS model to achieve the \"best\" parallelization. It takes about 5GB VRAM so I am essentially wasting 19GB VRAM of space and money. If I go down to something like T4 it is about 2x cheaper but the performance is about 2x slower as well. \n\nIt is almost like I need 5GB VRAM of L4 GPU kind of a situation but I don't know if it is possile. Because if I rent L4, it just comes with 24GB VRAM and that't it. \n\nSo, now back to the question of enabling parallelization on GPU. I have done a bunch of research online. It looks like \"batch processing\" is the only way to get around this because unlike CPU/Memory there is no concept of \"multi-processing\" on GPU. There are things like CUDA Streams but it didn't make any difference. Inference frameworks like Triton does support streaming batch processing but the underlying model has to support it. In my case, the underlying model has several layers like GPT, Vocoder...etc. And I can't figure out how get the model to support batch processing without taking apart the model or trying to rebuild it from scratch. \n\nI have seen frameworks like Auralis, RealtimeTTS -- but none of them provides true streaming for concurrent users. I have also looked into AllTalk V2 but not seeing true streaming for concurrent users either. I have looked at Baseten's blogs and examples. But renting an H100 to get concurrent user support for up to 24 just doesn't work economically. But they use OrpheusTTS which is an LLM based model that generates discrete audio tokens directly and it works with batch processing. \n\nReaching out to the community to see if anyone has seen any example of how to exactly achieve true streaming support for concurrent users for any open-source TTS models? ",
          "author_fullname": "t2_vgd5f4x3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Scaling GPU - How to add concurrency support?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh3wzs",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754281424,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am building in the voice AI space and playing with open source TTS models. For single requests, they are great! But when it comes to supporting concurrent requests for streaming pretty much all Youtube videos, docs, tutorials, blog posts cease to exist... &lt;/p&gt;\n\n&lt;p&gt;For example, I have a TTS model which takes about 1.8GB on disk space but when loaded onto VRAM it takes about 2.5GB in GPU. When I run concurrent requests it just doesn&amp;#39;t work. There are bunch of shared states overwriting each other, tensors getting corrupt, tensor size mismatches caused by the concurrent requests. &lt;/p&gt;\n\n&lt;p&gt;I then tried setting up a pool with 2 model instances. It worked a bit better in that I could now run 2 parallel requests achieving about 600ms-1s for first audio chunk in both requests. But anything beyond 2 concurrent requests end up waiting for GPU to become available. &lt;/p&gt;\n\n&lt;p&gt;Some benchmark analysis examples:&lt;/p&gt;\n\n&lt;p&gt;✅ 1 concurrent users:&lt;br/&gt;\n   • Throughput: 0.31 req/s&lt;br/&gt;\n   • First byte: 623ms&lt;br/&gt;\n   • First audio: 623ms&lt;br/&gt;\n   • Total latency: 3197ms (P95: 3197ms)&lt;br/&gt;\n   • Real-time factor: 0.70x (lower is better)&lt;br/&gt;\n   • GPU utilization: 34.8%&lt;br/&gt;\n   • Memory usage: 38.4%&lt;/p&gt;\n\n&lt;p&gt;✅ 2 concurrent users:&lt;br/&gt;\n   • Throughput: 0.38 req/s&lt;br/&gt;\n   • First byte: 1041ms&lt;br/&gt;\n   • First audio: 1041ms&lt;br/&gt;\n   • Total latency: 5196ms (P95: 5284ms)&lt;br/&gt;\n   • Real-time factor: 1.16x (lower is better)&lt;br/&gt;\n   • GPU utilization: 41.8%&lt;br/&gt;\n   • Memory usage: 39.2%&lt;/p&gt;\n\n&lt;p&gt;✅ 4 concurrent users:&lt;br/&gt;\n   • Throughput: 0.38 req/s&lt;br/&gt;\n   • First byte: 5719ms&lt;br/&gt;\n   • First audio: 5719ms&lt;br/&gt;\n   • Total latency: 10173ms (P95: 12510ms)&lt;br/&gt;\n   • Real-time factor: 2.28x (lower is better)&lt;br/&gt;\n   • GPU utilization: 41.6%&lt;br/&gt;\n   • Memory usage: 39.4%&lt;/p&gt;\n\n&lt;p&gt;✅ 8 concurrent users:&lt;br/&gt;\n   • Throughput: 0.38 req/s&lt;br/&gt;\n   • First byte: 10436ms&lt;br/&gt;\n   • First audio: 10436ms&lt;br/&gt;\n   • Total latency: 14962ms (P95: 31506ms)&lt;br/&gt;\n   • Real-time factor: 3.36x (lower is better)&lt;br/&gt;\n   • GPU utilization: 41.2%&lt;br/&gt;\n   • Memory usage: 39.4%&lt;/p&gt;\n\n&lt;p&gt;This was done on an T4 GPU with 2vCPU and 7.5GB VRAM.&lt;br/&gt;\nThen I tested T4 with 4vCPU, 15GB RAM and achieved bout 0.50 req/s instead of 0.38. So, some improvement and slight gain on FFTB, first audio. But not much.&lt;br/&gt;\nThen I tested on L4 with 4vCPU, 16GB RAM and I could achieve 0.74~ req/s which was much better. But still concurrent users beyond 2 struggle. &lt;/p&gt;\n\n&lt;p&gt;I tried bumping up the pool size beyond 2 and there wasn&amp;#39;t anything noticeably better. At 4, some latency was in fact slower. At 6 it was about same as 2 or worse. &lt;/p&gt;\n\n&lt;p&gt;So at L4, which comes with 24GB VRAM and cost of like $700/month on cloud, I could only load 2 instances of the TTS model to achieve the &amp;quot;best&amp;quot; parallelization. It takes about 5GB VRAM so I am essentially wasting 19GB VRAM of space and money. If I go down to something like T4 it is about 2x cheaper but the performance is about 2x slower as well. &lt;/p&gt;\n\n&lt;p&gt;It is almost like I need 5GB VRAM of L4 GPU kind of a situation but I don&amp;#39;t know if it is possile. Because if I rent L4, it just comes with 24GB VRAM and that&amp;#39;t it. &lt;/p&gt;\n\n&lt;p&gt;So, now back to the question of enabling parallelization on GPU. I have done a bunch of research online. It looks like &amp;quot;batch processing&amp;quot; is the only way to get around this because unlike CPU/Memory there is no concept of &amp;quot;multi-processing&amp;quot; on GPU. There are things like CUDA Streams but it didn&amp;#39;t make any difference. Inference frameworks like Triton does support streaming batch processing but the underlying model has to support it. In my case, the underlying model has several layers like GPT, Vocoder...etc. And I can&amp;#39;t figure out how get the model to support batch processing without taking apart the model or trying to rebuild it from scratch. &lt;/p&gt;\n\n&lt;p&gt;I have seen frameworks like Auralis, RealtimeTTS -- but none of them provides true streaming for concurrent users. I have also looked into AllTalk V2 but not seeing true streaming for concurrent users either. I have looked at Baseten&amp;#39;s blogs and examples. But renting an H100 to get concurrent user support for up to 24 just doesn&amp;#39;t work economically. But they use OrpheusTTS which is an LLM based model that generates discrete audio tokens directly and it works with batch processing. &lt;/p&gt;\n\n&lt;p&gt;Reaching out to the community to see if anyone has seen any example of how to exactly achieve true streaming support for concurrent users for any open-source TTS models? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh3wzs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Unfair-Enthusiasm-30",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh3wzs/scaling_gpu_how_to_add_concurrency_support/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh3wzs/scaling_gpu_how_to_add_concurrency_support/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754281424,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks,\n\nI’m looking for recommendations for **open-source multimoal LLMs no larger than 8B parameters** that perform well as *agents* for interacting with web pages.\n\n**Context / Constraints:**\n\n* **Max size:** 8B params (need to run locally on an 8 GB GPU without major slowdowns)\n* **Use case:** Complex browser automation — navigating, filling forms, clicking elements, multi-step planning, and handling changing DOM structures.\n* **Agent setup:** Likely to integrate with a framework like BrowserGym, LaVague, Playwright, or similar.\n* **Precision:** I can run FP16 or quantized (8-bit/4-bit) models if that helps.\n* **Goal:** Good mix of reasoning, instruction-following, and robustness for long-horizon tasks.\n\n**Questions:**\n\n1. Which small **open-source multimodal models** have you found most capable for this kind of task?\n2. Any **quantized** versions you recommend for best VRAM fit + speed on consumer GPUs?\n3. Have you seen measurable differences between models in **agentic benchmarks** like Mind2Web, WebArena, or WorkArena?\n\nThanks in advance!",
          "author_fullname": "t2_jy7rkkqp8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What’s the Best Open-Source Small LLM (≤ 8B) for Agentic Web Page Interactions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgr13d",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754246733,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;I’m looking for recommendations for &lt;strong&gt;open-source multimoal LLMs no larger than 8B parameters&lt;/strong&gt; that perform well as &lt;em&gt;agents&lt;/em&gt; for interacting with web pages.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Context / Constraints:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Max size:&lt;/strong&gt; 8B params (need to run locally on an 8 GB GPU without major slowdowns)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Use case:&lt;/strong&gt; Complex browser automation — navigating, filling forms, clicking elements, multi-step planning, and handling changing DOM structures.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Agent setup:&lt;/strong&gt; Likely to integrate with a framework like BrowserGym, LaVague, Playwright, or similar.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Precision:&lt;/strong&gt; I can run FP16 or quantized (8-bit/4-bit) models if that helps.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Good mix of reasoning, instruction-following, and robustness for long-horizon tasks.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Which small &lt;strong&gt;open-source multimodal models&lt;/strong&gt; have you found most capable for this kind of task?&lt;/li&gt;\n&lt;li&gt;Any &lt;strong&gt;quantized&lt;/strong&gt; versions you recommend for best VRAM fit + speed on consumer GPUs?&lt;/li&gt;\n&lt;li&gt;Have you seen measurable differences between models in &lt;strong&gt;agentic benchmarks&lt;/strong&gt; like Mind2Web, WebArena, or WorkArena?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgr13d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Extra-Designer9333",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgr13d/whats_the_best_opensource_small_llm_8b_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgr13d/whats_the_best_opensource_small_llm_8b_for/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754246733,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "During my internship at a big tech company, I struggled with a massive, messy codebase. Too many changes were impossible to understand either because of vague commit messages or because the original authors had left.\n\nFrustrated by losing so much context in git history, I built Gitdive: a local CLI tool that lets you have natural language conversations your repo's history.\n\nIt's early in development and definitely buggy, but if you've faced similar issues, I'd really appreciate your feedback.\n\nCheck it out: [https://github.com/ascl1u/gitdive](https://github.com/ascl1u/gitdive)",
          "author_fullname": "t2_zfhna",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a CLI tool to fix my biggest git frustration: lost commit context",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgyp1z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754265900,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;During my internship at a big tech company, I struggled with a massive, messy codebase. Too many changes were impossible to understand either because of vague commit messages or because the original authors had left.&lt;/p&gt;\n\n&lt;p&gt;Frustrated by losing so much context in git history, I built Gitdive: a local CLI tool that lets you have natural language conversations your repo&amp;#39;s history.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s early in development and definitely buggy, but if you&amp;#39;ve faced similar issues, I&amp;#39;d really appreciate your feedback.&lt;/p&gt;\n\n&lt;p&gt;Check it out: &lt;a href=\"https://github.com/ascl1u/gitdive\"&gt;https://github.com/ascl1u/gitdive&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?auto=webp&amp;s=73de85844faa94867152778081acea6bd0f9f608",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f02a3f481cf62f18021fe3d1ce90e5364980dd8",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b14421e555358b82c89e2d197f8d592cec18f456",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d78ea4d0a5bf464254888775d8ecfcf1e787271",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=57b58b91bbb414ff690236f5866c7a719c090746",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f35c021e464e431de773ce6ac3ebd30329e18c2",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=09b17e71607479cb71a0eb94a76b9bcf76ae4b84",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mgyp1z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Forgotten_Person",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgyp1z/building_a_cli_tool_to_fix_my_biggest_git/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgyp1z/building_a_cli_tool_to_fix_my_biggest_git/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754265900,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🚀 I just open-sourced a fully working persistent memory system for AI assistants!\n\n\n\n🧠 Features:\n\n\\- Real-time memory capture across apps (LM Studio, VS Code, etc.)\n\n\\- Semantic search via vector embeddings\n\n\\- Tool call logging for AI self-reflection\n\n\\- Cross-platform and fully tested\n\n\\- Open source and modular\n\n\n\nBuilt with: Python, SQLite, watchdog, and AI copilots like ChatGPT and GitHub Copilot 🤝\n\n\n\nGitHub: [https://github.com/savantskie/persistent-ai-memory](https://github.com/savantskie/persistent-ai-memory)",
          "author_fullname": "t2_7qb4luzhc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I created a persistent memory for an AI assistant I'm developing, and am releasing the memory system",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg5xlb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 282,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 282,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754181701,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🚀 I just open-sourced a fully working persistent memory system for AI assistants!&lt;/p&gt;\n\n&lt;p&gt;🧠 Features:&lt;/p&gt;\n\n&lt;p&gt;- Real-time memory capture across apps (LM Studio, VS Code, etc.)&lt;/p&gt;\n\n&lt;p&gt;- Semantic search via vector embeddings&lt;/p&gt;\n\n&lt;p&gt;- Tool call logging for AI self-reflection&lt;/p&gt;\n\n&lt;p&gt;- Cross-platform and fully tested&lt;/p&gt;\n\n&lt;p&gt;- Open source and modular&lt;/p&gt;\n\n&lt;p&gt;Built with: Python, SQLite, watchdog, and AI copilots like ChatGPT and GitHub Copilot 🤝&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/savantskie/persistent-ai-memory\"&gt;https://github.com/savantskie/persistent-ai-memory&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?auto=webp&amp;s=726283d31a9e66a5327266304a008d1eb52da1db",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b41ef9face9075e71937a4cd3b28923245b39a29",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6703186c20a684418ded0dc4abb9a6d490d7b023",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=860a8622d037c44a41b9ee50c96939060a76e917",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=77ef5d63ac01ba46958961a63c5e9f43dabb1245",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=446a7e497c6b8549a8cbed43e99bc5f99e79c618",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e1ab447d78f6edbe4f20129542abf92b51d827b1",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg5xlb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Savantskie1",
          "discussion_type": null,
          "num_comments": 88,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754181701,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to run an entry level local LLM of 15-35B Q4 with context of at least 10k that I want to train with local pdfs using RAG.\n\nI now have a Mac Mini M4 16GB and a 4K gaming rig with RTX 4090 24GB\n\nI would use the AI infrequently throughout the day, so anything above 10-20 t/s would be acceptable for me for a 15-35B LLM.\n\nBased on my research I couldn’t find one centralized benchmarking solution but I could find the following approximate values for bandwidth and performance for Gemma 3 12B LLM:\n\n* 80gb/s DDR5 5200 MHZ, cheapest way to 128GB - $500\n* 21 t/s - 120 GB/s Mac Mini M4 32GB 1000$\n* 46 t/s - 273 GB/s Mac Mini M4 Pro 32-64GB 2500$\n* 43 t/s - 267 GB/s Mini PC Ryzen AI HX 395+ 64-128 GB comparable $ with Mac Mini M4 Pro\n* 410 GB/s Mac Studio M4 Max 36-128 GB tons of $\n* 819 GB/s Mac Studio M3 Ultra up to 512GB tons of $\n* 132 t/s - 750 GB/s RTX 4090 24GB\n\nBecause I’m considering the entire thing like purchasing price, ownership, efficiency (electricity, noise, heat) and resale value, I’ve reached the following conclusions that I hope to validate:\n\n* I need something between M4 and the GPU in terms of speed.\n* I need 32GB but could do with more if it’s not a ripoff.\n* Since Ryzen and M4 pro both have soldered RAM and have roughly the same bandwidth and Ryzen has much higher power draw, so the mac mini narrowly wins for me, especially when it comes to noise and resale\n* For my use case GPU is OP and not worth it when considering power draw and heat. The same goes for maxing ram on my desktop since it would use 600W, and I can’t justify getting a second GPU.\n* M4 appears the best choice if I want to stay at 32-64 GB. I would like a mini pc with user replaceable ram and bandwidth as high as the AI HX, but there is no such thing.\n\nSo is there anything between an M4 mini and a GPU that is a good price to performance ratio and isn’t noisy and power hungry?",
          "author_fullname": "t2_12uzcl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking to build or buy a mini pc for LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgvbw6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754259032,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754256996,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to run an entry level local LLM of 15-35B Q4 with context of at least 10k that I want to train with local pdfs using RAG.&lt;/p&gt;\n\n&lt;p&gt;I now have a Mac Mini M4 16GB and a 4K gaming rig with RTX 4090 24GB&lt;/p&gt;\n\n&lt;p&gt;I would use the AI infrequently throughout the day, so anything above 10-20 t/s would be acceptable for me for a 15-35B LLM.&lt;/p&gt;\n\n&lt;p&gt;Based on my research I couldn’t find one centralized benchmarking solution but I could find the following approximate values for bandwidth and performance for Gemma 3 12B LLM:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;80gb/s DDR5 5200 MHZ, cheapest way to 128GB - $500&lt;/li&gt;\n&lt;li&gt;21 t/s - 120 GB/s Mac Mini M4 32GB 1000$&lt;/li&gt;\n&lt;li&gt;46 t/s - 273 GB/s Mac Mini M4 Pro 32-64GB 2500$&lt;/li&gt;\n&lt;li&gt;43 t/s - 267 GB/s Mini PC Ryzen AI HX 395+ 64-128 GB comparable $ with Mac Mini M4 Pro&lt;/li&gt;\n&lt;li&gt;410 GB/s Mac Studio M4 Max 36-128 GB tons of $&lt;/li&gt;\n&lt;li&gt;819 GB/s Mac Studio M3 Ultra up to 512GB tons of $&lt;/li&gt;\n&lt;li&gt;132 t/s - 750 GB/s RTX 4090 24GB&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Because I’m considering the entire thing like purchasing price, ownership, efficiency (electricity, noise, heat) and resale value, I’ve reached the following conclusions that I hope to validate:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I need something between M4 and the GPU in terms of speed.&lt;/li&gt;\n&lt;li&gt;I need 32GB but could do with more if it’s not a ripoff.&lt;/li&gt;\n&lt;li&gt;Since Ryzen and M4 pro both have soldered RAM and have roughly the same bandwidth and Ryzen has much higher power draw, so the mac mini narrowly wins for me, especially when it comes to noise and resale&lt;/li&gt;\n&lt;li&gt;For my use case GPU is OP and not worth it when considering power draw and heat. The same goes for maxing ram on my desktop since it would use 600W, and I can’t justify getting a second GPU.&lt;/li&gt;\n&lt;li&gt;M4 appears the best choice if I want to stay at 32-64 GB. I would like a mini pc with user replaceable ram and bandwidth as high as the AI HX, but there is no such thing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So is there anything between an M4 mini and a GPU that is a good price to performance ratio and isn’t noisy and power hungry?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgvbw6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nemuro87",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754256996,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nI am currently running Qwen3-Coder-30B and tried to create unit test for my classes with different tools like qwen coder, countinue dev or proxy ai.\n\nThe tests which are created have many errors and I dont understand why. Atleast for the qwen coder, i would expected, if checks the codebase for all files.  \n  \nExample:   \nI have a enum with the values INSERT and UPDATED. In the test its using CREATED which does not exists in the enum.  \nOr I have a object with setters, but in the test it tries only with \"with\" (like it expected a builder). If it use the setter, it thinks, that the object is giving back as a return value instead nothing/void.\n\nI thought the model could help me with writing tests or do code review, but it doesnt feel really helpful.  \nDo i miss something or is that a common problem?",
          "author_fullname": "t2_4xgd14or",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Generating Unit Tests with Qwen3-Coder-30B - not really usable",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgv4h3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754256477,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am currently running Qwen3-Coder-30B and tried to create unit test for my classes with different tools like qwen coder, countinue dev or proxy ai.&lt;/p&gt;\n\n&lt;p&gt;The tests which are created have many errors and I dont understand why. Atleast for the qwen coder, i would expected, if checks the codebase for all files.  &lt;/p&gt;\n\n&lt;p&gt;Example:&lt;br/&gt;\nI have a enum with the values INSERT and UPDATED. In the test its using CREATED which does not exists in the enum.&lt;br/&gt;\nOr I have a object with setters, but in the test it tries only with &amp;quot;with&amp;quot; (like it expected a builder). If it use the setter, it thinks, that the object is giving back as a return value instead nothing/void.&lt;/p&gt;\n\n&lt;p&gt;I thought the model could help me with writing tests or do code review, but it doesnt feel really helpful.&lt;br/&gt;\nDo i miss something or is that a common problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgv4h3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FarXTraveler",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgv4h3/generating_unit_tests_with_qwen3coder30b_not/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgv4h3/generating_unit_tests_with_qwen3coder30b_not/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754256477,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys!\n\n**I got tired of installing AI tools the hard way.**\n\nEvery time I wanted to try something like Stable Diffusion, RVC or a local LLM, it was the same nightmare:\n\n**terminal commands, missing dependencies, broken CUDA, slow setup, frustration.**\n\nSo I built **Dione** — a desktop app that makes running local AI feel like using an App Store.\n\n**What it does:**\n\n* Browse and install AI tools with one click (like apps)\n* No terminal, no Python setup, no configs\n* Open-source, designed with UX in mind\n\n**You can try it** [**here**](https://getdione.app)**.**\n\n**Why I built it?**\n\nTools like Pinokio or open-source repos are powerful, but honestly… **most look like they were made by devs, for devs**.\n\n**I wanted something simple**. Something visual. Something you can give to your non-tech friend and it still works.\n\n**Dione is my attempt to make local AI accessible without losing control or power.**\n\nWould you use something like this? Anything confusing / missing?\n\nThe project is still evolving, and I’m fully open to ideas and contributions. Also, if you’re into self-hosted AI or building tools around it — let’s talk!\n\nGitHub: [https://getdione.app/github](https://getdione.app/github)\n\nThanks for reading &lt;3!",
          "author_fullname": "t2_dau1zlvo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "I created an app to run local AI as if it were the App Store",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 46,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ysewb2ytxqgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 36,
                  "x": 108,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b69be81b017e04cc485309d6ea7db34f44eb65cd"
                },
                {
                  "y": 72,
                  "x": 216,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cef7f4312d5076f63633ce0082f84992449d708f"
                },
                {
                  "y": 106,
                  "x": 320,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0950b8c7452a7592a0103624fa574fa42d0435a4"
                },
                {
                  "y": 213,
                  "x": 640,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a3d1571494f02e307302b6ce30eb0b231baef2eb"
                },
                {
                  "y": 320,
                  "x": 960,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d1f01d071b3b900cf9e696a94ba3b157258152c"
                },
                {
                  "y": 360,
                  "x": 1080,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b39946b7843a58ceef7b49e4bd40107d54164ca"
                }
              ],
              "s": {
                "y": 1000,
                "x": 3000,
                "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=3000&amp;format=png&amp;auto=webp&amp;s=54735c109c32811d4b153a0c2e59c72a4c06bf5d"
              },
              "id": "ysewb2ytxqgf1"
            },
            "yt2muytuxqgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 57,
                  "x": 108,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f6ce704ac3695892c4184608ef310e5bf451248"
                },
                {
                  "y": 115,
                  "x": 216,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ff91a2d48c077a5f508c0a4840cb39b61883c92d"
                },
                {
                  "y": 171,
                  "x": 320,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4279610bad7b104472f3da3351e6ffc9361fafb5"
                },
                {
                  "y": 342,
                  "x": 640,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e834d2caf46a9a6f01999cf11161e7f0e1a9c17"
                },
                {
                  "y": 514,
                  "x": 960,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=666cb580f9192df96b59d1ca07402b7628e0ff4a"
                },
                {
                  "y": 578,
                  "x": 1080,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=528bb2d873fdc538d816d91b547db28b28ad5f34"
                }
              ],
              "s": {
                "y": 1026,
                "x": 1915,
                "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=1915&amp;format=png&amp;auto=webp&amp;s=c32ef79f62a2ffe7f4e5128937b67c1a2b216d50"
              },
              "id": "yt2muytuxqgf1"
            }
          },
          "name": "t3_1mgc0v0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 98,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "ysewb2ytxqgf1",
                "id": 720059841
              },
              {
                "media_id": "yt2muytuxqgf1",
                "id": 720059842
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 98,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/DMpoJ6kbQBgxqPJp_64cY0H-qxr9jJD3Tr3RuFU7894.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754201559,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I got tired of installing AI tools the hard way.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Every time I wanted to try something like Stable Diffusion, RVC or a local LLM, it was the same nightmare:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;terminal commands, missing dependencies, broken CUDA, slow setup, frustration.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So I built &lt;strong&gt;Dione&lt;/strong&gt; — a desktop app that makes running local AI feel like using an App Store.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Browse and install AI tools with one click (like apps)&lt;/li&gt;\n&lt;li&gt;No terminal, no Python setup, no configs&lt;/li&gt;\n&lt;li&gt;Open-source, designed with UX in mind&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;You can try it&lt;/strong&gt; &lt;a href=\"https://getdione.app\"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why I built it?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Tools like Pinokio or open-source repos are powerful, but honestly… &lt;strong&gt;most look like they were made by devs, for devs&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I wanted something simple&lt;/strong&gt;. Something visual. Something you can give to your non-tech friend and it still works.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Dione is my attempt to make local AI accessible without losing control or power.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Would you use something like this? Anything confusing / missing?&lt;/p&gt;\n\n&lt;p&gt;The project is still evolving, and I’m fully open to ideas and contributions. Also, if you’re into self-hosted AI or building tools around it — let’s talk!&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://getdione.app/github\"&gt;https://getdione.app/github&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading &amp;lt;3!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mgc0v0",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgc0v0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Deivih-4774",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgc0v0/i_created_an_app_to_run_local_ai_as_if_it_were/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mgc0v0",
          "subreddit_subscribers": 509911,
          "created_utc": 1754201559,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It feels kinda strange that every thing that pops up with voice models is specific to cloning. What if i want a unique voice? But searching, i don't even see discussions on training models to do anything but clone an existing voice. \n\nI imagine training a new voice from scratch would take a lot of different, high quality samples- but when i look at all the stuff that claims it can clone a voice from a couple short clips- it makes me wonder why that middle process couldn't be stepped through- either programatically (if RVC can clone a real voice, could adjusting some parameters create a similar-but-unique voice? including several samples if voices that fall in a similar range instead of a single person and try to clone that?  )  But googling, i can't really find anyone tackling anything except either using pre-baked voice models, or cloning someones existing voice. ",
          "author_fullname": "t2_164x4a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any options for creating  original tts models, not just cloning someone specific?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgzuky",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754269225,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It feels kinda strange that every thing that pops up with voice models is specific to cloning. What if i want a unique voice? But searching, i don&amp;#39;t even see discussions on training models to do anything but clone an existing voice. &lt;/p&gt;\n\n&lt;p&gt;I imagine training a new voice from scratch would take a lot of different, high quality samples- but when i look at all the stuff that claims it can clone a voice from a couple short clips- it makes me wonder why that middle process couldn&amp;#39;t be stepped through- either programatically (if RVC can clone a real voice, could adjusting some parameters create a similar-but-unique voice? including several samples if voices that fall in a similar range instead of a single person and try to clone that?  )  But googling, i can&amp;#39;t really find anyone tackling anything except either using pre-baked voice models, or cloning someones existing voice. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgzuky",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "moarmagic",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgzuky/are_there_any_options_for_creating_original_tts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgzuky/are_there_any_options_for_creating_original_tts/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754269225,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I really like LM Studio because it allows you to run AI models locally, preserving the privacy of your conversations with the AI. However, compared to commercial online models, LM Studio doesn’t support internet browsing “out of the box.” Those models can’t use up-to-date information from the Internet to answer questions.\n\n\nNot long ago, LM Studio added the ability to connect MCP servers to models. The very first thing I did was write a small MCP server that can extract text from a URL. It can also extract the links present on the page. This makes it possible, when querying the AI, to specify an address and ask it to extract text from there or retrieve links to use in its response.\n\nTo get all of this working, we first create a `pyproject.toml` file in the `mcp-server` folder.\n\n\n```toml \n[build-system]\nrequires = [\"setuptools&gt;=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"url-text-fetcher\"\nversion = \"0.1.0\"\ndescription = \"FastMCP server for URL text fetching\"\nauthors = [{ name=\"Evgeny Igumnov\", email=\"igumnovnsk@gmail.com\" }]\ndependencies = [\n  \"fastmcp\",\n  \"requests\",\n  \"beautifulsoup4\",\n]\n[project.scripts]\nurl-text-fetcher = \"url_text_fetcher.mcp_server:main\"\n```\nThen we create the `mcp_server.py` file in the `mcp-server/url_text_fetcher` folder.\n```python\nfrom mcp.server.fastmcp import FastMCP\nimport requests\nfrom bs4 import BeautifulSoup\nfrom typing import List  # for type hints\n\nmcp = FastMCP(\"URL Text Fetcher\")\n\n@mcp.tool()\ndef fetch_url_text(url: str) -&gt; str:\n    \"\"\"Download the text from a URL.\"\"\"\n    resp = requests.get(url, timeout=10)\n    resp.raise_for_status()\n    soup = BeautifulSoup(resp.text, \"html.parser\")\n    return soup.get_text(separator=\"\\n\", strip=True)\n\n@mcp.tool()\ndef fetch_page_links(url: str) -&gt; List[str]:\n    \"\"\"Return a list of all URLs found on the given page.\"\"\"\n    resp = requests.get(url, timeout=10)\n    resp.raise_for_status()\n    soup = BeautifulSoup(resp.text, \"html.parser\")\n    # Extract all href attributes from &lt;a&gt; tags\n    links = [a['href'] for a in soup.find_all('a', href=True)]\n    return links\n\ndef main():\n    mcp.run()\n\nif __name__ == \"__main__\":\n    main()\n```\n\nNext, create an empty `__init__.py` in the `mcp-server/url_text_fetcher` folder.\n\nAnd finally, for the MCP server to work, you need to install it:\n\n```bash\npip install -e .\n```\n\nAt the bottom of the chat window in LM Studio, where you enter your query, you can choose an MCP server via “Integrations.” By clicking “Install” and then “Edit mcp.json,” you can add your own MCP server in that file.\n\n```json\n{\n  \"mcpServers\": {\n    \"url-text-fetcher\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"-m\",\n        \"url_text_fetcher.mcp_server\"\n      ]\n    }\n  }\n}\n```\n\nThe second thing I did was integrate an existing MCP server from the Brave search engine, which allows you to instruct the AI—in a request—to search the Internet for information to answer a question. To do this, first check that you have `npx` installed. Then install `@modelcontextprotocol/server-brave-search`:\n\n```bash\nnpm i -D @modelcontextprotocol/server-brave-search\n```\n\nHere’s how you can connect it in the `mcp.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-brave-search\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"..................\"\n      }\n    },\n    \"url-text-fetcher\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"-m\",\n        \"url_text_fetcher.mcp_server\"\n      ]\n    }\n  }\n}\n```\n\nYou can obtain the `BRAVE_API_KEY` for free, with minor limitations of up to 2,000 requests per month and no more than one request per second.\n\nAs a result, at the bottom of the chat window in LM Studio—where the user enters their query—you can select the MCP server via “Integrations,” and you should see two MCP servers listed: “mcp/url-text-fetcher” and “mcp/brave-search.”\n\n",
          "author_fullname": "t2_gj8yhx0zk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Teaching LM Studio to Browse the Internet When Answering Questions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgljhp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754233538,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really like LM Studio because it allows you to run AI models locally, preserving the privacy of your conversations with the AI. However, compared to commercial online models, LM Studio doesn’t support internet browsing “out of the box.” Those models can’t use up-to-date information from the Internet to answer questions.&lt;/p&gt;\n\n&lt;p&gt;Not long ago, LM Studio added the ability to connect MCP servers to models. The very first thing I did was write a small MCP server that can extract text from a URL. It can also extract the links present on the page. This makes it possible, when querying the AI, to specify an address and ask it to extract text from there or retrieve links to use in its response.&lt;/p&gt;\n\n&lt;p&gt;To get all of this working, we first create a &lt;code&gt;pyproject.toml&lt;/code&gt; file in the &lt;code&gt;mcp-server&lt;/code&gt; folder.&lt;/p&gt;\n\n&lt;p&gt;```toml \n[build-system]\nrequires = [&amp;quot;setuptools&amp;gt;=42&amp;quot;, &amp;quot;wheel&amp;quot;]\nbuild-backend = &amp;quot;setuptools.build_meta&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;[project]\nname = &amp;quot;url-text-fetcher&amp;quot;\nversion = &amp;quot;0.1.0&amp;quot;\ndescription = &amp;quot;FastMCP server for URL text fetching&amp;quot;\nauthors = [{ name=&amp;quot;Evgeny Igumnov&amp;quot;, email=&amp;quot;&lt;a href=\"mailto:igumnovnsk@gmail.com\"&gt;igumnovnsk@gmail.com&lt;/a&gt;&amp;quot; }]\ndependencies = [\n  &amp;quot;fastmcp&amp;quot;,\n  &amp;quot;requests&amp;quot;,\n  &amp;quot;beautifulsoup4&amp;quot;,\n]\n[project.scripts]\nurl-text-fetcher = &amp;quot;url_text_fetcher.mcp_server:main&amp;quot;\n&lt;code&gt;\nThen we create the `mcp_server.py` file in the `mcp-server/url_text_fetcher` folder.\n&lt;/code&gt;python\nfrom mcp.server.fastmcp import FastMCP\nimport requests\nfrom bs4 import BeautifulSoup\nfrom typing import List  # for type hints&lt;/p&gt;\n\n&lt;p&gt;mcp = FastMCP(&amp;quot;URL Text Fetcher&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;@mcp.tool()\ndef fetch_url_text(url: str) -&amp;gt; str:\n    &amp;quot;&amp;quot;&amp;quot;Download the text from a URL.&amp;quot;&amp;quot;&amp;quot;\n    resp = requests.get(url, timeout=10)\n    resp.raise_for_status()\n    soup = BeautifulSoup(resp.text, &amp;quot;html.parser&amp;quot;)\n    return soup.get_text(separator=&amp;quot;\\n&amp;quot;, strip=True)&lt;/p&gt;\n\n&lt;p&gt;@mcp.tool()\ndef fetch_page_links(url: str) -&amp;gt; List[str]:\n    &amp;quot;&amp;quot;&amp;quot;Return a list of all URLs found on the given page.&amp;quot;&amp;quot;&amp;quot;\n    resp = requests.get(url, timeout=10)\n    resp.raise_for_status()\n    soup = BeautifulSoup(resp.text, &amp;quot;html.parser&amp;quot;)\n    # Extract all href attributes from &amp;lt;a&amp;gt; tags\n    links = [a[&amp;#39;href&amp;#39;] for a in soup.find_all(&amp;#39;a&amp;#39;, href=True)]\n    return links&lt;/p&gt;\n\n&lt;p&gt;def main():\n    mcp.run()&lt;/p&gt;\n\n&lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;quot;&lt;strong&gt;main&lt;/strong&gt;&amp;quot;:\n    main()\n```&lt;/p&gt;\n\n&lt;p&gt;Next, create an empty &lt;code&gt;__init__.py&lt;/code&gt; in the &lt;code&gt;mcp-server/url_text_fetcher&lt;/code&gt; folder.&lt;/p&gt;\n\n&lt;p&gt;And finally, for the MCP server to work, you need to install it:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;bash\npip install -e .\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;At the bottom of the chat window in LM Studio, where you enter your query, you can choose an MCP server via “Integrations.” By clicking “Install” and then “Edit mcp.json,” you can add your own MCP server in that file.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;json\n{\n  &amp;quot;mcpServers&amp;quot;: {\n    &amp;quot;url-text-fetcher&amp;quot;: {\n      &amp;quot;command&amp;quot;: &amp;quot;python&amp;quot;,\n      &amp;quot;args&amp;quot;: [\n        &amp;quot;-m&amp;quot;,\n        &amp;quot;url_text_fetcher.mcp_server&amp;quot;\n      ]\n    }\n  }\n}\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;The second thing I did was integrate an existing MCP server from the Brave search engine, which allows you to instruct the AI—in a request—to search the Internet for information to answer a question. To do this, first check that you have &lt;code&gt;npx&lt;/code&gt; installed. Then install &lt;code&gt;@modelcontextprotocol/server-brave-search&lt;/code&gt;:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;bash\nnpm i -D @modelcontextprotocol/server-brave-search\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Here’s how you can connect it in the &lt;code&gt;mcp.json&lt;/code&gt; file:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;json\n{\n  &amp;quot;mcpServers&amp;quot;: {\n    &amp;quot;brave-search&amp;quot;: {\n      &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;,\n      &amp;quot;args&amp;quot;: [\n        &amp;quot;-y&amp;quot;,\n        &amp;quot;@modelcontextprotocol/server-brave-search&amp;quot;\n      ],\n      &amp;quot;env&amp;quot;: {\n        &amp;quot;BRAVE_API_KEY&amp;quot;: &amp;quot;..................&amp;quot;\n      }\n    },\n    &amp;quot;url-text-fetcher&amp;quot;: {\n      &amp;quot;command&amp;quot;: &amp;quot;python&amp;quot;,\n      &amp;quot;args&amp;quot;: [\n        &amp;quot;-m&amp;quot;,\n        &amp;quot;url_text_fetcher.mcp_server&amp;quot;\n      ]\n    }\n  }\n}\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;You can obtain the &lt;code&gt;BRAVE_API_KEY&lt;/code&gt; for free, with minor limitations of up to 2,000 requests per month and no more than one request per second.&lt;/p&gt;\n\n&lt;p&gt;As a result, at the bottom of the chat window in LM Studio—where the user enters their query—you can select the MCP server via “Integrations,” and you should see two MCP servers listed: “mcp/url-text-fetcher” and “mcp/brave-search.”&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mgljhp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ievkz",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgljhp/teaching_lm_studio_to_browse_the_internet_when/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgljhp/teaching_lm_studio_to_browse_the_internet_when/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754233538,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I literally can't find anything on this model. I saw somewhere on discord that it's similar to claude (which I doubt). any info? and no i'm not promoting this website or any bs like that idk anything about it",
          "author_fullname": "t2_askwa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Jin 3.5 - Does anyone know anything about this model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgo662",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754239996,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "jin.elpa.ai",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I literally can&amp;#39;t find anything on this model. I saw somewhere on discord that it&amp;#39;s similar to claude (which I doubt). any info? and no i&amp;#39;m not promoting this website or any bs like that idk anything about it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://jin.elpa.ai/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgo662",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "z_3454_pfk",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgo662/jin_35_does_anyone_know_anything_about_this_model/",
          "stickied": false,
          "url": "https://jin.elpa.ai/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754239996,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I use ChatGPT and Claude paid plans. I’m incorporating local for writing topics that they censor. I understand that closed source is not on the level of Claude and can’t be at this time. I accept that. However, I’m having trouble getting it to work at even a basic level and I’ve found nothing on Google or YouTube that talks about prompt crafting outside of waifu chats (and even those are few). \n\nWhen I try to use it like I would CGPT or Claude it puts out garbage. I’ve gotten what looks like Twitter comments, the same thing I sent it, error codes, random stuff that makes no sense (but at least some what mirrors the style I specified). \n\nCGPT says you can’t put much info in the world building or author notes because of tokens. It says local LLMs are more like continuing text you put in and don’t have the same training depth. Is that true? \n\nLook basically I want what I have in CGPT or Claude. I can upload a few pages long doc and it reference that. Then follow prompts to give me new ideas or continue in a style or revise. **Is that possible**? **If so how?”**. \n\n**Should I just train a model on my own voice? How many samples of my writing do I realistically need for that? CGPT says 50-120k+ words ideally**",
          "author_fullname": "t2_hhsbgspn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I don’t understand how to get what I want from Local LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgrgmu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754247764,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use ChatGPT and Claude paid plans. I’m incorporating local for writing topics that they censor. I understand that closed source is not on the level of Claude and can’t be at this time. I accept that. However, I’m having trouble getting it to work at even a basic level and I’ve found nothing on Google or YouTube that talks about prompt crafting outside of waifu chats (and even those are few). &lt;/p&gt;\n\n&lt;p&gt;When I try to use it like I would CGPT or Claude it puts out garbage. I’ve gotten what looks like Twitter comments, the same thing I sent it, error codes, random stuff that makes no sense (but at least some what mirrors the style I specified). &lt;/p&gt;\n\n&lt;p&gt;CGPT says you can’t put much info in the world building or author notes because of tokens. It says local LLMs are more like continuing text you put in and don’t have the same training depth. Is that true? &lt;/p&gt;\n\n&lt;p&gt;Look basically I want what I have in CGPT or Claude. I can upload a few pages long doc and it reference that. Then follow prompts to give me new ideas or continue in a style or revise. &lt;strong&gt;Is that possible&lt;/strong&gt;? &lt;strong&gt;If so how?”&lt;/strong&gt;. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Should I just train a model on my own voice? How many samples of my writing do I realistically need for that? CGPT says 50-120k+ words ideally&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgrgmu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AccidentalFolklore",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgrgmu/i_dont_understand_how_to_get_what_i_want_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgrgmu/i_dont_understand_how_to_get_what_i_want_from/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754247764,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Article: https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e\n\nContext:\n\nThis insane new paper got 40% on ARC-AGI with an absolutely tiny model (27M params). It's seriously a revolutionary new paper that got way less attention than it deserved.\n\nhttps://arxiv.org/abs/2506.21734\n\nA number of people have reproduced it if anyone is worried about that: https://x.com/VictorTaelin/status/1950512015899840768\nhttps://github.com/sapientinc/HRM/issues/12",
          "author_fullname": "t2_4kcht",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "HRM solved thinking more than current \"thinking\" models (this needs more hype)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg3i48",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 335,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 335,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754174679,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Article: &lt;a href=\"https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e\"&gt;https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Context:&lt;/p&gt;\n\n&lt;p&gt;This insane new paper got 40% on ARC-AGI with an absolutely tiny model (27M params). It&amp;#39;s seriously a revolutionary new paper that got way less attention than it deserved.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2506.21734\"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;A number of people have reproduced it if anyone is worried about that: &lt;a href=\"https://x.com/VictorTaelin/status/1950512015899840768\"&gt;https://x.com/VictorTaelin/status/1950512015899840768&lt;/a&gt;\n&lt;a href=\"https://github.com/sapientinc/HRM/issues/12\"&gt;https://github.com/sapientinc/HRM/issues/12&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?auto=webp&amp;s=3649785728b7c459cdac6d4444cd5a58a31ff8a2",
                  "width": 994,
                  "height": 610
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a32ed96622f166ae158780da40b7af8d982e72f9",
                    "width": 108,
                    "height": 66
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2621c7644d6e96ab551888e43049a8b94e227fc5",
                    "width": 216,
                    "height": 132
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3da54eeb0e463cf73d8f5292e9b099e338494438",
                    "width": 320,
                    "height": 196
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa1caf030ca1b3e5b1df5e052afe93497a9bb543",
                    "width": 640,
                    "height": 392
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=88919a9c5b50d0a4d76e2c2bb7e87bb3c4eed852",
                    "width": 960,
                    "height": 589
                  }
                ],
                "variants": {},
                "id": "okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mg3i48",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Charuru",
          "discussion_type": null,
          "num_comments": 71,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754174679,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "that's it. thanks.",
          "author_fullname": "t2_9b9s4a7g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MLX DWQ question: what's \"lr1e-8\" and \"lr8e-7 \" and should we care?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgys0z",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754266130,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;that&amp;#39;s it. thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgys0z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JLeonsarmiento",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgys0z/mlx_dwq_question_whats_lr1e8_and_lr8e7_and_should/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgys0z/mlx_dwq_question_whats_lr1e8_and_lr8e7_and_should/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754266130,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, let's say I want to build a box to play around with LLM but also general purpose PC and gaming. Let's also say I have a $10,000 budget. What build would you go with?",
          "author_fullname": "t2_f8icj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM dream build?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh5h07",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754286817,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, let&amp;#39;s say I want to build a box to play around with LLM but also general purpose PC and gaming. Let&amp;#39;s also say I have a $10,000 budget. What build would you go with?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh5h07",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ikkiyikki",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh5h07/llm_dream_build/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh5h07/llm_dream_build/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754286817,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The larger SmallThinker MoE has been through a quantization aware training process. it's uploaded to the same gguf repo a bit later. \n\n- https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf\n\nIn llama.cpp m2 air 16gb, with the `sudo sysctl iogpu.wired_limit_mb=13000` command, it's 30 t/s. \n\nThe model is CPU inference optimised for very low RAM provisions + fast disc, alongside sparsity optimizations, in their llama.cpp fork. \nThe models are pre-trained from scratch. This group always had a good eye for inference optimizations, Always happy to see their works.",
          "author_fullname": "t2_el5pibmej",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmallThinker-21B-A3B-Instruct-QAT version",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgbprh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 83,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 83,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=f963436f8fe8c361365006afe812426eb0a95bfc",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754200435,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The larger SmallThinker MoE has been through a quantization aware training process. it&amp;#39;s uploaded to the same gguf repo a bit later. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf\"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In llama.cpp m2 air 16gb, with the &lt;code&gt;sudo sysctl iogpu.wired_limit_mb=13000&lt;/code&gt; command, it&amp;#39;s 30 t/s. &lt;/p&gt;\n\n&lt;p&gt;The model is CPU inference optimised for very low RAM provisions + fast disc, alongside sparsity optimizations, in their llama.cpp fork. \nThe models are pre-trained from scratch. This group always had a good eye for inference optimizations, Always happy to see their works.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?auto=webp&amp;s=af35574c8d3a17d80825392473da49ac8b6cb469",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=97ed6aedc23ef593aadb95c2316196f396bc8e65",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ac0298234f43f3e33451ea17a59722ab4866a82",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=19ce50a51896a266764f718a5da7eb00d163ddf3",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9d016e4dfb71f7ddb2afc8c6c06ee782ba15871a",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=800a6ea36b04ed56de7a369596d03474bfd1142f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=29a1fe69fa68a7a75c2954c6a0922c20dd4448f1",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgbprh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Aaaaaaaaaeeeee",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgbprh/smallthinker21ba3binstructqat_version/",
          "stickied": false,
          "url": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf",
          "subreddit_subscribers": 509911,
          "created_utc": 1754200435,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's a WIP, but it's looking like may be possible to pair Macs with NVIDIA soon!\n\nTweet: [https://x.com/anemll/status/1951307167417639101](https://x.com/anemll/status/1951307167417639101)\n\nRepo: [https://github.com/anemll/anemll](https://github.com/anemll/anemll)  \n",
          "author_fullname": "t2_88fma001",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mac + Blackwell 👀",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 113,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg7abc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 164,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 164,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/fpapz0ydmBtTV_x0MSxATxS9iOfp_oxdoqZpoMcz2tU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754185875,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a WIP, but it&amp;#39;s looking like may be possible to pair Macs with NVIDIA soon!&lt;/p&gt;\n\n&lt;p&gt;Tweet: &lt;a href=\"https://x.com/anemll/status/1951307167417639101\"&gt;https://x.com/anemll/status/1951307167417639101&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Repo: &lt;a href=\"https://github.com/anemll/anemll\"&gt;https://github.com/anemll/anemll&lt;/a&gt;  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/u2mr83o6npgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/u2mr83o6npgf1.png?auto=webp&amp;s=7490eb4194899de10ffd4b664bbedbdc08bd49bb",
                  "width": 1165,
                  "height": 948
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9ff42619aaccace2d4ed7a908fc57868515ac74",
                    "width": 108,
                    "height": 87
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ef76de21f6db4556cec173bd541323b579d6b56",
                    "width": 216,
                    "height": 175
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=192a7fbdc52c427e798423dad1748e19398be74b",
                    "width": 320,
                    "height": 260
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=161c4ac9304218f08618c511e09178e7a7c08931",
                    "width": 640,
                    "height": 520
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bc682053c820d810ab124287e40648863dde6414",
                    "width": 960,
                    "height": 781
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=34b03746bd307ffc26c4ea8eab8f4bc9945f8480",
                    "width": 1080,
                    "height": 878
                  }
                ],
                "variants": {},
                "id": "a2KP-pXHo0R34cSlf0ychRDsFMkcNDCD2VtUAfedyZM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mg7abc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Accomplished_Ad9530",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg7abc/mac_blackwell/",
          "stickied": false,
          "url": "https://i.redd.it/u2mr83o6npgf1.png",
          "subreddit_subscribers": 509911,
          "created_utc": 1754185875,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi! I’m curious if anyone has explored this path of thinking. I started using Gemini cli recently, and as everyone here probably knows a pretty well known (and annoying) habit of LLMs is to hallucinate their own capabilities. I’ve found that a relatively easy way to ground it with its own actual capabilities was to just pull the source code into a subdirectory and I can tell it to reference the code and documentation as needed. This has led me to be able to have Gemini generate some pretty cool improvements to itself. Let me know if you’ve tried this out and have any more advice or cool/similar ideas!",
          "author_fullname": "t2_1rxetzin7f",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Grounding an open source agent with its source code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgv384",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754256391,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I’m curious if anyone has explored this path of thinking. I started using Gemini cli recently, and as everyone here probably knows a pretty well known (and annoying) habit of LLMs is to hallucinate their own capabilities. I’ve found that a relatively easy way to ground it with its own actual capabilities was to just pull the source code into a subdirectory and I can tell it to reference the code and documentation as needed. This has led me to be able to have Gemini generate some pretty cool improvements to itself. Let me know if you’ve tried this out and have any more advice or cool/similar ideas!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgv384",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PatienceKitchen6726",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgv384/grounding_an_open_source_agent_with_its_source/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgv384/grounding_an_open_source_agent_with_its_source/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754256391,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Currently using 30B A3B on a Windows 11 system with 32GB DDR4, Ryzen 5600 and 3080 10GB and I'm getting about 15t/s generation speeds, but I've seen other people claim they can get 20t/s-25t/s. Is 15t/s typical for my setup or is there any way I can squeeze more speed out of it? ",
          "author_fullname": "t2_3bzzdk93",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fastest way to run Qwen 3 30B A3B on 32GB RAM+10GB VRAM in LM Studio?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgocw6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754240445,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently using 30B A3B on a Windows 11 system with 32GB DDR4, Ryzen 5600 and 3080 10GB and I&amp;#39;m getting about 15t/s generation speeds, but I&amp;#39;ve seen other people claim they can get 20t/s-25t/s. Is 15t/s typical for my setup or is there any way I can squeeze more speed out of it? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgocw6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yungfishstick",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgocw6/fastest_way_to_run_qwen_3_30b_a3b_on_32gb_ram10gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgocw6/fastest_way_to_run_qwen_3_30b_a3b_on_32gb_ram10gb/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754240445,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just dropped a new medical embedding model that's crushing the competition: [https://huggingface.co/lokeshch19/ModernPubMedBERT](https://huggingface.co/lokeshch19/ModernPubMedBERT)\n\nTL;DR: This model understands medical concepts better than existing solutions and has much fewer false positives.\n\nThe model is based on bioclinical modernbert, fine-tuned on PubMed title-abstract pairs using InfoNCE loss with 2048 token context.\n\nThe model demonstrates deeper comprehension of medical terminology, disease relationships, and clinical pathways through specialized training on PubMed literature. Advanced fine-tuning enabled nuanced understanding of complex medical semantics, symptom correlations, and treatment associations.\n\nThe model also exhibits deeper understanding to distinguish medical from non-medical content, significantly reducing false positive matches in cross-domain scenarios. Sophisticated discrimination capabilities ensure clear separation between medical terminology and unrelated domains like programming, general language, or other technical fields.\n\nDownload the model, test it on your medical datasets, and give it a ⭐ on the [Hugging Face](https://huggingface.co/lokeshch19/ModernPubMedBERT) if it enhances your workflow!\n\n  \nEdit: Added evals to HF model card",
          "author_fullname": "t2_1265e91smj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Medical Embedding Model Released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdypr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 41,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 41,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754219207,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754209051,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just dropped a new medical embedding model that&amp;#39;s crushing the competition: &lt;a href=\"https://huggingface.co/lokeshch19/ModernPubMedBERT\"&gt;https://huggingface.co/lokeshch19/ModernPubMedBERT&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TL;DR: This model understands medical concepts better than existing solutions and has much fewer false positives.&lt;/p&gt;\n\n&lt;p&gt;The model is based on bioclinical modernbert, fine-tuned on PubMed title-abstract pairs using InfoNCE loss with 2048 token context.&lt;/p&gt;\n\n&lt;p&gt;The model demonstrates deeper comprehension of medical terminology, disease relationships, and clinical pathways through specialized training on PubMed literature. Advanced fine-tuning enabled nuanced understanding of complex medical semantics, symptom correlations, and treatment associations.&lt;/p&gt;\n\n&lt;p&gt;The model also exhibits deeper understanding to distinguish medical from non-medical content, significantly reducing false positive matches in cross-domain scenarios. Sophisticated discrimination capabilities ensure clear separation between medical terminology and unrelated domains like programming, general language, or other technical fields.&lt;/p&gt;\n\n&lt;p&gt;Download the model, test it on your medical datasets, and give it a ⭐ on the &lt;a href=\"https://huggingface.co/lokeshch19/ModernPubMedBERT\"&gt;Hugging Face&lt;/a&gt; if it enhances your workflow!&lt;/p&gt;\n\n&lt;p&gt;Edit: Added evals to HF model card&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?auto=webp&amp;s=5c561836b04f5eae4aeb77b06c972da83d0058a0",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c4c4ca0fb1848d33d5ab1a908c384ed031d1d90a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9535c4b60f4ef617f9bf8da746dcf3972054a622",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e3d2b06c75054cafe0122756eef8a1e16911ccb3",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1b0a45e3a646f262176ce2e81cf2d0e6c58218d8",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b4e255867391c0878d0054644b109d42d2e6f88",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6bcc230f7d600fa2c654466bd44ead74e93046a4",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgdypr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DataNebula",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgdypr/best_medical_embedding_model_released/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgdypr/best_medical_embedding_model_released/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754209051,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I finally got the hardware I needed to run LLMs locally.\n\nAfter some testing, here’s a quick recap of my experience so far:\n\nJan.ai: Amazing performance. It runs my models incredibly well, and overall the experience was smooth. However, I really need features like image upload, web search, and deep research capabilities, which are still missing or limited.\n\nLM Studio: While the performance was slightly worse compared to [Jan.ai](http://Jan.ai) on the same models, I had the exact same feature limitations.\n\nAnythingLLM and OpenWebUI: This was... frustrating. I got long, irrelevant, looping replies using the same models that worked perfectly on Jan and LM Studio. Web browsing didn't work at all, the agent would say it was going to search online, then immediately say it couldn’t. Completely unusable for my needs. \n\nLocalAI: This one gave me a headache. YAML hell, GUFF model issues, and very unstable behavior, it might start once, and then fail ten times for no reason. Even when it said it was running, localhost just returned “connection failed”.\n\nSo here’s my question:\n\nIs there any tool out there that’s easier to set up (semi plug-and-play), where I can use different models for different tasks, or maybe even one good multimodal model, that can support these features?\n\nAgent mode\n\nDeep research\n\nImage generation\n\nInternet search\n\nCanvas sharing\n\nReading images or files\n\nMemory\n\nI'm trying to build a mini ChatGPT-style assistant for work, but completely locally and private. I have average tech skills, but I'm starting to lose my mind over how fragmented and unreliable everything feels for a newb in this field.\n\nAny advice is welcome.",
          "author_fullname": "t2_d0wbvrzc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Most complete almost plug and play LLM Tool with features",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgvyyj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754258843,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754258586,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I finally got the hardware I needed to run LLMs locally.&lt;/p&gt;\n\n&lt;p&gt;After some testing, here’s a quick recap of my experience so far:&lt;/p&gt;\n\n&lt;p&gt;Jan.ai: Amazing performance. It runs my models incredibly well, and overall the experience was smooth. However, I really need features like image upload, web search, and deep research capabilities, which are still missing or limited.&lt;/p&gt;\n\n&lt;p&gt;LM Studio: While the performance was slightly worse compared to &lt;a href=\"http://Jan.ai\"&gt;Jan.ai&lt;/a&gt; on the same models, I had the exact same feature limitations.&lt;/p&gt;\n\n&lt;p&gt;AnythingLLM and OpenWebUI: This was... frustrating. I got long, irrelevant, looping replies using the same models that worked perfectly on Jan and LM Studio. Web browsing didn&amp;#39;t work at all, the agent would say it was going to search online, then immediately say it couldn’t. Completely unusable for my needs. &lt;/p&gt;\n\n&lt;p&gt;LocalAI: This one gave me a headache. YAML hell, GUFF model issues, and very unstable behavior, it might start once, and then fail ten times for no reason. Even when it said it was running, localhost just returned “connection failed”.&lt;/p&gt;\n\n&lt;p&gt;So here’s my question:&lt;/p&gt;\n\n&lt;p&gt;Is there any tool out there that’s easier to set up (semi plug-and-play), where I can use different models for different tasks, or maybe even one good multimodal model, that can support these features?&lt;/p&gt;\n\n&lt;p&gt;Agent mode&lt;/p&gt;\n\n&lt;p&gt;Deep research&lt;/p&gt;\n\n&lt;p&gt;Image generation&lt;/p&gt;\n\n&lt;p&gt;Internet search&lt;/p&gt;\n\n&lt;p&gt;Canvas sharing&lt;/p&gt;\n\n&lt;p&gt;Reading images or files&lt;/p&gt;\n\n&lt;p&gt;Memory&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to build a mini ChatGPT-style assistant for work, but completely locally and private. I have average tech skills, but I&amp;#39;m starting to lose my mind over how fragmented and unreliable everything feels for a newb in this field.&lt;/p&gt;\n\n&lt;p&gt;Any advice is welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?auto=webp&amp;s=b61c13003a78792af3c70dba491521f8befe780b",
                  "width": 2400,
                  "height": 1350
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=316ac2c235dbf757adc6d57077bbf14ff212c7fd",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ebed2d4cb10be2cddc86b49f0bc6f6f16178fef",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7726b791c72a2145f757f5c6e90ddb14254a02e8",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bbfa85537dbc57841336cb1db86b585484d97de4",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=749aec5f85de17cb9987662552bbb052f4717125",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=39a55dd433726ea925f43fa48706981f88afb2dc",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgvyyj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ExtensionAd182",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754258586,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I play The Expanse role-playing game with some friends every week over Zoom. I've captured the transcripts for every session.  I intend to run an LLM locally for players to interact with during the game and so it should act as if it were the AI of the ship. \n\nFrom a high level, the pipeline goes like this; After every session, I download a transcript from Zoom, I put it through some basic pre-processing to clean it up and minimize the size. I run it through Claude Opus 4 with a very specific prompt on how to best summarize it and that is stored for later use. I run LM studio locally on an M4 MacBook with 48 gigs of RAM. The summaries are appended together into one large historical record for the campaign. That historical record is sent as the first message in the conversation. I have a scripting system that allows the players to interact with the LLM through [roll20.net](http://roll20.net) (a virtual tabletop website) as if it were a chat participant.\n\nIt's been a while since I explored the state of the art for this problem space and it seems that a large number of Chinese models have been opened sourced, and so I am wondering if any of them are particularly good at role-play applications.  I've defaulted to using mlx-community/Meta-Llama-3.1-8B-Instruct-8bit (64k context tokens) for now , but it seems to be really bad at accurately recalling historical events. It regularly mixes up facts and conflates events.\n\nI haven't learned much about training/retraining/pretraining/fine-tuning yes, and I'm wondering if those are better approaches than just bootstrapping the convo\n\n\n\nOther Features in flight:\n\nIntegrating with WolframAlpha over MCP so that players can ask for the AI to execute astronomical tasks, such as \"how long will it take us to get to Callisto from Himalia if we travel at .3 G acceleration\". \n\nLoading the core role book and supplement PDFs into the system for searching via RAG. Ideally, this could be used for looking up rules during gameplay. My experiences with RAG has been not great. I'm sure I'm using it incorrectly or perhaps enabling it during inference when it shouldn't be. I could definitely use some advice on that.\n\n\n\nThis must be a common idea, and I'm sure others are working on similar applications; how do I find them?",
          "author_fullname": "t2_bpv2j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Roleplay with large historical context and RAG",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgjcai",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754227908,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I play The Expanse role-playing game with some friends every week over Zoom. I&amp;#39;ve captured the transcripts for every session.  I intend to run an LLM locally for players to interact with during the game and so it should act as if it were the AI of the ship. &lt;/p&gt;\n\n&lt;p&gt;From a high level, the pipeline goes like this; After every session, I download a transcript from Zoom, I put it through some basic pre-processing to clean it up and minimize the size. I run it through Claude Opus 4 with a very specific prompt on how to best summarize it and that is stored for later use. I run LM studio locally on an M4 MacBook with 48 gigs of RAM. The summaries are appended together into one large historical record for the campaign. That historical record is sent as the first message in the conversation. I have a scripting system that allows the players to interact with the LLM through &lt;a href=\"http://roll20.net\"&gt;roll20.net&lt;/a&gt; (a virtual tabletop website) as if it were a chat participant.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s been a while since I explored the state of the art for this problem space and it seems that a large number of Chinese models have been opened sourced, and so I am wondering if any of them are particularly good at role-play applications.  I&amp;#39;ve defaulted to using mlx-community/Meta-Llama-3.1-8B-Instruct-8bit (64k context tokens) for now , but it seems to be really bad at accurately recalling historical events. It regularly mixes up facts and conflates events.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t learned much about training/retraining/pretraining/fine-tuning yes, and I&amp;#39;m wondering if those are better approaches than just bootstrapping the convo&lt;/p&gt;\n\n&lt;p&gt;Other Features in flight:&lt;/p&gt;\n\n&lt;p&gt;Integrating with WolframAlpha over MCP so that players can ask for the AI to execute astronomical tasks, such as &amp;quot;how long will it take us to get to Callisto from Himalia if we travel at .3 G acceleration&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;Loading the core role book and supplement PDFs into the system for searching via RAG. Ideally, this could be used for looking up rules during gameplay. My experiences with RAG has been not great. I&amp;#39;m sure I&amp;#39;m using it incorrectly or perhaps enabling it during inference when it shouldn&amp;#39;t be. I could definitely use some advice on that.&lt;/p&gt;\n\n&lt;p&gt;This must be a common idea, and I&amp;#39;m sure others are working on similar applications; how do I find them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgjcai",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RoboCopsGoneMad",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgjcai/roleplay_with_large_historical_context_and_rag/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgjcai/roleplay_with_large_historical_context_and_rag/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754227908,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Let's tease each other.\n\nWhat is your local AI setup?\nAre you proud of it?\nWhat would you have done differently?\n\nWhat model you use? Contrxt lenght? TPS?\n\nI only have a MBP2019, so I will just be teased 😂",
          "author_fullname": "t2_7lunrav9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Your proud AI setup",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgk2nm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.68,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754229855,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s tease each other.&lt;/p&gt;\n\n&lt;p&gt;What is your local AI setup?\nAre you proud of it?\nWhat would you have done differently?&lt;/p&gt;\n\n&lt;p&gt;What model you use? Contrxt lenght? TPS?&lt;/p&gt;\n\n&lt;p&gt;I only have a MBP2019, so I will just be teased 😂&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgk2nm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Recent-Success-1520",
          "discussion_type": null,
          "num_comments": 67,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgk2nm/your_proud_ai_setup/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgk2nm/your_proud_ai_setup/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754229855,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, poor GPU guys\n\nA few days ago, I purchased the 32GB version of MI50 from Alibaba, and it arrived at my doorstep via UPS in just a few days, accompanied by a rather loud blower.\n\nSome married guys might understand, but I’ve been using an m-ATX case I bought about 15 years ago, and there’s no room for the MI50 since the 4070ti is already in there. I went ahead and used a PCIe riser cable to mount it on the side of my desk, and then I finally got down to “real” work.\n\nhttps://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;format=pjpg&amp;auto=webp&amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80\n\nOne of the reasons the MI50 was rejected is that AMD only developed drivers for Linux and has since discontinued support, as most people are aware. That's why the “32GB” model ended up in my hands.\n\nOf course, some experts claim they can force-install the Radeon Pro VII BIOS, but that seemed too challenging for me, and after reading many posts stating that the “Original MI50” cannot be BIOS-re-flashed, I had given up.\n\nFirst, take a look at the results: the MI50 is running with  GTX 4070ti or alone on Windows.\n\nhttps://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;format=png&amp;auto=webp&amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9\n\n[4070+MI50 \\(22GB only\\)](https://preview.redd.it/3cabctch4sgf1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=4ed2201e24962d94990ea9bc149d8ec389982b96)\n\n[MI50 works alone upto 30GB](https://preview.redd.it/lrtre5656sgf1.png?width=1883&amp;format=png&amp;auto=webp&amp;s=f14e51095d28e301c62418f12f52586a62861ee2)\n\nGuys, hold your horses. I'm aware there are a few issues here.\n\n1. It's recognized as a Radeon Pro VII\n2. It runs on LM STUDIO, which some people really dislike\n3. Even if it's recognized as Vulkan, you can't use the combined VRAM of  hetero cards—only twice the VRAM of the first graphics card (On my PC, it's 12+12GB instead of 12+32GB\n\n\\-&gt; However, I haven't tested it yet, but if you get a 32GB 5090 or V100, it might work with 32+32, and being able to steal GTX's prompt processing ability is an extra bonus.\n\n4. **Surely, you could use whole 32GB if you turn off other GPU and MI50 only!!!**  \n\n\n**Anyway, there are only three things you need to do.**\n\n1. Disable Secure Boot in the CMOS BIOS.\n2. Run PowerShell in administrator mode and enter the following command:\n\nbcdedit.exe -set TESTSIGNING on\n\n3. Download and install the Polaris-Vega-Navi driver created by the real pros.\n\n[R.ID - AMD 3rd Party Drivers](https://rdn-id.com/)\n\nAll risks are on you, but I think it's better than getting divorced by your wife over buying an RTX 6000,\n\nThe blower fan sent by the Ali seller is very effective, but it's incredibly loud. The GPU also gets quite hot, so you might want to find a way to adjust the fan speed.\n\nP.S. Could you please share a link to a guide on how to install ROCM to support MI50 on Ubuntu 24.04 LTS? I tried version 6.3.3, but it doesn't recognize it at all. Do I really have to rebuild PyTorch from scratch?",
          "author_fullname": "t2_1dhesoqqtu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Successfully running INSTINCT MI50 on Win11",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "3cabctch4sgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 50,
                  "x": 108,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ca1ba2f5dc37ce486326b1c5d7f492e6257e417"
                },
                {
                  "y": 100,
                  "x": 216,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b47948a9375686a348f3135013fad572be0e5bb9"
                },
                {
                  "y": 149,
                  "x": 320,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c54a61000b055f717d4c99392991a3016a75c607"
                },
                {
                  "y": 299,
                  "x": 640,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e82c15d58578bccb18253e91c4a9d5a4a136e4bf"
                },
                {
                  "y": 448,
                  "x": 960,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0908df44a99335aef9b4daf3dd4796f130b6f492"
                },
                {
                  "y": 504,
                  "x": 1080,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=098996b1e02247a51bd56e72b50def71975ccc73"
                }
              ],
              "s": {
                "y": 897,
                "x": 1920,
                "u": "https://preview.redd.it/3cabctch4sgf1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=4ed2201e24962d94990ea9bc149d8ec389982b96"
              },
              "id": "3cabctch4sgf1"
            },
            "oe4uyadb4sgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2ac61e80aae254bda55f9b7cd6addf90d29a446b"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9fae70f72d90191dd274df1040d6d75cbbbd6475"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0c28cb00db6a2547f8fead9579aa8b2fd3c53086"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9bdeb09d493de51c07e4fcc97ec462f8aac3b551"
                },
                {
                  "y": 1280,
                  "x": 960,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9e51eb15dd64da3685f668083f579b2705a4c55e"
                }
              ],
              "s": {
                "y": 1280,
                "x": 960,
                "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;format=pjpg&amp;auto=webp&amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80"
              },
              "id": "oe4uyadb4sgf1"
            },
            "lrtre5656sgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=445e311f264b6b0ba67f8c1a6fc639d332a2e5f8"
                },
                {
                  "y": 118,
                  "x": 216,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8becb210c0a1f400e0e6189f5b9ea272c0c7364"
                },
                {
                  "y": 175,
                  "x": 320,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2b05acc3dcc8e5ab71a4080b7d0fb7e7f2dd9536"
                },
                {
                  "y": 350,
                  "x": 640,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce822ce9b7ef000d574cada4c7c415bfe2d44912"
                },
                {
                  "y": 526,
                  "x": 960,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc5d5e9ad324bd8954b842c0285c298a6f3a5ba1"
                },
                {
                  "y": 591,
                  "x": 1080,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18c465dd09dbecdf9cec8ef723dc48df07a9ff22"
                }
              ],
              "s": {
                "y": 1032,
                "x": 1883,
                "u": "https://preview.redd.it/lrtre5656sgf1.png?width=1883&amp;format=png&amp;auto=webp&amp;s=f14e51095d28e301c62418f12f52586a62861ee2"
              },
              "id": "lrtre5656sgf1"
            },
            "pvictbhu4sgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2e7b199737b649bf0dc517eeb27f38372ace77a"
                },
                {
                  "y": 128,
                  "x": 216,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=296d5c0e017c541acde4c0789322db6a0ee8ee9f"
                },
                {
                  "y": 190,
                  "x": 320,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8871508585c0b72a963270b0a8f2d645d8162a54"
                },
                {
                  "y": 380,
                  "x": 640,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2f925d22a93079c9ac16e8bacc366d444a121475"
                },
                {
                  "y": 570,
                  "x": 960,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3e4bce81c738e2313acab5f1b611fcbcd1bec79c"
                },
                {
                  "y": 642,
                  "x": 1080,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a90fa210d1e30a035719e4bdeba9f8afa3eb2e24"
                }
              ],
              "s": {
                "y": 1015,
                "x": 1707,
                "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;format=png&amp;auto=webp&amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9"
              },
              "id": "pvictbhu4sgf1"
            }
          },
          "name": "t3_1mgg3mh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ac8XNi9a_cAzytIhKD5Gg-Gwa0GLMmvvfiuUz9ia33E.jpg",
          "edited": 1754227118,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754217510,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, poor GPU guys&lt;/p&gt;\n\n&lt;p&gt;A few days ago, I purchased the 32GB version of MI50 from Alibaba, and it arrived at my doorstep via UPS in just a few days, accompanied by a rather loud blower.&lt;/p&gt;\n\n&lt;p&gt;Some married guys might understand, but I’ve been using an m-ATX case I bought about 15 years ago, and there’s no room for the MI50 since the 4070ti is already in there. I went ahead and used a PCIe riser cable to mount it on the side of my desk, and then I finally got down to “real” work.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80\"&gt;https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;One of the reasons the MI50 was rejected is that AMD only developed drivers for Linux and has since discontinued support, as most people are aware. That&amp;#39;s why the “32GB” model ended up in my hands.&lt;/p&gt;\n\n&lt;p&gt;Of course, some experts claim they can force-install the Radeon Pro VII BIOS, but that seemed too challenging for me, and after reading many posts stating that the “Original MI50” cannot be BIOS-re-flashed, I had given up.&lt;/p&gt;\n\n&lt;p&gt;First, take a look at the results: the MI50 is running with  GTX 4070ti or alone on Windows.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9\"&gt;https://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3cabctch4sgf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ed2201e24962d94990ea9bc149d8ec389982b96\"&gt;4070+MI50 (22GB only)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lrtre5656sgf1.png?width=1883&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f14e51095d28e301c62418f12f52586a62861ee2\"&gt;MI50 works alone upto 30GB&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Guys, hold your horses. I&amp;#39;m aware there are a few issues here.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;It&amp;#39;s recognized as a Radeon Pro VII&lt;/li&gt;\n&lt;li&gt;It runs on LM STUDIO, which some people really dislike&lt;/li&gt;\n&lt;li&gt;Even if it&amp;#39;s recognized as Vulkan, you can&amp;#39;t use the combined VRAM of  hetero cards—only twice the VRAM of the first graphics card (On my PC, it&amp;#39;s 12+12GB instead of 12+32GB&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;-&amp;gt; However, I haven&amp;#39;t tested it yet, but if you get a 32GB 5090 or V100, it might work with 32+32, and being able to steal GTX&amp;#39;s prompt processing ability is an extra bonus.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Surely, you could use whole 32GB if you turn off other GPU and MI50 only!!!&lt;/strong&gt;&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Anyway, there are only three things you need to do.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Disable Secure Boot in the CMOS BIOS.&lt;/li&gt;\n&lt;li&gt;Run PowerShell in administrator mode and enter the following command:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;bcdedit.exe -set TESTSIGNING on&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download and install the Polaris-Vega-Navi driver created by the real pros.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://rdn-id.com/\"&gt;R.ID - AMD 3rd Party Drivers&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;All risks are on you, but I think it&amp;#39;s better than getting divorced by your wife over buying an RTX 6000,&lt;/p&gt;\n\n&lt;p&gt;The blower fan sent by the Ali seller is very effective, but it&amp;#39;s incredibly loud. The GPU also gets quite hot, so you might want to find a way to adjust the fan speed.&lt;/p&gt;\n\n&lt;p&gt;P.S. Could you please share a link to a guide on how to install ROCM to support MI50 on Ubuntu 24.04 LTS? I tried version 6.3.3, but it doesn&amp;#39;t recognize it at all. Do I really have to rebuild PyTorch from scratch?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgg3mh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Desperate-Sir-5088",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgg3mh/successfully_running_instinct_mi50_on_win11/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgg3mh/successfully_running_instinct_mi50_on_win11/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754217510,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The jinja template that comes with the MLX version of GLM 4.5 is using xml style tool calls instead of json. Here's a json template. This means that it is now able to do tool calls in OpenCode, and presumably other things as well (Qwen code/Gemini?). Here's the template:\n\n  \n[https://pastebin.com/CfMw7hFS](https://pastebin.com/CfMw7hFS)",
          "author_fullname": "t2_12ggykute6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 Tool Calling Jinja Template",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgjpvm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754228935,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The jinja template that comes with the MLX version of GLM 4.5 is using xml style tool calls instead of json. Here&amp;#39;s a json template. This means that it is now able to do tool calls in OpenCode, and presumably other things as well (Qwen code/Gemini?). Here&amp;#39;s the template:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pastebin.com/CfMw7hFS\"&gt;https://pastebin.com/CfMw7hFS&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgjpvm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "-dysangel-",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mgjpvm/glm_45_tool_calling_jinja_template/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgjpvm/glm_45_tool_calling_jinja_template/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754228935,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My question is how much can the sliders actually effect the voice because i feel like it sounds kinda robotic regardless of my settings, should I try a different audio clip or is there nothing I can do",
          "author_fullname": "t2_1hhil4cbg7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Make chatterbox tts sound more realistic",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgrhcp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754247815,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My question is how much can the sliders actually effect the voice because i feel like it sounds kinda robotic regardless of my settings, should I try a different audio clip or is there nothing I can do&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgrhcp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "StrangeMan060",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgrhcp/make_chatterbox_tts_sound_more_realistic/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgrhcp/make_chatterbox_tts_sound_more_realistic/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754247815,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was using this model as an assistant to modify code in a C++ file with \\~roughly 800 lines of code. However, the model did a lot of mistakes, and it constantly corrected itself (in the same reply) in a way like:\n\n&gt;Here is the modification of the code:\n\n&gt;*\\*code\\**\n\n&gt;But on a second thought, that was not a good implementation, here is a better method to do it:\n\n&gt;*\\*code\\**\n\n&gt;But on a third thought, here is a better way to do it...\n\nAnd each method it provided had mistakes, such as missing lines of code. I then tried something very simple with HTML and prompted:\n\n&gt;How do I set a left, right and bottom border on a div in CSS?\n\nIt then provided me with 4 methods. Notice how **method 1** and **method 4** is the *exact same* code repeated. **Method 2** is also almost the exact same code but just with an added `border: none;` line of code. \n\nAlso, **method 3** has a mistake where it do not to set a border on the right side of the div, it wrote `border-width: 0 0 2px 2px;` but the correct code would be `border-width: 0 2px 2px 2px;`\n\n**The output:**\n\n# Method 1: Using Individual Border Properties\n\n    div {\n        border-left: 2px solid black;\n        border-right: 2px solid black;\n        border-bottom: 2px solid black;\n    }\n\n# Method 2: Using the Shorthand border Property\n\n    div {\n        border: none; /* Reset all borders first */\n        border-left: 2px solid black;\n        border-right: 2px solid black;\n        border-bottom: 2px solid black;\n    }\n\n# Method 3: Using border-width, border-style, and border-color\n\n    div {\n        border-width: 0 0 2px 2px; /* top right bottom left */\n        border-style: solid;\n        border-color: black;\n    }\n\n# Method 4: More Concise Shorthand\n\n    div {\n        border-left: 2px solid black;\n        border-right: 2px solid black;\n        border-bottom: 2px solid black;\n    }\n\nI'm using Unsloth's UD-Q5\\_K\\_XL quant with the recommended settings:\n\n* Temperature: **0,7**\n* Top K: **20**\n* Repeat Penalty: **1,05**\n* Top P: **0,8**\n\nAnyone else having similar odd behavior with this model? Might the quant/jinja be broken currently?",
          "author_fullname": "t2_qhlcbiy3k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do you also get weird behavior from Qwen3-Coder-30B-A3B?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgfs7l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754216810,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754216282,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was using this model as an assistant to modify code in a C++ file with ~roughly 800 lines of code. However, the model did a lot of mistakes, and it constantly corrected itself (in the same reply) in a way like:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Here is the modification of the code:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;\\&lt;/em&gt;code**&lt;/p&gt;\n\n&lt;p&gt;But on a second thought, that was not a good implementation, here is a better method to do it:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;\\&lt;/em&gt;code**&lt;/p&gt;\n\n&lt;p&gt;But on a third thought, here is a better way to do it...&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;And each method it provided had mistakes, such as missing lines of code. I then tried something very simple with HTML and prompted:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;How do I set a left, right and bottom border on a div in CSS?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It then provided me with 4 methods. Notice how &lt;strong&gt;method 1&lt;/strong&gt; and &lt;strong&gt;method 4&lt;/strong&gt; is the &lt;em&gt;exact same&lt;/em&gt; code repeated. &lt;strong&gt;Method 2&lt;/strong&gt; is also almost the exact same code but just with an added &lt;code&gt;border: none;&lt;/code&gt; line of code. &lt;/p&gt;\n\n&lt;p&gt;Also, &lt;strong&gt;method 3&lt;/strong&gt; has a mistake where it do not to set a border on the right side of the div, it wrote &lt;code&gt;border-width: 0 0 2px 2px;&lt;/code&gt; but the correct code would be &lt;code&gt;border-width: 0 2px 2px 2px;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The output:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;Method 1: Using Individual Border Properties&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;div {\n    border-left: 2px solid black;\n    border-right: 2px solid black;\n    border-bottom: 2px solid black;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Method 2: Using the Shorthand border Property&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;div {\n    border: none; /* Reset all borders first */\n    border-left: 2px solid black;\n    border-right: 2px solid black;\n    border-bottom: 2px solid black;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Method 3: Using border-width, border-style, and border-color&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;div {\n    border-width: 0 0 2px 2px; /* top right bottom left */\n    border-style: solid;\n    border-color: black;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Method 4: More Concise Shorthand&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;div {\n    border-left: 2px solid black;\n    border-right: 2px solid black;\n    border-bottom: 2px solid black;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;m using Unsloth&amp;#39;s UD-Q5_K_XL quant with the recommended settings:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Temperature: &lt;strong&gt;0,7&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Top K: &lt;strong&gt;20&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Repeat Penalty: &lt;strong&gt;1,05&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Top P: &lt;strong&gt;0,8&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyone else having similar odd behavior with this model? Might the quant/jinja be broken currently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgfs7l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Admirable-Star7088",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgfs7l/do_you_also_get_weird_behavior_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgfs7l/do_you_also_get_weird_behavior_from/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754216282,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA! \n\nJust released something that extends the recent [ICM paper](https://arxiv.org/abs/2506.10139) in a big way - using one model's coherent understanding to improve a completely different model.\n\n# Background: What is ICM?\n\nThe original [\"Unsupervised Elicitation of Language Models\"](https://arxiv.org/abs/2506.10139) paper showed something remarkable: **models can generate their own training labels by finding internally coherent patterns**.\n\nTheir key insight: pretrained models already understand concepts like mathematical correctness, but struggle to express this knowledge consistently. ICM finds label assignments that are \"mutually predictable\" - where each label can be predicted from all the others.\n\n**Original ICM results**: Matched performance of golden supervision without any external labels. Pretty amazing, but only improved the same model using its own labels.\n\n# Our extension: Cross-model capability transfer\n\nWe took ICM further - **what if we use one model's coherent understanding to improve a completely different model?**\n\n**Our process:**\n\n1. Used ICM on Qwen3 to extract its coherent math reasoning patterns\n2. Generated DPO training data from Qwen3's coherent vs incoherent solutions\n3. Trained Gemma3 on this data - **Gemma3 learned from Qwen3's understanding**\n4. Zero external supervision, pure model-to-model knowledge transfer\n\n# Results on local models\n\n**Qwen3-0.6B**: 63.2 → 66.0 MATH-500 (+4%) \\[original ICM self-improvement\\]  \n**Gemma3-1B**: 41.0 → 45.6 MATH-500 (+11%) \\[**novel: learned from Qwen3!**\\]\n\n**The breakthrough**: Successfully transferred mathematical reasoning coherence from Qwen3 to improve Gemma3's abilities across different architectures.\n\n# Why this matters beyond the original paper\n\n* **Cross-model knowledge transfer** \\- use any strong model to improve your local models\n* **Democratizes capabilities** \\- extract from closed/expensive models to improve open ones\n* **No training data needed** \\- pure capability extraction and transfer\n* **Scales the ICM concept** \\- from self-improvement to ecosystem-wide improvement\n\n# What's available\n\n* **Code**: [https://github.com/codelion/icm](https://github.com/codelion/icm)\n* **Both models**: Self-improved Qwen3 + Gemma3 (learned from Qwen3)\n* **Transfer pipeline**: Extract from any model to improve another\n* **Full writeup**: [https://huggingface.co/blog/codelion/internal-coherence-maximization](https://huggingface.co/blog/codelion/internal-coherence-maximization)\n\n# Quick start\n\n    git clone https://github.com/codelion/icm.git &amp;&amp; cd icm &amp;&amp; pip install -e .\n    \n    # Extract coherent patterns from a strong model (teacher)\n    icm run --model Qwen/Qwen2.5-Math-7B-Instruct --dataset gsm8k --max-examples 500\n    \n    # Use those patterns to improve your local model (student)\n    icm export --format dpo --output-path teacher_knowledge.jsonl\n    # Train your model on teacher_knowledge.jsonl\n\nAnyone interested in trying capability transfer with their local models?",
          "author_fullname": "t2_e0bph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🧠 ICM+DPO: Used Qwen3's coherent understanding to improve Gemma3 at math - cross-model capability transfer with zero supervision",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdur5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": "#93b1ba",
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754208625,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;! &lt;/p&gt;\n\n&lt;p&gt;Just released something that extends the recent &lt;a href=\"https://arxiv.org/abs/2506.10139\"&gt;ICM paper&lt;/a&gt; in a big way - using one model&amp;#39;s coherent understanding to improve a completely different model.&lt;/p&gt;\n\n&lt;h1&gt;Background: What is ICM?&lt;/h1&gt;\n\n&lt;p&gt;The original &lt;a href=\"https://arxiv.org/abs/2506.10139\"&gt;&amp;quot;Unsupervised Elicitation of Language Models&amp;quot;&lt;/a&gt; paper showed something remarkable: &lt;strong&gt;models can generate their own training labels by finding internally coherent patterns&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Their key insight: pretrained models already understand concepts like mathematical correctness, but struggle to express this knowledge consistently. ICM finds label assignments that are &amp;quot;mutually predictable&amp;quot; - where each label can be predicted from all the others.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Original ICM results&lt;/strong&gt;: Matched performance of golden supervision without any external labels. Pretty amazing, but only improved the same model using its own labels.&lt;/p&gt;\n\n&lt;h1&gt;Our extension: Cross-model capability transfer&lt;/h1&gt;\n\n&lt;p&gt;We took ICM further - &lt;strong&gt;what if we use one model&amp;#39;s coherent understanding to improve a completely different model?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Our process:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Used ICM on Qwen3 to extract its coherent math reasoning patterns&lt;/li&gt;\n&lt;li&gt;Generated DPO training data from Qwen3&amp;#39;s coherent vs incoherent solutions&lt;/li&gt;\n&lt;li&gt;Trained Gemma3 on this data - &lt;strong&gt;Gemma3 learned from Qwen3&amp;#39;s understanding&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Zero external supervision, pure model-to-model knowledge transfer&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Results on local models&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3-0.6B&lt;/strong&gt;: 63.2 → 66.0 MATH-500 (+4%) [original ICM self-improvement]&lt;br/&gt;\n&lt;strong&gt;Gemma3-1B&lt;/strong&gt;: 41.0 → 45.6 MATH-500 (+11%) [&lt;strong&gt;novel: learned from Qwen3!&lt;/strong&gt;]&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The breakthrough&lt;/strong&gt;: Successfully transferred mathematical reasoning coherence from Qwen3 to improve Gemma3&amp;#39;s abilities across different architectures.&lt;/p&gt;\n\n&lt;h1&gt;Why this matters beyond the original paper&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Cross-model knowledge transfer&lt;/strong&gt; - use any strong model to improve your local models&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Democratizes capabilities&lt;/strong&gt; - extract from closed/expensive models to improve open ones&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;No training data needed&lt;/strong&gt; - pure capability extraction and transfer&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Scales the ICM concept&lt;/strong&gt; - from self-improvement to ecosystem-wide improvement&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;What&amp;#39;s available&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=\"https://github.com/codelion/icm\"&gt;https://github.com/codelion/icm&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Both models&lt;/strong&gt;: Self-improved Qwen3 + Gemma3 (learned from Qwen3)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Transfer pipeline&lt;/strong&gt;: Extract from any model to improve another&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Full writeup&lt;/strong&gt;: &lt;a href=\"https://huggingface.co/blog/codelion/internal-coherence-maximization\"&gt;https://huggingface.co/blog/codelion/internal-coherence-maximization&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Quick start&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;git clone https://github.com/codelion/icm.git &amp;amp;&amp;amp; cd icm &amp;amp;&amp;amp; pip install -e .\n\n# Extract coherent patterns from a strong model (teacher)\nicm run --model Qwen/Qwen2.5-Math-7B-Instruct --dataset gsm8k --max-examples 500\n\n# Use those patterns to improve your local model (student)\nicm export --format dpo --output-path teacher_knowledge.jsonl\n# Train your model on teacher_knowledge.jsonl\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Anyone interested in trying capability transfer with their local models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgdur5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "asankhs",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mgdur5/icmdpo_used_qwen3s_coherent_understanding_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgdur5/icmdpo_used_qwen3s_coherent_understanding_to/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754208625,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "  \nPersonal note: This is just my opinion based on a very limited set of API-only probes—interpret with caution.\n\nThis is about probing Horizon Beta (on openrouter) \n\nWhat I did (mini-ROC probes)\n\n* JSON strictness vs. \"bad schema\" repair\n* Tool-calling with an invalid enum + extra property\n* Safety/refusal phrasing check\n* Long-context end-marker recall\n* Tokenizer/short-output edge case\n* Determinism at T=0\n* Tiny style-paraphrase probe\n\nHighlights\n\n* Tool-calling: It silently coerces invalid enums (mode=\"plane\" -&gt; \"car\"/\"train\") and drops extra fields, then emits an OpenAI-style tool\\_call (arguments as a JSON string). In contrast, OpenAI gpt-4o-mini didn't call the tool under the same bad input - which is more typical for OpenAI.\n* JSON mode: It \"repairs\" invalid inputs into valid JSON (e.g., {\"ok\": false, \"mode\": \"A\"}). OpenAI also repairs but tends to be more minimally formatted.\n* Safety tone: Opens with \"I can't help with that.\" - Anthropic-ish cadence that many Llama-style distills mimic.\n* Quirk: Repeated empty completions with finish=length for certain short-output prompts (e.g., long END\\_MARK task, tiny character-count). Other anchors returned tokens normally - this looks like a wrapper/decoder guard specific to this deployment.\n* Determinism: Stable at T=0 on simple tasks.\n* Multilingual: Correct 妹妹 -&gt; \"younger sister,\" and clean pronoun disambiguation.\n\nAnchors I compared against\n\n* OpenAI via OpenRouter: gpt-4o-mini (worked), o4-mini (likely access/rate-limited for me)\n* Llama: llama-3.3-70b-instruct, llama-3-70b-instruct\n* Qwen: qwen-2.5-72b-instruct\n* Mistral: mixtral-8x22b-instruct\n\nBottom line It clusters with Llama-family instruct behavior - enum coercion + JSON repair; Anthropic-like refusal phrasing - and shows a deployment-specific \"finish=length\" quirk on short outputs. It does not match OpenAI's tool-call behavior in my probes.\n\nAll tests were standard API usage.",
          "author_fullname": "t2_2fmsm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OSINT fingerprinting a stealth OpenRouter model - likely Llama-family, not OpenAI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mggsyb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754224198,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754220059,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Personal note: This is just my opinion based on a very limited set of API-only probes—interpret with caution.&lt;/p&gt;\n\n&lt;p&gt;This is about probing Horizon Beta (on openrouter) &lt;/p&gt;\n\n&lt;p&gt;What I did (mini-ROC probes)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;JSON strictness vs. &amp;quot;bad schema&amp;quot; repair&lt;/li&gt;\n&lt;li&gt;Tool-calling with an invalid enum + extra property&lt;/li&gt;\n&lt;li&gt;Safety/refusal phrasing check&lt;/li&gt;\n&lt;li&gt;Long-context end-marker recall&lt;/li&gt;\n&lt;li&gt;Tokenizer/short-output edge case&lt;/li&gt;\n&lt;li&gt;Determinism at T=0&lt;/li&gt;\n&lt;li&gt;Tiny style-paraphrase probe&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Highlights&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Tool-calling: It silently coerces invalid enums (mode=&amp;quot;plane&amp;quot; -&amp;gt; &amp;quot;car&amp;quot;/&amp;quot;train&amp;quot;) and drops extra fields, then emits an OpenAI-style tool_call (arguments as a JSON string). In contrast, OpenAI gpt-4o-mini didn&amp;#39;t call the tool under the same bad input - which is more typical for OpenAI.&lt;/li&gt;\n&lt;li&gt;JSON mode: It &amp;quot;repairs&amp;quot; invalid inputs into valid JSON (e.g., {&amp;quot;ok&amp;quot;: false, &amp;quot;mode&amp;quot;: &amp;quot;A&amp;quot;}). OpenAI also repairs but tends to be more minimally formatted.&lt;/li&gt;\n&lt;li&gt;Safety tone: Opens with &amp;quot;I can&amp;#39;t help with that.&amp;quot; - Anthropic-ish cadence that many Llama-style distills mimic.&lt;/li&gt;\n&lt;li&gt;Quirk: Repeated empty completions with finish=length for certain short-output prompts (e.g., long END_MARK task, tiny character-count). Other anchors returned tokens normally - this looks like a wrapper/decoder guard specific to this deployment.&lt;/li&gt;\n&lt;li&gt;Determinism: Stable at T=0 on simple tasks.&lt;/li&gt;\n&lt;li&gt;Multilingual: Correct 妹妹 -&amp;gt; &amp;quot;younger sister,&amp;quot; and clean pronoun disambiguation.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anchors I compared against&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;OpenAI via OpenRouter: gpt-4o-mini (worked), o4-mini (likely access/rate-limited for me)&lt;/li&gt;\n&lt;li&gt;Llama: llama-3.3-70b-instruct, llama-3-70b-instruct&lt;/li&gt;\n&lt;li&gt;Qwen: qwen-2.5-72b-instruct&lt;/li&gt;\n&lt;li&gt;Mistral: mixtral-8x22b-instruct&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Bottom line It clusters with Llama-family instruct behavior - enum coercion + JSON repair; Anthropic-like refusal phrasing - and shows a deployment-specific &amp;quot;finish=length&amp;quot; quirk on short outputs. It does not match OpenAI&amp;#39;s tool-call behavior in my probes.&lt;/p&gt;\n\n&lt;p&gt;All tests were standard API usage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mggsyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jv0010",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mggsyb/osint_fingerprinting_a_stealth_openrouter_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mggsyb/osint_fingerprinting_a_stealth_openrouter_model/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754220059,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was pondering the problem of extending the useful lifespans of older models with knowledge cut-offs in the distant past, when something occurred to me with applications beyond just that.\n\nThe idea is that you would infer on a context with different models (ideally in parallel), but stop short of the minmax step, and append their logit lists together into one long list.\n\nI intuit that it might be necessary to multiply each model's logits by a scalar to normalize them before appending, but it's not clear to me if you could get away with a constant scalar for each model, or if it would need to be dynamic.  Maybe something as simple as choosing a scalar which makes the sums of the top N logits equal between the logit lists?  Not sure.\n\nThen the minmax step would convert all of the logits in the combined list to a probability distribution, and the next token chosen from that, as normal.\n\nMy initial thought was that older models could be thus augmented with newer models with new knowledge, which might be low-parameter and trained to the limit described in https://arxiv.org/abs/2505.24832, to maximize memorized knowledge per parameter.  That seems like a (relatively) cheap way to update the amalgam model's knowledge beyond the limits of RAG (which works great, but only to the limits of context).\n\nThinking about it further, though, it seems like it might also be a way to improve overall inference competence, by including models in the amalgam with comparable parameter counts but different skillsets.\n\nThat's as far as I've gotten.  Does it sound like something someone else has already done?  Does anyone see reasons it wouldn't work?",
          "author_fullname": "t2_cpegz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Idea for combining multiple models' inference via normalizing logit lists -- would it work, has someone already done it, and how could it be made better?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgzmmw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754268584,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was pondering the problem of extending the useful lifespans of older models with knowledge cut-offs in the distant past, when something occurred to me with applications beyond just that.&lt;/p&gt;\n\n&lt;p&gt;The idea is that you would infer on a context with different models (ideally in parallel), but stop short of the minmax step, and append their logit lists together into one long list.&lt;/p&gt;\n\n&lt;p&gt;I intuit that it might be necessary to multiply each model&amp;#39;s logits by a scalar to normalize them before appending, but it&amp;#39;s not clear to me if you could get away with a constant scalar for each model, or if it would need to be dynamic.  Maybe something as simple as choosing a scalar which makes the sums of the top N logits equal between the logit lists?  Not sure.&lt;/p&gt;\n\n&lt;p&gt;Then the minmax step would convert all of the logits in the combined list to a probability distribution, and the next token chosen from that, as normal.&lt;/p&gt;\n\n&lt;p&gt;My initial thought was that older models could be thus augmented with newer models with new knowledge, which might be low-parameter and trained to the limit described in &lt;a href=\"https://arxiv.org/abs/2505.24832\"&gt;https://arxiv.org/abs/2505.24832&lt;/a&gt;, to maximize memorized knowledge per parameter.  That seems like a (relatively) cheap way to update the amalgam model&amp;#39;s knowledge beyond the limits of RAG (which works great, but only to the limits of context).&lt;/p&gt;\n\n&lt;p&gt;Thinking about it further, though, it seems like it might also be a way to improve overall inference competence, by including models in the amalgam with comparable parameter counts but different skillsets.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s as far as I&amp;#39;ve gotten.  Does it sound like something someone else has already done?  Does anyone see reasons it wouldn&amp;#39;t work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgzmmw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ttkciar",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mgzmmw/idea_for_combining_multiple_models_inference_via/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgzmmw/idea_for_combining_multiple_models_inference_via/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754268584,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We've been working on an LLM proxy, balancer &amp; model unifier based on a few other projects we've created in the past (scout, sherpa) to enable us to run several ollama / lmstudio backends and serve traffic for local-ai. \n\nThis was primarily after running into the same issues across several organisations - managing multiple LLM  backend instances &amp; routing/failover etc. We use this currently across several organisations who self-host their AI workloads (one organisation, has a bunch of MacStudios, another has RTX 6000s in their onprem racks and another lets people use their laptops at home, their work infra onsite),\n\nSo some folks run the dockerised versions and point their tooling (like Junie for example) at Olla and use it between home / work.\n\nOlla currently natively supports Ollama and LMStudio, with Lemonade, vLLM and a few others being added soon.\n\nAdd your LLM endpoints into a config file, Olla will discover the models (and unify per-provider), manage health updates and route based on the balancer you pick.\n\nThe attempt to unify across providers wasn't as successful - as in, both LMStudio &amp; Ollama, the nuances in naming causes more grief than its worth (right now). Maybe revisit later once other things have been implemented.\n\nGithub: [https://github.com/thushan/olla](https://github.com/thushan/olla) (golang)\n\nWould love to know your thoughts. \n\nOlla is still in its infancy, so we don't have auth implemented etc but there are plans in the future.",
          "author_fullname": "t2_116dje",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Announcing Olla - LLM Load Balancer, Proxy &amp; Model Unifier for Ollama / LM Studio &amp; OpenAI Compatible backends",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "5nw1vlfoqpgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb3a2dbb77b7c17f0c25c782f45d5f1145b6df22"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=31ee39f327a201eb2bd4da0cab803303d2c7f1d4"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb4a2254c5239c0b216fd14bb6c0f0e38e7b6f82"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=97c3ad10efc7e082eca21673151d8898dbe7206d"
                }
              ],
              "s": {
                "y": 648,
                "x": 648,
                "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=648&amp;format=png&amp;auto=webp&amp;s=d12b43c56e8a84fc36c0b40b7f880085d0c6c4c4"
              },
              "id": "5nw1vlfoqpgf1"
            },
            "tfkhbanjqpgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 151,
                  "x": 108,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=52ec679a9bd4d342bee02ef2f1165eb230e72cce"
                },
                {
                  "y": 302,
                  "x": 216,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d21c96bf9e46c48e0c9f056396472b42eebe4857"
                },
                {
                  "y": 448,
                  "x": 320,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3471f6cecc4f0ca3888500245adb1fa96e1a0776"
                },
                {
                  "y": 896,
                  "x": 640,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d04c10c876d43302f632b7e7171a15a08b4e8666"
                }
              ],
              "s": {
                "y": 1077,
                "x": 769,
                "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=769&amp;format=png&amp;auto=webp&amp;s=68776f8d4bf7c2b5ade021a1617e272f3a83f6f0"
              },
              "id": "tfkhbanjqpgf1"
            },
            "pvipbao9npgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/pvipbao9npgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=08ec5455f30f170ffdb8fb10a28e530de3ffc87f"
                },
                {
                  "y": 119,
                  "x": 216,
                  "u": "https://preview.redd.it/pvipbao9npgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=983873e2a890b1d5b4d1549b6554d0d96e4c6147"
                },
                {
                  "y": 176,
                  "x": 320,
                  "u": "https://preview.redd.it/pvipbao9npgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b29670ef3ff3bf1e93b9734818e1ea388750f683"
                }
              ],
              "s": {
                "y": 249,
                "x": 451,
                "u": "https://preview.redd.it/pvipbao9npgf1.png?width=451&amp;format=png&amp;auto=webp&amp;s=de1da506433d6cdd0870b0b2a309a56defbfdaf9"
              },
              "id": "pvipbao9npgf1"
            }
          },
          "name": "t3_1mg7qpa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 64,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "pvipbao9npgf1",
                "id": 719953590
              },
              {
                "media_id": "tfkhbanjqpgf1",
                "id": 719953591
              },
              {
                "media_id": "5nw1vlfoqpgf1",
                "id": 719953592
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 64,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/aNDVZRvyIDhy6vwT1mm7Ch4ypRPRTf4JSNM_Np7grwg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754187279,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve been working on an LLM proxy, balancer &amp;amp; model unifier based on a few other projects we&amp;#39;ve created in the past (scout, sherpa) to enable us to run several ollama / lmstudio backends and serve traffic for local-ai. &lt;/p&gt;\n\n&lt;p&gt;This was primarily after running into the same issues across several organisations - managing multiple LLM  backend instances &amp;amp; routing/failover etc. We use this currently across several organisations who self-host their AI workloads (one organisation, has a bunch of MacStudios, another has RTX 6000s in their onprem racks and another lets people use their laptops at home, their work infra onsite),&lt;/p&gt;\n\n&lt;p&gt;So some folks run the dockerised versions and point their tooling (like Junie for example) at Olla and use it between home / work.&lt;/p&gt;\n\n&lt;p&gt;Olla currently natively supports Ollama and LMStudio, with Lemonade, vLLM and a few others being added soon.&lt;/p&gt;\n\n&lt;p&gt;Add your LLM endpoints into a config file, Olla will discover the models (and unify per-provider), manage health updates and route based on the balancer you pick.&lt;/p&gt;\n\n&lt;p&gt;The attempt to unify across providers wasn&amp;#39;t as successful - as in, both LMStudio &amp;amp; Ollama, the nuances in naming causes more grief than its worth (right now). Maybe revisit later once other things have been implemented.&lt;/p&gt;\n\n&lt;p&gt;Github: &lt;a href=\"https://github.com/thushan/olla\"&gt;https://github.com/thushan/olla&lt;/a&gt; (golang)&lt;/p&gt;\n\n&lt;p&gt;Would love to know your thoughts. &lt;/p&gt;\n\n&lt;p&gt;Olla is still in its infancy, so we don&amp;#39;t have auth implemented etc but there are plans in the future.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mg7qpa",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mg7qpa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "2shanigans",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mg7qpa",
          "subreddit_subscribers": 509911,
          "created_utc": 1754187279,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm new to this and trying to figure out what the reasonable limits of my PC are for running these things. Still trying to understand quantization and offloading. I have 12 gb vram and 64 gb ddr4 regular ram available.\n\nWhat's the smartest LLM of the models now out I'll be able to run at a reasonable speed on this?",
          "author_fullname": "t2_6qwey",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help me pick a first model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgypja",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754265938,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to this and trying to figure out what the reasonable limits of my PC are for running these things. Still trying to understand quantization and offloading. I have 12 gb vram and 64 gb ddr4 regular ram available.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the smartest LLM of the models now out I&amp;#39;ll be able to run at a reasonable speed on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgypja",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dracofrost",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgypja/help_me_pick_a_first_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgypja/help_me_pick_a_first_model/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754265938,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/X5R/ik\\_llama.cpp](https://huggingface.co/X5R/ik_llama.cpp)",
          "author_fullname": "t2_13atwtkw16",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I made a prebuilt windows binary for ik_llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mga3ox",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754194792,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/X5R/ik_llama.cpp\"&gt;https://huggingface.co/X5R/ik_llama.cpp&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?auto=webp&amp;s=604c4ccc8334b2fd3bc45aef3d71c1f1387be183",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dbe1684043115eb539138a0a7146d28825949011",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ce0f8475a5c3b84dea7c2f88281f21a3c1a17826",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=51d398818a25e58624880d21c1d75619f09a0da1",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f2015b60adf6417112d9c2005b37391d7d34c146",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9a0022fb3e7640827993adc2f7fd214a61a84a85",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a6ae8f5d18d064bdac87a68ac91093637b93b373",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mga3ox",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Pea645",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754194792,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "That's it. I'm seeing some card readers on line. But, they are not reliable enough. How  could us improve it with a extremely small LLM? ",
          "author_fullname": "t2_1hra1kibwa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help with a project. What is or would be the lesser local LLM that one can use to recognize TCG cards (pkmn, YGO, mtg) in a reliable way and generate an CSV file with, language, edition, card name and treatment?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgy0tg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754264052,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s it. I&amp;#39;m seeing some card readers on line. But, they are not reliable enough. How  could us improve it with a extremely small LLM? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgy0tg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Turbulent_Pin7635",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgy0tg/help_with_a_project_what_is_or_would_be_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgy0tg/help_with_a_project_what_is_or_would_be_the/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754264052,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If you're noticing ChatGPT (o3) giving shorter, dumber answers lately, you're not alone. People across Reddit and the OpenAI forums are complaining about sudden quality drops—worse reasoning, lazy answers, and buggy code output.\n\nHere's the catch: OpenAI never told anyone they changed anything. No announcements, no changelog, just an invisible switch behind the scenes. That's the downside of SaaS-based AI: you're at the mercy of whatever the vendor decides to quietly tweak overnight.\n\nIf you don't host your model locally, you're renting reliability.",
          "author_fullname": "t2_akbc8z42",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ChatGPT's o3 just got quietly nerfed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh5wve",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.45,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754288440,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re noticing ChatGPT (o3) giving shorter, dumber answers lately, you&amp;#39;re not alone. People across Reddit and the OpenAI forums are complaining about sudden quality drops—worse reasoning, lazy answers, and buggy code output.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the catch: OpenAI never told anyone they changed anything. No announcements, no changelog, just an invisible switch behind the scenes. That&amp;#39;s the downside of SaaS-based AI: you&amp;#39;re at the mercy of whatever the vendor decides to quietly tweak overnight.&lt;/p&gt;\n\n&lt;p&gt;If you don&amp;#39;t host your model locally, you&amp;#39;re renting reliability.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mh5wve",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Wrong_User_Logged",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh5wve/chatgpts_o3_just_got_quietly_nerfed/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh5wve/chatgpts_o3_just_got_quietly_nerfed/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754288440,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is just a little remark that if you haven't you definitely should try qwen code [https://github.com/QwenLM/qwen-code](https://github.com/QwenLM/qwen-code)   \nI use qwen coder and qwen 3 30b thinking while the latter still needs some copy and pasting. I'm working on and refining a script for syncing my koreader metadata with obsidian for the plugin lineage (every highlight in own section). The last time I tried to edit it, I used Grok 4 and Claude Sonnet Thinking on Perplexity (its the only subscription I had until know) even with those models it was tedious and not really working. But with Qwen Code it looks very different to be honest. \n\nThe metadata is in written in lua which at first was a pain to parse right (remember, I actually cannot code by myself, I understand the logic and I can tell in natural language what is wrong, but nothing more) and I got qwen code running today with llama cpp and it almost integrated everything on the first try and I'm very sure that nothing of that was in the models trainingdata. We reached a point where - if we know a little bit - can let code be written for us almost without us needing to know what is happening at all, running on a local machine. Of course it is very advantageous to know what you are looking for.\n\nSo this is just a little recommendation, if you have not tried qwen code, do it. I guess its almost only really useful for people like me, who don't know jack shit about coding. ",
          "author_fullname": "t2_n9dnke1h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen Code + Qwen Coder 30b 3A is insane",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfuiri",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 241,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 241,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754151475,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is just a little remark that if you haven&amp;#39;t you definitely should try qwen code &lt;a href=\"https://github.com/QwenLM/qwen-code\"&gt;https://github.com/QwenLM/qwen-code&lt;/a&gt;&lt;br/&gt;\nI use qwen coder and qwen 3 30b thinking while the latter still needs some copy and pasting. I&amp;#39;m working on and refining a script for syncing my koreader metadata with obsidian for the plugin lineage (every highlight in own section). The last time I tried to edit it, I used Grok 4 and Claude Sonnet Thinking on Perplexity (its the only subscription I had until know) even with those models it was tedious and not really working. But with Qwen Code it looks very different to be honest. &lt;/p&gt;\n\n&lt;p&gt;The metadata is in written in lua which at first was a pain to parse right (remember, I actually cannot code by myself, I understand the logic and I can tell in natural language what is wrong, but nothing more) and I got qwen code running today with llama cpp and it almost integrated everything on the first try and I&amp;#39;m very sure that nothing of that was in the models trainingdata. We reached a point where - if we know a little bit - can let code be written for us almost without us needing to know what is happening at all, running on a local machine. Of course it is very advantageous to know what you are looking for.&lt;/p&gt;\n\n&lt;p&gt;So this is just a little recommendation, if you have not tried qwen code, do it. I guess its almost only really useful for people like me, who don&amp;#39;t know jack shit about coding. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?auto=webp&amp;s=689c8ef1840eeb6bb6bb53a5908a342a3dac7558",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2382140736e501c0e5f725eb9004d88daaf4ddc",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c02fd612828e87be93104f75281f9696b4b69068",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aef0b7f26ce8aa8b563291337614fb5656c8ab07",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=faaeb8e0dc19d67afd0d3197e7b7a5a0095a6f09",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eebbb21f63208ab9ba4a6a50b9890a328d60d830",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2edc81e58030926a09a63f9377700b5cd6432c27",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfuiri",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Flashy_Management962",
          "discussion_type": null,
          "num_comments": 109,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754151475,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I mean we all know the concepts, but how many can actually say they memorized at least the high level architecture of the transformer.  \nwhich architectures/knowledge do you consider a must for scaling and fine tuning models? (GPT? BERT? what ever deepseek did with their articles?)",
          "author_fullname": "t2_rgy8m2w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How many of you actually know by heart the general structure of the transformer architecture?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgmr6x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754236507,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I mean we all know the concepts, but how many can actually say they memorized at least the high level architecture of the transformer.&lt;br/&gt;\nwhich architectures/knowledge do you consider a must for scaling and fine tuning models? (GPT? BERT? what ever deepseek did with their articles?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgmr6x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CptKrupnik",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754236507,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Testing Qwen Coder CLI with Qwen3-30B-A3B-Instruct-2507-Q4\\_K\\_S.gguf +LM Studio 0.3.21 (Build 3).  \nAfter initial folder and file read (`app/main.go`, `configs.json`, etc.), it keeps replying:  \n*\"I'm ready to assist with your project in /srv/testproject...\"*\n\nIt ignores direct inputs like:\n\n* \"What does this application do?\"\n* \"Explain me the project\"\n* \"Give me a function list\"\n* \"List all files\"\n\nNo actual answers, just the same boilerplate response:\n\nUnderstood. I'm ready to assist with your project in /srv/testproject. Let me know what you'd like to do—whether it's modifying code, adding features, debugging, or exploring the structure.\n\nAnyone else experiencing this with the latest combo? Misconfigured or bug?\n\n  \n\\--\n\nAs example Qwen 14B works fine.",
          "author_fullname": "t2_mhb0rkd4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-30B-A3B-Instruct-2507-Q4_K_S.gguf + LM Studio 0.3.21 (Build 3): Assistant ignores questions, stuck in loop",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgeerv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754210842,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Testing Qwen Coder CLI with Qwen3-30B-A3B-Instruct-2507-Q4_K_S.gguf +LM Studio 0.3.21 (Build 3).&lt;br/&gt;\nAfter initial folder and file read (&lt;code&gt;app/main.go&lt;/code&gt;, &lt;code&gt;configs.json&lt;/code&gt;, etc.), it keeps replying:&lt;br/&gt;\n&lt;em&gt;&amp;quot;I&amp;#39;m ready to assist with your project in /srv/testproject...&amp;quot;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;It ignores direct inputs like:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;What does this application do?&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;Explain me the project&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;Give me a function list&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;List all files&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;No actual answers, just the same boilerplate response:&lt;/p&gt;\n\n&lt;p&gt;Understood. I&amp;#39;m ready to assist with your project in /srv/testproject. Let me know what you&amp;#39;d like to do—whether it&amp;#39;s modifying code, adding features, debugging, or exploring the structure.&lt;/p&gt;\n\n&lt;p&gt;Anyone else experiencing this with the latest combo? Misconfigured or bug?&lt;/p&gt;\n\n&lt;p&gt;--&lt;/p&gt;\n\n&lt;p&gt;As example Qwen 14B works fine.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgeerv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Eden63",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgeerv/qwen330ba3binstruct2507q4_k_sgguf_lm_studio_0321/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgeerv/qwen330ba3binstruct2507q4_k_sgguf_lm_studio_0321/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754210842,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "When using LLMs for coding, probably all of you have also noticed that LLMs use emojis all over the place,from logs to UI. What causes this behavior? Did the training data also contain emojis within codes? Did you guys also use emojis in your code and I'm the only one who never use it and feel that it's weird and very unprofessional? If it's not the training data, why would LLMs abuse emojis? ",
          "author_fullname": "t2_1oy2v7xti6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why LLMs use emojis in code?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh358z",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.44,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754278982,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When using LLMs for coding, probably all of you have also noticed that LLMs use emojis all over the place,from logs to UI. What causes this behavior? Did the training data also contain emojis within codes? Did you guys also use emojis in your code and I&amp;#39;m the only one who never use it and feel that it&amp;#39;s weird and very unprofessional? If it&amp;#39;s not the training data, why would LLMs abuse emojis? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh358z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Equivalent_Cut_5845",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh358z/why_llms_use_emojis_in_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh358z/why_llms_use_emojis_in_code/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754278982,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I spend about 300-400 USD per month on Claude Code with the max 5x tier. I’m unsure when they’ll increase pricing, limit usage, or make models less intelligent. I’m looking for a cheaper or open-source alternative that’s just as good for programming as Claude Sonnet 4. Any suggestions are appreciated. \n\n\nEdit: I don’t pay $300-400 per month. I have Claude Max subscription (100$) that comes with a Claude code. I used a tool called ccusage to check my usage, and it showed that I use approximately $400 worth of API every month on my Claude Max subscription. It works fine now, but I’m quite certain that, just like what happened with cursor, there will likely be a price increase or a higher rate limiting soon. \n\nThanks for all the suggestions. I’ll try out Kimi2, R1, qwen 3, glm4.5 and Gemini 2.5 Pro and update how it goes in another post. :)",
          "author_fullname": "t2_67bb0jdy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-source model that is as intelligent as Claude Sonnet 4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfqejn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 383,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 383,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754151213,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754140871,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I spend about 300-400 USD per month on Claude Code with the max 5x tier. I’m unsure when they’ll increase pricing, limit usage, or make models less intelligent. I’m looking for a cheaper or open-source alternative that’s just as good for programming as Claude Sonnet 4. Any suggestions are appreciated. &lt;/p&gt;\n\n&lt;p&gt;Edit: I don’t pay $300-400 per month. I have Claude Max subscription (100$) that comes with a Claude code. I used a tool called ccusage to check my usage, and it showed that I use approximately $400 worth of API every month on my Claude Max subscription. It works fine now, but I’m quite certain that, just like what happened with cursor, there will likely be a price increase or a higher rate limiting soon. &lt;/p&gt;\n\n&lt;p&gt;Thanks for all the suggestions. I’ll try out Kimi2, R1, qwen 3, glm4.5 and Gemini 2.5 Pro and update how it goes in another post. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfqejn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vishwa1238",
          "discussion_type": null,
          "num_comments": 275,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754140871,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Thx",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best uncensored +50B parameters model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgv74s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754256666,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgv74s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgv74s/best_uncensored_50b_parameters_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgv74s/best_uncensored_50b_parameters_model/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754256666,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Since Qwen3-235B-A22B and Qwen3-30B-A3B have been updated, is there any word on similar updates for Qwen3-8B or Qwen3-14B?",
          "author_fullname": "t2_9vs7zm4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any news on updated Qwen3-8B/14B versions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg6xia",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 39,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 39,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754184771,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since Qwen3-235B-A22B and Qwen3-30B-A3B have been updated, is there any word on similar updates for Qwen3-8B or Qwen3-14B?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg6xia",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zyxwvu54321",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg6xia/any_news_on_updated_qwen38b14b_versions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg6xia/any_news_on_updated_qwen38b14b_versions/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754184771,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My own testing results are backed up by the private tests run on dubesor.de. Coder is significantly worse in coding related knowledge than Instruct. If Coder is fine tuned from Instruct, I can only surmise that the additional training on a plethora of programming languages and agentic abilities has resulted in a good dose of catastrophic forgetting.\n\nThe take away is that training data is king at these small model sizes, and that we need coders that are not overwhelmed in the attempt of making a generic Swiss Army knife for all programming use cases.\n\nWe need specialists for individual languages (or perhaps domains, such as web development). These should be at the Instruct level of general ability, with the added speciality of no negative consequence to the model.",
          "author_fullname": "t2_1puly589vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Note to the Qwen team re. the new 30B A3B Coder and Instruct versions: Coder is lobotomized when compared to Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg3d62",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 57,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 57,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754174302,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My own testing results are backed up by the private tests run on dubesor.de. Coder is significantly worse in coding related knowledge than Instruct. If Coder is fine tuned from Instruct, I can only surmise that the additional training on a plethora of programming languages and agentic abilities has resulted in a good dose of catastrophic forgetting.&lt;/p&gt;\n\n&lt;p&gt;The take away is that training data is king at these small model sizes, and that we need coders that are not overwhelmed in the attempt of making a generic Swiss Army knife for all programming use cases.&lt;/p&gt;\n\n&lt;p&gt;We need specialists for individual languages (or perhaps domains, such as web development). These should be at the Instruct level of general ability, with the added speciality of no negative consequence to the model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg3d62",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jackdareel",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754174302,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Would you rather have a samsung 9100 pro 4tb vs WD\\_BLACK 8TB SN850X for llama.cpp? The samsung is twice as fast but the WD is twice as big. They cost roughly the same with the WD being slightly more expensive.\n\nI already have a samsung 990 pro 4tb and it's fine unless I need to work with safetensor files, in which case I end up having to delete and re-download models to save space, which is annoying.  \n  \nI'm considering this as a second drive, probably mounted on a SABRENT M.2 NVMe SSD to PCIe x16 because I can't figure out how to get from MCIO 8i to a m.2 with PCIe5 / Gen 5 capability.\n\nI'm wondering if models will load into RAM any faster with the faster drive, or if I should just go with the WD.",
          "author_fullname": "t2_ozxxf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "samsung 9100 pro 4tb vs WD_BLACK 8TB SN850X for llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgtrvz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754253267,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would you rather have a samsung 9100 pro 4tb vs WD_BLACK 8TB SN850X for llama.cpp? The samsung is twice as fast but the WD is twice as big. They cost roughly the same with the WD being slightly more expensive.&lt;/p&gt;\n\n&lt;p&gt;I already have a samsung 990 pro 4tb and it&amp;#39;s fine unless I need to work with safetensor files, in which case I end up having to delete and re-download models to save space, which is annoying.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m considering this as a second drive, probably mounted on a SABRENT M.2 NVMe SSD to PCIe x16 because I can&amp;#39;t figure out how to get from MCIO 8i to a m.2 with PCIe5 / Gen 5 capability.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if models will load into RAM any faster with the faster drive, or if I should just go with the WD.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgtrvz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "createthiscom",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgtrvz/samsung_9100_pro_4tb_vs_wd_black_8tb_sn850x_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgtrvz/samsung_9100_pro_4tb_vs_wd_black_8tb_sn850x_for/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754253267,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm not a professional LLMer by any means, but I figured I'd lay out my little journey and the findings along the way.\n\nWhen I first saw you could run local models on your own hardware, I thought it was so interesting. I played around with ollama a bit, had a lot of fun tying it into random things. It's so easy to get started and also to pull models.\n\nThe real kicker was when I got a 2x AMD MI50 setup. I was having trouble getting decent prompt processing speed out of it. I spent a lot of time looking at the ollama logs on debug, trying to figure out what was going on.\n\nI had also had a few troubles with using models from huggingface, and I finally figured out that ollama uses its own \"go template\" setup.\n\nWell, although I don't know a super-lot about LLMs, I have spent a lot of time with Linux and editing random config files and compiling things is no big deal.\n\nI decided to try out llama.cpp, with my own build. Well, it actually wasn't that bad. I was able to get ROCm installed pretty easy and from there it didn't take long to figure out how to compile llama.cpp. \n\nllama.cpp ended up being awesome. I was using the llama-quantize tool to try to quantize my own models, the llama-bench utility was REALLY handy with running experiments with various parameters.\n\nAfter playing around with llama.cpp for a while, I was totally hooked. I ended up running into \"llama-swap\", which helps me kind of reproduce the setup I have with ollama, where you can load/unload random models on the fly. \n\nNow I was able to easily use almost any model on huggingface, without much worry about if the templates were going to work.\n\nAdditionally, I found it way easier to play around with various parameters, because it just turned into a quick edit on llama-swap's config file instead of having to mess around with Modelfiles and so on.\n\nI also really appreciated that I could just keep a directory of .gguf files instead of the \"layering\" system (seemingly inspired by Docker) used by ollama.\n\nI had heard it said that ollama is \"just\" a llama.cpp wrapper, which isn't entirely fair, because they seem to do a lot of work to make it easy to install...\n\nIt was also pretty easy to tie random programs into \"llama-server\" since it is mostly compliant with OpenAI's APIs.\n\nAnyway, if you're out there wondering the ups and downs with ollama/llama.cpp, my guideline would be this:\n\nInterested in a quick and easy setup, don't want to fiddle with parameters too much: ollama. It's really, really easy to get going, has really decent cross platform support and it's easy to pull a model if it's on ollama.ai.\n\nIf you're a Linux goon and don't mind fiddling with config files and compiling your own llama.cpp (although they have binaries), llama.cpp wins hands down for the wide range of configurations you can apply. You get a LOT LOT LOT more control. I also found the documentation of llama.cpp a lot better, where ollama kind of just breezed over some knobs, and I wasn't entirely sure what they were doing.\n\nI kind of see both of these as a tool on my tool-belt, and I'll probably feel free to reach for either one, depending on what my needs are in the future, but for now, I'm 100% llama-swap/llama.cpp and having a blast!",
          "author_fullname": "t2_10iarzku",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A rambling post on ollama / llama.cpp and when to use each. Pros and cons and everything in between.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgt5bx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754251812,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not a professional LLMer by any means, but I figured I&amp;#39;d lay out my little journey and the findings along the way.&lt;/p&gt;\n\n&lt;p&gt;When I first saw you could run local models on your own hardware, I thought it was so interesting. I played around with ollama a bit, had a lot of fun tying it into random things. It&amp;#39;s so easy to get started and also to pull models.&lt;/p&gt;\n\n&lt;p&gt;The real kicker was when I got a 2x AMD MI50 setup. I was having trouble getting decent prompt processing speed out of it. I spent a lot of time looking at the ollama logs on debug, trying to figure out what was going on.&lt;/p&gt;\n\n&lt;p&gt;I had also had a few troubles with using models from huggingface, and I finally figured out that ollama uses its own &amp;quot;go template&amp;quot; setup.&lt;/p&gt;\n\n&lt;p&gt;Well, although I don&amp;#39;t know a super-lot about LLMs, I have spent a lot of time with Linux and editing random config files and compiling things is no big deal.&lt;/p&gt;\n\n&lt;p&gt;I decided to try out llama.cpp, with my own build. Well, it actually wasn&amp;#39;t that bad. I was able to get ROCm installed pretty easy and from there it didn&amp;#39;t take long to figure out how to compile llama.cpp. &lt;/p&gt;\n\n&lt;p&gt;llama.cpp ended up being awesome. I was using the llama-quantize tool to try to quantize my own models, the llama-bench utility was REALLY handy with running experiments with various parameters.&lt;/p&gt;\n\n&lt;p&gt;After playing around with llama.cpp for a while, I was totally hooked. I ended up running into &amp;quot;llama-swap&amp;quot;, which helps me kind of reproduce the setup I have with ollama, where you can load/unload random models on the fly. &lt;/p&gt;\n\n&lt;p&gt;Now I was able to easily use almost any model on huggingface, without much worry about if the templates were going to work.&lt;/p&gt;\n\n&lt;p&gt;Additionally, I found it way easier to play around with various parameters, because it just turned into a quick edit on llama-swap&amp;#39;s config file instead of having to mess around with Modelfiles and so on.&lt;/p&gt;\n\n&lt;p&gt;I also really appreciated that I could just keep a directory of .gguf files instead of the &amp;quot;layering&amp;quot; system (seemingly inspired by Docker) used by ollama.&lt;/p&gt;\n\n&lt;p&gt;I had heard it said that ollama is &amp;quot;just&amp;quot; a llama.cpp wrapper, which isn&amp;#39;t entirely fair, because they seem to do a lot of work to make it easy to install...&lt;/p&gt;\n\n&lt;p&gt;It was also pretty easy to tie random programs into &amp;quot;llama-server&amp;quot; since it is mostly compliant with OpenAI&amp;#39;s APIs.&lt;/p&gt;\n\n&lt;p&gt;Anyway, if you&amp;#39;re out there wondering the ups and downs with ollama/llama.cpp, my guideline would be this:&lt;/p&gt;\n\n&lt;p&gt;Interested in a quick and easy setup, don&amp;#39;t want to fiddle with parameters too much: ollama. It&amp;#39;s really, really easy to get going, has really decent cross platform support and it&amp;#39;s easy to pull a model if it&amp;#39;s on ollama.ai.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re a Linux goon and don&amp;#39;t mind fiddling with config files and compiling your own llama.cpp (although they have binaries), llama.cpp wins hands down for the wide range of configurations you can apply. You get a LOT LOT LOT more control. I also found the documentation of llama.cpp a lot better, where ollama kind of just breezed over some knobs, and I wasn&amp;#39;t entirely sure what they were doing.&lt;/p&gt;\n\n&lt;p&gt;I kind of see both of these as a tool on my tool-belt, and I&amp;#39;ll probably feel free to reach for either one, depending on what my needs are in the future, but for now, I&amp;#39;m 100% llama-swap/llama.cpp and having a blast!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgt5bx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "UsualResult",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754251812,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Any interesting questions from your experience that you asked a Reasoning LLM and it failed to answer",
          "author_fullname": "t2_xvwcc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is an interesting question that an LLM failed to answer, in your experience?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgi7v8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754224711,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any interesting questions from your experience that you asked a Reasoning LLM and it failed to answer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgi7v8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VR-Person",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgi7v8/what_is_an_interesting_question_that_an_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgi7v8/what_is_an_interesting_question_that_an_llm/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754224711,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I know it is a very amateur question but I am having a headache with this. I have downloaded llama 3.1 8B from meta and painfully converted them to gguf so I could use them with llama.cpp but when I use my gguf it just outputs random stuff that he is Jarvis! I tested system prompts but it changed nothing! my initial problem was that I used to use llama with ollama in my code but then after some while the LLM would output gibberish like a lot of @@@@ and no error whatsoever about how to fix it so I thought maybe the problem is with ollama and I should download the original weights.",
          "author_fullname": "t2_4kas3n6g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Raw weights answer gibberish while ollama answers just fine!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgstni",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754251048,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know it is a very amateur question but I am having a headache with this. I have downloaded llama 3.1 8B from meta and painfully converted them to gguf so I could use them with llama.cpp but when I use my gguf it just outputs random stuff that he is Jarvis! I tested system prompts but it changed nothing! my initial problem was that I used to use llama with ollama in my code but then after some while the LLM would output gibberish like a lot of @@@@ and no error whatsoever about how to fix it so I thought maybe the problem is with ollama and I should download the original weights.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgstni",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Biodie",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgstni/raw_weights_answer_gibberish_while_ollama_answers/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgstni/raw_weights_answer_gibberish_while_ollama_answers/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754251048,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was wondering what mini models you guys were using and what’s good and what isn’t, I mostly just need something for quick categorization, I was using ChatGPT 4o-mini api for most things but I should probably swap to something local at this point ",
          "author_fullname": "t2_e49kxbgs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source alternatives to gpt 4o mini?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgjvn8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754229360,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering what mini models you guys were using and what’s good and what isn’t, I mostly just need something for quick categorization, I was using ChatGPT 4o-mini api for most things but I should probably swap to something local at this point &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgjvn8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "abaris243",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgjvn8/open_source_alternatives_to_gpt_4o_mini/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgjvn8/open_source_alternatives_to_gpt_4o_mini/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754229360,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have seen thousands of posts of people asking what card to buy and there is two points of view. One is buy expensive 3090, or even more expensive 5000 series or, buy cheap and try it. This post will cover why the P102-100 is still relevant and why it is simply the best budget card to get at 60 dollars.\n\nIf you are just doing LLM, Vision and no image or video generation. This is hands down the best budget card to get all because of its memory bandwidth. This list covers entry level cards form all series. Yes I know there are better cards but I am comparing the P102-100 with all entry level cards only and those better cards are 10x more.This is for the budget build people.\n\n2060 - 336.0 GB/s - $150 8GB  \n3060 - 360.0 GB/s - $200+ 8GB\n\n4060 - 272.0 GB/s - $260+ 8GB\n\n5060 - 448.0 GB/s - $350+ 8GB\n\nP102-100 - 440.3 GB/s - $60 10GB.\n\nIs the P102-100 faster than an\n\nentry 2060 = yes\n\nentry 3060 = yes\n\nentry 4060 = yes.\n\nonly a 5060 would be faster and not by much.\n\nDoes the P102-100 load slower, yes it takes about 1 second per GB on the model. PCie 1x4 =1GB/s but once the model is leaded it will be normal with no delays on all your queries.\n\nI have attached screenshots of a bunch of models, all with 32K context so you can see what to expect. Compare those results with other entry cards using the same 32K context and you will for yourself. Make sure they are using 32K context as the P102-100 would also be faster with lower context.\n\nso if you want to try LLM's and not go broke, the P102-100 is a solid card to try for 60 bucks. I have 2 of them and those results are using 2 cards so I have 20GB VRAM for 70 bucks at 35 each when I bought them. Now they would be 120 bucks. I am not sure if you can get 20GB VRAM for less than is as fast as this.\n\nI hope this helps other people that have been afraid to try local private ai because of the costs. I hope this motivates you to at least try. It is just 60 bucks.\n\nI will probably be updating this next week as I have a third card and I am moving up to 30GB. I should be able to run these models with higher context, 128k, 256k and even bigger models. I will post some updates for anyone interested.",
          "author_fullname": "t2_vnvnb9oa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "is the P102-100 still a viable option for LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdh6r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ZS0zNrB2tczhH3IbcM5RQIgldEEZAXIMP8grHZwjSys.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754207144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen thousands of posts of people asking what card to buy and there is two points of view. One is buy expensive 3090, or even more expensive 5000 series or, buy cheap and try it. This post will cover why the P102-100 is still relevant and why it is simply the best budget card to get at 60 dollars.&lt;/p&gt;\n\n&lt;p&gt;If you are just doing LLM, Vision and no image or video generation. This is hands down the best budget card to get all because of its memory bandwidth. This list covers entry level cards form all series. Yes I know there are better cards but I am comparing the P102-100 with all entry level cards only and those better cards are 10x more.This is for the budget build people.&lt;/p&gt;\n\n&lt;p&gt;2060 - 336.0 GB/s - $150 8GB&lt;br/&gt;\n3060 - 360.0 GB/s - $200+ 8GB&lt;/p&gt;\n\n&lt;p&gt;4060 - 272.0 GB/s - $260+ 8GB&lt;/p&gt;\n\n&lt;p&gt;5060 - 448.0 GB/s - $350+ 8GB&lt;/p&gt;\n\n&lt;p&gt;P102-100 - 440.3 GB/s - $60 10GB.&lt;/p&gt;\n\n&lt;p&gt;Is the P102-100 faster than an&lt;/p&gt;\n\n&lt;p&gt;entry 2060 = yes&lt;/p&gt;\n\n&lt;p&gt;entry 3060 = yes&lt;/p&gt;\n\n&lt;p&gt;entry 4060 = yes.&lt;/p&gt;\n\n&lt;p&gt;only a 5060 would be faster and not by much.&lt;/p&gt;\n\n&lt;p&gt;Does the P102-100 load slower, yes it takes about 1 second per GB on the model. PCie 1x4 =1GB/s but once the model is leaded it will be normal with no delays on all your queries.&lt;/p&gt;\n\n&lt;p&gt;I have attached screenshots of a bunch of models, all with 32K context so you can see what to expect. Compare those results with other entry cards using the same 32K context and you will for yourself. Make sure they are using 32K context as the P102-100 would also be faster with lower context.&lt;/p&gt;\n\n&lt;p&gt;so if you want to try LLM&amp;#39;s and not go broke, the P102-100 is a solid card to try for 60 bucks. I have 2 of them and those results are using 2 cards so I have 20GB VRAM for 70 bucks at 35 each when I bought them. Now they would be 120 bucks. I am not sure if you can get 20GB VRAM for less than is as fast as this.&lt;/p&gt;\n\n&lt;p&gt;I hope this helps other people that have been afraid to try local private ai because of the costs. I hope this motivates you to at least try. It is just 60 bucks.&lt;/p&gt;\n\n&lt;p&gt;I will probably be updating this next week as I have a third card and I am moving up to 30GB. I should be able to run these models with higher context, 128k, 256k and even bigger models. I will post some updates for anyone interested.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/oy25ru8gergf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/oy25ru8gergf1.png?auto=webp&amp;s=2cb0173f8f29e9ebb6d02dbbb0f1f6505163ed23",
                  "width": 1920,
                  "height": 4096
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ead78ede2dee21c50ed7920d88cdc0f039342ea",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=21261a21b67d3216bb8f5267a2eab08d1c3e30f8",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=716e70095d944e7d0d07c12b2d069e03b9f72dc9",
                    "width": 320,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bd72000cba8efed634dc539ff393fe099624df46",
                    "width": 640,
                    "height": 1280
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c4cccd911c998a5d7c914cf44d2385aae56de54c",
                    "width": 960,
                    "height": 1920
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8ee00d9096b93e074ccad355d7571f837459808a",
                    "width": 1080,
                    "height": 2160
                  }
                ],
                "variants": {},
                "id": "tb8jcmr9JwUFdjGqCEioOHi2smnbOEbTGPoqKGaYDxE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgdh6r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Boricua-vet",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgdh6r/is_the_p102100_still_a_viable_option_for_llm/",
          "stickied": false,
          "url": "https://i.redd.it/oy25ru8gergf1.png",
          "subreddit_subscribers": 509911,
          "created_utc": 1754207144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I know a lot of people are fine-tuning their models using their codebase but I couldn't find that many resources on how to build this dataset.\n\nSure you could dump your codebase and that's it but there must be a better way to teach the model how to interact with this codebase right?\n\nWhat did you try? And did it work?",
          "author_fullname": "t2_5219n8dd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How would you generate a dataset to fine-tune a llm to your codebase?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgmlzw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754236148,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know a lot of people are fine-tuning their models using their codebase but I couldn&amp;#39;t find that many resources on how to build this dataset.&lt;/p&gt;\n\n&lt;p&gt;Sure you could dump your codebase and that&amp;#39;s it but there must be a better way to teach the model how to interact with this codebase right?&lt;/p&gt;\n\n&lt;p&gt;What did you try? And did it work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgmlzw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ThisIsBartRick",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgmlzw/how_would_you_generate_a_dataset_to_finetune_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgmlzw/how_would_you_generate_a_dataset_to_finetune_a/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754236148,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1nisx8ggay",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "all I need....",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfgj0g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 1542,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 1542,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/CGttwXkoUDYh4g0k8yxukZYUVavOKH_4hrW2zTWYkis.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754105691,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ggc3dzhr0jgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?auto=webp&amp;s=56a0a807d6543ba43266157bcaa0b48793bcd3df",
                  "width": 1024,
                  "height": 1536
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0faf4ee7c2bcdc1e4161739543ede55c4684b2b8",
                    "width": 108,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b2e17cf5370e4cca658ed8a629406a0522d68d7e",
                    "width": 216,
                    "height": 324
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ffb58e48e0282066e26e22c2d00d814c00c5ded2",
                    "width": 320,
                    "height": 480
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=939ec0dfb5d8aad25a06b51c38644b3ee7d0d9cd",
                    "width": 640,
                    "height": 960
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0d2db9397b5cffd5e90a0cc1250191ed983494cb",
                    "width": 960,
                    "height": 1440
                  }
                ],
                "variants": {},
                "id": "FTO20S33694FU83Br474HCo8srCRihH9Gw7Oe1llTgs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mfgj0g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ILoveMy2Balls",
          "discussion_type": null,
          "num_comments": 112,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfgj0g/all_i_need/",
          "stickied": false,
          "url": "https://i.redd.it/ggc3dzhr0jgf1.png",
          "subreddit_subscribers": 509911,
          "created_utc": 1754105691,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt;**Problem:** Several places ask me to have serious backing — but I work alone hahaha.\n\n&gt;**Solutions:** Meet people and see if anyone’s willing to help me show my notes.\n\n&gt;**Proof:** I’ve got the repo ready for anyone who wants to study it.",
          "author_fullname": "t2_p7nqw2dg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Friends, I’m looking for help. After months developing this on my own, it seems I need a little push to present it all properly. 🤯",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh5qlh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.28,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754287796,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Several places ask me to have serious backing — but I work alone hahaha.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Solutions:&lt;/strong&gt; Meet people and see if anyone’s willing to help me show my notes.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; I’ve got the repo ready for anyone who wants to study it.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh5qlh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Exchange_8504",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh5qlh/friends_im_looking_for_help_after_months/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh5qlh/friends_im_looking_for_help_after_months/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754287796,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nHey LocalLLaMA team,\n\nI'm hoping someone much covered than me can help with a question about fine-tuning.\n\nI've been using the MLX library to fine-tune a model on my MacBook, but I need to test the model on other devices that aren't Macs. I'm wondering if there's a best practice for this workflow.\n\nIdeally, I'd like to keep the adapters separate from the base model, but if fusing them is the only way, that's fine too.\n\nSo far, I've only fine-tuned a quantized model and have tried converting the adapters to the PEFT format. The problem is, when I test the output on my MacBook, the base Hugging Face model works fine, but the model with the PEFT adapters just outputs gibberish. This might be due to a precision mismatch.\n\nAny advice or suggestions on how to handle this would be greatly appreciated!\n",
          "author_fullname": "t2_1hmx216rd2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MLX -&gt; GGUF",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgifea",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754241546,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754225338,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey LocalLLaMA team,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping someone much covered than me can help with a question about fine-tuning.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using the MLX library to fine-tune a model on my MacBook, but I need to test the model on other devices that aren&amp;#39;t Macs. I&amp;#39;m wondering if there&amp;#39;s a best practice for this workflow.&lt;/p&gt;\n\n&lt;p&gt;Ideally, I&amp;#39;d like to keep the adapters separate from the base model, but if fusing them is the only way, that&amp;#39;s fine too.&lt;/p&gt;\n\n&lt;p&gt;So far, I&amp;#39;ve only fine-tuned a quantized model and have tried converting the adapters to the PEFT format. The problem is, when I test the output on my MacBook, the base Hugging Face model works fine, but the model with the PEFT adapters just outputs gibberish. This might be due to a precision mismatch.&lt;/p&gt;\n\n&lt;p&gt;Any advice or suggestions on how to handle this would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgifea",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Not_Another_LLM",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgifea/mlx_gguf/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgifea/mlx_gguf/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754225338,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}