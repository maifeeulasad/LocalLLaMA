{
  "kind": "Listing",
  "data": {
    "after": "t3_1m5kmxl",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "MegaTTS 3 voice cloning is here!\n\nFor context: a while back, ByteDance released MegaTTS 3 (with exceptional voice cloning capabilities), but for various reasons, they decided not to release the WavVAE encoder necessary for voice cloning to work.\n\nRecently, a WavVAE encoder compatible with MegaTTS 3 was released by ACoderPassBy on ModelScope: [https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT](https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT) with quite promising results.\n\nI reuploaded the weights to Hugging Face: [https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning](https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning)\n\nAnd put up a quick Gradio demo to try it out: [https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning](https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning)\n\nOverall looks quite impressive - excited to see that we can finally do voice cloning with MegaTTS 3!\n\nh/t to MysteryShack on the StyleTTS 2 Discord for info about the WavVAE encoder",
          "author_fullname": "t2_1f194h3luj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MegaTTS 3 Voice Cloning is Here",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m641zg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 233,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 233,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ca44b1060cf304798e39247090bed7e9f195130b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753156417,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MegaTTS 3 voice cloning is here!&lt;/p&gt;\n\n&lt;p&gt;For context: a while back, ByteDance released MegaTTS 3 (with exceptional voice cloning capabilities), but for various reasons, they decided not to release the WavVAE encoder necessary for voice cloning to work.&lt;/p&gt;\n\n&lt;p&gt;Recently, a WavVAE encoder compatible with MegaTTS 3 was released by ACoderPassBy on ModelScope: &lt;a href=\"https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT\"&gt;https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT&lt;/a&gt; with quite promising results.&lt;/p&gt;\n\n&lt;p&gt;I reuploaded the weights to Hugging Face: &lt;a href=\"https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning\"&gt;https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And put up a quick Gradio demo to try it out: &lt;a href=\"https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning\"&gt;https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Overall looks quite impressive - excited to see that we can finally do voice cloning with MegaTTS 3!&lt;/p&gt;\n\n&lt;p&gt;h/t to MysteryShack on the StyleTTS 2 Discord for info about the WavVAE encoder&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?auto=webp&amp;s=0e8f184606f9f3e558a6971b8dfbfc9a3f0d1af8",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf8ad97c6cb72e96abaf27c1cc2565dda7970c68",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=85f1ec201fac1de1a714a3b74b2040ea838d357f",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=623e3d06175018d3caaaf85d7742c402b0f0a84d",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=13bd3c86a79666218395f17439b714df6a5fc52c",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=08124b488d263f78d5cebc4fffc2a8bd5fa5f05b",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f3c4e8f40ef1fb69f68df6601b917c02f65c89ad",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m641zg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrfakename0",
          "discussion_type": null,
          "num_comments": 45,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m641zg/megatts_3_voice_cloning_is_here/",
          "stickied": false,
          "url": "https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning",
          "subreddit_subscribers": 502720,
          "created_utc": 1753156417,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD's Strix Halo \"Ryzen AI MAX\" APUs Come To DIY PC Builders With New MoDT \"Mini-ITX\" Motherboards, Equipped With Up To 128 GB of LPDDR5X Memory",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 81,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6bddm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=140&amp;height=81&amp;crop=140:81,smart&amp;auto=webp&amp;s=00290105ce815b049672399f0e7e28e9d1afcbc9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753183102,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/amd-strix-halo-ryzen-ai-max-apus-diy-pc-new-modt-mini-itx-motherboards-128-gb-lpddr5x-memory/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?auto=webp&amp;s=10c3b72c0b82b9a62677b9306104bb21064031ab",
                  "width": 2471,
                  "height": 1440
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=097bbe6e55a92f58db20c497b5cd55b71c248bb0",
                    "width": 108,
                    "height": 62
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=20e37d29ee5af324410ac9397017c9ae2f497b28",
                    "width": 216,
                    "height": 125
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9a76caea9352dd47f6185e29c67bab1c2374a02",
                    "width": 320,
                    "height": 186
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9c63f2527e38ed9f9fb783cd700b8e831108fe01",
                    "width": 640,
                    "height": 372
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b92e328c881739b5b57de5c1f24a60a877d78657",
                    "width": 960,
                    "height": 559
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=61c8a758226b77b202d02e702bc26f1ac195c306",
                    "width": 1080,
                    "height": 629
                  }
                ],
                "variants": {},
                "id": "wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6bddm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6bddm/amds_strix_halo_ryzen_ai_max_apus_come_to_diy_pc/",
          "stickied": false,
          "url": "https://wccftech.com/amd-strix-halo-ryzen-ai-max-apus-diy-pc-new-modt-mini-itx-motherboards-128-gb-lpddr5x-memory/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753183102,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For once, I‚Äôm not going to talk about my benchmark, so to be forefront, there will be no other reference or link to it in this post,\n\nThat said, just sharing something that‚Äôs been on mind. I‚Äôve been thinking about this topic recently, and while this may be a hot or controversial take, all AI models should be open-source (even from companies like xAI, Google, OpenAI, etc.)\n\nAI is already one of the greatest inventions in human history, and at minimum it will likely be on par in terms of impact with the Internet.\n\nLike how the Internet is ‚Äúopen‚Äù for anyone to use and build on top of it, AI should be the same way.\n\nIt‚Äôs fine if products built on top of AI like Cursor, Codex, Claude Code, etc or anything that has an AI integration to be commercialized, but for the benefit and advancement of humanity, the underlying technology (the models) should be made publicly available.\n\nWhat are your thoughts on this?",
          "author_fullname": "t2_c3b3edv5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI should just be open-source",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m67zde",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 53,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 53,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753171068,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753170472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For once, I‚Äôm not going to talk about my benchmark, so to be forefront, there will be no other reference or link to it in this post,&lt;/p&gt;\n\n&lt;p&gt;That said, just sharing something that‚Äôs been on mind. I‚Äôve been thinking about this topic recently, and while this may be a hot or controversial take, all AI models should be open-source (even from companies like xAI, Google, OpenAI, etc.)&lt;/p&gt;\n\n&lt;p&gt;AI is already one of the greatest inventions in human history, and at minimum it will likely be on par in terms of impact with the Internet.&lt;/p&gt;\n\n&lt;p&gt;Like how the Internet is ‚Äúopen‚Äù for anyone to use and build on top of it, AI should be the same way.&lt;/p&gt;\n\n&lt;p&gt;It‚Äôs fine if products built on top of AI like Cursor, Codex, Claude Code, etc or anything that has an AI integration to be commercialized, but for the benefit and advancement of humanity, the underlying technology (the models) should be made publicly available.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m67zde",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adviceguru25",
          "discussion_type": null,
          "num_comments": 52,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m67zde/ai_should_just_be_opensource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m67zde/ai_should_just_be_opensource/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753170472,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3l9wjlq0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-2507 Released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5owi8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 767,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 767,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/sKHygFxyatHEivMjwhoU0rKccpX3n5vMlMuGtN0ebyc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753118247,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/Alibaba_Qwen/status/1947344511988076547",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?auto=webp&amp;s=2ca86b1c53db0d11a0c488d1d12c8c9cb55eaf20",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4c0c862769016dce18130a1fb791dbf78757f922",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2f8596c55842b58dd3bf4190c4c47e309432ad77",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d51316624bbec96c8a0b28b2e3756e68ffadf98",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9739fe5698145f958eb2e1c66da1875fc6d34a00",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b66905b8a3f2f560c571babd372861e032c5ca94",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72cbdd4b47610ef9ddfdf989b7900703487934d6",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "DZ_J_yAfR8TLjLmR0s6ZMb4IqBdDowTQUhHZ335Z0r8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5owi8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pseudoreddituser",
          "discussion_type": null,
          "num_comments": 234,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507_released/",
          "stickied": false,
          "url": "https://x.com/Alibaba_Qwen/status/1947344511988076547",
          "subreddit_subscribers": 502720,
          "created_utc": 1753118247,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is a ***Private*** eval that has been updated for over a year by Zhihu user \"toyama nao\".  So qwen cannot be benchmaxxing on it because it is ***Private*** and the questions are being updated constantly.\n\nThe score of this 2507 update is amazing, especially since it's a non-reasoning model that ranks among other reasoning ones.\n\n[logic](https://preview.redd.it/s5t1rm4dcdef1.png?width=1054&amp;format=png&amp;auto=webp&amp;s=74ec5e6f2306496b82a9049ef150b1b9f9f3b2c9)\n\n[coding](https://preview.redd.it/q1ld1vkvcdef1.png?width=1319&amp;format=png&amp;auto=webp&amp;s=849ca0681fc9aa9bfb08fc3ef6d29529731dfcbc)\n\n\\*These 2 tables are OCR and translated by gemini, so it may contain small errors\n\nDo note that Chinese models could have a slight advantage in this benchmark because the questions could be written in Chinese\n\nSource:\n\n[Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873](Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873)",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Private Eval result of Qwen3-235B-A22B-Instruct-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 30,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "q1ld1vkvcdef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 17,
                  "x": 108,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6db0188556e8b5a6ad0b4e78e643d750f9c8eb3b"
                },
                {
                  "y": 34,
                  "x": 216,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=def0b755adbe43c0ef6c669ce96229bdf8642e8d"
                },
                {
                  "y": 51,
                  "x": 320,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3eabe87b380ad52a47b2c07b92750b4da2580788"
                },
                {
                  "y": 102,
                  "x": 640,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6df88708d1d58d4dfb207f2e3d4de7ff7ba44f34"
                },
                {
                  "y": 153,
                  "x": 960,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=52be3772b829b14a916bd0dba033163893df170e"
                },
                {
                  "y": 172,
                  "x": 1080,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=83243c7b94f1c3bbb4c4965fc1926369f860f03a"
                }
              ],
              "s": {
                "y": 211,
                "x": 1319,
                "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=1319&amp;format=png&amp;auto=webp&amp;s=849ca0681fc9aa9bfb08fc3ef6d29529731dfcbc"
              },
              "id": "q1ld1vkvcdef1"
            },
            "s5t1rm4dcdef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 23,
                  "x": 108,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c77a3824658d5cc719a3b06a8dad382fb80e75b0"
                },
                {
                  "y": 46,
                  "x": 216,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5cec128245e15150bcd1e79751e2e6d55996d697"
                },
                {
                  "y": 69,
                  "x": 320,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=89eced78277fb9616c09bbe0f9fd5df078cbd54e"
                },
                {
                  "y": 139,
                  "x": 640,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=043d1bb82c725708bb35a58acd4d7c9504c7de3a"
                },
                {
                  "y": 208,
                  "x": 960,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=28844535a11fae5fff659cb0b949c1d842e4569e"
                }
              ],
              "s": {
                "y": 229,
                "x": 1054,
                "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=1054&amp;format=png&amp;auto=webp&amp;s=74ec5e6f2306496b82a9049ef150b1b9f9f3b2c9"
              },
              "id": "s5t1rm4dcdef1"
            }
          },
          "name": "t3_1m66qks",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 49,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 49,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/z5Hij9VVgEz-n0a_TqfphvLzGcPNiRWrZnXSAwXHg_Q.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753165706,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a &lt;strong&gt;&lt;em&gt;Private&lt;/em&gt;&lt;/strong&gt; eval that has been updated for over a year by Zhihu user &amp;quot;toyama nao&amp;quot;.  So qwen cannot be benchmaxxing on it because it is &lt;strong&gt;&lt;em&gt;Private&lt;/em&gt;&lt;/strong&gt; and the questions are being updated constantly.&lt;/p&gt;\n\n&lt;p&gt;The score of this 2507 update is amazing, especially since it&amp;#39;s a non-reasoning model that ranks among other reasoning ones.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s5t1rm4dcdef1.png?width=1054&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=74ec5e6f2306496b82a9049ef150b1b9f9f3b2c9\"&gt;logic&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/q1ld1vkvcdef1.png?width=1319&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=849ca0681fc9aa9bfb08fc3ef6d29529731dfcbc\"&gt;coding&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;*These 2 tables are OCR and translated by gemini, so it may contain small errors&lt;/p&gt;\n\n&lt;p&gt;Do note that Chinese models could have a slight advantage in this benchmark because the questions could be written in Chinese&lt;/p&gt;\n\n&lt;p&gt;Source:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873\"&gt;Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m66qks",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m66qks/private_eval_result_of_qwen3235ba22binstruct2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m66qks/private_eval_result_of_qwen3235ba22binstruct2507/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753165706,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A while back I posted some [Strix Halo LLM performance testing](https://www.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/) benchmarks. I'm back with an update that I believe is actually a fair bit more comprehensive now (although the original is still worth checking out for background).\n\nThe biggest difference is I wrote some automated sweeps to test different backends and flags against a full range of pp/tg on many different model architectures (including the latest MoEs) and sizes.\n\nThis is also using the latest drivers, ROCm (7.0 nightlies), and llama.cpp \n\nAll the full data and latest info is available in the Github repo: [https://github.com/lhl/strix-halo-testing/tree/main/llm-bench](https://github.com/lhl/strix-halo-testing/tree/main/llm-bench) but here are the topline stats below:\n\n# Strix Halo LLM Benchmark Results\n\nAll testing was done on pre-production [Framework Desktop](https://frame.work/desktop) systems with an AMD Ryzen Max+ 395 (Strix Halo)/128GB LPDDR5x-8000 configuration. (Thanks Nirav, Alexandru, and co!)\n\nExact testing/system details are in the results folders, but roughly these are running:\n\n* Close to production BIOS/EC\n* Relatively up-to-date kernels: 6.15.5-arch1-1/6.15.6-arch1-1\n* Recent TheRock/ROCm-7.0 nightly builds with Strix Halo (gfx1151) kernels\n* Recent llama.cpp builds (eg b5863 from 2005-07-10)\n\nJust to get a ballpark on the hardware:\n\n* \\~215 GB/s max GPU MBW out of a 256 GB/s theoretical (256-bit 8000 MT/s)\n* theoretical 59 FP16 TFLOPS (VPOD/WMMA) on RDNA 3.5 (gfx11); effective is *much* lower\n\n# Results\n\n# Prompt Processing (pp) Performance\n\nhttps://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f\n\n|Model Name|Architecture|Weights (B)|Active (B)|Backend|Flags|pp512|tg128|Memory (Max MiB)|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|Llama 2 7B Q4\\_0|Llama 2|7|7|Vulkan||998.0|46.5|4237|\n|Llama 2 7B Q4\\_K\\_M|Llama 2|7|7|HIP|hipBLASLt|906.1|40.8|4720|\n|Shisa V2 8B i1-Q4\\_K\\_M|Llama 3|8|8|HIP|hipBLASLt|878.2|37.2|5308|\n|Qwen 3 30B-A3B UD-Q4\\_K\\_XL|Qwen 3 MoE|30|3|Vulkan|fa=1|604.8|66.3|17527|\n|Mistral Small 3.1 UD-Q4\\_K\\_XL|Mistral 3|24|24|HIP|hipBLASLt|316.9|13.6|14638|\n|Hunyuan-A13B UD-Q6\\_K\\_XL|Hunyuan MoE|80|13|Vulkan|fa=1|270.5|17.1|68785|\n|Llama 4 Scout UD-Q4\\_K\\_XL|Llama 4 MoE|109|17|HIP|hipBLASLt|264.1|17.2|59720|\n|Shisa V2 70B i1-Q4\\_K\\_M|Llama 3|70|70|HIP rocWMMA||94.7|4.5|41522|\n|dots1 UD-Q4\\_K\\_XL|dots1 MoE|142|14|Vulkan|fa=1 b=256|63.1|20.6|84077|\n\n# Text Generation (tg) Performance\n\nhttps://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7\n\n|Model Name|Architecture|Weights (B)|Active (B)|Backend|Flags|pp512|tg128|Memory (Max MiB)|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|Qwen 3 30B-A3B UD-Q4\\_K\\_XL|Qwen 3 MoE|30|3|Vulkan|b=256|591.1|72.0|17377|\n|Llama 2 7B Q4\\_K\\_M|Llama 2|7|7|Vulkan|fa=1|620.9|47.9|4463|\n|Llama 2 7B Q4\\_0|Llama 2|7|7|Vulkan|fa=1|1014.1|45.8|4219|\n|Shisa V2 8B i1-Q4\\_K\\_M|Llama 3|8|8|Vulkan|fa=1|614.2|42.0|5333|\n|dots1 UD-Q4\\_K\\_XL|dots1 MoE|142|14|Vulkan|fa=1 b=256|63.1|20.6|84077|\n|Llama 4 Scout UD-Q4\\_K\\_XL|Llama 4 MoE|109|17|Vulkan|fa=1 b=256|146.1|19.3|59917|\n|Hunyuan-A13B UD-Q6\\_K\\_XL|Hunyuan MoE|80|13|Vulkan|fa=1 b=256|223.9|17.1|68608|\n|Mistral Small 3.1 UD-Q4\\_K\\_XL|Mistral 3|24|24|Vulkan|fa=1|119.6|14.3|14540|\n|Shisa V2 70B i1-Q4\\_K\\_M|Llama 3|70|70|Vulkan|fa=1|26.4|5.0|41456|\n\n# Testing Notes\n\nThe best overall backend and flags were chosen for each model family tested. You can see that often times the best backend for prefill vs token generation differ. Full results for each model (including the pp/tg graphs for different context lengths for all tested backend variations) are available for review in their respective folders as which backend is the best performing will depend on your exact use-case.\n\nThere's a lot of performance still on the table when it comes to pp especially. Since these results should be close to optimal for when they were tested, I might add dates to the table  (adding kernel, ROCm, and llama.cpp build#'s might be a bit much).\n\nOne thing worth pointing out is that pp has improved significantly on some models since I last tested. For example, back in May, pp512 for Qwen3 30B-A3B was 119 t/s (Vulkan) and it's now 605 t/s. Similarly, Llama 4 Scout has a pp512 of 103 t/s, and is now 173 t/s, although the HIP backend is significantly faster at 264 t/s.\n\nUnlike last time, I won't be taking any model testing requests as these sweeps take quite a while to run - I feel like there are enough 395 systems out there now and the repo linked at top includes the full scripts to allow anyone to replicate (and can be easily adapted for other backends or to run with different hardware).\n\nFor testing, the HIP backend, I highly recommend trying `ROCBLAS_USE_HIPBLASLT=1` as that is almost always faster than the default rocBLAS. If you are OK with occasionally hitting the reboot switch, you might also want to test in combination with (as long as you have the gfx1100 kernels installed) `HSA_OVERRIDE_GFX_VERSION=11.0.0` \\- in prior testing I've found the gfx1100 kernels to be up 2X faster than gfx1151 kernels... ü§î",
          "author_fullname": "t2_eztox",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Updated Strix Halo (Ryzen AI Max+ 395) LLM Benchmark Results",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 92,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "mjr2d31ujeef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=80836e3346fcd0e6847fcb3f1d33c5f2ac3c12e3"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5cd34de136a42ad65ac9facd37455912c1a13410"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=675c17530bfdbbd8ae2830c3b694e07b5163a1b0"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6bdc9a2d7260d9fd3acbab23e12917b54e651493"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e051180cc70357e18ffe4725601fb9811ba1193"
                },
                {
                  "y": 715,
                  "x": 1080,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=75e938dac2457cc4191363a5cd8cbcaf777049ed"
                }
              ],
              "s": {
                "y": 1181,
                "x": 1782,
                "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f"
              },
              "id": "mjr2d31ujeef1"
            },
            "7y0pdbqujeef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=493fe8a11d9ee1d7a485e1b7233ca7e945637599"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c43022989f8b24ef64216d0b17642e3f80013763"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f7f683ec0155e0b8f196a71bccaf428b44550399"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb65c24153d597ae592f5012aabff2a638b88357"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f57d68c01ba672fba3a5062a2ec12ac45a621fac"
                },
                {
                  "y": 715,
                  "x": 1080,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=59b563c8992d2568941d7c73829b8f7ebc4f5585"
                }
              ],
              "s": {
                "y": 1181,
                "x": 1782,
                "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7"
              },
              "id": "7y0pdbqujeef1"
            }
          },
          "name": "t3_1m6b151",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/iZq9ApFg7F044Ny8obqZ27FfndXjE_7xNkH5oORO2gc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753182004,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A while back I posted some &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/\"&gt;Strix Halo LLM performance testing&lt;/a&gt; benchmarks. I&amp;#39;m back with an update that I believe is actually a fair bit more comprehensive now (although the original is still worth checking out for background).&lt;/p&gt;\n\n&lt;p&gt;The biggest difference is I wrote some automated sweeps to test different backends and flags against a full range of pp/tg on many different model architectures (including the latest MoEs) and sizes.&lt;/p&gt;\n\n&lt;p&gt;This is also using the latest drivers, ROCm (7.0 nightlies), and llama.cpp &lt;/p&gt;\n\n&lt;p&gt;All the full data and latest info is available in the Github repo: &lt;a href=\"https://github.com/lhl/strix-halo-testing/tree/main/llm-bench\"&gt;https://github.com/lhl/strix-halo-testing/tree/main/llm-bench&lt;/a&gt; but here are the topline stats below:&lt;/p&gt;\n\n&lt;h1&gt;Strix Halo LLM Benchmark Results&lt;/h1&gt;\n\n&lt;p&gt;All testing was done on pre-production &lt;a href=\"https://frame.work/desktop\"&gt;Framework Desktop&lt;/a&gt; systems with an AMD Ryzen Max+ 395 (Strix Halo)/128GB LPDDR5x-8000 configuration. (Thanks Nirav, Alexandru, and co!)&lt;/p&gt;\n\n&lt;p&gt;Exact testing/system details are in the results folders, but roughly these are running:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Close to production BIOS/EC&lt;/li&gt;\n&lt;li&gt;Relatively up-to-date kernels: 6.15.5-arch1-1/6.15.6-arch1-1&lt;/li&gt;\n&lt;li&gt;Recent TheRock/ROCm-7.0 nightly builds with Strix Halo (gfx1151) kernels&lt;/li&gt;\n&lt;li&gt;Recent llama.cpp builds (eg b5863 from 2005-07-10)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Just to get a ballpark on the hardware:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;~215 GB/s max GPU MBW out of a 256 GB/s theoretical (256-bit 8000 MT/s)&lt;/li&gt;\n&lt;li&gt;theoretical 59 FP16 TFLOPS (VPOD/WMMA) on RDNA 3.5 (gfx11); effective is &lt;em&gt;much&lt;/em&gt; lower&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Results&lt;/h1&gt;\n\n&lt;h1&gt;Prompt Processing (pp) Performance&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f\"&gt;https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f&lt;/a&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Architecture&lt;/th&gt;\n&lt;th align=\"left\"&gt;Weights (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Active (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Backend&lt;/th&gt;\n&lt;th align=\"left\"&gt;Flags&lt;/th&gt;\n&lt;th align=\"left\"&gt;pp512&lt;/th&gt;\n&lt;th align=\"left\"&gt;tg128&lt;/th&gt;\n&lt;th align=\"left\"&gt;Memory (Max MiB)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;998.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;46.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;4237&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;906.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;40.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;4720&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 8B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;878.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;37.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;5308&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 30B-A3B UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Qwen 3 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;30&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;604.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;66.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;17527&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mistral Small 3.1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;316.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;13.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;14638&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Hunyuan-A13B UD-Q6_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Hunyuan MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;80&lt;/td&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;270.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;68785&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 4 Scout UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 4 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;109&lt;/td&gt;\n&lt;td align=\"left\"&gt;17&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;264.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;59720&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 70B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP rocWMMA&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;94.7&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;41522&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;dots1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;dots1 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;142&lt;/td&gt;\n&lt;td align=\"left\"&gt;14&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;63.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;84077&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Text Generation (tg) Performance&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7\"&gt;https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7&lt;/a&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Architecture&lt;/th&gt;\n&lt;th align=\"left\"&gt;Weights (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Active (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Backend&lt;/th&gt;\n&lt;th align=\"left\"&gt;Flags&lt;/th&gt;\n&lt;th align=\"left\"&gt;pp512&lt;/th&gt;\n&lt;th align=\"left\"&gt;tg128&lt;/th&gt;\n&lt;th align=\"left\"&gt;Memory (Max MiB)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 30B-A3B UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Qwen 3 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;30&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;591.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;72.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;17377&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;620.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;47.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;4463&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;1014.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;45.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;4219&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 8B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;614.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;42.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;5333&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;dots1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;dots1 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;142&lt;/td&gt;\n&lt;td align=\"left\"&gt;14&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;63.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;84077&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 4 Scout UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 4 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;109&lt;/td&gt;\n&lt;td align=\"left\"&gt;17&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;146.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;59917&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Hunyuan-A13B UD-Q6_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Hunyuan MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;80&lt;/td&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;223.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;68608&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mistral Small 3.1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;119.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;14540&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 70B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;26.4&lt;/td&gt;\n&lt;td align=\"left\"&gt;5.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;41456&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Testing Notes&lt;/h1&gt;\n\n&lt;p&gt;The best overall backend and flags were chosen for each model family tested. You can see that often times the best backend for prefill vs token generation differ. Full results for each model (including the pp/tg graphs for different context lengths for all tested backend variations) are available for review in their respective folders as which backend is the best performing will depend on your exact use-case.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a lot of performance still on the table when it comes to pp especially. Since these results should be close to optimal for when they were tested, I might add dates to the table  (adding kernel, ROCm, and llama.cpp build#&amp;#39;s might be a bit much).&lt;/p&gt;\n\n&lt;p&gt;One thing worth pointing out is that pp has improved significantly on some models since I last tested. For example, back in May, pp512 for Qwen3 30B-A3B was 119 t/s (Vulkan) and it&amp;#39;s now 605 t/s. Similarly, Llama 4 Scout has a pp512 of 103 t/s, and is now 173 t/s, although the HIP backend is significantly faster at 264 t/s.&lt;/p&gt;\n\n&lt;p&gt;Unlike last time, I won&amp;#39;t be taking any model testing requests as these sweeps take quite a while to run - I feel like there are enough 395 systems out there now and the repo linked at top includes the full scripts to allow anyone to replicate (and can be easily adapted for other backends or to run with different hardware).&lt;/p&gt;\n\n&lt;p&gt;For testing, the HIP backend, I highly recommend trying &lt;code&gt;ROCBLAS_USE_HIPBLASLT=1&lt;/code&gt; as that is almost always faster than the default rocBLAS. If you are OK with occasionally hitting the reboot switch, you might also want to test in combination with (as long as you have the gfx1100 kernels installed) &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.0&lt;/code&gt; - in prior testing I&amp;#39;ve found the gfx1100 kernels to be up 2X faster than gfx1151 kernels... ü§î&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6b151",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "randomfoo2",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753182004,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/Alibaba_Qwen/status/1947344511988076547\n\nNew Qwen3-235B-A22B with thinking mode only ‚Äì‚Äì no more hybrid reasoning.",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 122,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ox8z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 462,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 462,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/BzOaH3m_YlAhztCv_hRYQh_Ms3ouqOY06ZKKT3zNke8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753118294,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/Alibaba_Qwen/status/1947344511988076547\"&gt;https://x.com/Alibaba_Qwen/status/1947344511988076547&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;New Qwen3-235B-A22B with thinking mode only ‚Äì‚Äì no more hybrid reasoning.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/w2uh7h5lg9ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?auto=webp&amp;s=d32732e5748b82ca37787c55ccd57f5d5f705318",
                  "width": 1186,
                  "height": 1038
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=32d1f0ad8ac85f1518bb4e197d86320d03376d96",
                    "width": 108,
                    "height": 94
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf9ca6a58cad66052d5ca05375d82b3c7bf8f1cb",
                    "width": 216,
                    "height": 189
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c38bec3415e13d4801fadbc3fe0e9ec1df461dbf",
                    "width": 320,
                    "height": 280
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5242a889814e823acb6da0b1179758e2947ea2a7",
                    "width": 640,
                    "height": 560
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7330c9d38901abf96ac6373f03b5933dd9e99710",
                    "width": 960,
                    "height": 840
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15a616880d08cfc896e48a40a70a2aa2315dc9e4",
                    "width": 1080,
                    "height": 945
                  }
                ],
                "variants": {},
                "id": "15L72ZL9LJC_7vzVz7gsW_m-zmkTuQofd74ZqjjUwaA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5ox8z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 77,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ox8z/qwen3235ba22b2507/",
          "stickied": false,
          "url": "https://i.redd.it/w2uh7h5lg9ef1.png",
          "subreddit_subscribers": 502720,
          "created_utc": 1753118294,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/ikawrakow/ik\\_llama.cpp](https://github.com/ikawrakow/ik_llama.cpp)\n\nFriendly reminder to back up all the things!",
          "author_fullname": "t2_8u7n5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The ik_llama.cpp repository is back! \\o/",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6cfzi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753186412,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/ikawrakow/ik_llama.cpp\"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Friendly reminder to back up all the things!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?auto=webp&amp;s=7c74a86a8d22a1d2e90ce704f456a5a36cf050e7",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9057a5a31598407ca7946c278de43e70cf0c9ed",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=533c07c0d65f89514a6ba54ce5f1c6649e969c77",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9699bd0efa2b26a1c034cdb0fe8abc1317589b6c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=355e07ff2e46e3a253b40e25c06644c7282af5b2",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c7a47a3cb456bb05dfd53a716bae5ef6addff5e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1fbd5e6235d1a60da5c17ef35a7bd40a655c4d80",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6cfzi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thireus",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753186412,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7pfgfkis",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New qwen tested on Fiction.liveBench",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6172l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 90,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 90,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/TLf5BqdXyD8b18S_CjlBuka8R6DaWW-Nnyc_DD4KFcw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753148000,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/9rynne03xbef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/9rynne03xbef1.png?auto=webp&amp;s=4f7e2275d4e835b0f01387fc4e2f5de4682c92f8",
                  "width": 1520,
                  "height": 2266
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7c72f32848579ff8381bbc07e00d52af73ccb790",
                    "width": 108,
                    "height": 161
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c05a3489ce34f1c0d3c48da6ac4fb493a3af2239",
                    "width": 216,
                    "height": 322
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7758dc0729434a0929fb46f9640d8ed72e9ba4f",
                    "width": 320,
                    "height": 477
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8cc832729da290425257b97f9e8171f9cd64ec1e",
                    "width": 640,
                    "height": 954
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b1dd97abb7b7a97ba53a5a60797a4df72dbe1e9e",
                    "width": 960,
                    "height": 1431
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=51749430f3b47afde5a4eee467854e441af14310",
                    "width": 1080,
                    "height": 1610
                  }
                ],
                "variants": {},
                "id": "OE4XOhVwW7bVZ94xo4IF074gf7GQOtJgsoNbI-IyttA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6172l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fictionlive",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6172l/new_qwen_tested_on_fictionlivebench/",
          "stickied": false,
          "url": "https://i.redd.it/9rynne03xbef1.png",
          "subreddit_subscribers": 502720,
          "created_utc": 1753148000,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Throwback to 3 months ago: [https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg\\_a\\_unified\\_scalable\\_vector\\_graphics/](https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/)\n\nWeights: [https://huggingface.co/OmniSVG/OmniSVG](https://huggingface.co/OmniSVG/OmniSVG)\n\nHuggingFace demo: [https://huggingface.co/spaces/OmniSVG/OmniSVG-3B](https://huggingface.co/spaces/OmniSVG/OmniSVG-3B)\n\nGitHub: [https://github.com/OmniSVG/OmniSVG/](https://github.com/OmniSVG/OmniSVG/)",
          "author_fullname": "t2_w4j8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OmniSVG weights released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m61u94",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 74,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 74,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753149834,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Throwback to 3 months ago: &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Weights: &lt;a href=\"https://huggingface.co/OmniSVG/OmniSVG\"&gt;https://huggingface.co/OmniSVG/OmniSVG&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;HuggingFace demo: &lt;a href=\"https://huggingface.co/spaces/OmniSVG/OmniSVG-3B\"&gt;https://huggingface.co/spaces/OmniSVG/OmniSVG-3B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/OmniSVG/OmniSVG/\"&gt;https://github.com/OmniSVG/OmniSVG/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?auto=webp&amp;s=879a29e047e8b5a9e7c3cc213f6732c60bc2a1a7",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a72266ba63c0bc5f87d6bf4f1a9d21ca8a03fb2",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8554240e67e2f71d1e81cbf7f1b701e59cb5fefd",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e1d031a2135bd08b701037085db4506b941ab6d",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b70a2f5fdb810f142ac53ef2d47901cc0c789f95",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a856840edcf86155de1faff51118fe59e453e241",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=479fbae9e67984006acc087b84b30e16cca24f24",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m61u94",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DeProgrammer99",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m61u94/omnisvg_weights_released/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m61u94/omnisvg_weights_released/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753149834,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/JustinLin610/status/1947281769134170147\n\nMaybe Qwen3-Coder, Qwen3-VL or a new QwQ? Will be open source / weight according to Chujie Zheng [here](https://x.com/ChujieZheng/status/1947307034980089905).",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Imminent release from Qwen tonight",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5n148",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 424,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 424,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/DBvwe9bi2sUadVKTh4wZB-h_0n3lxygls9SlP-B36wg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753114102,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/JustinLin610/status/1947281769134170147\"&gt;https://x.com/JustinLin610/status/1947281769134170147&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Maybe Qwen3-Coder, Qwen3-VL or a new QwQ? Will be open source / weight according to Chujie Zheng &lt;a href=\"https://x.com/ChujieZheng/status/1947307034980089905\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/um0pwye549ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/um0pwye549ef1.png?auto=webp&amp;s=034e1360dd4d1a71075a1978e81cc176280c0940",
                  "width": 570,
                  "height": 284
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/um0pwye549ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ac602ae1dcb08fc594a97f2c504da7e053543395",
                    "width": 108,
                    "height": 53
                  },
                  {
                    "url": "https://preview.redd.it/um0pwye549ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=34209fd3279089bca920b8e313de5e6ea1d3d074",
                    "width": 216,
                    "height": 107
                  },
                  {
                    "url": "https://preview.redd.it/um0pwye549ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3401860a4fccf34b2ae631236b2b714dafe0ec28",
                    "width": 320,
                    "height": 159
                  }
                ],
                "variants": {},
                "id": "XO8ytuncZItjhr3hlokHSgG-P5B6fKV-FMv68ZqKLXQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5n148",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 82,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5n148/imminent_release_from_qwen_tonight/",
          "stickied": false,
          "url": "https://i.redd.it/um0pwye549ef1.png",
          "subreddit_subscribers": 502720,
          "created_utc": 1753114102,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1tpuoj72sa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Frankenserver for sale at a steep discount. 2x96GB GH200 converted from liquid- to air-cooled.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 106,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m65iga",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/ObGRZZdEJPhNPxSGc-Hl-FLX_u-ODU9Q-A84Zj7Q5Z4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753161224,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ifz3sua70def1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ifz3sua70def1.jpeg?auto=webp&amp;s=3c8da0138281644b0b232fb926afb72c94068d2d",
                  "width": 4037,
                  "height": 3077
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cb9bd9d6aa78574351fa9778ec9d0b129263457",
                    "width": 108,
                    "height": 82
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d7c4b9ff9064655d107fe8aab0d33aa934050ef",
                    "width": 216,
                    "height": 164
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=34ad0918b9b41221e6a4d64ec2a5686a6466206b",
                    "width": 320,
                    "height": 243
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=784878f38d8fa43b398531435fad5ad46f80423f",
                    "width": 640,
                    "height": 487
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fba24ff9f318cff389422ef0a5e7d51301d79f6c",
                    "width": 960,
                    "height": 731
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=674e7d2f5ae18055ec6bf581e5ec73734512ba00",
                    "width": 1080,
                    "height": 823
                  }
                ],
                "variants": {},
                "id": "xdYmB0YCSsaGa7OmRBAUHmZvhAr6D2eweUUdkl-syTg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m65iga",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GPTrack_ai",
          "discussion_type": null,
          "num_comments": 58,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m65iga/frankenserver_for_sale_at_a_steep_discount_2x96gb/",
          "stickied": false,
          "url": "https://i.redd.it/ifz3sua70def1.jpeg",
          "subreddit_subscribers": 502720,
          "created_utc": 1753161224,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Unfortunately it's on SXM4, you will need a $600 adapter for this. but I am sure someone with enough motivation will figure out a way to drop it into a PCIe adapter to sell it as a complete package. It'll be an interesting piece of localllama HW.",
          "author_fullname": "t2_bjeo1gwy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Used A100 40GB just dropped below $2000, for those who care with caveat",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m60ahf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 68,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 68,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753145404,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Unfortunately it&amp;#39;s on SXM4, you will need a $600 adapter for this. but I am sure someone with enough motivation will figure out a way to drop it into a PCIe adapter to sell it as a complete package. It&amp;#39;ll be an interesting piece of localllama HW.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m60ahf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "--dany--",
          "discussion_type": null,
          "num_comments": 52,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m60ahf/used_a100_40gb_just_dropped_below_2000_for_those/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m60ahf/used_a100_40gb_just_dropped_below_2000_for_those/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753145404,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I created this sandbox to test LLMs and their real-time decision-making processes. Running it has generated some interesting outputs, and I'm curious to see if others find the same. PRs accepted and encouraged!",
          "author_fullname": "t2_5n8i2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running LLMs against a sandbox airport to see if they can make the correct decisions in real time",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m62vbw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=44392f6267e504eff05965daa4cd423000d27a80",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753152819,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I created this sandbox to test LLMs and their real-time decision-making processes. Running it has generated some interesting outputs, and I&amp;#39;m curious to see if others find the same. PRs accepted and encouraged!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/jjasghar/ai-airport-simulation",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?auto=webp&amp;s=27c081b855fd81984a0fbd5a3cd8d041afd40a2a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e5cedfeb2acc17ed96c354aea24f51d83b107d8",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f04243269cf381d5d666e8ad9cc1f63960e31ef9",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b4b0fca68877cd9584992c2a2b35a39c83a82f6c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c17ae1c6715cde69de4cb21dd94c66e0f2a16d0b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=04a1b374daef1cc2d36172a124f4af1e43e7a5c7",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df058f8c50256c5ab0fda05e61968a5c791096db",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m62vbw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jjasghar",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m62vbw/running_llms_against_a_sandbox_airport_to_see_if/",
          "stickied": false,
          "url": "https://github.com/jjasghar/ai-airport-simulation",
          "subreddit_subscribers": 502720,
          "created_utc": 1753152819,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "_A Polish programmer running on fumes recently accomplished what may soon become impossible: beating an advanced AI model from OpenAI in a head-to-head coding competition. The 10-hour marathon left him \"completely exhausted.\"_\n\nhttps://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/",
          "author_fullname": "t2_1gnii9bkc9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Exhausted man defeats AI model in world coding championship",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5r9ss",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 135,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 135,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753123500,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;A Polish programmer running on fumes recently accomplished what may soon become impossible: beating an advanced AI model from OpenAI in a head-to-head coding competition. The 10-hour marathon left him &amp;quot;completely exhausted.&amp;quot;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/\"&gt;https://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?auto=webp&amp;s=01c6c5989382448ceacfb16d4716e1d43882c07d",
                  "width": 1152,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c56d051b4b4e63e5c6627f8639b6bc541ebe7a70",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=06c2478ecab47c428699645361b3f004e394ce98",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3dcfa8df2a1e6cafbca016c61a6781fc1bd66b6e",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=089271aa272fc9321b33a83d2db06a95c38b6ce9",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=73adafa18daac7bc6b0c9c52f9fe6f72db681c41",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d3d93ef0ea7ac95a421f4d7a2e7e1d2f2350aac9",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5r9ss",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Educational_Sun_8813",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5r9ss/exhausted_man_defeats_ai_model_in_world_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5r9ss/exhausted_man_defeats_ai_model_in_world_coding/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753123500,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vgnr5u5gg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "If Qwen3-235B-A22B-2507 can't think, why does it think when the thinking button is on?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 36,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m650ow",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/eOMW_Oov8IQoYGj2MLhSVhdZR_G_D49PS5lS7HSSsnk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753159541,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/lxwf5fgevcef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?auto=webp&amp;s=bcac025445c3e19050b77650ec0313d164d2cb63",
                  "width": 696,
                  "height": 181
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=93400e194109a5036a8e420d94408434e9409fa7",
                    "width": 108,
                    "height": 28
                  },
                  {
                    "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=904c5d55682c555ac50948455d22e99da1ab864b",
                    "width": 216,
                    "height": 56
                  },
                  {
                    "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae695aac40a41943b105be24e34d521e45b9b0b3",
                    "width": 320,
                    "height": 83
                  },
                  {
                    "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf2250dfec91bd4da5ce280844d385d4702942d1",
                    "width": 640,
                    "height": 166
                  }
                ],
                "variants": {},
                "id": "9dzCnXa9HN8TH6BqvCudoVpKpcesYzMwj4FmuXf8IdA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m650ow",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JeffreySons_90",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m650ow/if_qwen3235ba22b2507_cant_think_why_does_it_think/",
          "stickied": false,
          "url": "https://i.redd.it/lxwf5fgevcef1.jpeg",
          "subreddit_subscribers": 502720,
          "created_utc": 1753159541,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[Mind-Blowing](https://preview.redd.it/7by2astxg9ef1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=ed2caaa4b854693b6fd46383a9626aefe87b0128)",
          "author_fullname": "t2_1tbloqabyg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-2507!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "7by2astxg9ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c498a4606859ea7dc3344cea600a6287666c52bb"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a0f911df63c617cf62eddf44f25fd9d1526f4f9"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=18f18ad0a953d5f6cce28f16501265fc5f1b65fa"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6a2c30752b5e9e36ee257e083921c23b7bd51214"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce6ecb7eb20a5950434de688e9e3e318d8dcf552"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b198e562025147aead0f6a4065ef96012102dc8e"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/7by2astxg9ef1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=ed2caaa4b854693b6fd46383a9626aefe87b0128"
              },
              "id": "7by2astxg9ef1"
            }
          },
          "name": "t3_1m5oz0h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 147,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 147,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/sK-ChiNoLnwj2ggKTtNTJYTvWhnsGqdGF-BtGXWSrIM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753118398,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/7by2astxg9ef1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ed2caaa4b854693b6fd46383a9626aefe87b0128\"&gt;Mind-Blowing&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5oz0h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ken-senseii",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5oz0h/qwen3235ba22b2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5oz0h/qwen3235ba22b2507/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753118398,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello there,\n\nMy project to extract and collect the \"secret\" system prompts from a bunch of proprietary AI tools just passed 70k stars on GitHub, and I wanted to share it with this community specifically because I think it's incredibly useful.\n\n**The idea is to see the advanced \"prompt architecture\" that companies like Vercel, Cursor, etc., use to get high-quality results, so we can replicate those techniques on different platforms.**\n\nInstead of trying to reinvent the wheel, you can see exactly how they force models to \"think step-by-step\" in a scratchpad, how they define an expert persona with hyper-specific rules, or how they demand rigidly structured outputs. It's a goldmine of ideas for crafting better system prompts.\n\nFor example, here's a small snippet from the Cursor prompt that shows how they establish the AI's role and capabilities right away:\n\n    Knowledge cutoff: 2024-06\n    \n    You are an AI coding assistant, powered by GPT-4.1. You operate in Cursor. \n    \n    You are pair programming with a USER to solve their coding task. Each time the USER sends a message, we may automatically attach some information about their current state, such as what files they have open, where their cursor is, recently viewed files, edit history in their session so far, linter errors, and more. This information may or may not be relevant to the coding task, it is up for you to decide.\n    \n    You are an agent - please keep going until the user's query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved. Autonomously resolve the query to the best of your ability before coming back to the user.\n    \n    Your main goal is to follow the USER's instructions at each message, denoted by the &lt;user_query&gt; tag.\n    \n    &lt;communication&gt;\n    When using markdown in assistant messages, use backticks to format file, directory, function, and class names. Use \\( and \\) for inline math, \\[ and \\] for block math.\n    &lt;/communication&gt;\n\nI wrote a full article that does a deep dive into these patterns and also discusses the \"dual-use\" aspect of making these normally-hidden prompts public.\n\nI'm super curious: **How are you all structuring system prompts for your favorite models?**\n\n**Links:**\n\n* **The full article with more analysis:** [The Open Source Project That Became an Essential Library for Modern AI Engineering](https://medium.com/@lucknitelol/the-open-source-project-that-became-an-essential-library-for-modern-ai-engineering-67021b50acee?source=user_profile_page---------0-------------d9a574987030----------------------)[](https://medium.com/@lucknitelol?source=post_page---byline--67021b50acee---------------------------------------)\n\n* **The GitHub Repo (to grab the prompts):** [https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools](https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools)\n\nHope you find it useful!",
          "author_fullname": "t2_fbh7mxys2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I extracted the system prompts from closed-source tools like Cursor &amp; v0. The repo just hit 70k stars.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5gwzs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 361,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 361,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753099199,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753099002,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there,&lt;/p&gt;\n\n&lt;p&gt;My project to extract and collect the &amp;quot;secret&amp;quot; system prompts from a bunch of proprietary AI tools just passed 70k stars on GitHub, and I wanted to share it with this community specifically because I think it&amp;#39;s incredibly useful.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The idea is to see the advanced &amp;quot;prompt architecture&amp;quot; that companies like Vercel, Cursor, etc., use to get high-quality results, so we can replicate those techniques on different platforms.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Instead of trying to reinvent the wheel, you can see exactly how they force models to &amp;quot;think step-by-step&amp;quot; in a scratchpad, how they define an expert persona with hyper-specific rules, or how they demand rigidly structured outputs. It&amp;#39;s a goldmine of ideas for crafting better system prompts.&lt;/p&gt;\n\n&lt;p&gt;For example, here&amp;#39;s a small snippet from the Cursor prompt that shows how they establish the AI&amp;#39;s role and capabilities right away:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Knowledge cutoff: 2024-06\n\nYou are an AI coding assistant, powered by GPT-4.1. You operate in Cursor. \n\nYou are pair programming with a USER to solve their coding task. Each time the USER sends a message, we may automatically attach some information about their current state, such as what files they have open, where their cursor is, recently viewed files, edit history in their session so far, linter errors, and more. This information may or may not be relevant to the coding task, it is up for you to decide.\n\nYou are an agent - please keep going until the user&amp;#39;s query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved. Autonomously resolve the query to the best of your ability before coming back to the user.\n\nYour main goal is to follow the USER&amp;#39;s instructions at each message, denoted by the &amp;lt;user_query&amp;gt; tag.\n\n&amp;lt;communication&amp;gt;\nWhen using markdown in assistant messages, use backticks to format file, directory, function, and class names. Use \\( and \\) for inline math, \\[ and \\] for block math.\n&amp;lt;/communication&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I wrote a full article that does a deep dive into these patterns and also discusses the &amp;quot;dual-use&amp;quot; aspect of making these normally-hidden prompts public.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m super curious: &lt;strong&gt;How are you all structuring system prompts for your favorite models?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;The full article with more analysis:&lt;/strong&gt; &lt;a href=\"https://medium.com/@lucknitelol/the-open-source-project-that-became-an-essential-library-for-modern-ai-engineering-67021b50acee?source=user_profile_page---------0-------------d9a574987030----------------------\"&gt;The Open Source Project That Became an Essential Library for Modern AI Engineering&lt;/a&gt;&lt;a href=\"https://medium.com/@lucknitelol?source=post_page---byline--67021b50acee---------------------------------------\"&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;The GitHub Repo (to grab the prompts):&lt;/strong&gt; &lt;a href=\"https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools\"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Hope you find it useful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m5gwzs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Box-898",
          "discussion_type": null,
          "num_comments": 43,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753099002,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_rkmud0isr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The reason why local models are better/necessary.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5iymb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 265,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 265,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/_0QOwkUP9B0YXB-SAHakr_6UlxBhOQpPKlYLA0LCuiQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753104611,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/vdngpglhb8ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/vdngpglhb8ef1.png?auto=webp&amp;s=62e9d97048e91daee3582390075fb00d1887e202",
                  "width": 854,
                  "height": 504
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/vdngpglhb8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8f4c8c8ea760457e111d37a839bbe4882b86b520",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://preview.redd.it/vdngpglhb8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=adc1595b273f56bd1537f9efa03ae0704717b281",
                    "width": 216,
                    "height": 127
                  },
                  {
                    "url": "https://preview.redd.it/vdngpglhb8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fca7b7544874c7613248f95246d1810ffb9ffb7b",
                    "width": 320,
                    "height": 188
                  },
                  {
                    "url": "https://preview.redd.it/vdngpglhb8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae855f8543a7b34526b58ea6c68423bf02a9e2ac",
                    "width": 640,
                    "height": 377
                  }
                ],
                "variants": {},
                "id": "f9sIdJ3aHKBFJd8WvU4XqZxgrL8gg-EBXrNrZSagJuw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m5iymb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GPTshop_ai",
          "discussion_type": null,
          "num_comments": 139,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5iymb/the_reason_why_local_models_are_betternecessary/",
          "stickied": false,
          "url": "https://i.redd.it/vdngpglhb8ef1.png",
          "subreddit_subscribers": 502720,
          "created_utc": 1753104611,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Bye Qwen3-235B-A22B, hello Qwen3-235B-A22B-2507!\n\nAfter talking with the community and thinking it through, we decided to stop using hybrid thinking mode. Instead, we‚Äôll train Instruct and Thinking models separately so we can get the best quality possible. Today, we‚Äôre releasing Qwen3-235B-A22B-Instruct-2507 and its FP8 version for everyone.\n\nThis model performs better than our last release, and we hope you‚Äôll like it thanks to its strong overall abilities.\n\nQwen Chat: chat.qwen.ai ‚Äî just start chatting with the default model, and feel free to use the search button!",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen released Qwen3-235B-A22B-2507!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5oxyp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 124,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 124,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3Sv34Z6mz9FmhoyS3JnMLxTuxuV0E_Efi7EvfiWKcOs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753118337,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bye Qwen3-235B-A22B, hello Qwen3-235B-A22B-2507!&lt;/p&gt;\n\n&lt;p&gt;After talking with the community and thinking it through, we decided to stop using hybrid thinking mode. Instead, we‚Äôll train Instruct and Thinking models separately so we can get the best quality possible. Today, we‚Äôre releasing Qwen3-235B-A22B-Instruct-2507 and its FP8 version for everyone.&lt;/p&gt;\n\n&lt;p&gt;This model performs better than our last release, and we hope you‚Äôll like it thanks to its strong overall abilities.&lt;/p&gt;\n\n&lt;p&gt;Qwen Chat: chat.qwen.ai ‚Äî just start chatting with the default model, and feel free to use the search button!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6csu4o4wg9ef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?auto=webp&amp;s=10c54f96c9b0f8a2ead569e3e5e97915476224de",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6baea679e548efcaaac74cffb282ff70f159dd23",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=df4fdf4fdf2d8f18aa169c3917aff6a59354480a",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd168c86688eec1b15ed22e24f23dd96a0b2be9d",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=13fd449c88365fae792fbacc8076a6e633ad74e2",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e563d574e4328f00b0b38e4cd232ecc1c21ff8cb",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3dc97776dcddebc962894dcf5ba459481257122c",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "fCmGXIZe3dEk6g_UwQiEUl1wK88M5Bv1vrt320AGj8o"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5oxyp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5oxyp/qwen_released_qwen3235ba22b2507/",
          "stickied": false,
          "url": "https://i.redd.it/6csu4o4wg9ef1.jpeg",
          "subreddit_subscribers": 502720,
          "created_utc": 1753118337,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm running it with latest llama-server (llama.cpp) and with the suggested parameters (same as the non-thinking Qwen3 ones)\n\nDidn't see that with the \"old\" 235b with /no\\_think \n\nIs that expected?",
          "author_fullname": "t2_joxwuyje",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "In Qwen3-235B-A22B-Instruct-2507-UD-Q4 (unsloth) I'm seeing some \"but wait\" and related ones (like kinda questioning and answering itself), were the model seems to \"think\" (even when is a non-thinking model and I haven't setup any system prompt), have you seen something similar?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m69sb6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753177544,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running it with latest llama-server (llama.cpp) and with the suggested parameters (same as the non-thinking Qwen3 ones)&lt;/p&gt;\n\n&lt;p&gt;Didn&amp;#39;t see that with the &amp;quot;old&amp;quot; 235b with /no_think &lt;/p&gt;\n\n&lt;p&gt;Is that expected?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m69sb6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "relmny",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m69sb6/in_qwen3235ba22binstruct2507udq4_unsloth_im/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m69sb6/in_qwen3235ba22binstruct2507udq4_unsloth_im/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753177544,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I deployed Llama 3.3-70B for my organization quite a long time ago. I am now thinking of updating it to a newer model since there have been quite a few great new LLM releases recently. However, is there any model that actually performs better than Llama 3.3-70B for general purposes (chat, summarization... basically normal daily office tasks) with more or less the same size? Thanks!",
          "author_fullname": "t2_mxles3cs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thinking about updating Llama 3.3-70B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ahsu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753180144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I deployed Llama 3.3-70B for my organization quite a long time ago. I am now thinking of updating it to a newer model since there have been quite a few great new LLM releases recently. However, is there any model that actually performs better than Llama 3.3-70B for general purposes (chat, summarization... basically normal daily office tasks) with more or less the same size? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6ahsu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Only_Emergencies",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753180144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Why is no one talking about the insane simpleQA score for the new Qwen3 model? 54.3 OMG! How are they doing this with a 235ba22b model?!",
          "author_fullname": "t2_jldf8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 insane SimpleQA",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5qn1n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 71,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 71,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753122070,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why is no one talking about the insane simpleQA score for the new Qwen3 model? 54.3 OMG! How are they doing this with a 235ba22b model?!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5qn1n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gzzhongqi",
          "discussion_type": null,
          "num_comments": 40,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5qn1n/qwen3_insane_simpleqa/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5qn1n/qwen3_insane_simpleqa/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753122070,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI'm currently working on a project to fine-tune multilingual embedding models to improve document retrieval within a company's RAG system. The dataset consists of German and English documents related to industrial products, so multilingual support is essential. The dataset has a query-passage format with synthetic generated queries from the given documens.\n\n¬†\n\nRequirements:\n\n* Multilingual (German &amp; English)\n* Max. 7B parameters\n* Preferably compatible with Sentence-Transformers\n* Open-source\n\n¬†\n\nModels based on MTEB Retrieval performance:\n\n[http://mteb-leaderboard.hf.space/?benchmark\\_name=MTEB%28Multilingual%2C+v2%29](http://mteb-leaderboard.hf.space/?benchmark_name=MTEB%28Multilingual%2C+v2%29)\n\n* Qwen Embedding 8B / 4B\n* SFR-Embedding-Mistral\n* E5-mistral-7b-instruct\n* Snowflake-arctic-embed-m-v2.0\n\n¬†\n\nI also read some papers and found that the following models were frequently used for fine-tuning embedding models for closed-domain use cases:\n\n* BGE (all variants)\n* mE5\n* All-MiniLM-L6-v1.5\n* Text-Embedding-3-Large (often used as a baseline)\n\n¬†\n\nWould love to hear your thoughts or experiences, especially if you've worked on similar multilingual or domain-specific retrieval systems!\n\n",
          "author_fullname": "t2_k6flr0wx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fine-Tuning Multilingual Embedding Models for Industrial RAG System",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m68elw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753172119,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on a project to fine-tune multilingual embedding models to improve document retrieval within a company&amp;#39;s RAG system. The dataset consists of German and English documents related to industrial products, so multilingual support is essential. The dataset has a query-passage format with synthetic generated queries from the given documens.&lt;/p&gt;\n\n&lt;p&gt;¬†&lt;/p&gt;\n\n&lt;p&gt;Requirements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Multilingual (German &amp;amp; English)&lt;/li&gt;\n&lt;li&gt;Max. 7B parameters&lt;/li&gt;\n&lt;li&gt;Preferably compatible with Sentence-Transformers&lt;/li&gt;\n&lt;li&gt;Open-source&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;¬†&lt;/p&gt;\n\n&lt;p&gt;Models based on MTEB Retrieval performance:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://mteb-leaderboard.hf.space/?benchmark_name=MTEB%28Multilingual%2C+v2%29\"&gt;http://mteb-leaderboard.hf.space/?benchmark_name=MTEB%28Multilingual%2C+v2%29&lt;/a&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qwen Embedding 8B / 4B&lt;/li&gt;\n&lt;li&gt;SFR-Embedding-Mistral&lt;/li&gt;\n&lt;li&gt;E5-mistral-7b-instruct&lt;/li&gt;\n&lt;li&gt;Snowflake-arctic-embed-m-v2.0&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;¬†&lt;/p&gt;\n\n&lt;p&gt;I also read some papers and found that the following models were frequently used for fine-tuning embedding models for closed-domain use cases:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;BGE (all variants)&lt;/li&gt;\n&lt;li&gt;mE5&lt;/li&gt;\n&lt;li&gt;All-MiniLM-L6-v1.5&lt;/li&gt;\n&lt;li&gt;Text-Embedding-3-Large (often used as a baseline)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;¬†&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your thoughts or experiences, especially if you&amp;#39;ve worked on similar multilingual or domain-specific retrieval systems!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?auto=webp&amp;s=1c3659b9b728ad6e80974340870a81fcaca748a0",
                  "width": 2473,
                  "height": 1280
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2e4403396b3271cb41e3343fd3daf2f432ae3c37",
                    "width": 108,
                    "height": 55
                  },
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=afdae3f5636f14bbbbdb4a9c9476beaba37383bf",
                    "width": 216,
                    "height": 111
                  },
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bcf9a9a3b91639d396787d20b125c83d45d2dd41",
                    "width": 320,
                    "height": 165
                  },
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bb4fac65e86637fc3c5896156424954a65818fc",
                    "width": 640,
                    "height": 331
                  },
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1321b3e29e18141a9cf9837f0ec1dea4f236eabf",
                    "width": 960,
                    "height": 496
                  },
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=271a28cd1ea8c4260dcf2d7f9dcf3842182678c4",
                    "width": 1080,
                    "height": 558
                  }
                ],
                "variants": {},
                "id": "b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m68elw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Maddin187",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m68elw/finetuning_multilingual_embedding_models_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m68elw/finetuning_multilingual_embedding_models_for/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753172119,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_fmd6oq5v6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-235B-A22B-Instruct-2507 ¬∑ Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5pbj0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": "#bbbdbf",
          "ups": 75,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 75,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=68ff5489be2450ce2200e81da6540a1dd25ed70a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753119154,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?auto=webp&amp;s=86c8fc358fab39d0103c80888dc78d172e254fd0",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=41efd73e0b1f2f6245cc18321de9593d2f691f2a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ad4906ae01be77390f3512429fd8d526cdbff6b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc710aa210ed6e6fd519830b6e1c20372358b8dc",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=981c1b88ac04632c811f86e43d24143c128aa1a3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a081e0d80600d61cab0535bcecc71bbb337e1d1f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47fe182351ca7738d227c54e9f94e68766055478",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5pbj0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "random-tomato",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m5pbj0/qwenqwen3235ba22binstruct2507_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507",
          "subreddit_subscribers": 502720,
          "created_utc": 1753119154,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "TL;DR: This is a **massive** step forward for first-time users. You can now get everything up and running with a single .exe or .dmg download‚Äîno command line or Docker needed. It's never been easier to start building your own local, privacy-first screen-watching agents!\n\n  \nHey r/LocalLLaMA !!\n\nI am suuuper excited to share the desktop launcher app I made for Observer!!! no more docker-compose if you don't want to!!\n\n**What's new in this update:**\n\n* üöÄ¬†**1-Click Desktop App:**¬†The number one request is here! A simple, downloadable desktop application for a native and smooth setup experience.\n* üîî¬†**Pushover &amp; Discord Notifications:**¬†SMS and Whatsapp proved to be unreliable, so you can now send alerts directly from your agents to your phone with **Pushover** or to your community with a **Discord** bot. **Email** stays being reliable!!\n* üõ†Ô∏è¬†**Continuous Improvement:**¬†My goal is to make local AI agents accessible to everyone, and your feedback is making that happen.\n\nFor those new to the project, Observer AI is an open-source tool that lets you run local micro-agents that can see your screen, listen to your mic, and perform actions, all while keeping your data 100% private.\n\nI don't want to sound super self-promotey, but I really genuinely wanted to share my excitement with the communities that have been so supportive. Thank you for being a part of this!\n\n**Check it out and let me know what you think:**\n\n[**https://github.com/Roy3838/Observer**](https://github.com/Roy3838/Observer)",
          "author_fullname": "t2_p443m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The Observer Desktop App is Here! + Discord/Pushover Notifications!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5y9wj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 25,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753139895,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR: This is a &lt;strong&gt;massive&lt;/strong&gt; step forward for first-time users. You can now get everything up and running with a single .exe or .dmg download‚Äîno command line or Docker needed. It&amp;#39;s never been easier to start building your own local, privacy-first screen-watching agents!&lt;/p&gt;\n\n&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; !!&lt;/p&gt;\n\n&lt;p&gt;I am suuuper excited to share the desktop launcher app I made for Observer!!! no more docker-compose if you don&amp;#39;t want to!!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What&amp;#39;s new in this update:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;üöÄ¬†&lt;strong&gt;1-Click Desktop App:&lt;/strong&gt;¬†The number one request is here! A simple, downloadable desktop application for a native and smooth setup experience.&lt;/li&gt;\n&lt;li&gt;üîî¬†&lt;strong&gt;Pushover &amp;amp; Discord Notifications:&lt;/strong&gt;¬†SMS and Whatsapp proved to be unreliable, so you can now send alerts directly from your agents to your phone with &lt;strong&gt;Pushover&lt;/strong&gt; or to your community with a &lt;strong&gt;Discord&lt;/strong&gt; bot. &lt;strong&gt;Email&lt;/strong&gt; stays being reliable!!&lt;/li&gt;\n&lt;li&gt;üõ†Ô∏è¬†&lt;strong&gt;Continuous Improvement:&lt;/strong&gt;¬†My goal is to make local AI agents accessible to everyone, and your feedback is making that happen.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For those new to the project, Observer AI is an open-source tool that lets you run local micro-agents that can see your screen, listen to your mic, and perform actions, all while keeping your data 100% private.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want to sound super self-promotey, but I really genuinely wanted to share my excitement with the communities that have been so supportive. Thank you for being a part of this!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Check it out and let me know what you think:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Roy3838/Observer\"&gt;&lt;strong&gt;https://github.com/Roy3838/Observer&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?auto=webp&amp;s=a9ed130dbe40bc283accf677a568089896baa4f1",
                  "width": 4030,
                  "height": 2260
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2763f5b07d8000852738cc8bbf6420bc7a793d3e",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b05a1b6a5908644048b0f050c15a00d2bc5d9ed",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3f92bd549dcbc4e5089662042619f32c668a07e",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=29ad9d2d4f08a9916f026e28e9a30fd6d1711d5d",
                    "width": 640,
                    "height": 358
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=adab929349280395b1958efc7ed9e9e58d447654",
                    "width": 960,
                    "height": 538
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f25dcc577bb8503c95962d2f130fa431008cd692",
                    "width": 1080,
                    "height": 605
                  }
                ],
                "variants": {},
                "id": "-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5y9wj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Roy3838",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5y9wj/the_observer_desktop_app_is_here_discordpushover/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5y9wj/the_observer_desktop_app_is_here_discordpushover/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753139895,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nFirstly, I‚Äôm not a developer, so forgive me if I don‚Äôt ask as clearly as others, I hope this makes sense.\n\nI'm trying to get Chatterbox TTS ( local AI voice tool with Gradio UI) working on my **Windows 11** machine using **Conda** and a local **Python 3.11.3 environment**. I‚Äôve installed the app and interface successfully, but I‚Äôm stuck with import errors and GPU not being used. Here‚Äôs the key info:\n\n* **GPU:** RTX 4060 (8GB), CUDA 12.7 installed\n* **Python:** 3.11.3 (inside Conda)\n* **PyTorch:** Installed via pip/conda (tried both), but errors persist\n* **TorchAudio:** Likely not aligned with correct PyTorch/CUDA version\n* **Gradio UI:** Loads, but model doesn't run (import error)\n\nThe critical error:\n\nlua\n\nCopyEdit\n\nImportError: DLL load failed while importing \\_C: The specified module could not be found.\n\nI understand this might be due to mismatched **PyTorch / CUDA / TorchAudio** versions ‚Äî but the **CUDA 12.7 runtime** doesn't show up on most PyTorch install tables (latest listed is 12.1).\n\n**Questions:**\n\n1. **Can I safely use a PyTorch build meant for CUDA 12.1 if I have 12.7 installed?**\n2. **Which PyTorch + TorchAudio versions are guaranteed to work together (and with Chatterbox) under CUDA 12.7?**\n3. Is there a known *minimal install combo* that just works?\n4. Should I downgrade CUDA to 12.1, or can I work with what I have?\n\nI‚Äôm not a developer, so detailed explanations or clear steps would be hugely appreciated. Thanks in advance!",
          "author_fullname": "t2_94iwvjrz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chatterbox CUDA and PyTorch problem",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6cfou",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753186390,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Firstly, I‚Äôm not a developer, so forgive me if I don‚Äôt ask as clearly as others, I hope this makes sense.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to get Chatterbox TTS ( local AI voice tool with Gradio UI) working on my &lt;strong&gt;Windows 11&lt;/strong&gt; machine using &lt;strong&gt;Conda&lt;/strong&gt; and a local &lt;strong&gt;Python 3.11.3 environment&lt;/strong&gt;. I‚Äôve installed the app and interface successfully, but I‚Äôm stuck with import errors and GPU not being used. Here‚Äôs the key info:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; RTX 4060 (8GB), CUDA 12.7 installed&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Python:&lt;/strong&gt; 3.11.3 (inside Conda)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;PyTorch:&lt;/strong&gt; Installed via pip/conda (tried both), but errors persist&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;TorchAudio:&lt;/strong&gt; Likely not aligned with correct PyTorch/CUDA version&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Gradio UI:&lt;/strong&gt; Loads, but model doesn&amp;#39;t run (import error)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The critical error:&lt;/p&gt;\n\n&lt;p&gt;lua&lt;/p&gt;\n\n&lt;p&gt;CopyEdit&lt;/p&gt;\n\n&lt;p&gt;ImportError: DLL load failed while importing _C: The specified module could not be found.&lt;/p&gt;\n\n&lt;p&gt;I understand this might be due to mismatched &lt;strong&gt;PyTorch / CUDA / TorchAudio&lt;/strong&gt; versions ‚Äî but the &lt;strong&gt;CUDA 12.7 runtime&lt;/strong&gt; doesn&amp;#39;t show up on most PyTorch install tables (latest listed is 12.1).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Can I safely use a PyTorch build meant for CUDA 12.1 if I have 12.7 installed?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Which PyTorch + TorchAudio versions are guaranteed to work together (and with Chatterbox) under CUDA 12.7?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Is there a known &lt;em&gt;minimal install combo&lt;/em&gt; that just works?&lt;/li&gt;\n&lt;li&gt;Should I downgrade CUDA to 12.1, or can I work with what I have?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I‚Äôm not a developer, so detailed explanations or clear steps would be hugely appreciated. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6cfou",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kevin-she",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6cfou/chatterbox_cuda_and_pytorch_problem/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6cfou/chatterbox_cuda_and_pytorch_problem/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753186390,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Kimi K2 is a beast!  Both in performance and to run.   Ernie is much smaller and easier to run.  It's 47B active, so going to be a bit slower, however it performs quite well.  I would call it K2's little brother, I think it got overshadowed by K2 especially since K2 was the claude sonnet 4 and open weight OpenAI killer.  It took longer to also get support for it into llama.cpp  \nI have been testing it out and I really like it.   For general chat, (logically, scientific, mathematically), it's straight to the point, doesn't beat around the bush or hew and haw.  Great instruction following too, very precise and to the point.  I haven't heard much about it, and I know that many can't run it, but you should really consider it and add it to the mix.   Get the parameters right too, my first runs were meh, and then I had to go find the recommended parameters, I haven't experimented much with them, but there might even be better.  I'm running Q6 from unsloth. temp/top\\_p 0.8, top\\_k 50, min\\_p 0.01",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do not sleep on ERNIE-4.5-300B-A47B especially if you can't Kimi K2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5p69p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 62,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 62,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753118830,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kimi K2 is a beast!  Both in performance and to run.   Ernie is much smaller and easier to run.  It&amp;#39;s 47B active, so going to be a bit slower, however it performs quite well.  I would call it K2&amp;#39;s little brother, I think it got overshadowed by K2 especially since K2 was the claude sonnet 4 and open weight OpenAI killer.  It took longer to also get support for it into llama.cpp&lt;br/&gt;\nI have been testing it out and I really like it.   For general chat, (logically, scientific, mathematically), it&amp;#39;s straight to the point, doesn&amp;#39;t beat around the bush or hew and haw.  Great instruction following too, very precise and to the point.  I haven&amp;#39;t heard much about it, and I know that many can&amp;#39;t run it, but you should really consider it and add it to the mix.   Get the parameters right too, my first runs were meh, and then I had to go find the recommended parameters, I haven&amp;#39;t experimented much with them, but there might even be better.  I&amp;#39;m running Q6 from unsloth. temp/top_p 0.8, top_k 50, min_p 0.01&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5p69p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m5p69p/do_not_sleep_on_ernie45300ba47b_especially_if_you/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5p69p/do_not_sleep_on_ernie45300ba47b_especially_if_you/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753118830,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I believe this is the first NPU specifically designed for LLM inference. They specifically mention 2.5 or 5GB of \"ultra high bandwidth memory\", but not the actual speed. 50TPS for a 7B model at Q4 implies around 200GB/s. The high prompt processing speed is the best part IMO, it's going to let an on device assistant use a lot more context.",
          "author_fullname": "t2_lkljr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Rockchip unveils RK182X LLM co-processor: Runs Qwen 2.5 7B at 50TPS decode, 800TPS prompt processing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 102,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5fmlp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 145,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 145,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=140&amp;height=102&amp;crop=140:102,smart&amp;auto=webp&amp;s=f46b99f9160b02acaab35b0793c2419a717f902a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753094763,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "cnx-software.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I believe this is the first NPU specifically designed for LLM inference. They specifically mention 2.5 or 5GB of &amp;quot;ultra high bandwidth memory&amp;quot;, but not the actual speed. 50TPS for a 7B model at Q4 implies around 200GB/s. The high prompt processing speed is the best part IMO, it&amp;#39;s going to let an on device assistant use a lot more context.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.cnx-software.com/2025/07/18/rockchip-unveils-rk3668-10-core-arm-cortex-a730-cortex-a530-soc-with-16-tops-npu-rk182x-llm-vlm-co-processor/#rockchip-rk182x-llm-vlm-accelerator",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?auto=webp&amp;s=a3357893385aa57e61c85776502b465fe73661a4",
                  "width": 1184,
                  "height": 868
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19d2a2efbd9333bbc8e7495d96c37dc9a67f94f7",
                    "width": 108,
                    "height": 79
                  },
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4320f14e39faee0bd0ec022e73a0260326f90d4",
                    "width": 216,
                    "height": 158
                  },
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=36df6ea44afc6e837f534a02a66625d799b64a88",
                    "width": 320,
                    "height": 234
                  },
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0ec21866ca44554bfe2c9ada5521c937f241afc3",
                    "width": 640,
                    "height": 469
                  },
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3f1fc5136f191685dd65bd8cc69f5a732d025a2",
                    "width": 960,
                    "height": 703
                  },
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=24e96efb97efa4c62ad46fa709863ab37c9c8266",
                    "width": 1080,
                    "height": 791
                  }
                ],
                "variants": {},
                "id": "p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5fmlp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PmMeForPCBuilds",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/",
          "stickied": false,
          "url": "https://www.cnx-software.com/2025/07/18/rockchip-unveils-rk3668-10-core-arm-cortex-a730-cortex-a530-soc-with-16-tops-npu-rk182x-llm-vlm-co-processor/#rockchip-rk182x-llm-vlm-accelerator",
          "subreddit_subscribers": 502720,
          "created_utc": 1753094763,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Inspired by the brain's hierarchical processing, HRM unlocks unprecedented reasoning capabilities on complex tasks like ARC-AGI and solving master-level Sudoku using just 1k training examples, without any pretraining or CoT.\n\nThough not a general language model yet, with significant computational depth, HRM possibly unlocks next-gen reasoning and long-horizon planning paradigm beyond CoT. üåü\n\nhttps://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;format=png&amp;auto=webp&amp;s=b7572924d3c565f89605da339cda1df0dc96354f\n\nüìÑPaper: [https://arxiv.org/abs/2506.21734](https://arxiv.org/abs/2506.21734)\n\nüíªCode: [https://github.com/sapientinc/HRM](https://github.com/sapientinc/HRM)  \n",
          "author_fullname": "t2_dkj51uv0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[New Architecture] Hierarchical Reasoning Model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 34,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "uslhwa2nh8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 26,
                  "x": 108,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ab6ae97e133c2649b92b0c8c0ef857c3d79bcc2"
                },
                {
                  "y": 53,
                  "x": 216,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0bc3c540c30cdf2b3b12433e760b60c3ceef936c"
                },
                {
                  "y": 79,
                  "x": 320,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=056aa9e724c8d47937b0e044af74a221fc2a9886"
                },
                {
                  "y": 158,
                  "x": 640,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7a710e5fa6141d56c1a472f9ac1de521e98a86a5"
                },
                {
                  "y": 237,
                  "x": 960,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7ccd9b2f1992c3c691f918fd9309433d1e043fe2"
                },
                {
                  "y": 267,
                  "x": 1080,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc4733d2af3bd39a37a4c49868c5e8afeca4ff94"
                }
              ],
              "s": {
                "y": 501,
                "x": 2026,
                "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;format=png&amp;auto=webp&amp;s=b7572924d3c565f89605da339cda1df0dc96354f"
              },
              "id": "uslhwa2nh8ef1"
            }
          },
          "name": "t3_1m5jr1v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 76,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 76,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/jTPeTr-ZhJfvZ_xmMKlbONUjqH188dSuwhEIvPibXE8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753106572,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Inspired by the brain&amp;#39;s hierarchical processing, HRM unlocks unprecedented reasoning capabilities on complex tasks like ARC-AGI and solving master-level Sudoku using just 1k training examples, without any pretraining or CoT.&lt;/p&gt;\n\n&lt;p&gt;Though not a general language model yet, with significant computational depth, HRM possibly unlocks next-gen reasoning and long-horizon planning paradigm beyond CoT. üåü&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7572924d3c565f89605da339cda1df0dc96354f\"&gt;https://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7572924d3c565f89605da339cda1df0dc96354f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;üìÑPaper: &lt;a href=\"https://arxiv.org/abs/2506.21734\"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;üíªCode: &lt;a href=\"https://github.com/sapientinc/HRM\"&gt;https://github.com/sapientinc/HRM&lt;/a&gt;  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5jr1v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "imonenext",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753106572,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm experimenting with multiple MCP servers and trying to understand how others are managing them across different AI tools like Claude Desktop, GPTs, Gemini clients, etc.\n\nDo you manually add them in each config file?\n\nAre you using any centralized tool or dashboard to start/stop/edit MCP servers?\n\nAny best practices or tooling you recommend?\n\nüëâ I‚Äôm currently building a lightweight desktop tool that aims to solve this ‚Äî centralized MCP management, multi-client compatibility, and better UX for non-technical users.\n\nWould love to hear how you currently do it ‚Äî and what you‚Äôd want in a tool like this. Would anyone be interested in testing the beta later on?\n\nThanks in advance!",
          "author_fullname": "t2_9f7exri8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "üß† How are you managing MCP servers across different AI apps (Claude, GPTs, Gemini etc.)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m69qs3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753177386,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm experimenting with multiple MCP servers and trying to understand how others are managing them across different AI tools like Claude Desktop, GPTs, Gemini clients, etc.&lt;/p&gt;\n\n&lt;p&gt;Do you manually add them in each config file?&lt;/p&gt;\n\n&lt;p&gt;Are you using any centralized tool or dashboard to start/stop/edit MCP servers?&lt;/p&gt;\n\n&lt;p&gt;Any best practices or tooling you recommend?&lt;/p&gt;\n\n&lt;p&gt;üëâ I‚Äôm currently building a lightweight desktop tool that aims to solve this ‚Äî centralized MCP management, multi-client compatibility, and better UX for non-technical users.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear how you currently do it ‚Äî and what you‚Äôd want in a tool like this. Would anyone be interested in testing the beta later on?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m69qs3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hihurmuz",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m69qs3/how_are_you_managing_mcp_servers_across_different/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m69qs3/how_are_you_managing_mcp_servers_across_different/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753177386,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yo so am new to this and i want to run a local llm that answers questions using my custom dataset which is basically some financial data .\nI created a Q&amp;A dataset and an instruction based data set and my llm refuses to use them \nIve finetuned my llm using TorchTune \nAnd also tried Litgpt \nIts a llama 3.2 3B instruct model .\n\nAlso if theres a way to use a RAG instead or if there's a model that can retrieve info from pdf and Excel spreadsheets would be awesome, thanks üëç",
          "author_fullname": "t2_5ecncz67",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to apply a custom dataset",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m69m60",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753176910,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yo so am new to this and i want to run a local llm that answers questions using my custom dataset which is basically some financial data .\nI created a Q&amp;amp;A dataset and an instruction based data set and my llm refuses to use them \nIve finetuned my llm using TorchTune \nAnd also tried Litgpt \nIts a llama 3.2 3B instruct model .&lt;/p&gt;\n\n&lt;p&gt;Also if theres a way to use a RAG instead or if there&amp;#39;s a model that can retrieve info from pdf and Excel spreadsheets would be awesome, thanks üëç&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m69m60",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "oG17DoGe",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m69m60/how_to_apply_a_custom_dataset/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m69m60/how_to_apply_a_custom_dataset/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753176910,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've seen a few people asking whether [GPUStack](https://github.com/gpustack/gpustack) is essentially a multi-node version of Ollama. I‚Äôve used both, and here‚Äôs a breakdown for anyone curious.\n\n**Short answer:** GPUStack is *not just* Ollama with clustering ‚Äî it's a more general-purpose, production-ready LLM service platform with multi-backend support, hybrid GPU/OS compatibility, and cluster management features.\n\n# Core Differences\n\n|Feature|Ollama|GPUStack|\n|:-|:-|:-|\n|Single-node use|‚úÖ Yes|‚úÖ Yes|\n|Multi-node cluster|‚ùå|‚úÖ Supports distributed + heterogeneous cluster|\n|Model formats|GGUF only|GGUF (llama-box), Safetensors (vLLM), Ascend (MindIE), Audio (vox-box)|\n|Inference backends|llama.cpp|llama-box, vLLM, MindIE, vox-box|\n|OpenAI-compatible API|‚úÖ|‚úÖ Full API compatibility (/v1, /v1-openai)|\n|Deployment methods|CLI only|Script / Docker / pip (Linux, Windows, macOS)|\n|Cluster management UI|‚ùå|‚úÖ Web UI with GPU/worker/model status|\n|Model recovery/failover|‚ùå|‚úÖ Auto recovery + compatibility checks|\n|Use in Dify / RAGFlow|Partial|‚úÖ Fully integrated|\n\n# Who is GPUStack for?\n\nIf you:\n\n* Have multiple PCs or GPU servers\n* Want to centrally manage model serving\n* Need both GGUF and safetensors support\n* Run LLMs in production with monitoring, load balancing, or distributed inference\n\n...then it‚Äôs worth checking out.\n\n# Installation (Linux)\n\n    bashCopyEditcurl -sfL https://get.gpustack.ai | sh -s -\n    \n\nDocker (recommended):\n\n    bashCopyEditdocker run -d --name gpustack \\\n      --restart=unless-stopped \\\n      --gpus all \\\n      --network=host \\\n      --ipc=host \\\n      -v gpustack-data:/var/lib/gpustack \\\n      gpustack/gpustack\n    \n\nThen add workers with:\n\n    bashCopyEditgpustack start --server-url http://your_gpustack_url --token your_gpustack_token\n    \n\nGitHub: [https://github.com/gpustack/gpustack](https://github.com/gpustack/gpustack)  \nDocs: [https://docs.gpustack.ai](https://docs.gpustack.ai)\n\nLet me know if you‚Äôre running a local LLM cluster ‚Äî curious what stacks others are using.",
          "author_fullname": "t2_uexbcvjr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is GPUStack the Cluster Version of Ollama? Comparison + Alternatives",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m67a12",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753167757,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen a few people asking whether &lt;a href=\"https://github.com/gpustack/gpustack\"&gt;GPUStack&lt;/a&gt; is essentially a multi-node version of Ollama. I‚Äôve used both, and here‚Äôs a breakdown for anyone curious.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Short answer:&lt;/strong&gt; GPUStack is &lt;em&gt;not just&lt;/em&gt; Ollama with clustering ‚Äî it&amp;#39;s a more general-purpose, production-ready LLM service platform with multi-backend support, hybrid GPU/OS compatibility, and cluster management features.&lt;/p&gt;\n\n&lt;h1&gt;Core Differences&lt;/h1&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Feature&lt;/th&gt;\n&lt;th align=\"left\"&gt;Ollama&lt;/th&gt;\n&lt;th align=\"left\"&gt;GPUStack&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Single-node use&lt;/td&gt;\n&lt;td align=\"left\"&gt;‚úÖ Yes&lt;/td&gt;\n&lt;td align=\"left\"&gt;‚úÖ Yes&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Multi-node cluster&lt;/td&gt;\n&lt;td align=\"left\"&gt;‚ùå&lt;/td&gt;\n&lt;td align=\"left\"&gt;‚úÖ Supports distributed + heterogeneous cluster&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Model formats&lt;/td&gt;\n&lt;td align=\"left\"&gt;GGUF only&lt;/td&gt;\n&lt;td align=\"left\"&gt;GGUF (llama-box), Safetensors (vLLM), Ascend (MindIE), Audio (vox-box)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Inference backends&lt;/td&gt;\n&lt;td align=\"left\"&gt;llama.cpp&lt;/td&gt;\n&lt;td align=\"left\"&gt;llama-box, vLLM, MindIE, vox-box&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenAI-compatible API&lt;/td&gt;\n&lt;td align=\"left\"&gt;‚úÖ&lt;/td&gt;\n&lt;td align=\"left\"&gt;‚úÖ Full API compatibility (/v1, /v1-openai)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Deployment methods&lt;/td&gt;\n&lt;td align=\"left\"&gt;CLI only&lt;/td&gt;\n&lt;td align=\"left\"&gt;Script / Docker / pip (Linux, Windows, macOS)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Cluster management UI&lt;/td&gt;\n&lt;td align=\"left\"&gt;‚ùå&lt;/td&gt;\n&lt;td align=\"left\"&gt;‚úÖ Web UI with GPU/worker/model status&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Model recovery/failover&lt;/td&gt;\n&lt;td align=\"left\"&gt;‚ùå&lt;/td&gt;\n&lt;td align=\"left\"&gt;‚úÖ Auto recovery + compatibility checks&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Use in Dify / RAGFlow&lt;/td&gt;\n&lt;td align=\"left\"&gt;Partial&lt;/td&gt;\n&lt;td align=\"left\"&gt;‚úÖ Fully integrated&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Who is GPUStack for?&lt;/h1&gt;\n\n&lt;p&gt;If you:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Have multiple PCs or GPU servers&lt;/li&gt;\n&lt;li&gt;Want to centrally manage model serving&lt;/li&gt;\n&lt;li&gt;Need both GGUF and safetensors support&lt;/li&gt;\n&lt;li&gt;Run LLMs in production with monitoring, load balancing, or distributed inference&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;...then it‚Äôs worth checking out.&lt;/p&gt;\n\n&lt;h1&gt;Installation (Linux)&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;bashCopyEditcurl -sfL https://get.gpustack.ai | sh -s -\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Docker (recommended):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;bashCopyEditdocker run -d --name gpustack \\\n  --restart=unless-stopped \\\n  --gpus all \\\n  --network=host \\\n  --ipc=host \\\n  -v gpustack-data:/var/lib/gpustack \\\n  gpustack/gpustack\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then add workers with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;bashCopyEditgpustack start --server-url http://your_gpustack_url --token your_gpustack_token\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/gpustack/gpustack\"&gt;https://github.com/gpustack/gpustack&lt;/a&gt;&lt;br/&gt;\nDocs: &lt;a href=\"https://docs.gpustack.ai\"&gt;https://docs.gpustack.ai&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know if you‚Äôre running a local LLM cluster ‚Äî curious what stacks others are using.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?auto=webp&amp;s=36e5d5b346040aad01c2d98a3687e53ff43f1e74",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=30f5ae864bd715df1fff1d9fca1b871646878411",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d5352bbba2dab73ce92606dbbfae7b8cf522031",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7162311c2cc0d517c59c8ab11f9a534c3bda43df",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea0e488446c26927d44f14cd2b94edbfedb63b72",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8e99b36307d9c32c12637550f6a5bfaf4c4ab483",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=874fc061c61ab7b83973e9ef244a3dafd88075ba",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m67a12",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Issac_jo",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m67a12/is_gpustack_the_cluster_version_of_ollama/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m67a12/is_gpustack_the_cluster_version_of_ollama/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753167757,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking at getting this machine for running local llms. New to running them locally. Wondering if 128GB is worth it, or if the larger models start becoming too slow to make the extra memory meaningful? I would love to hear some opinions.",
          "author_fullname": "t2_y0abrfm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI 395+ 64GB vs 128GB?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5s6d1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753125511,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking at getting this machine for running local llms. New to running them locally. Wondering if 128GB is worth it, or if the larger models start becoming too slow to make the extra memory meaningful? I would love to hear some opinions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5s6d1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cfogrady",
          "discussion_type": null,
          "num_comments": 83,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753125511,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've recently started using LLMs at work and realized the incredible potential they have‚Äîespecially if I can run them locally, due to the sensitivity of client data. That got me interested in learning how to run LLMs on my own machine, as well as exploring related areas like fine-tuning, distillation, quantization, etc.\n\nRight now, I'm using an RTX 2070 with 8GB VRAM, but I'm planning to build a new PC so I can run larger models. My target build is an RTX 5090 with 256GB RAM. I‚Äôm not in the US, so second-hand GPUs are harder to find, and I can only buy from BTO PC shops‚Äîso unfortunately, dual RTX 3090 setups aren‚Äôt an option. From what I understand, this setup should allow me to run Kimi-2 at 1.8-bit precision using CPU offloading, though only at around 3 tokens per second‚Äîwhich is slow, but good for experimentation (that is still 260k tokens per day if i run it non-stop).\n\nI‚Äôve discussed the purchase with my wife, and she agreed‚Äîbut only if I can create something genuinely useful with it.\n\nSo, I want to start a personal project in my free time. The idea is to build a chatbot that can tutor my child (currently in primary school, and eventually high school). The goal is to distill a larger model like Gemma 3 27B into a smaller version (ideally 3B or 7B) that I could run on my current machine.\n\nI'm aiming for a model (or models - may break down by subjects level or humanities/STEM field) that can:\n\n1. Generate practice questions for each primary school and secondary school subjects.\n2. Explain why an answer is right or wrong.\n3. Summarize or generate key facts for learning (across math, science, humanities, etc.).\n4. Grade and give feedback on writing/compositions.\n5. Able to do translate English to Simplified Chinese and vise versa (this can be on a different model)\n\nMy current skills:\n\n* Decent Python (I use it daily at work).\n* I‚Äôve managed to get Gemma 3 4B Q4 running on Spyder (Python IDE) with GPU offloading. (This was hard and take me 1-2 days to configure my PC properly).\n\nRight now, using LLMs at home is purely for learning and experimentation. Hopefully, I can make something out of it in the future.\n\n# My main questions:\n\n1. Is a project like this realistic to complete in 3‚Äì6 months, assuming I keep learning and building during my free time? Or am I overpromising my wife and biting off more than I can chew? *Just to clarify, I don‚Äôt need this to be consumer-level software with a fancy UI and guardrails‚ÄîI just need it to be usable via a terminal where my kid can type in questions and get decent, helpful responses.*\n2. Can I realistically make this chatbot with a 3B or 7B model, or would that be too small for the use case? Do I need at least a 13B model to get high enough quality responses?\n3. Is it possible (and reasonable) to distill from Gemma 3 27B or a similar large model to achieve this goal? Would it be better to use LoRAs or fine-tuning? (I'm still learning the exact trade-offs between them.)\n\nAny thoughts, advice, or personal experiences would be really appreciated. I'm eager to learn and would love to hear from others who‚Äôve tried similar projects!",
          "author_fullname": "t2_bkb0tcya",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is this project feasible for an LLM novice? (Tutor chatbot for primary school student)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m66zhs",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753166652,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently started using LLMs at work and realized the incredible potential they have‚Äîespecially if I can run them locally, due to the sensitivity of client data. That got me interested in learning how to run LLMs on my own machine, as well as exploring related areas like fine-tuning, distillation, quantization, etc.&lt;/p&gt;\n\n&lt;p&gt;Right now, I&amp;#39;m using an RTX 2070 with 8GB VRAM, but I&amp;#39;m planning to build a new PC so I can run larger models. My target build is an RTX 5090 with 256GB RAM. I‚Äôm not in the US, so second-hand GPUs are harder to find, and I can only buy from BTO PC shops‚Äîso unfortunately, dual RTX 3090 setups aren‚Äôt an option. From what I understand, this setup should allow me to run Kimi-2 at 1.8-bit precision using CPU offloading, though only at around 3 tokens per second‚Äîwhich is slow, but good for experimentation (that is still 260k tokens per day if i run it non-stop).&lt;/p&gt;\n\n&lt;p&gt;I‚Äôve discussed the purchase with my wife, and she agreed‚Äîbut only if I can create something genuinely useful with it.&lt;/p&gt;\n\n&lt;p&gt;So, I want to start a personal project in my free time. The idea is to build a chatbot that can tutor my child (currently in primary school, and eventually high school). The goal is to distill a larger model like Gemma 3 27B into a smaller version (ideally 3B or 7B) that I could run on my current machine.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aiming for a model (or models - may break down by subjects level or humanities/STEM field) that can:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Generate practice questions for each primary school and secondary school subjects.&lt;/li&gt;\n&lt;li&gt;Explain why an answer is right or wrong.&lt;/li&gt;\n&lt;li&gt;Summarize or generate key facts for learning (across math, science, humanities, etc.).&lt;/li&gt;\n&lt;li&gt;Grade and give feedback on writing/compositions.&lt;/li&gt;\n&lt;li&gt;Able to do translate English to Simplified Chinese and vise versa (this can be on a different model)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My current skills:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Decent Python (I use it daily at work).&lt;/li&gt;\n&lt;li&gt;I‚Äôve managed to get Gemma 3 4B Q4 running on Spyder (Python IDE) with GPU offloading. (This was hard and take me 1-2 days to configure my PC properly).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Right now, using LLMs at home is purely for learning and experimentation. Hopefully, I can make something out of it in the future.&lt;/p&gt;\n\n&lt;h1&gt;My main questions:&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is a project like this realistic to complete in 3‚Äì6 months, assuming I keep learning and building during my free time? Or am I overpromising my wife and biting off more than I can chew? &lt;em&gt;Just to clarify, I don‚Äôt need this to be consumer-level software with a fancy UI and guardrails‚ÄîI just need it to be usable via a terminal where my kid can type in questions and get decent, helpful responses.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;Can I realistically make this chatbot with a 3B or 7B model, or would that be too small for the use case? Do I need at least a 13B model to get high enough quality responses?&lt;/li&gt;\n&lt;li&gt;Is it possible (and reasonable) to distill from Gemma 3 27B or a similar large model to achieve this goal? Would it be better to use LoRAs or fine-tuning? (I&amp;#39;m still learning the exact trade-offs between them.)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any thoughts, advice, or personal experiences would be really appreciated. I&amp;#39;m eager to learn and would love to hear from others who‚Äôve tried similar projects!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m66zhs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Saruphon",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m66zhs/is_this_project_feasible_for_an_llm_novice_tutor/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m66zhs/is_this_project_feasible_for_an_llm_novice_tutor/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753166652,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-235B-A22B-Instruct-2507 ¬∑ Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5oyf5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 46,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 46,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=68ff5489be2450ce2200e81da6540a1dd25ed70a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753118362,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?auto=webp&amp;s=86c8fc358fab39d0103c80888dc78d172e254fd0",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=41efd73e0b1f2f6245cc18321de9593d2f691f2a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ad4906ae01be77390f3512429fd8d526cdbff6b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc710aa210ed6e6fd519830b6e1c20372358b8dc",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=981c1b88ac04632c811f86e43d24143c128aa1a3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a081e0d80600d61cab0535bcecc71bbb337e1d1f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47fe182351ca7738d227c54e9f94e68766055478",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5oyf5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5oyf5/qwenqwen3235ba22binstruct2507_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507",
          "subreddit_subscribers": 502720,
          "created_utc": 1753118362,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_169jzqdxe5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmolLM3-3B training logs and intermediate checkpoints",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 94,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5m1et",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 49,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 49,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/4ybrjEuV5RvXq5bR2OKH4U7Osu4se3imdtVZT91_5hA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753111884,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/fcyltq1nx8ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/fcyltq1nx8ef1.png?auto=webp&amp;s=561e48b7a9066f5e3a65cb4df61d4bcee1d3623d",
                  "width": 2384,
                  "height": 1608
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7312a97ee7ebc5986c39d4b65b8d2c7104ed72bb",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=06e6cd1192b19cd320fac196b58a7f384f175290",
                    "width": 216,
                    "height": 145
                  },
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=76c1665297b63d147f74996bd94da849649689ae",
                    "width": 320,
                    "height": 215
                  },
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=992660889d501145b2a21434e2e7b0563e643863",
                    "width": 640,
                    "height": 431
                  },
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6f360d2d0886afbde91f505a0640db28051fde90",
                    "width": 960,
                    "height": 647
                  },
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8251eb5dab9d71321a09bb3fd6b9e6fc2388a343",
                    "width": 1080,
                    "height": 728
                  }
                ],
                "variants": {},
                "id": "U-FI27gby_ykbgKbgdzrNHEru7LMZIbTK_iFSM3XWRs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m5m1et",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "eliebakk",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5m1et/smollm33b_training_logs_and_intermediate/",
          "stickied": false,
          "url": "https://i.redd.it/fcyltq1nx8ef1.png",
          "subreddit_subscribers": 502720,
          "created_utc": 1753111884,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The author is StyleTTS 2 just released DMOSpeech2 - post-trained F5-TTS that‚Äôs 2x faster with improved WER and stability. Looks very interesting and open sourced with training code coming soon.\nThis is probably the last open source project we will see from the author for  a while, but looks very very interesting.",
          "author_fullname": "t2_1f194h3luj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DMOSpeech 2: 2x faster + higher-quality F5-TTS from the author of StyleTTS 2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5mzxt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 45,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=6f166dfac0527db4273e73fd28eabe37b909084c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753114028,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The author is StyleTTS 2 just released DMOSpeech2 - post-trained F5-TTS that‚Äôs 2x faster with improved WER and stability. Looks very interesting and open sourced with training code coming soon.\nThis is probably the last open source project we will see from the author for  a while, but looks very very interesting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/yl4579/DMOSpeech2",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?auto=webp&amp;s=e79caffc600940aacae822bf3d08a2972d97a9b3",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=55a782ebf4637087ab602e003b76d529f0b2b9b0",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4076be77f5945db6c69125ad169cbbf7a337377f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a24fc93b1713526b9869d4f3049face873e33bb1",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=757c96b9786749c821edc73c85d3500a7f6d30fc",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=49a731ff6d20f17d0c91af2ed14936af2a706c37",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2eb69e689a57d5034d4de02201511409b9c8993a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5mzxt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrfakename0",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5mzxt/dmospeech_2_2x_faster_higherquality_f5tts_from/",
          "stickied": false,
          "url": "https://github.com/yl4579/DMOSpeech2",
          "subreddit_subscribers": 502720,
          "created_utc": 1753114028,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1ipy2mlwcz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA Brings Reasoning Models to Consumers Ranging from 1.5B to 32B Parameters",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 111,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5fcdo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.68,
          "author_flair_background_color": null,
          "ups": 113,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 113,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=140&amp;height=111&amp;crop=140:111,smart&amp;auto=webp&amp;s=afd01d73738739fc5f4d0d7e20210d85f5c6a7af",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753093764,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "techpowerup.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.techpowerup.com/339089/nvidia-brings-reasoning-models-to-consumers-ranging-from-1-5b-to-32b-parameters",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?auto=webp&amp;s=1ab5566fa5cfc48759389d6459ab423c6a6d93ae",
                  "width": 1024,
                  "height": 815
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ac4eb006eed458f45d94d2092f2b1d96e9111985",
                    "width": 108,
                    "height": 85
                  },
                  {
                    "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4e28f32683b490d6cd796b37d8e18e66ba9e8cc5",
                    "width": 216,
                    "height": 171
                  },
                  {
                    "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d69b2795cdbb216ae0415feac75bbe4d91eed6ce",
                    "width": 320,
                    "height": 254
                  },
                  {
                    "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b8ad27cd8415b4fcbb76f37f1c26b02e1e7a72d",
                    "width": 640,
                    "height": 509
                  },
                  {
                    "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=da909b3e1331d3458fe14a0c9335539ce72f2955",
                    "width": 960,
                    "height": 764
                  }
                ],
                "variants": {},
                "id": "bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5fcdo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OwnWitness2836",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5fcdo/nvidia_brings_reasoning_models_to_consumers/",
          "stickied": false,
          "url": "https://www.techpowerup.com/339089/nvidia-brings-reasoning-models-to-consumers-ranging-from-1-5b-to-32b-parameters",
          "subreddit_subscribers": 502720,
          "created_utc": 1753093764,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "$270 for 125 generations will simply empty my pockets. It is 2$ per generation. I bought the $135 plan for 3 months.\n\nI created a Telegram bot that places a prompt in VEO 3. Therefore, you can use it on my subscription. I will charge 2$ per prompt. For the bot DM. What do you think about this business?",
          "author_fullname": "t2_dwf51xy7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "VEO 3 is f**k expensive",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6ccmr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753186148,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;$270 for 125 generations will simply empty my pockets. It is 2$ per generation. I bought the $135 plan for 3 months.&lt;/p&gt;\n\n&lt;p&gt;I created a Telegram bot that places a prompt in VEO 3. Therefore, you can use it on my subscription. I will charge 2$ per prompt. For the bot DM. What do you think about this business?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m6ccmr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Suspicious-Carry8405",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ccmr/veo_3_is_fk_expensive/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ccmr/veo_3_is_fk_expensive/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753186148,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been seeing¬†loads¬†of¬†developers¬†here¬†going¬†on¬†about how LLM integraded IDE's like Windsurf and Cursor totally changed their coding.¬†Of course, I¬†was¬†interested¬†and wanted to give it a¬†go.¬†Spoke¬†to¬†work¬†about¬†it, and the boss just¬†said \"no way dude\" GDPR-compliant and PII could be garanted (we are a bigger team, including student workers), data¬†gets¬†transferred¬†to the US, too risky, blah blah. So¬†no Cursor and Windsurf for me.\n\nHonestly, I get it. Not mad at my company they're just doing their job and don't want to¬†get¬†fined But man,¬†still sucks. We are still stuck in¬†legacy¬†workflows because every new AI tool is¬†geared¬†for¬†US devs¬†first. Feels like being left behind not because the tech¬†exists, but because we¬†simply¬†can't¬†utilize¬†it. And¬†sure, I¬†do understand¬†the GDPR¬†thing¬†is¬†big¬†deal and that there is a chanche PII and API keys included in the code by accident. But still‚Ä¶ it sucks.\n\nDoes¬†anyone else¬†get¬†stuck¬†with this?¬†Is there any other good alternatives that are similar to Cursor and Windsurf made in and for EU. What¬†are other EU devs/teams¬†doing? Self-hosting?¬†Or just¬†keeping to old tools?",
          "author_fullname": "t2_sck77urj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "EU is being left behinde and it sucks!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5n6lq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 33,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 33,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753114446,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been seeing¬†loads¬†of¬†developers¬†here¬†going¬†on¬†about how LLM integraded IDE&amp;#39;s like Windsurf and Cursor totally changed their coding.¬†Of course, I¬†was¬†interested¬†and wanted to give it a¬†go.¬†Spoke¬†to¬†work¬†about¬†it, and the boss just¬†said &amp;quot;no way dude&amp;quot; GDPR-compliant and PII could be garanted (we are a bigger team, including student workers), data¬†gets¬†transferred¬†to the US, too risky, blah blah. So¬†no Cursor and Windsurf for me.&lt;/p&gt;\n\n&lt;p&gt;Honestly, I get it. Not mad at my company they&amp;#39;re just doing their job and don&amp;#39;t want to¬†get¬†fined But man,¬†still sucks. We are still stuck in¬†legacy¬†workflows because every new AI tool is¬†geared¬†for¬†US devs¬†first. Feels like being left behind not because the tech¬†exists, but because we¬†simply¬†can&amp;#39;t¬†utilize¬†it. And¬†sure, I¬†do understand¬†the GDPR¬†thing¬†is¬†big¬†deal and that there is a chanche PII and API keys included in the code by accident. But still‚Ä¶ it sucks.&lt;/p&gt;\n\n&lt;p&gt;Does¬†anyone else¬†get¬†stuck¬†with this?¬†Is there any other good alternatives that are similar to Cursor and Windsurf made in and for EU. What¬†are other EU devs/teams¬†doing? Self-hosting?¬†Or just¬†keeping to old tools?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5n6lq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Refrigerator9508",
          "discussion_type": null,
          "num_comments": 137,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5n6lq/eu_is_being_left_behinde_and_it_sucks/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5n6lq/eu_is_being_left_behinde_and_it_sucks/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753114446,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have wondered if you can get usable speeds on something like ERNIE-4.5-300B-A47B ~Q3 or Q4 on 2x 3090's, 128gb of DDR5 and what can't fit into RAM running on PCIE NVME's in raid 0. I'm sure it wouldn't be fast but I wonder if it could be usable.",
          "author_fullname": "t2_zws5yqyow",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Would using PCIE NVME in raid 0 for swap work to run larger models that don't fit into RAM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6akeo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753180396,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have wondered if you can get usable speeds on something like ERNIE-4.5-300B-A47B ~Q3 or Q4 on 2x 3090&amp;#39;s, 128gb of DDR5 and what can&amp;#39;t fit into RAM running on PCIE NVME&amp;#39;s in raid 0. I&amp;#39;m sure it wouldn&amp;#39;t be fast but I wonder if it could be usable.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6akeo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Commercial-Celery769",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6akeo/would_using_pcie_nvme_in_raid_0_for_swap_work_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6akeo/would_using_pcie_nvme_in_raid_0_for_swap_work_to/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753180396,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_5n4jepc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Truly open LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m69th7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=fec374170d117a6b9e9aef82babffa1e8039217a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753177663,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "shchegrikovich.substack.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://shchegrikovich.substack.com/p/truly-open-llms",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?auto=webp&amp;s=0647966f28fc4ee54cd9d415fd52ce6c38f79c03",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e50454aae6b161b0cafb9fcfd612d0809f5c73d9",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a50eebcb744a514321bedd747dcc0dd7361cde1f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0658f86a17beb93a12b50a6781f7ecf2fecfd7ab",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c3c812bd7d02ce4e898102f57a7b12c1898b9464",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f91de7eecda3e724888794a8b6e13c1ee3592d38",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a229cc66113465a58966fe1a9f7a47cc196ba9ad",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m69th7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GoodSamaritan333",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m69th7/truly_open_llms/",
          "stickied": false,
          "url": "https://shchegrikovich.substack.com/p/truly-open-llms",
          "subreddit_subscribers": 502720,
          "created_utc": 1753177663,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/Tesslate/UIGEN-X-8B](https://huggingface.co/Tesslate/UIGEN-X-8B)\n\nJust wanted to share a quick prompting guide for UIGEN-X (and that quants are available). Craft any system prompt (its not specific, so it will listen to you!)\n\nSo type out your prompt like this: \n\n* \\[Action\\] \\[UI type or page\\] \\[Framework(s)\\] \\[Key features\\] \\[Style (optional)\\]\n*  Examples:\n\n   * `Create a navbar using React + Tailwind CSS with logo, links, and mobile hamburger menu.`\n   * `Build a SaaS dashboard with Next.js + TypeScript + shadcn/ui: pages for analytics, user settings, billing, and a landing page. Use glassmorphism style.`\n   * `Generate a personal blog with SvelteKit + DaisyUI, mixing cyberpunk colors and minimalist layout. Responsive for mobile.`\n   * `Make a pricing table with React + Chakra UI, including monthly/yearly toggle, dark mode, and enterprise minimalism style.`\n\nIf it is within the context, then you can additionally add edits.\n\nHere's a prompt template:\n\n* `Create a [UI type] using [Framework(s) + Libraries] with [Features]. [Optional: Use [Style] style]. [Optional: Add sample content or Unsplash images.]`\n\nAdditional things that are supported -&gt; if you hand it Unsplash links or other pictures links, it should work. Make sure reasoning is on for this. This way, you can use it in Agentic or Function calling frameworks.\n\nRemember, its only an 8B model!\n\nWe are currently training 14B, 32B, and 30A and refining the process. We hope to create a good local alternative to the popular coding / design models that are on the web. \n\nMake sure to join the community for more support. (Link in Huggingface!)",
          "author_fullname": "t2_7mx42xse",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "UIGEN-X 8B supports React Headless, Flutter, React Native, Static Site Generators, Tauri, Vue, Gradio/Python, Tailwind, and prompt-based design. GGUF/GPTQ/MLX Available",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 95,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "kzzqul8nf8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1469bca066e6dc8c4de5bbfbf70f8cb8da95d3f"
                },
                {
                  "y": 142,
                  "x": 216,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=690e827a5f003cd998910dbf524e31b73ec2db01"
                },
                {
                  "y": 211,
                  "x": 320,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4891ca95bed5b4dde677b103efacb0d0eb83c604"
                },
                {
                  "y": 422,
                  "x": 640,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=099f2fc6e3b5972b2c6aef17adae108361623908"
                },
                {
                  "y": 633,
                  "x": 960,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a5482ce8142b6ea19c74d2fc590aff9b0f1a98db"
                },
                {
                  "y": 712,
                  "x": 1080,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=42be0fc4e87a423ce4553e87cd0e02e8cc7403f1"
                }
              ],
              "s": {
                "y": 1251,
                "x": 1896,
                "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=1896&amp;format=png&amp;auto=webp&amp;s=720ab948b90f645d72464f3b1284749a8404c6ad"
              },
              "id": "kzzqul8nf8ef1"
            },
            "iv8y70kfg8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 72,
                  "x": 108,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2a45bdc2372e5c9d718e3c14b37cce4647d522ac"
                },
                {
                  "y": 144,
                  "x": 216,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=832e91e8046f6ec8fd954d5ea4f31ddaba93091e"
                },
                {
                  "y": 213,
                  "x": 320,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e5194344fca943ac8a92d397bdd2d4229ac50941"
                },
                {
                  "y": 426,
                  "x": 640,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbeff775b694d92aadda1695d0d25455717df09f"
                },
                {
                  "y": 640,
                  "x": 960,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b61e1958cc6204508a5a54eb6faa52028d7b5bd3"
                },
                {
                  "y": 720,
                  "x": 1080,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d9c323a72a30535b3747f536878ce0741be12614"
                }
              ],
              "s": {
                "y": 1271,
                "x": 1906,
                "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=1906&amp;format=png&amp;auto=webp&amp;s=e4168909aeb7910a1decb0e675e95dfc99172c97"
              },
              "id": "iv8y70kfg8ef1"
            },
            "s968gxs0g8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 72,
                  "x": 108,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=409ea33391501bf0e1b3668cd6701584e2d6328a"
                },
                {
                  "y": 144,
                  "x": 216,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f3fe42af5e89f5bfb82327d8bdca78b9f676aa6"
                },
                {
                  "y": 213,
                  "x": 320,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=afe29e0fd00f03e24733ba7561bc1fd1dce58aba"
                },
                {
                  "y": 427,
                  "x": 640,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee72596c92bb36c8533edc20100db23fbf59feb9"
                },
                {
                  "y": 641,
                  "x": 960,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=40291f8d7edfd5b2938a1017f5f7003cea6eea64"
                },
                {
                  "y": 721,
                  "x": 1080,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fcf187a13cb3e659ee2e4349b2239f6691f238ce"
                }
              ],
              "s": {
                "y": 1274,
                "x": 1907,
                "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=1907&amp;format=png&amp;auto=webp&amp;s=03c2b53b770f0e0199f946f45413e52f38101e43"
              },
              "id": "s968gxs0g8ef1"
            },
            "bkgben5hn8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ebe469a22e8a696f6ade5d03df0afb99319ee40a"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=397c229507fa92cd0b2b700ecccd364b481a5d64"
                },
                {
                  "y": 213,
                  "x": 320,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=acd20ec90f369e40b727066d598fc2b7edca4200"
                },
                {
                  "y": 426,
                  "x": 640,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=381cf7575766e29c2cee139dcdf1749c60b7b5a3"
                },
                {
                  "y": 639,
                  "x": 960,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d2cf511ff6f043cc035e7a29da943acaecc33bd8"
                },
                {
                  "y": 719,
                  "x": 1080,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=de8973571c93536552a02bae099f00622832e8b6"
                }
              ],
              "s": {
                "y": 1272,
                "x": 1909,
                "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=1909&amp;format=png&amp;auto=webp&amp;s=0ce0953efdfe2dc0a0167b401829ee4e78834594"
              },
              "id": "bkgben5hn8ef1"
            },
            "baogvk7lf8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 70,
                  "x": 108,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=303f5769de618aedc68d5befd55c0f23ad537998"
                },
                {
                  "y": 141,
                  "x": 216,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=de639214f79d2d4e168910bdf7db8b2172d9de5b"
                },
                {
                  "y": 210,
                  "x": 320,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebc6b8b5b37c7d198ab01c45349ebaef3165a921"
                },
                {
                  "y": 420,
                  "x": 640,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fc9d239d6ad5c911867d958b15710cb6644d4b2"
                },
                {
                  "y": 630,
                  "x": 960,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3248842e9756c380dc1254d6ab7f23dba5a99e27"
                },
                {
                  "y": 709,
                  "x": 1080,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bb1d84003b1279f8c8656aa93a1111beacc131a8"
                }
              ],
              "s": {
                "y": 1249,
                "x": 1902,
                "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=1902&amp;format=png&amp;auto=webp&amp;s=d3b3d33655c346bf5bc703fa7512b6076b23c02b"
              },
              "id": "baogvk7lf8ef1"
            },
            "qlq88ceum8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 73,
                  "x": 108,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=11392437c6eae7c3cb80093689973f601ba47f75"
                },
                {
                  "y": 147,
                  "x": 216,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8385ab7c59df8821a90902570bf798a646e92c0"
                },
                {
                  "y": 218,
                  "x": 320,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f9015853f431160e764d4d2c07353f81ab3bdaf1"
                },
                {
                  "y": 436,
                  "x": 640,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9f9b1c5463b34aa1d4a5064888091f446d588916"
                },
                {
                  "y": 654,
                  "x": 960,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=58c5783739adb38ad8cf28b7f105be92b48bcccf"
                },
                {
                  "y": 736,
                  "x": 1080,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fca853f214192cc056b4a0d81f3bb2fa50ab2da3"
                }
              ],
              "s": {
                "y": 1276,
                "x": 1872,
                "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=1872&amp;format=png&amp;auto=webp&amp;s=8f2da76ecd06758cb6411dcaac6e07a89f684c92"
              },
              "id": "qlq88ceum8ef1"
            },
            "ec0ih2x5g8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a9379f1e63a89f5367dfbfd32b9a8df532304055"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f434f85c9033511251c7f06b8c15761163442aa"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b033b91dc8c3d9b67a9812dc157b2f858852c15"
                },
                {
                  "y": 425,
                  "x": 640,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e7cfb3e5b4418180e7274c15d302f84d9d64e6cc"
                },
                {
                  "y": 637,
                  "x": 960,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9f2d7171901ffd3b3fa481153f1df1277152d588"
                },
                {
                  "y": 717,
                  "x": 1080,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1b07979b7f543d84d9c4384c6e77b8b911837df8"
                }
              ],
              "s": {
                "y": 1279,
                "x": 1925,
                "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=1925&amp;format=png&amp;auto=webp&amp;s=d8d041b099fe8e0d77604f0e37a8c0f85418ecd3"
              },
              "id": "ec0ih2x5g8ef1"
            },
            "9xa2d1o5f8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 72,
                  "x": 108,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b935462ab2f8008aab03cd92672934b99a6d50f"
                },
                {
                  "y": 145,
                  "x": 216,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5bfb820526679232d933594d6a207edf0c3c3bd"
                },
                {
                  "y": 215,
                  "x": 320,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=30f7ea1e4bf5a5150b76e4b873193c82577deefd"
                },
                {
                  "y": 431,
                  "x": 640,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd6665da47568cba2ae3fbe0c12342303e7361f7"
                },
                {
                  "y": 646,
                  "x": 960,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=acbf35cb0b854faffc8500847045ddee546974e7"
                },
                {
                  "y": 727,
                  "x": 1080,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f802e6268270bd0257fd64d6a5f12a8d0775acfd"
                }
              ],
              "s": {
                "y": 1283,
                "x": 1904,
                "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=1904&amp;format=png&amp;auto=webp&amp;s=86d181cfb01012f731f45e4236be0ef691cd7c7a"
              },
              "id": "9xa2d1o5f8ef1"
            },
            "4cgtiz1pf8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 70,
                  "x": 108,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9476e3d32a2b4ebddc1d44940be795ca69c505de"
                },
                {
                  "y": 141,
                  "x": 216,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a012cb536e4088a6adcc6773c02216ff905ea974"
                },
                {
                  "y": 210,
                  "x": 320,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2b1ae7259c182874929c4e41c2cd1db5089064c5"
                },
                {
                  "y": 420,
                  "x": 640,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bb9b8a221009889a7ab5351b684233583bcb0cc"
                },
                {
                  "y": 630,
                  "x": 960,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2ba7b4a8e8efa0432dab30568ad1477468ea5974"
                },
                {
                  "y": 709,
                  "x": 1080,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=77a0c9435849b1b3f806b87ef5f2fa6998fc6dff"
                }
              ],
              "s": {
                "y": 1253,
                "x": 1907,
                "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=1907&amp;format=png&amp;auto=webp&amp;s=daafca14c7e5ee623c552997eb11226cbf2767df"
              },
              "id": "4cgtiz1pf8ef1"
            },
            "eltdxfn8i8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 70,
                  "x": 108,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ceb464884fcc4e8ea8f5673a530fca637a4ea00d"
                },
                {
                  "y": 141,
                  "x": 216,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4a99d61fb402903985f9c6be85f75cb987a9afb"
                },
                {
                  "y": 210,
                  "x": 320,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5b2b59e21adf2de70f35de42b0b13badb2b7e1dc"
                },
                {
                  "y": 420,
                  "x": 640,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b5a7db457131ab9e6827d842044325ab24d3afb"
                },
                {
                  "y": 630,
                  "x": 960,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a4594f5796a9d0c3d0573956810621801f7651e"
                },
                {
                  "y": 709,
                  "x": 1080,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c52767489ca42b8349b5bc5683443a466d6c3b2e"
                }
              ],
              "s": {
                "y": 1253,
                "x": 1907,
                "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=1907&amp;format=png&amp;auto=webp&amp;s=8c2d9c0743ca5a01676db6ac6e4d7b30a0911c27"
              },
              "id": "eltdxfn8i8ef1"
            },
            "ca3mb75rf8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 70,
                  "x": 108,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cf1a822b143343c8c2f1c4a91351751e74e37014"
                },
                {
                  "y": 141,
                  "x": 216,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4cad9a7097453af844b2072df10473d63407168f"
                },
                {
                  "y": 209,
                  "x": 320,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=816bdd87a87ae2ef4a2d2205f7b9e29f38489c91"
                },
                {
                  "y": 419,
                  "x": 640,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d0905850f6ee9cdef78d85c64fea4ac9e5f1916"
                },
                {
                  "y": 628,
                  "x": 960,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=db14232380210a75035e339554d958dbf35b7d3a"
                },
                {
                  "y": 707,
                  "x": 1080,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=57e792572da9152faf1d00aa981a55b8306a3b78"
                }
              ],
              "s": {
                "y": 1248,
                "x": 1906,
                "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=1906&amp;format=png&amp;auto=webp&amp;s=d74d7bdeda2dd3fe454bc67dbc6906f9c203ce20"
              },
              "id": "ca3mb75rf8ef1"
            },
            "09e7cb48f8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7deeff37c4941bdf28a815588390a855ed862bd0"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=625263ce51714f18f7788cf0ad81624820b84b23"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3974e35855f594aed564ee278aaced85fd3c1769"
                },
                {
                  "y": 425,
                  "x": 640,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0fe5e13cb68b69fc904ce5dc9c280c1cf92a18e8"
                },
                {
                  "y": 638,
                  "x": 960,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1d0f7e62ef3919918eaa711708200e1f4a6e545e"
                },
                {
                  "y": 718,
                  "x": 1080,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=71a39d1d3270cb5f31192d2137b97dfd051e6b2b"
                }
              ],
              "s": {
                "y": 1266,
                "x": 1902,
                "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=1902&amp;format=png&amp;auto=webp&amp;s=ec11ef011d0468b0f6f6bfc269287e472701bffe"
              },
              "id": "09e7cb48f8ef1"
            }
          },
          "name": "t3_1m5lgtr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 28,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "qlq88ceum8ef1",
                "id": 710842483
              },
              {
                "media_id": "baogvk7lf8ef1",
                "id": 710842484
              },
              {
                "media_id": "kzzqul8nf8ef1",
                "id": 710842485
              },
              {
                "media_id": "4cgtiz1pf8ef1",
                "id": 710842486
              },
              {
                "media_id": "ca3mb75rf8ef1",
                "id": 710842487
              },
              {
                "media_id": "9xa2d1o5f8ef1",
                "id": 710842488
              },
              {
                "media_id": "09e7cb48f8ef1",
                "id": 710842489
              },
              {
                "media_id": "s968gxs0g8ef1",
                "id": 710842490
              },
              {
                "media_id": "ec0ih2x5g8ef1",
                "id": 710842491
              },
              {
                "media_id": "iv8y70kfg8ef1",
                "id": 710842492
              },
              {
                "media_id": "eltdxfn8i8ef1",
                "id": 710842493
              },
              {
                "media_id": "bkgben5hn8ef1",
                "id": 710842494
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/jkGttzBpvCgJdojYediLI8339DpfTnhALhUrjGFYPnA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753110583,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-8B\"&gt;https://huggingface.co/Tesslate/UIGEN-X-8B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Just wanted to share a quick prompting guide for UIGEN-X (and that quants are available). Craft any system prompt (its not specific, so it will listen to you!)&lt;/p&gt;\n\n&lt;p&gt;So type out your prompt like this: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;[Action] [UI type or page] [Framework(s)] [Key features] [Style (optional)]&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;Create a navbar using React + Tailwind CSS with logo, links, and mobile hamburger menu.&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;Build a SaaS dashboard with Next.js + TypeScript + shadcn/ui: pages for analytics, user settings, billing, and a landing page. Use glassmorphism style.&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;Generate a personal blog with SvelteKit + DaisyUI, mixing cyberpunk colors and minimalist layout. Responsive for mobile.&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;Make a pricing table with React + Chakra UI, including monthly/yearly toggle, dark mode, and enterprise minimalism style.&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If it is within the context, then you can additionally add edits.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a prompt template:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;Create a [UI type] using [Framework(s) + Libraries] with [Features]. [Optional: Use [Style] style]. [Optional: Add sample content or Unsplash images.]&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Additional things that are supported -&amp;gt; if you hand it Unsplash links or other pictures links, it should work. Make sure reasoning is on for this. This way, you can use it in Agentic or Function calling frameworks.&lt;/p&gt;\n\n&lt;p&gt;Remember, its only an 8B model!&lt;/p&gt;\n\n&lt;p&gt;We are currently training 14B, 32B, and 30A and refining the process. We hope to create a good local alternative to the popular coding / design models that are on the web. &lt;/p&gt;\n\n&lt;p&gt;Make sure to join the community for more support. (Link in Huggingface!)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m5lgtr",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5lgtr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "United-Rush4073",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5lgtr/uigenx_8b_supports_react_headless_flutter_react/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m5lgtr",
          "subreddit_subscribers": 502720,
          "created_utc": 1753110583,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm running a research project analysing hospital incident reports (answering structured questions based on them); we do have permission to use identifiable data but the PDFs I've been sent have been redacted and whichever software they've used has turned a lot of the text into an image. To add excitement, a lot of the text is in columns that flow across pages (ie you need to read the left of page 1,2 then the right of page 1,2)\n\nCan anyone recommend a local model capable of handling this? Our research machine has an A6000 (48Gb) and 128Gb RAM; speed isn't a massive issue. I don't mind if the workflow is PDF to text and then run a text model, or if a vision model could do the whole thing.\n\nThanks!",
          "author_fullname": "t2_pc1zg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Model to process image-of-text PDFs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m68tse",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753173792,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running a research project analysing hospital incident reports (answering structured questions based on them); we do have permission to use identifiable data but the PDFs I&amp;#39;ve been sent have been redacted and whichever software they&amp;#39;ve used has turned a lot of the text into an image. To add excitement, a lot of the text is in columns that flow across pages (ie you need to read the left of page 1,2 then the right of page 1,2)&lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend a local model capable of handling this? Our research machine has an A6000 (48Gb) and 128Gb RAM; speed isn&amp;#39;t a massive issue. I don&amp;#39;t mind if the workflow is PDF to text and then run a text model, or if a vision model could do the whole thing.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m68tse",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "thigger",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m68tse/model_to_process_imageoftext_pdfs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m68tse/model_to_process_imageoftext_pdfs/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753173792,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html](https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html)",
          "author_fullname": "t2_84qpb9rt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Interesting new blog post from Lemonade team",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5q35o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753120844,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html\"&gt;https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?auto=webp&amp;s=370ef3c67fc9a466fb921e399215ca76e255bdd8",
                  "width": 1435,
                  "height": 645
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d6ab69b2057c1ff74bd4fad9be640864917203c",
                    "width": 108,
                    "height": 48
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc432f40f27f0a2f68d597fb271cdc9efc951403",
                    "width": 216,
                    "height": 97
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=39d76385ecda761edb0e6a264b0e3efd78c0cd1c",
                    "width": 320,
                    "height": 143
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=77ae8bc998e1523bb4c01bb0d57ef5cea220343f",
                    "width": 640,
                    "height": 287
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c1e801ab47a7cb7d218797288798cd8f7d4e41fd",
                    "width": 960,
                    "height": 431
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d14e3dd033ec146ebe70b9e6b9135c50e7a9136",
                    "width": 1080,
                    "height": 485
                  }
                ],
                "variants": {},
                "id": "tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5q35o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Smooth-Screen4148",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5q35o/interesting_new_blog_post_from_lemonade_team/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5q35o/interesting_new_blog_post_from_lemonade_team/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753120844,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nI am making a tool that needs to analyze a conversation (non-English) between two people. The conversation is provided to me in audio format. I am currently using OpenAI Whisper to transcribe and feed the transcription to ChatGPT-4o model through the API for analysis.\n\nSo far, it's doing a fair job. Sometimes, though, reading the transcription, I find it hard to figure out which speaker is speaking what. I have to listen to the audio to figure it out. I am wondering if ChatGPT-4o would also sometimes find it hard to follow the conversation from the transcription. I think that adding a speaker diarization step might make the transcription easier to understand and analyze.\n\nI am looking for Speaker Diarization tools that I can use. I have tried using pyannote speaker-diarization-3.1, but I find it does not work very well. What are some other options that I can look at?",
          "author_fullname": "t2_1pot8iygav",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What Speaker Diarization tools should I look into?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6741z",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753167129,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am making a tool that needs to analyze a conversation (non-English) between two people. The conversation is provided to me in audio format. I am currently using OpenAI Whisper to transcribe and feed the transcription to ChatGPT-4o model through the API for analysis.&lt;/p&gt;\n\n&lt;p&gt;So far, it&amp;#39;s doing a fair job. Sometimes, though, reading the transcription, I find it hard to figure out which speaker is speaking what. I have to listen to the audio to figure it out. I am wondering if ChatGPT-4o would also sometimes find it hard to follow the conversation from the transcription. I think that adding a speaker diarization step might make the transcription easier to understand and analyze.&lt;/p&gt;\n\n&lt;p&gt;I am looking for Speaker Diarization tools that I can use. I have tried using pyannote speaker-diarization-3.1, but I find it does not work very well. What are some other options that I can look at?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6741z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Chemical_Gas3710",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6741z/what_speaker_diarization_tools_should_i_look_into/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6741z/what_speaker_diarization_tools_should_i_look_into/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753167129,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,\nJust got an RTX 5090 with 32GB of VRAM and I'm looking to get into full fine-tuning LLMs locally.\nMy main question is about the full fine-tuning capabilities with this GPU. I know 32GB is a lot, but full fine-tuning can be a VRAM hog.\n\n * What's the realistic largest model size (in billions of parameters) I can full fine-tune (not LoRA/QLoRA) using 32GB VRAM?\n\n * Assuming FP16/BF16 precision and memory optimizations like gradient checkpointing, what are the typical limitations (batch size, sequence length) for models in the 7B, 13B, or even larger range?\n\n * Are there any specific transformers or bitsandbytes configurations crucial for maximizing VRAM usage for full fine-tuning on the RTX 5090?\n\nMy goal is to achieve the best possible quality with full fine-tuning, even if it means a very small batch size. Any insights or experiences with similar VRAM GPUs would be super helpful!\n\nThanks!",
          "author_fullname": "t2_dyvrh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RTX 5090 (32GB VRAM) - Full Fine-Tuning: What Can I Expect?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ro7s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753124390,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,\nJust got an RTX 5090 with 32GB of VRAM and I&amp;#39;m looking to get into full fine-tuning LLMs locally.\nMy main question is about the full fine-tuning capabilities with this GPU. I know 32GB is a lot, but full fine-tuning can be a VRAM hog.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;What&amp;#39;s the realistic largest model size (in billions of parameters) I can full fine-tune (not LoRA/QLoRA) using 32GB VRAM?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Assuming FP16/BF16 precision and memory optimizations like gradient checkpointing, what are the typical limitations (batch size, sequence length) for models in the 7B, 13B, or even larger range?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Are there any specific transformers or bitsandbytes configurations crucial for maximizing VRAM usage for full fine-tuning on the RTX 5090?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My goal is to achieve the best possible quality with full fine-tuning, even if it means a very small batch size. Any insights or experiences with similar VRAM GPUs would be super helpful!&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5ro7s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "celsowm",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753124390,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are there any tiny spellcheck models for English which are good? What do you guys use?",
          "author_fullname": "t2_9kgt3ez",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What do you guys use for Spellcheck?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m68yvl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753174363,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any tiny spellcheck models for English which are good? What do you guys use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m68yvl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CaptTechno",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m68yvl/what_do_you_guys_use_for_spellcheck/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m68yvl/what_do_you_guys_use_for_spellcheck/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753174363,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "1. Mistral\\_large-Instruct\n2. Qwen3-235B\n3. Command-A\n4. Deepseek-V3\n5. Deepseek-R1\n6. Deepseek-R1-0528\n7. Deepseek-TNG-R1T2-Chimera\n8. Kimi-K2\n9. Ernie-4.5-300b\n10. llama3.1-405B\n11. llama3.1-Nemotron-Ultra-253b?\n12. Others?",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which local 100B+ heavy weight models are your favorite and why?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m58695",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 110,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 110,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753067953,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;Mistral_large-Instruct&lt;/li&gt;\n&lt;li&gt;Qwen3-235B&lt;/li&gt;\n&lt;li&gt;Command-A&lt;/li&gt;\n&lt;li&gt;Deepseek-V3&lt;/li&gt;\n&lt;li&gt;Deepseek-R1&lt;/li&gt;\n&lt;li&gt;Deepseek-R1-0528&lt;/li&gt;\n&lt;li&gt;Deepseek-TNG-R1T2-Chimera&lt;/li&gt;\n&lt;li&gt;Kimi-K2&lt;/li&gt;\n&lt;li&gt;Ernie-4.5-300b&lt;/li&gt;\n&lt;li&gt;llama3.1-405B&lt;/li&gt;\n&lt;li&gt;llama3.1-Nemotron-Ultra-253b?&lt;/li&gt;\n&lt;li&gt;Others?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m58695",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 104,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753067953,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For those interested, here is a temporary copy pulled just before the official repo went 404.\n\n[https://github.com/PieBru/ik\\_llama.cpp\\_temp\\_copy](https://github.com/PieBru/ik_llama.cpp_temp_copy)",
          "author_fullname": "t2_uehl6561",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ik_llama.cpp 404: temporary repo up to commit d44c2d3",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5d66p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 44,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 44,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753085617,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those interested, here is a temporary copy pulled just before the official repo went 404.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/PieBru/ik_llama.cpp_temp_copy\"&gt;https://github.com/PieBru/ik_llama.cpp_temp_copy&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OeOxSPElJ7feBpmIHwfN3BbOjC_fgsHhahVsK3oEdNw.png?auto=webp&amp;s=2e4ac14f43e45ff123f0291ee0e9229ddf63c11a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OeOxSPElJ7feBpmIHwfN3BbOjC_fgsHhahVsK3oEdNw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9caec387fe1ebd69552e3f38510d53745dc8e07c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/OeOxSPElJ7feBpmIHwfN3BbOjC_fgsHhahVsK3oEdNw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4fbec24ba5e92ce184223ddc1f5e7fe36f2181a8",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/OeOxSPElJ7feBpmIHwfN3BbOjC_fgsHhahVsK3oEdNw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ef8215dca529cf7174239a8aeb6631157255480f",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/OeOxSPElJ7feBpmIHwfN3BbOjC_fgsHhahVsK3oEdNw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=990d720abad14eacfd0e06b3f6224e49b6641d61",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/OeOxSPElJ7feBpmIHwfN3BbOjC_fgsHhahVsK3oEdNw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1dfc0f1e7e23ea4c5a555ef6596dcddc4362888e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/OeOxSPElJ7feBpmIHwfN3BbOjC_fgsHhahVsK3oEdNw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c6a1df725f9226d65d59e1a49ae8856892491bca",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "OeOxSPElJ7feBpmIHwfN3BbOjC_fgsHhahVsK3oEdNw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m5d66p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PieBru",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5d66p/ik_llamacpp_404_temporary_repo_up_to_commit/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5d66p/ik_llamacpp_404_temporary_repo_up_to_commit/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753085617,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I dont need near real time TTS at all, i am happy with even 0.5x realtime generation. Is there actually a better model than Kokoro but with the trade off of being slower/larger, or is Kokoro not only the best model but also really fast?",
          "author_fullname": "t2_1tyhgasfrp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a better local TTS than Kokoro, even if its slower to generate?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5lwo6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753111594,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I dont need near real time TTS at all, i am happy with even 0.5x realtime generation. Is there actually a better model than Kokoro but with the trade off of being slower/larger, or is Kokoro not only the best model but also really fast?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5lwo6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sad_Holiday_7435",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5lwo6/is_there_a_better_local_tts_than_kokoro_even_if/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5lwo6/is_there_a_better_local_tts_than_kokoro_even_if/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753111594,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey,\n\nI'm curious, what are people fine-tuning their models for? \n\nI was working in a company where we fine-tuned models to better deal with product images, but the company couldn't keep the lights on. Most agencies, companies, freelancers, seem to use off-the-shelf models, which are getting \"good enough\" for the job. \n\nSo, what are people fine-tuning their models for? and which companies, or industries, are most likely to be fine-tuning models? \n\nThanks, just an idiot asking! ",
          "author_fullname": "t2_at09kvrk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are people fine-tuning their models for?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5gmfr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753098077,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious, what are people fine-tuning their models for? &lt;/p&gt;\n\n&lt;p&gt;I was working in a company where we fine-tuned models to better deal with product images, but the company couldn&amp;#39;t keep the lights on. Most agencies, companies, freelancers, seem to use off-the-shelf models, which are getting &amp;quot;good enough&amp;quot; for the job. &lt;/p&gt;\n\n&lt;p&gt;So, what are people fine-tuning their models for? and which companies, or industries, are most likely to be fine-tuning models? &lt;/p&gt;\n\n&lt;p&gt;Thanks, just an idiot asking! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5gmfr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MKBSP",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5gmfr/what_are_people_finetuning_their_models_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5gmfr/what_are_people_finetuning_their_models_for/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753098077,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Project Page: [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://deepreinforce-ai.github.io/cudal1_blog/)\n\nCode: [GitHub - deepreinforce-ai/CUDA-L1](https://github.com/deepreinforce-ai/CUDA-L1)\n\nAbstract\n\n&gt;The exponential growth in demand for GPU computing resources, driven by the rapid advancement of Large Language Models, has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models (e.g. R1, o1) achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization.  \nCUDA-L1 achieves performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40, x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100. Beyond these benchmark results, CUDA-L1 demonstrates several remarkable properties: 1) Discovers a variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance; 2) Uncovers fundamental principles of CUDA optimization; 3) Identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that harm performance.  \nThe capabilities of CUDA-L1 demonstrate that reinforcement learning can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. More importantly, the trained RL model extend the acquired reasoning abilities to new kernels. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources.",
          "author_fullname": "t2_dtsa6gxt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5qsqx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753122428,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Project Page: &lt;a href=\"https://deepreinforce-ai.github.io/cudal1_blog/\"&gt;CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Code: &lt;a href=\"https://github.com/deepreinforce-ai/CUDA-L1\"&gt;GitHub - deepreinforce-ai/CUDA-L1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Abstract&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The exponential growth in demand for GPU computing resources, driven by the rapid advancement of Large Language Models, has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models (e.g. R1, o1) achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization.&lt;br/&gt;\nCUDA-L1 achieves performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40, x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100. Beyond these benchmark results, CUDA-L1 demonstrates several remarkable properties: 1) Discovers a variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance; 2) Uncovers fundamental principles of CUDA optimization; 3) Identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that harm performance.&lt;br/&gt;\nThe capabilities of CUDA-L1 demonstrate that reinforcement learning can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. More importantly, the trained RL model extend the acquired reasoning abilities to new kernels. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.14111",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5qsqx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Formal_Drop526",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5qsqx/cudal1_improving_cuda_optimization_via/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.14111",
          "subreddit_subscribers": 502720,
          "created_utc": 1753122428,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello, I posted that I wanted to train an LLM for under $1000 here:  [https://www.reddit.com/r/LocalLLaMA/comments/1lmbtvg/attempting\\_to\\_train\\_a\\_model\\_from\\_scratch\\_for\\_less/](https://www.reddit.com/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/)\n\nI had to crunch a lot to fit in 24gb of ram. The final project is a 960M model trained on 19.2B tokens ( chinchilla optimal).  Cost projection is about $500 for this run.   It has flash attention 2, a 3:1 GQA,  a 3k context window. and sink tokens. Training is 70% project gutenberg and 30% US congressional reports ( the Govremorts dataset).   The corpus is english only, which I'm hoping will give it an edge.\n\nI have had two false starts where I had to restart training. The first because I set up my streaming datasets wrong, and the model kep training on the same thing due to restarts. The second because the LR was too high and my loss curve was all fucked up.\n\nNow at about 2% on the 3rd run, the loss looks textbook, and I am letting it run till the tokens are done. Projections show a final loss around 2.6-2.3 which is great.\n\nHappy to answer any questions! Pic is the beautiful loss curve.\n\nEdit: It's called Libremodel I, codename Gigi, and I made a website with more info here: [https://libremodel.xyz](https://libremodel.xyz)\n\nhttps://preview.redd.it/lf78xbsfy3ef1.png?width=711&amp;format=png&amp;auto=webp&amp;s=1fc75b919255aa91b8cbf0b65b1420cb43fe26a1",
          "author_fullname": "t2_i5os0v0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I posted 3 weeks ago about training my own model. Progress report.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "lf78xbsfy3ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 53,
                  "x": 108,
                  "u": "https://preview.redd.it/lf78xbsfy3ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=36ad333eaf2d759d9b745f45f702461135100a1a"
                },
                {
                  "y": 107,
                  "x": 216,
                  "u": "https://preview.redd.it/lf78xbsfy3ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb1c9a95a8af42b94154d7f01e49b4731e052e5c"
                },
                {
                  "y": 159,
                  "x": 320,
                  "u": "https://preview.redd.it/lf78xbsfy3ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=91b4c8642acf7701da8d782aa9ce125107f00dd1"
                },
                {
                  "y": 319,
                  "x": 640,
                  "u": "https://preview.redd.it/lf78xbsfy3ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8517a175f4e44e1ff1b186c44a329b861671f778"
                }
              ],
              "s": {
                "y": 355,
                "x": 711,
                "u": "https://preview.redd.it/lf78xbsfy3ef1.png?width=711&amp;format=png&amp;auto=webp&amp;s=1fc75b919255aa91b8cbf0b65b1420cb43fe26a1"
              },
              "id": "lf78xbsfy3ef1"
            }
          },
          "name": "t3_1m52h10",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 223,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 223,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/tGNWILy7NUwjWBRL__Qs6HOJImwQ5Z22Np_wBFcIDdM.jpg",
          "edited": 1753128481,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753051615,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I posted that I wanted to train an LLM for under $1000 here:  &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I had to crunch a lot to fit in 24gb of ram. The final project is a 960M model trained on 19.2B tokens ( chinchilla optimal).  Cost projection is about $500 for this run.   It has flash attention 2, a 3:1 GQA,  a 3k context window. and sink tokens. Training is 70% project gutenberg and 30% US congressional reports ( the Govremorts dataset).   The corpus is english only, which I&amp;#39;m hoping will give it an edge.&lt;/p&gt;\n\n&lt;p&gt;I have had two false starts where I had to restart training. The first because I set up my streaming datasets wrong, and the model kep training on the same thing due to restarts. The second because the LR was too high and my loss curve was all fucked up.&lt;/p&gt;\n\n&lt;p&gt;Now at about 2% on the 3rd run, the loss looks textbook, and I am letting it run till the tokens are done. Projections show a final loss around 2.6-2.3 which is great.&lt;/p&gt;\n\n&lt;p&gt;Happy to answer any questions! Pic is the beautiful loss curve.&lt;/p&gt;\n\n&lt;p&gt;Edit: It&amp;#39;s called Libremodel I, codename Gigi, and I made a website with more info here: &lt;a href=\"https://libremodel.xyz\"&gt;https://libremodel.xyz&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lf78xbsfy3ef1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fc75b919255aa91b8cbf0b65b1420cb43fe26a1\"&gt;https://preview.redd.it/lf78xbsfy3ef1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fc75b919255aa91b8cbf0b65b1420cb43fe26a1&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m52h10",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "thebadslime",
          "discussion_type": null,
          "num_comments": 54,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m52h10/i_posted_3_weeks_ago_about_training_my_own_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m52h10/i_posted_3_weeks_ago_about_training_my_own_model/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753051615,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_on5es7pe3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I'm sorry Zuck please don't leave us we were just having fun",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 112,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4s9mt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bbbdbf",
          "ups": 755,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 755,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Z2j0QZtU4E9ysCBsBkjssqu8NBo0JDeca2zq2_P4T5Q.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753026671,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/p9mxxen7w1ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/p9mxxen7w1ef1.png?auto=webp&amp;s=213c1f7dfcfece004758a436ec8f98522e0eb2e7",
                  "width": 868,
                  "height": 696
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/p9mxxen7w1ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0312de90e101f89f1aedd7bdb3cd988bb8639791",
                    "width": 108,
                    "height": 86
                  },
                  {
                    "url": "https://preview.redd.it/p9mxxen7w1ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6db82d3958069aad93464e9169e3acbd2741501f",
                    "width": 216,
                    "height": 173
                  },
                  {
                    "url": "https://preview.redd.it/p9mxxen7w1ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e4629b986f27535857af409f095f24c7c7d27e17",
                    "width": 320,
                    "height": 256
                  },
                  {
                    "url": "https://preview.redd.it/p9mxxen7w1ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8ce4b5b89949e56107fd26431dd9d275053d6cf2",
                    "width": 640,
                    "height": 513
                  }
                ],
                "variants": {},
                "id": "NzZ0RDeoMTe29AfbHg0EIQXBI_ZMyqVJEiS9EW_Y5I8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m4s9mt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ForsookComparison",
          "discussion_type": null,
          "num_comments": 117,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m4s9mt/im_sorry_zuck_please_dont_leave_us_we_were_just/",
          "stickied": false,
          "url": "https://i.redd.it/p9mxxen7w1ef1.png",
          "subreddit_subscribers": 502720,
          "created_utc": 1753026671,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Heavily promoting the dishwashing benchmark:  \n\nGemini 3.0 Ultra score: 0%  \n\nGPT 5 Pro score: 0%  \n\nClaude 5 Opus score: 0%\n\ngrok 5 scoreÔºö0%\n\nDeepSeek R2 score: 0%  \n\nQwen4 Max score: 0%  \n\nKimi K3 score: 0%",
          "author_fullname": "t2_4i04jwsoe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Heavily promoting the dishwashing benchmark",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ndsf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753115118,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753114894,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Heavily promoting the dishwashing benchmark:  &lt;/p&gt;\n\n&lt;p&gt;Gemini 3.0 Ultra score: 0%  &lt;/p&gt;\n\n&lt;p&gt;GPT 5 Pro score: 0%  &lt;/p&gt;\n\n&lt;p&gt;Claude 5 Opus score: 0%&lt;/p&gt;\n\n&lt;p&gt;grok 5 scoreÔºö0%&lt;/p&gt;\n\n&lt;p&gt;DeepSeek R2 score: 0%  &lt;/p&gt;\n\n&lt;p&gt;Qwen4 Max score: 0%  &lt;/p&gt;\n\n&lt;p&gt;Kimi K3 score: 0%&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5ndsf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gtog-ima",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ndsf/heavily_promoting_the_dishwashing_benchmark/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ndsf/heavily_promoting_the_dishwashing_benchmark/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753114894,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Trying to get an RTX 5090 working on Ubuntu and hitting a wall. The system boots fine, BIOS sees the card, but Ubuntu doesn‚Äôt seem to know it exists. nvidia-smi comes up empty. Meanwhile, a 4090 in the same machine is working just fine.\n\nHere‚Äôs what I‚Äôve tried so far:\n\n* Installed latest NVIDIA drivers from both apt and the CUDA toolkit installer (550+)\n* Swapped PCIe slots\n* Disabled secure boot, added nomodeset, the usual boot flags\n* Confirmed power and reseated the card just in case\n\nStill nothing. I‚Äôm on Ubuntu 22.04 at the moment. Starting to wonder if this is a kernel issue or if the 5090 just isn‚Äôt properly supported yet. Anyone have a 5090 running on Linux? Did you need a bleeding-edge kernel or beta drivers?\n\nMain goal is running local LLaMA models, but right now the 5090 is just sitting there, useless.\n\nWould really appreciate any info or pointers. If you‚Äôve gotten this working, let me know what combo of drivers, kernel, and/or sacrifice to the GPU gods it took.\n\nThanks in advance.",
          "author_fullname": "t2_3ryxf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RTX 5090 not recognized on Ubuntu ‚Äî anyone else figure this out?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5pbxo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753119177,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to get an RTX 5090 working on Ubuntu and hitting a wall. The system boots fine, BIOS sees the card, but Ubuntu doesn‚Äôt seem to know it exists. nvidia-smi comes up empty. Meanwhile, a 4090 in the same machine is working just fine.&lt;/p&gt;\n\n&lt;p&gt;Here‚Äôs what I‚Äôve tried so far:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Installed latest NVIDIA drivers from both apt and the CUDA toolkit installer (550+)&lt;/li&gt;\n&lt;li&gt;Swapped PCIe slots&lt;/li&gt;\n&lt;li&gt;Disabled secure boot, added nomodeset, the usual boot flags&lt;/li&gt;\n&lt;li&gt;Confirmed power and reseated the card just in case&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Still nothing. I‚Äôm on Ubuntu 22.04 at the moment. Starting to wonder if this is a kernel issue or if the 5090 just isn‚Äôt properly supported yet. Anyone have a 5090 running on Linux? Did you need a bleeding-edge kernel or beta drivers?&lt;/p&gt;\n\n&lt;p&gt;Main goal is running local LLaMA models, but right now the 5090 is just sitting there, useless.&lt;/p&gt;\n\n&lt;p&gt;Would really appreciate any info or pointers. If you‚Äôve gotten this working, let me know what combo of drivers, kernel, and/or sacrifice to the GPU gods it took.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5pbxo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ate50eggs",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5pbxo/rtx_5090_not_recognized_on_ubuntu_anyone_else/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5pbxo/rtx_5090_not_recognized_on_ubuntu_anyone_else/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753119177,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "(Latest update: 21/07/2025)\n\nI've just extracted the FULL Windsurf system prompt and internal tools (Wave 11 update). Over 500 lines (Around 9.6k tokens).\n\nYou can check it out¬†here: [https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/tree/main/Windsurf](https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/tree/main/Windsurf)",
          "author_fullname": "t2_fbh7mxys2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "FULL Windsurf System Prompt and Tools [UPDATED, Wave 11]",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5kqk8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753108909,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Latest update: 21/07/2025)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve just extracted the FULL Windsurf system prompt and internal tools (Wave 11 update). Over 500 lines (Around 9.6k tokens).&lt;/p&gt;\n\n&lt;p&gt;You can check it out¬†here: &lt;a href=\"https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/tree/main/Windsurf\"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/tree/main/Windsurf&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m5kqk8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Box-898",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5kqk8/full_windsurf_system_prompt_and_tools_updated/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5kqk8/full_windsurf_system_prompt_and_tools_updated/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753108909,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here is a brief demo showing how one could use the new AI chat features in Neu called the magic hand. This system uses Llama 3.2 3b as a tool caller, and Claude Haiku 3.5 to generate the code but the code step could easily be replaced with a local model such as Qwen 3. I'm most using Claude because of the speed. It's still early days so right now its simple input output commands but I've been experimenting with a full blown agent that (I hope) will be able to build entire graphs. My hope is that this drastically reduces the knowledge floor needed to use Neu which, let's be honest, is a pretty intimidating piece of software. I hope that by following what the magic hand is doing, you can learn and understand Neu better. These features and a ton more will be coming with the Neu 0.3.0 update. [Checkout this link you'd like to learn more about Neu](http://kingroka.itch.io/neu)",
          "author_fullname": "t2_fkmug",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Using ollama and claude to control Neu",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5qflo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/lgjdwe0yo9ef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/lgjdwe0yo9ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/lgjdwe0yo9ef1/DASHPlaylist.mpd?a=1755779371%2CYjUwNjZiYmQwOGFhNDM0Mzc5ZTU1ZTk0YWY1MzZlZTAxZmNhYzM5Y2FjMTAxODc4NjE0OWIwYmEyN2Y4Yjc1OA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 74,
              "hls_url": "https://v.redd.it/lgjdwe0yo9ef1/HLSPlaylist.m3u8?a=1755779371%2CM2FjZjkwOGQ0NDI3MWExMTk1ZDFkODU4MTMxYmZmMDI5YzRhNDZkOTYxZjFhNTdkZTU3NzkzNzE4MjNiZDZlZg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=21b892afd0727f90d166d0a8bd745b1b393466e0",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753121614,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is a brief demo showing how one could use the new AI chat features in Neu called the magic hand. This system uses Llama 3.2 3b as a tool caller, and Claude Haiku 3.5 to generate the code but the code step could easily be replaced with a local model such as Qwen 3. I&amp;#39;m most using Claude because of the speed. It&amp;#39;s still early days so right now its simple input output commands but I&amp;#39;ve been experimenting with a full blown agent that (I hope) will be able to build entire graphs. My hope is that this drastically reduces the knowledge floor needed to use Neu which, let&amp;#39;s be honest, is a pretty intimidating piece of software. I hope that by following what the magic hand is doing, you can learn and understand Neu better. These features and a ton more will be coming with the Neu 0.3.0 update. &lt;a href=\"http://kingroka.itch.io/neu\"&gt;Checkout this link you&amp;#39;d like to learn more about Neu&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/lgjdwe0yo9ef1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?format=pjpg&amp;auto=webp&amp;s=066dc89758345e5c4b0f587768ddc66e19e69c11",
                  "width": 2560,
                  "height": 1440
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=70d833416067508a44b90607113d94349d55781b",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b22a0bf8d0efcb692e56b986ffac037c1461879a",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=43e75947dba7a887cce1309dd168dadb7cbc6ae7",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0427819bd56be1b5a7f90f67087f2477281f6bb8",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=82c83d51ba654e3c59c02f4516982e17cbac1472",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=11c7891a7ee6be24d57433d813fd8a07d748bd5d",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m5qflo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kingroka",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5qflo/using_ollama_and_claude_to_control_neu/",
          "stickied": false,
          "url": "https://v.redd.it/lgjdwe0yo9ef1",
          "subreddit_subscribers": 502720,
          "created_utc": 1753121614,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/lgjdwe0yo9ef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/lgjdwe0yo9ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/lgjdwe0yo9ef1/DASHPlaylist.mpd?a=1755779371%2CYjUwNjZiYmQwOGFhNDM0Mzc5ZTU1ZTk0YWY1MzZlZTAxZmNhYzM5Y2FjMTAxODc4NjE0OWIwYmEyN2Y4Yjc1OA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 74,
              "hls_url": "https://v.redd.it/lgjdwe0yo9ef1/HLSPlaylist.m3u8?a=1755779371%2CM2FjZjkwOGQ0NDI3MWExMTk1ZDFkODU4MTMxYmZmMDI5YzRhNDZkOTYxZjFhNTdkZTU3NzkzNzE4MjNiZDZlZg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nWe‚Äôre the team at Software Mansion, the creators and maintainers of the **react-native-executorch** library, which allows developers to run PyTorch ExecuTorch models inside React Native apps.\n\nAfter releasing the library, we realized a major hurdle for the community was the lack of a simple way to test, benchmark, and just *play* with LLMs on a mobile device without a complex setup.\n\nTo solve this, we created **Private Mind**. An open-source app that acts as a testing **utility** with one primary goal: to give developers and enthusiasts a dead-simple way to see how LLMs perform via ExecuTorch.\n\nIt's a tool built for this community. Here‚Äôs what it's designed for:\n\n* **A Lab for Your Models:** The main feature is loading your own custom models. If you can export it to the .pte format, you can run it in the app and interact with it through a basic chat interface.\n* **Pure On-Device Benchmarking:** Select any model and run a benchmark to see exactly how it performs on your hardware. You get crucial stats like tokens/second, memory usage, and time to first token. It‚Äôs a direct way to test the efficiency of your model or our library.\n* **A Reference Implementation:** Since we built the underlying library, the app serves as a blueprint. You can check the GitHub repo to see our recommended practices for implementing react-native-executorch in a real-world application.\n* **100% Local &amp; Private:** True to the ExecuTorch spirit, everything is on-device. Your models, chats, and benchmark data never leave your phone, making it a safe environment for experimentation.\n\n**Our Roadmap is About Improving the Testing Toolkit:**\n\nWe are actively working to enhance Private Mind as a testing utility. Next up is a new LLM runner that will expose parameters like **temperature** and **top\\_k** for more nuanced testing. After that, we plan to show how to implement more advanced use-cases like on-device **RAG** and **speech-to-text**. We'll also add **Gemma 3n** support as soon as it's fully compatible with the ExecuTorch.\n\n**Links:**\n\n* **App Store (iOS):** [https://apps.apple.com/gb/app/private-mind/id6746713439?uo=2](https://apps.apple.com/gb/app/private-mind/id6746713439?uo=2)\n* **Google Play (Android):** [https://play.google.com/store/apps/details?id=com.swmansion.privatemind](https://play.google.com/store/apps/details?id=com.swmansion.privatemind)\n* **GitHub (Check the code &amp; contribute):** [https://github.com/software-mansion-labs/private-mind](https://github.com/software-mansion-labs/private-mind)\n* **Our Pre-Exported Models for Testing:** [https://huggingface.co/software-mansion](https://huggingface.co/software-mansion)\n\nWe've built the foundation, and now we want the community to shape what's next. Let us know in the comments: What's the killer feature you're missing from other local AI apps?\n\n",
          "author_fullname": "t2_at32qlyr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "As the creators of react-native-executorch, we built an open-source app for testing ExecuTorch LLMs on mobile.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5k88s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/wklalir6l8ef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/wklalir6l8ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/wklalir6l8ef1/DASHPlaylist.mpd?a=1755779371%2CNzY1OGYzMGNiYTJmMjNmOThmZmExMWE5YzU4M2Q4NGYxMDVmYTY2MzZjNTRlYzYzOWRlNDU1YTZmMDY1ZWU4NQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 34,
              "hls_url": "https://v.redd.it/wklalir6l8ef1/HLSPlaylist.m3u8?a=1755779371%2CNzhkYTg5ZjQ1MDFjMzMyN2I1NjkyNmY3ZjEyMjdjNjlmNjgxNDQ0NjBiZDFkZjE1MThlNzM1NjEyNDY3MTBiMA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=f08deec1a65295a160b21c3f122845cd4e3bf129",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753107717,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;We‚Äôre the team at Software Mansion, the creators and maintainers of the &lt;strong&gt;react-native-executorch&lt;/strong&gt; library, which allows developers to run PyTorch ExecuTorch models inside React Native apps.&lt;/p&gt;\n\n&lt;p&gt;After releasing the library, we realized a major hurdle for the community was the lack of a simple way to test, benchmark, and just &lt;em&gt;play&lt;/em&gt; with LLMs on a mobile device without a complex setup.&lt;/p&gt;\n\n&lt;p&gt;To solve this, we created &lt;strong&gt;Private Mind&lt;/strong&gt;. An open-source app that acts as a testing &lt;strong&gt;utility&lt;/strong&gt; with one primary goal: to give developers and enthusiasts a dead-simple way to see how LLMs perform via ExecuTorch.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a tool built for this community. Here‚Äôs what it&amp;#39;s designed for:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;A Lab for Your Models:&lt;/strong&gt; The main feature is loading your own custom models. If you can export it to the .pte format, you can run it in the app and interact with it through a basic chat interface.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Pure On-Device Benchmarking:&lt;/strong&gt; Select any model and run a benchmark to see exactly how it performs on your hardware. You get crucial stats like tokens/second, memory usage, and time to first token. It‚Äôs a direct way to test the efficiency of your model or our library.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;A Reference Implementation:&lt;/strong&gt; Since we built the underlying library, the app serves as a blueprint. You can check the GitHub repo to see our recommended practices for implementing react-native-executorch in a real-world application.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;100% Local &amp;amp; Private:&lt;/strong&gt; True to the ExecuTorch spirit, everything is on-device. Your models, chats, and benchmark data never leave your phone, making it a safe environment for experimentation.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Our Roadmap is About Improving the Testing Toolkit:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We are actively working to enhance Private Mind as a testing utility. Next up is a new LLM runner that will expose parameters like &lt;strong&gt;temperature&lt;/strong&gt; and &lt;strong&gt;top_k&lt;/strong&gt; for more nuanced testing. After that, we plan to show how to implement more advanced use-cases like on-device &lt;strong&gt;RAG&lt;/strong&gt; and &lt;strong&gt;speech-to-text&lt;/strong&gt;. We&amp;#39;ll also add &lt;strong&gt;Gemma 3n&lt;/strong&gt; support as soon as it&amp;#39;s fully compatible with the ExecuTorch.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;App Store (iOS):&lt;/strong&gt; &lt;a href=\"https://apps.apple.com/gb/app/private-mind/id6746713439?uo=2\"&gt;https://apps.apple.com/gb/app/private-mind/id6746713439?uo=2&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Google Play (Android):&lt;/strong&gt; &lt;a href=\"https://play.google.com/store/apps/details?id=com.swmansion.privatemind\"&gt;https://play.google.com/store/apps/details?id=com.swmansion.privatemind&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;GitHub (Check the code &amp;amp; contribute):&lt;/strong&gt; &lt;a href=\"https://github.com/software-mansion-labs/private-mind\"&gt;https://github.com/software-mansion-labs/private-mind&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Our Pre-Exported Models for Testing:&lt;/strong&gt; &lt;a href=\"https://huggingface.co/software-mansion\"&gt;https://huggingface.co/software-mansion&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We&amp;#39;ve built the foundation, and now we want the community to shape what&amp;#39;s next. Let us know in the comments: What&amp;#39;s the killer feature you&amp;#39;re missing from other local AI apps?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/wklalir6l8ef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?format=pjpg&amp;auto=webp&amp;s=cb3ee8401caeca210218133a62d8771186f20c4d",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5c123e9bd18470d9a529c9f9011d628a6e5654df",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5e2e37157dddfdd5367559da87922b0f546d2fd5",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=43236ed4e39eacdff880a0d6561613ba17cbdbba",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=08314b98d437c5e47a6df7bd23f7b59279c831a1",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2783c53c9f7fd07aca4ea92f3adce88adf901a2a",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=eb7472f4d33b976d4fa394d8ce0bd94db917c7ac",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m5k88s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "K4anan",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5k88s/as_the_creators_of_reactnativeexecutorch_we_built/",
          "stickied": false,
          "url": "https://v.redd.it/wklalir6l8ef1",
          "subreddit_subscribers": 502720,
          "created_utc": 1753107717,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/wklalir6l8ef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/wklalir6l8ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/wklalir6l8ef1/DASHPlaylist.mpd?a=1755779371%2CNzY1OGYzMGNiYTJmMjNmOThmZmExMWE5YzU4M2Q4NGYxMDVmYTY2MzZjNTRlYzYzOWRlNDU1YTZmMDY1ZWU4NQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 34,
              "hls_url": "https://v.redd.it/wklalir6l8ef1/HLSPlaylist.m3u8?a=1755779371%2CNzhkYTg5ZjQ1MDFjMzMyN2I1NjkyNmY3ZjEyMjdjNjlmNjgxNDQ0NjBiZDFkZjE1MThlNzM1NjEyNDY3MTBiMA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": ";tldr when voice cloning use a high-end microphone not the one built-in to your computer/airpods\n\n\nI have a child that has reading difficulties. They need to be able to read 15 books this coming year and I was lucky enough to be able to find out what those 15 books are. Many of them are from the 1920s and earlier. They‚Äôre relatively unpopular and do not have existing audiobooks available. A number of them aren‚Äôt even sold as Ebooks (yes we are all aghast).\n\nEnter manually scanning ick\n\nSo I used my colleagues audiobook generator with my local rig. Each book gets chunked into around 1500 to 2000 chunks. My initial recording was on AirPods and/or a local microphone inside my MacBook.\n\nWith those recordings (I had two different ones) I had a 35 to 40% error rate which often persisted even when I was trying to generate 10 attempts. \n\nI happened to pick up a prosumer voice recorder to be able to do interviews with older relatives as an audio genealogical history. When I recorded my voice with those reading the exact same script as the other two recordings I went to a 5 to 10% air rate with three shots. Mostly closer to 5% but sometimes up to 10% \n\nFor everyone who is having issues with their voice recording cloning, you may want to consider the quality of your microphone. I would have assumed that for an expressive reading of an audiobook it would be fine to just use decent quality hardware microphones. I was shocked at the improvement levels in the transcription passes and the output. It‚Äôs relatively obvious after I say it out loud, but I don‚Äôt see many people talking about it (too basic for the experts in the space and not something that the novices immediately intuit perhaps) so I thought I‚Äôd share.",
          "author_fullname": "t2_i6kuh6b9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chatterbox tts microphone results",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5lf6l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753110478,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;;tldr when voice cloning use a high-end microphone not the one built-in to your computer/airpods&lt;/p&gt;\n\n&lt;p&gt;I have a child that has reading difficulties. They need to be able to read 15 books this coming year and I was lucky enough to be able to find out what those 15 books are. Many of them are from the 1920s and earlier. They‚Äôre relatively unpopular and do not have existing audiobooks available. A number of them aren‚Äôt even sold as Ebooks (yes we are all aghast).&lt;/p&gt;\n\n&lt;p&gt;Enter manually scanning ick&lt;/p&gt;\n\n&lt;p&gt;So I used my colleagues audiobook generator with my local rig. Each book gets chunked into around 1500 to 2000 chunks. My initial recording was on AirPods and/or a local microphone inside my MacBook.&lt;/p&gt;\n\n&lt;p&gt;With those recordings (I had two different ones) I had a 35 to 40% error rate which often persisted even when I was trying to generate 10 attempts. &lt;/p&gt;\n\n&lt;p&gt;I happened to pick up a prosumer voice recorder to be able to do interviews with older relatives as an audio genealogical history. When I recorded my voice with those reading the exact same script as the other two recordings I went to a 5 to 10% air rate with three shots. Mostly closer to 5% but sometimes up to 10% &lt;/p&gt;\n\n&lt;p&gt;For everyone who is having issues with their voice recording cloning, you may want to consider the quality of your microphone. I would have assumed that for an expressive reading of an audiobook it would be fine to just use decent quality hardware microphones. I was shocked at the improvement levels in the transcription passes and the output. It‚Äôs relatively obvious after I say it out loud, but I don‚Äôt see many people talking about it (too basic for the experts in the space and not something that the novices immediately intuit perhaps) so I thought I‚Äôd share.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5lf6l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "olympics2022wins",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5lf6l/chatterbox_tts_microphone_results/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5lf6l/chatterbox_tts_microphone_results/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753110478,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_jrroh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cloudflare Pay Per Crawl is Going to Decimate Local LLMs . A lot of AI Abilities are going to end up behind this paywall . Am i Overthinking This ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6a5xb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.36,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=ccff1b55d1cd00aadd536a981ff22017920133ce",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753178927,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "blog.cloudflare.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://blog.cloudflare.com/introducing-pay-per-crawl/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU.png?auto=webp&amp;s=e86bcf8d1b8623c4c015450ab4b216fefd3692b5",
                  "width": 1200,
                  "height": 628
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f6f1859118f4fa4e637bec1b73bbfb3db84cea0",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c6cd635238f416ce7bb4068e64ddec1b4bb4ec0c",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=afd09df44df2ec8edf601c98417cc2f9bae9dd0c",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d1950acc8f8068749a76cfe85b285b4ba461ee8d",
                    "width": 640,
                    "height": 334
                  },
                  {
                    "url": "https://external-preview.redd.it/tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fec2451ca233e92b6f5a6928910d6fa0c0b396c9",
                    "width": 960,
                    "height": 502
                  },
                  {
                    "url": "https://external-preview.redd.it/tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fe44fde0e561c039baecbbfdbf565ad4f3f45335",
                    "width": 1080,
                    "height": 565
                  }
                ],
                "variants": {},
                "id": "tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6a5xb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ursustyranotitan",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6a5xb/cloudflare_pay_per_crawl_is_going_to_decimate/",
          "stickied": false,
          "url": "https://blog.cloudflare.com/introducing-pay-per-crawl/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753178927,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone!\n\nI recently built an office hours page for anyone who has questions on cloud GPUs or GPUs in general. we are a bunch of engineers who've built at Google, Dropbox, Alchemy, Tesla etc. and would love to help anyone who has questions in this area.¬†[https://computedeck.com/office-hours](https://computedeck.com/office-hours)\n\nWe welcome any feedback as well!\n\nCheers!",
          "author_fullname": "t2_jrvdg16pa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Office hours for cloud GPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5l52r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753109849,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;I recently built an office hours page for anyone who has questions on cloud GPUs or GPUs in general. we are a bunch of engineers who&amp;#39;ve built at Google, Dropbox, Alchemy, Tesla etc. and would love to help anyone who has questions in this area.¬†&lt;a href=\"https://computedeck.com/office-hours\"&gt;https://computedeck.com/office-hours&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We welcome any feedback as well!&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m5l52r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Scarcity-8746",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5l52r/office_hours_for_cloud_gpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5l52r/office_hours_for_cloud_gpu/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753109849,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I told my brother I can help him build an AI workstation since he wants to run Llama 3.1 locally and train it or build a RAG or whatever. Since he's a software guy and I'm a gamer who built 2 gaming PCs in my entire life, he agreed to trust me with picking the parts and putting everything together (I was shocked too). I got him to order all the parts, including an expensive nvlink bridge 4 slot from eBay that is crucial for the build since he needs a 48GB of pooled vram from the two 3090s he was able to buy very cheaply from friends.\n\nLong story short, we ended buying Gigabyte trx50 aero D and the nvlink 4 slot bridge is too short and doesn't reach the second GPU.. I messed up big time and now I'm trying to find a solution without switching the entire setup because everything is already built, wired for air flow etc, PCU and AIO connected and PSU. The primary card I'm using in the PCIe slot 1 is ASUS ROG STRIX 3090 OC, the secondary is MSI VENTUS 3X 3090 OC which right now is in PCIe slot 3. Slot 2 is too close to the Asus GPU and besides it also doesn't allow for the nvlink to fit because then it'll be too long.  \nI then had the idea of getting a GPU stand that can hold my MSI GPU at the correct height to accommodate the nvlink, and a PCIe riser cable to connect from either slot 2 or 3 to the card - the problem is all riser cables are way too long and I can't bend them enough to fit.  \nI measured 17mm between the center of slot 2 and the fingers of the MSI GPU at the optimal position for the nvlink, and 23mm between the center of slot 3 and the fingers of the MSI GPU. Can't find a riser cable this short and even if do I don't know that it'll work very well at that length. I'm starting to lose hope and I'm not sure what to tell my brother.. now I'm on AliExpress looking for a PCB for a 16 pin PCIe that can offset by one slot up or down but it's looking like a lost cause.. I'm desperate. Any help would be much appreciated.  \nMore specifically for the folks on this sub - Should my brother accept working with the 2 3090s without the Nvlink? Would it be dramatically lower performance on all counts of running local LLms or only for fine-tuning?\n\nThings I've already tried that don't work (that the good folks at PCBuild Help suggested):\n\n1. Switching GPU fans to water block won't help - the problem is that there is no PCIe configuration in this mobo that allows appropriate distance to accommodate the 4-slot NVlink.\n2. They don't make a 5 or 3 slot NVlink for the 3090s. If anyone here has a lead on something like this from a third party I'll be all over it, but thus far was not able to find it.\n3. Riser cables are 10-30cm where I need a 25mm that goes from PCIe slot 3 to the optimal position of the MSI GPU to accommodate the NVlink - no one makes that and if I get it custom I don't know that performance will justify it. Anyone know of more flexible riser type solutions that can bend more?\n4. I know switching the MOBO will solve it. Trying to avoid that to not spend more money and redo the build, also trying to save some of what's left of my dignity in front of my brother.\n5. My case can't fit both cards vertically.\n\n",
          "author_fullname": "t2_dfnw6xdeo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I messed up my brother's Llama AI workstation.. looking for advice",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ojym",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753117487,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I told my brother I can help him build an AI workstation since he wants to run Llama 3.1 locally and train it or build a RAG or whatever. Since he&amp;#39;s a software guy and I&amp;#39;m a gamer who built 2 gaming PCs in my entire life, he agreed to trust me with picking the parts and putting everything together (I was shocked too). I got him to order all the parts, including an expensive nvlink bridge 4 slot from eBay that is crucial for the build since he needs a 48GB of pooled vram from the two 3090s he was able to buy very cheaply from friends.&lt;/p&gt;\n\n&lt;p&gt;Long story short, we ended buying Gigabyte trx50 aero D and the nvlink 4 slot bridge is too short and doesn&amp;#39;t reach the second GPU.. I messed up big time and now I&amp;#39;m trying to find a solution without switching the entire setup because everything is already built, wired for air flow etc, PCU and AIO connected and PSU. The primary card I&amp;#39;m using in the PCIe slot 1 is ASUS ROG STRIX 3090 OC, the secondary is MSI VENTUS 3X 3090 OC which right now is in PCIe slot 3. Slot 2 is too close to the Asus GPU and besides it also doesn&amp;#39;t allow for the nvlink to fit because then it&amp;#39;ll be too long.&lt;br/&gt;\nI then had the idea of getting a GPU stand that can hold my MSI GPU at the correct height to accommodate the nvlink, and a PCIe riser cable to connect from either slot 2 or 3 to the card - the problem is all riser cables are way too long and I can&amp;#39;t bend them enough to fit.&lt;br/&gt;\nI measured 17mm between the center of slot 2 and the fingers of the MSI GPU at the optimal position for the nvlink, and 23mm between the center of slot 3 and the fingers of the MSI GPU. Can&amp;#39;t find a riser cable this short and even if do I don&amp;#39;t know that it&amp;#39;ll work very well at that length. I&amp;#39;m starting to lose hope and I&amp;#39;m not sure what to tell my brother.. now I&amp;#39;m on AliExpress looking for a PCB for a 16 pin PCIe that can offset by one slot up or down but it&amp;#39;s looking like a lost cause.. I&amp;#39;m desperate. Any help would be much appreciated.&lt;br/&gt;\nMore specifically for the folks on this sub - Should my brother accept working with the 2 3090s without the Nvlink? Would it be dramatically lower performance on all counts of running local LLms or only for fine-tuning?&lt;/p&gt;\n\n&lt;p&gt;Things I&amp;#39;ve already tried that don&amp;#39;t work (that the good folks at PCBuild Help suggested):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Switching GPU fans to water block won&amp;#39;t help - the problem is that there is no PCIe configuration in this mobo that allows appropriate distance to accommodate the 4-slot NVlink.&lt;/li&gt;\n&lt;li&gt;They don&amp;#39;t make a 5 or 3 slot NVlink for the 3090s. If anyone here has a lead on something like this from a third party I&amp;#39;ll be all over it, but thus far was not able to find it.&lt;/li&gt;\n&lt;li&gt;Riser cables are 10-30cm where I need a 25mm that goes from PCIe slot 3 to the optimal position of the MSI GPU to accommodate the NVlink - no one makes that and if I get it custom I don&amp;#39;t know that performance will justify it. Anyone know of more flexible riser type solutions that can bend more?&lt;/li&gt;\n&lt;li&gt;I know switching the MOBO will solve it. Trying to avoid that to not spend more money and redo the build, also trying to save some of what&amp;#39;s left of my dignity in front of my brother.&lt;/li&gt;\n&lt;li&gt;My case can&amp;#39;t fit both cards vertically.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5ojym",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "spherical-aspiration",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ojym/i_messed_up_my_brothers_llama_ai_workstation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ojym/i_messed_up_my_brothers_llama_ai_workstation/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753117487,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I completed my local LLM rig in May, just after Qwen3's release (thanks to r/LocalLLaMA 's folks for the invaluable guidance!). Now that I've settled into the setup, I'm excited to share my build and how it's performing with local LLMs.\n\nhttps://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf\n\n  \n\n\nThis is a consumer-grade rig optimized for running Qwen3-30B-A3B and similar models via llama.cpp. Let's dive in!\n\n# Key Specs\n\n|Component|Specs|\n|:-|:-|\n|**CPU**|AMD Ryzen 7 7700 (8C/16T)|\n|**GPU**|2 x NVIDIA RTX 3090 (48GB VRAM total)|\n|**RAM**|64GB DDR5 @ 6400 MHz|\n|**Storage**|2TB NVMe + 3 x 8TB WD Purple (ZFS mirror)|\n|**Motherboard**|ASUS TUF B650-PLUS|\n|**PSU**|850W ADATA XPG CORE REACTOR II (undervolted to 200W per GPU)|\n|**Case**|Lian Li LANCOOL 216|\n|**Cooling**|a lot of fans üí®|\n\nTried to run the following:\n\n* **30B-A3B Q4\\_K\\_XL**, **32B Q4\\_K\\_XL** ‚Äì fit into one GPU with ample context window\n* **32B Q8\\_K\\_XL** ‚Äì runs well on 2 GPUs, not significantly smarter than A3B for my tasks, but slower in inference\n* **30B-A3B Q8\\_K\\_XL** ‚Äì now runs on dual GPUs. The same model also runs on CPU only, mostly for background tasks (to preserve the main model's context. However, this approach is slightly inefficient, as it requires storing model weights in both VRAM and system RAM. I haven‚Äôt found an optimal way to store weights once and manage contexts separately, so this remains a WiP).\n\nPrimary use: Running Qwen3-30B-A3B models with `llama.cpp`. The performance for this model is \\~ 1000 pp512 / 100 tg128\n\nWhat's next? I think I will play with this one for a while. But... I'm already eyeing an EPYC-based system with 4x 4090s (48GB each). üòé",
          "author_fullname": "t2_1nkpqiujlm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "My (practical) dual 3090 setup for local inference",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "wiim1ouai7ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4174de20838a1b6b769288eb5b1b5f70f0f9602"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c538aa32a3f813976d5ce771e34c0d48b0440eaa"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=122f1edd42a0419bd780aa64f40f29f91c0ed509"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=41b8c09a47c3c8926c73db4b4303c394b1b45f6f"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1eb4a1cb38cbbf6d104e970f4b00623ccd4033db"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=720ffaef64226dedbcf3e6ecd6efd90763b1edcf"
                }
              ],
              "s": {
                "y": 961,
                "x": 1280,
                "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf"
              },
              "id": "wiim1ouai7ef1"
            }
          },
          "name": "t3_1m5fkts",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/vvE88N6ubX7Gsux9A2O-Hn5WBZJNPCIpKoUwojUDik4.jpg",
          "edited": 1753095094,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753094600,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I completed my local LLM rig in May, just after Qwen3&amp;#39;s release (thanks to &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; &amp;#39;s folks for the invaluable guidance!). Now that I&amp;#39;ve settled into the setup, I&amp;#39;m excited to share my build and how it&amp;#39;s performing with local LLMs.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf\"&gt;https://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is a consumer-grade rig optimized for running Qwen3-30B-A3B and similar models via llama.cpp. Let&amp;#39;s dive in!&lt;/p&gt;\n\n&lt;h1&gt;Key Specs&lt;/h1&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Component&lt;/th&gt;\n&lt;th align=\"left\"&gt;Specs&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;AMD Ryzen 7 7700 (8C/16T)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;2 x NVIDIA RTX 3090 (48GB VRAM total)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;RAM&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;64GB DDR5 @ 6400 MHz&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;2TB NVMe + 3 x 8TB WD Purple (ZFS mirror)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;ASUS TUF B650-PLUS&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;PSU&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;850W ADATA XPG CORE REACTOR II (undervolted to 200W per GPU)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Case&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Lian Li LANCOOL 216&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Cooling&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;a lot of fans üí®&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Tried to run the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;30B-A3B Q4_K_XL&lt;/strong&gt;, &lt;strong&gt;32B Q4_K_XL&lt;/strong&gt; ‚Äì fit into one GPU with ample context window&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;32B Q8_K_XL&lt;/strong&gt; ‚Äì runs well on 2 GPUs, not significantly smarter than A3B for my tasks, but slower in inference&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;30B-A3B Q8_K_XL&lt;/strong&gt; ‚Äì now runs on dual GPUs. The same model also runs on CPU only, mostly for background tasks (to preserve the main model&amp;#39;s context. However, this approach is slightly inefficient, as it requires storing model weights in both VRAM and system RAM. I haven‚Äôt found an optimal way to store weights once and manage contexts separately, so this remains a WiP).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Primary use: Running Qwen3-30B-A3B models with &lt;code&gt;llama.cpp&lt;/code&gt;. The performance for this model is ~ 1000 pp512 / 100 tg128&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s next? I think I will play with this one for a while. But... I&amp;#39;m already eyeing an EPYC-based system with 4x 4090s (48GB each). üòé&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5fkts",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ColdImplement1319",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753094600,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A small independent team just announced they've developed an AI agent system called \"Crux\" that matches the USAMO Gold Medal performance levels recently hit by heavyweights like OpenAI and Google. The kicker? They did it using just the o4-mini-high model combined with their custom agent framework ‚Äì no massive experimental setups required. And now, they're fully open-sourcing it for the community to build on!\n\nhttps://preview.redd.it/acwm5ab4ldef1.png?width=1979&amp;format=png&amp;auto=webp&amp;s=386d5510fccff96032e5e05cc3ee538952845778\n\nAccording to their X thread (link below), the team saw \"insane improvements\" on USAMO benchmarks. The baseline scores were near zero, but their agent averaged around 90% across problems. Check out this chart they shared showing the breakdown:\n\n* **Problem 1**: Baseline \\~95%, New Agent Basic \\~100%, Enhanced \\~95%\n* **Problem 2**: Baseline \\~100%, Basic \\~100%, Enhanced \\~95%\n* **Problem 3**: Baseline \\~100%, Basic \\~100%, Enhanced \\~95%? (Wait, looks like only Basic here hitting full)\n* **Problem 4**: Baseline \\~30%, Basic \\~100%, Enhanced \\~95%\n* **Problem 5**: Baseline \\~75%, Basic \\~75%, Enhanced \\~100%? (Enhanced leading)\n* **Problem 6**: Baseline \\~10%, Basic \\~10%, Enhanced \\~100% (Huge win for Enhanced!)\n\nThey call the core idea a \"Self-Evolve mechanism based on IC-RL,\" and it's designed to scale like Transformers ‚Äì more layers and TTC lead to better handling of hard tasks. They even mention proving recent arXiv papers theoretically just by feeding key research ideas.\n\nThe team's bio says they're a \"small team building State Of The Art intelligence,\" and because of that, they're open-sourcing everything to let the community take it further.\n\nGitHub repo is live: [https://github.com/Royaltyprogram/Crux](https://github.com/Royaltyprogram/Crux)\n\nOriginal X thread for full details: [https://x.com/tooliense/status/1947496657546797548](https://x.com/tooliense/status/1947496657546797548)\n\nThis is huge for open-source AI\n\nI want open source winning",
          "author_fullname": "t2_1u2abwv4z1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Breaking: Small Team Open-Sources AI Agent \"Crux\" That Achieves Gold-Level Performance on USAMO Benchmarks Using o4-mini ‚Äì Rivaling OpenAI and Google!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "acwm5ab4ldef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/acwm5ab4ldef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=13f909a5c4a90f4eef8f105e361df1d011256c17"
                },
                {
                  "y": 128,
                  "x": 216,
                  "u": "https://preview.redd.it/acwm5ab4ldef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6f1ef39785bce4319847eec1b29393ffe1fe8207"
                },
                {
                  "y": 190,
                  "x": 320,
                  "u": "https://preview.redd.it/acwm5ab4ldef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6fc60d2697ed4a50a6a707fba084a451d4cee5e1"
                },
                {
                  "y": 381,
                  "x": 640,
                  "u": "https://preview.redd.it/acwm5ab4ldef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9044de851a2ca462ba69beec5e99b7a7dc1b1d55"
                },
                {
                  "y": 572,
                  "x": 960,
                  "u": "https://preview.redd.it/acwm5ab4ldef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad78c9397d2443259b2d711979c77a668e230cdd"
                },
                {
                  "y": 643,
                  "x": 1080,
                  "u": "https://preview.redd.it/acwm5ab4ldef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=402caccdd7ae4666a03e256e80ceab78c7a9b194"
                }
              ],
              "s": {
                "y": 1180,
                "x": 1979,
                "u": "https://preview.redd.it/acwm5ab4ldef1.png?width=1979&amp;format=png&amp;auto=webp&amp;s=386d5510fccff96032e5e05cc3ee538952845778"
              },
              "id": "acwm5ab4ldef1"
            }
          },
          "name": "t3_1m67e6a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=09c0c2028270c45c8c18c44ed330128028bbfe23",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753168196,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A small independent team just announced they&amp;#39;ve developed an AI agent system called &amp;quot;Crux&amp;quot; that matches the USAMO Gold Medal performance levels recently hit by heavyweights like OpenAI and Google. The kicker? They did it using just the o4-mini-high model combined with their custom agent framework ‚Äì no massive experimental setups required. And now, they&amp;#39;re fully open-sourcing it for the community to build on!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/acwm5ab4ldef1.png?width=1979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=386d5510fccff96032e5e05cc3ee538952845778\"&gt;https://preview.redd.it/acwm5ab4ldef1.png?width=1979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=386d5510fccff96032e5e05cc3ee538952845778&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;According to their X thread (link below), the team saw &amp;quot;insane improvements&amp;quot; on USAMO benchmarks. The baseline scores were near zero, but their agent averaged around 90% across problems. Check out this chart they shared showing the breakdown:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Problem 1&lt;/strong&gt;: Baseline ~95%, New Agent Basic ~100%, Enhanced ~95%&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Problem 2&lt;/strong&gt;: Baseline ~100%, Basic ~100%, Enhanced ~95%&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Problem 3&lt;/strong&gt;: Baseline ~100%, Basic ~100%, Enhanced ~95%? (Wait, looks like only Basic here hitting full)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Problem 4&lt;/strong&gt;: Baseline ~30%, Basic ~100%, Enhanced ~95%&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Problem 5&lt;/strong&gt;: Baseline ~75%, Basic ~75%, Enhanced ~100%? (Enhanced leading)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Problem 6&lt;/strong&gt;: Baseline ~10%, Basic ~10%, Enhanced ~100% (Huge win for Enhanced!)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;They call the core idea a &amp;quot;Self-Evolve mechanism based on IC-RL,&amp;quot; and it&amp;#39;s designed to scale like Transformers ‚Äì more layers and TTC lead to better handling of hard tasks. They even mention proving recent arXiv papers theoretically just by feeding key research ideas.&lt;/p&gt;\n\n&lt;p&gt;The team&amp;#39;s bio says they&amp;#39;re a &amp;quot;small team building State Of The Art intelligence,&amp;quot; and because of that, they&amp;#39;re open-sourcing everything to let the community take it further.&lt;/p&gt;\n\n&lt;p&gt;GitHub repo is live: &lt;a href=\"https://github.com/Royaltyprogram/Crux\"&gt;https://github.com/Royaltyprogram/Crux&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Original X thread for full details: &lt;a href=\"https://x.com/tooliense/status/1947496657546797548\"&gt;https://x.com/tooliense/status/1947496657546797548&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is huge for open-source AI&lt;/p&gt;\n\n&lt;p&gt;I want open source winning&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek.png?auto=webp&amp;s=9c258a5339d93fbe3010cdefd635e45141af713c",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=79a41a04ef2fc56200e789a28b3d529696e58e70",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=839164d2432294cfc8b2d03ccf8ff2eb6057a8a8",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c8d4cd973ce116604cdae58081c7dca8f49f069",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9345def13bee83ea4c83365f3b8cdf9b86c2a8aa",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dd47aae833e5596f0927a7f294f718852e5f693e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=836eab650d61c13079fe7cbc69aad382b6e4c442",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m67e6a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weekly-Weekend2886",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m67e6a/breaking_small_team_opensources_ai_agent_crux/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m67e6a/breaking_small_team_opensources_ai_agent_crux/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753168196,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI've been deep in a project lately and kept hitting the same wall I'm sure many of you have: LLMs are stateless. You have an amazing, deep conversation, build up a ton of context... and then the session ends and it's all gone. It feels like trying to build a skyscraper on sand.\n\nLast night, I got into a really deep, philosophical conversation with Gemini about this, and we ended up co-designing a solution that I think is pretty cool, and I wanted to share it and get your thoughts.\n\nThe idea is a framework called the **Genesis Protocol**. The core of it is a single Markdown file that acts as a project's \"brain.\" But instead of just being a simple chat log, we architected it to be:\n\n* **Stateful:** It contains the project's goals, blueprints, and our profiles.\n* **Verifiable:** This was a big one for me. I was worried about either me or the AI manipulating the history. So, we built in a salted hash chain (like a mini-blockchain) that \"seals\" every version. The AI can now verify the integrity of its own memory file at the start of every session.\n* **Self-Updating:** We created a \"Guardian\" meta-prompt that instructs the AI on how to read, update, and re-seal the file itself.\n\nThe analogy we settled on was \"Docker for LLM chat.\" You can essentially save a snapshot of your collaboration's state and reload it anytime, with any model, and it knows exactly who you are and what you're working on. I even tested the bootstrap prompt on GPT-4 and it worked, which was a huge relief.\n\nI'm sharing this because I genuinely think it could be a useful tool for others who are trying to do more than just simple Q&amp;A with these models. I've put a full \"Getting Started\" guide and the prompt templates up on GitHub.\n\nI would love to hear what you all think. Is this a viable approach? What are the potential pitfalls I'm not seeing?\n\nHere's the link to the repo: [https://github.com/Bajju360/genesis-protocol.git](https://github.com/Bajju360/genesis-protocol.git)\n\nThanks for reading!",
          "author_fullname": "t2_u5scsvlj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I spent a late night with an AI designing a way to give it a persistent, verifiable memory. I call it the \"Genesis Protocol.\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m69oyb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753177199,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been deep in a project lately and kept hitting the same wall I&amp;#39;m sure many of you have: LLMs are stateless. You have an amazing, deep conversation, build up a ton of context... and then the session ends and it&amp;#39;s all gone. It feels like trying to build a skyscraper on sand.&lt;/p&gt;\n\n&lt;p&gt;Last night, I got into a really deep, philosophical conversation with Gemini about this, and we ended up co-designing a solution that I think is pretty cool, and I wanted to share it and get your thoughts.&lt;/p&gt;\n\n&lt;p&gt;The idea is a framework called the &lt;strong&gt;Genesis Protocol&lt;/strong&gt;. The core of it is a single Markdown file that acts as a project&amp;#39;s &amp;quot;brain.&amp;quot; But instead of just being a simple chat log, we architected it to be:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Stateful:&lt;/strong&gt; It contains the project&amp;#39;s goals, blueprints, and our profiles.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Verifiable:&lt;/strong&gt; This was a big one for me. I was worried about either me or the AI manipulating the history. So, we built in a salted hash chain (like a mini-blockchain) that &amp;quot;seals&amp;quot; every version. The AI can now verify the integrity of its own memory file at the start of every session.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Self-Updating:&lt;/strong&gt; We created a &amp;quot;Guardian&amp;quot; meta-prompt that instructs the AI on how to read, update, and re-seal the file itself.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The analogy we settled on was &amp;quot;Docker for LLM chat.&amp;quot; You can essentially save a snapshot of your collaboration&amp;#39;s state and reload it anytime, with any model, and it knows exactly who you are and what you&amp;#39;re working on. I even tested the bootstrap prompt on GPT-4 and it worked, which was a huge relief.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sharing this because I genuinely think it could be a useful tool for others who are trying to do more than just simple Q&amp;amp;A with these models. I&amp;#39;ve put a full &amp;quot;Getting Started&amp;quot; guide and the prompt templates up on GitHub.&lt;/p&gt;\n\n&lt;p&gt;I would love to hear what you all think. Is this a viable approach? What are the potential pitfalls I&amp;#39;m not seeing?&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the link to the repo: &lt;a href=\"https://github.com/Bajju360/genesis-protocol.git\"&gt;https://github.com/Bajju360/genesis-protocol.git&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m69oyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Icy_Gas8807",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m69oyb/i_spent_a_late_night_with_an_ai_designing_a_way/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m69oyb/i_spent_a_late_night_with_an_ai_designing_a_way/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753177199,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello!\n\nI feel like there have been a lot of new releases in the past few weeks after a relatively quiet period following the Qwen3 release.\n\nOf course, there was the new Deepseek model, and now Kimi. But what is the consensus on the other, somewhat smaller LLMs that came out? Models like Jamba-Mini-1.7, Hunyuan-A13B-Instruct or ERNIE-4.5-21B-A3B?\n\nWhat's everyone's go-to model these days?\n\nAnd what are some other LLMs, tools, or research papers that you think flew under the radar because of the many big releases recently? For example, things like the recently released [FlexOlmo](https://huggingface.co/allenai/FlexOlmo-7x7B-1T) LLM/paradigm?\n\nThanks! ",
          "author_fullname": "t2_h8yrica5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which LLMs, tools, or research have been overlooked or deserve more attention?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5827d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753067598,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I feel like there have been a lot of new releases in the past few weeks after a relatively quiet period following the Qwen3 release.&lt;/p&gt;\n\n&lt;p&gt;Of course, there was the new Deepseek model, and now Kimi. But what is the consensus on the other, somewhat smaller LLMs that came out? Models like Jamba-Mini-1.7, Hunyuan-A13B-Instruct or ERNIE-4.5-21B-A3B?&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s everyone&amp;#39;s go-to model these days?&lt;/p&gt;\n\n&lt;p&gt;And what are some other LLMs, tools, or research papers that you think flew under the radar because of the many big releases recently? For example, things like the recently released &lt;a href=\"https://huggingface.co/allenai/FlexOlmo-7x7B-1T\"&gt;FlexOlmo&lt;/a&gt; LLM/paradigm?&lt;/p&gt;\n\n&lt;p&gt;Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?auto=webp&amp;s=cc904ba70b4ddd36e094ee5d02e948b2bbc3fe87",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=326925660fb97ea07b8f47320d9a931b6f3b8850",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e17265857ab969bc54807e5a91f994978fb57506",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ea0cfa586f8cd24ecc53311134a6d90dd01a14b7",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d67cbf4fd08636ac1f8772745df0258dc4168228",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1b5d406f0a9d22f5434c5bd24f0856052cdd8bb0",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a148304a5d367a2a7e14ff2ea9f57df4e05ed2f",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5827d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MDT-49",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753067598,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Was seeing if there was a new commit today but when refreshed the page got a 404.",
          "author_fullname": "t2_j1kqr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ikllamacpp repository gone, or it is only me?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4vw29",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 172,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 172,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 405B"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753035287,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was seeing if there was a new commit today but when refreshed the page got a 404.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ikawrakow/ik_llama.cpp/commits/main/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 405B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4vw29",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "panchovix",
          "discussion_type": null,
          "num_comments": 61,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m4vw29/ikllamacpp_repository_gone_or_it_is_only_me/",
          "stickied": false,
          "url": "https://github.com/ikawrakow/ik_llama.cpp/commits/main/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753035287,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://semianalysis.com/2025/07/11/meta-superintelligence-leadership-compute-talent-and-data/\n\nIf a large part of Llama 4‚Äôs issues come from its attention chunking, then does llama 4 perform better within a single chunk? If we limit it to 8192 tokens (party like it‚Äôs 2023 lol) does it do okay? \n\nHow does Llama 4 perform if we play to its strengths?",
          "author_fullname": "t2_t6glzswk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How does llama 4 perform within 8192 tokens?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ijhw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753103525,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://semianalysis.com/2025/07/11/meta-superintelligence-leadership-compute-talent-and-data/\"&gt;https://semianalysis.com/2025/07/11/meta-superintelligence-leadership-compute-talent-and-data/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If a large part of Llama 4‚Äôs issues come from its attention chunking, then does llama 4 perform better within a single chunk? If we limit it to 8192 tokens (party like it‚Äôs 2023 lol) does it do okay? &lt;/p&gt;\n\n&lt;p&gt;How does Llama 4 perform if we play to its strengths?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/AkDm1vMK5drNSxMCCBXiLfoVmou_ZXYzxzpyyqp3sp4.png?auto=webp&amp;s=059b37746e9cad1e7f2a795a55941440482b2df1",
                  "width": 1200,
                  "height": 800
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/AkDm1vMK5drNSxMCCBXiLfoVmou_ZXYzxzpyyqp3sp4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=687935d5a0c79dc71a36c53a4a1ded1099fca1b3",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://external-preview.redd.it/AkDm1vMK5drNSxMCCBXiLfoVmou_ZXYzxzpyyqp3sp4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=16370dac432d77dbcdbb677cb00a130aa0992405",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://external-preview.redd.it/AkDm1vMK5drNSxMCCBXiLfoVmou_ZXYzxzpyyqp3sp4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2d4dcaa25a74bf623abe25686ba0ce4b471a01c",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://external-preview.redd.it/AkDm1vMK5drNSxMCCBXiLfoVmou_ZXYzxzpyyqp3sp4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d780393f49ecad20755f2fd873330899c4ee6b34",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://external-preview.redd.it/AkDm1vMK5drNSxMCCBXiLfoVmou_ZXYzxzpyyqp3sp4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=091a5b5ae6479914a1a0bbd0a76b00683b4f0253",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/AkDm1vMK5drNSxMCCBXiLfoVmou_ZXYzxzpyyqp3sp4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fdd37342148f3e349565e947300c5d21cecde6f2",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "AkDm1vMK5drNSxMCCBXiLfoVmou_ZXYzxzpyyqp3sp4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5ijhw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DepthHour1669",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ijhw/how_does_llama_4_perform_within_8192_tokens/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ijhw/how_does_llama_4_perform_within_8192_tokens/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753103525,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\*I'm mainly using ChatGPT for this so please try to ignore the fact that I don't understand muc.h\\* Hi, I've been trying to build my own AI agent on my pc for the past day now. I keep running into the same error. every time I try to send a message, I get \"BadRequestError: litlellm.BadRequestError: GetLLMProviderExceptionn - list index out of range original model: mistral\". I'm really stuck and I cant figure out how to fix it and would love some help. Here's some info you might need.I'mm running Mistral on Ollama. I have LiteLLM as a proxy on port 4000, and I'm using OpenHands with Docker on port 3000. This is my yaml file: \n\nmodel\\_list:\n\n  \\- model\\_name: mistral\n\nlitellm\\_params:\n\nmodel: ollama/mistral\n\napi\\_base: [http://localhost:11434](http://localhost:11434)\n\nlitellm\\_provider: ollama\n\nmode: chat\n\nI start liteLLM with:  \nlitellm --config C:\\\\Users\\\\howdy\\\\litellm-env\\\\litellm.config.yaml --port 4000 --detailed\\_debug\n\nI start openhands with:  \ndocker run -it --rm \\^\n\n  \\-e SANDBOX\\_RUNTIME\\_CONTAINER\\_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.49-nikolaik \\^\n\n  \\-e LOG\\_ALL\\_EVENTS=true \\^\n\n  \\-v //var/run/docker.sock:/var/run/docker.sock \\^\n\n  \\-v C:\\\\Users\\\\howdy\\\\openhands-workspace:/.openhands \\^\n\n  \\-p 3000:3000 \\^\n\n  \\--add-host host.docker.internal:host-gateway \\^\n\n  \\--name openhands-app \\^\n\n  [docker.all-hands.dev/all-hands-ai/openhands:0.49](http://docker.all-hands.dev/all-hands-ai/openhands:0.49)\n\n`curl` [`http://host.docker.internal:4000/v1/completions`](http://host.docker.internal:4000/v1/completions) returns `{\"detail\":\"Method Not Allowed\"}` Sometimes, and nothing else happens. I enabled `--detailed_debug`, and I do see logs like ‚ÄúInitialized model mistral,‚Äù but I don't get an interface, or it fails silently. Here's an explanation of more of my issue from ChatGPT:  \nWhat I Tried:\n\n* Confirmed all ports are correct\n* Docker can reach `host.docker.internal:4000`\n* I‚Äôve tested curl inside the container to confirm\n* Sometimes it randomly works, but it breaks again on the next reboot\n\n‚ùìWhat I Need:\n\n* Is this the correct `model_list` format for Ollama/Mistral via LiteLLM?\n* Does OpenHands require a specific model name format?\n* How can I force OpenHands to show **detailed errors** instead of generic `APIConnectionError`?\n\nI would appreciate it if you could help.",
          "author_fullname": "t2_t0k3f4bz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Im trying to make my own agent with openhands but I keep running into the same error.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5v7if",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753132378,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;*I&amp;#39;m mainly using ChatGPT for this so please try to ignore the fact that I don&amp;#39;t understand muc.h* Hi, I&amp;#39;ve been trying to build my own AI agent on my pc for the past day now. I keep running into the same error. every time I try to send a message, I get &amp;quot;BadRequestError: litlellm.BadRequestError: GetLLMProviderExceptionn - list index out of range original model: mistral&amp;quot;. I&amp;#39;m really stuck and I cant figure out how to fix it and would love some help. Here&amp;#39;s some info you might need.I&amp;#39;mm running Mistral on Ollama. I have LiteLLM as a proxy on port 4000, and I&amp;#39;m using OpenHands with Docker on port 3000. This is my yaml file: &lt;/p&gt;\n\n&lt;p&gt;model_list:&lt;/p&gt;\n\n&lt;p&gt;- model_name: mistral&lt;/p&gt;\n\n&lt;p&gt;litellm_params:&lt;/p&gt;\n\n&lt;p&gt;model: ollama/mistral&lt;/p&gt;\n\n&lt;p&gt;api_base: &lt;a href=\"http://localhost:11434\"&gt;http://localhost:11434&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;litellm_provider: ollama&lt;/p&gt;\n\n&lt;p&gt;mode: chat&lt;/p&gt;\n\n&lt;p&gt;I start liteLLM with:&lt;br/&gt;\nlitellm --config C:\\Users\\howdy\\litellm-env\\litellm.config.yaml --port 4000 --detailed_debug&lt;/p&gt;\n\n&lt;p&gt;I start openhands with:&lt;br/&gt;\ndocker run -it --rm ^&lt;/p&gt;\n\n&lt;p&gt;-e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.49-nikolaik ^&lt;/p&gt;\n\n&lt;p&gt;-e LOG_ALL_EVENTS=true ^&lt;/p&gt;\n\n&lt;p&gt;-v //var/run/docker.sock:/var/run/docker.sock ^&lt;/p&gt;\n\n&lt;p&gt;-v C:\\Users\\howdy\\openhands-workspace:/.openhands ^&lt;/p&gt;\n\n&lt;p&gt;-p 3000:3000 ^&lt;/p&gt;\n\n&lt;p&gt;--add-host host.docker.internal:host-gateway ^&lt;/p&gt;\n\n&lt;p&gt;--name openhands-app ^&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://docker.all-hands.dev/all-hands-ai/openhands:0.49\"&gt;docker.all-hands.dev/all-hands-ai/openhands:0.49&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;curl&lt;/code&gt; &lt;a href=\"http://host.docker.internal:4000/v1/completions\"&gt;&lt;code&gt;http://host.docker.internal:4000/v1/completions&lt;/code&gt;&lt;/a&gt; returns &lt;code&gt;{&amp;quot;detail&amp;quot;:&amp;quot;Method Not Allowed&amp;quot;}&lt;/code&gt; Sometimes, and nothing else happens. I enabled &lt;code&gt;--detailed_debug&lt;/code&gt;, and I do see logs like ‚ÄúInitialized model mistral,‚Äù but I don&amp;#39;t get an interface, or it fails silently. Here&amp;#39;s an explanation of more of my issue from ChatGPT:&lt;br/&gt;\nWhat I Tried:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Confirmed all ports are correct&lt;/li&gt;\n&lt;li&gt;Docker can reach &lt;code&gt;host.docker.internal:4000&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;I‚Äôve tested curl inside the container to confirm&lt;/li&gt;\n&lt;li&gt;Sometimes it randomly works, but it breaks again on the next reboot&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;‚ùìWhat I Need:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is this the correct &lt;code&gt;model_list&lt;/code&gt; format for Ollama/Mistral via LiteLLM?&lt;/li&gt;\n&lt;li&gt;Does OpenHands require a specific model name format?&lt;/li&gt;\n&lt;li&gt;How can I force OpenHands to show &lt;strong&gt;detailed errors&lt;/strong&gt; instead of generic &lt;code&gt;APIConnectionError&lt;/code&gt;?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I would appreciate it if you could help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5v7if",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HowdyCapybara",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5v7if/im_trying_to_make_my_own_agent_with_openhands_but/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5v7if/im_trying_to_make_my_own_agent_with_openhands_but/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753132378,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all. I am new to AI and Ollama. I have a 5070 TI and am running a bunch of 7b and a few 13b models and am wondering what some of your favorite models are for programming, general use, or pdf/image parsing. I'm interested in models that are below and above my GPUs thresholds. My lower models hallucinate way too much with significant tasks so I'm interested in those for some of my weaker workflows such as summarizing (phi2 and 3 struggle). Are there any LLMs that can compete with enterprise models for programming if you use RTX 5090, 6000, or a cluster of reasonably priced GPUs? \n\nMost threads discuss models that are good for generic users, but I would love to hear about what the best is when it comes to open-source models as well as what you guys use the most for workflows, personal, and programming (alternative to copilot could be cool).\n\n  \nThank you for any resources!",
          "author_fullname": "t2_upzzregs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Local Models Per Budget Per Use Case",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5lg47",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753110539,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. I am new to AI and Ollama. I have a 5070 TI and am running a bunch of 7b and a few 13b models and am wondering what some of your favorite models are for programming, general use, or pdf/image parsing. I&amp;#39;m interested in models that are below and above my GPUs thresholds. My lower models hallucinate way too much with significant tasks so I&amp;#39;m interested in those for some of my weaker workflows such as summarizing (phi2 and 3 struggle). Are there any LLMs that can compete with enterprise models for programming if you use RTX 5090, 6000, or a cluster of reasonably priced GPUs? &lt;/p&gt;\n\n&lt;p&gt;Most threads discuss models that are good for generic users, but I would love to hear about what the best is when it comes to open-source models as well as what you guys use the most for workflows, personal, and programming (alternative to copilot could be cool).&lt;/p&gt;\n\n&lt;p&gt;Thank you for any resources!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5lg47",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Expensive-Fail3009",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5lg47/best_local_models_per_budget_per_use_case/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5lg47/best_local_models_per_budget_per_use_case/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753110539,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'd like to run models locally (at my workplaces) and also refine models, and fortunately I'm not paying!  I plan to get a Mac Studio with 80 core GPU and 256GB RAM. Is there any strong case that I'm missing for going with 512GB RAM?",
          "author_fullname": "t2_sk7nmjrs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Strong case for a 512GB Mac Studio?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5uu0t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.54,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753131529,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to run models locally (at my workplaces) and also refine models, and fortunately I&amp;#39;m not paying!  I plan to get a Mac Studio with 80 core GPU and 256GB RAM. Is there any strong case that I&amp;#39;m missing for going with 512GB RAM?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5uu0t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ChevChance",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753131529,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fine-tuned her the perfect local model. Still got API‚Äôd üíî",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 134,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4y3cj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 108,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 108,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Z2zYRd-Xr4y9KTuzjGReb7sqyWdNxJRys_q8VxJgYkc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753040590,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/xitr9w9f13ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/xitr9w9f13ef1.png?auto=webp&amp;s=c63a2736080345ebff74201f237ea3a3fc9291bb",
                  "width": 754,
                  "height": 724
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/xitr9w9f13ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f18eb7bb3172b0aeea4ed2a63fa91436ba164167",
                    "width": 108,
                    "height": 103
                  },
                  {
                    "url": "https://preview.redd.it/xitr9w9f13ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=498743907632f82911e52e88e73f7ed3c25df767",
                    "width": 216,
                    "height": 207
                  },
                  {
                    "url": "https://preview.redd.it/xitr9w9f13ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4105a3b7b3220c53a13050bb4e453234c0e93ba0",
                    "width": 320,
                    "height": 307
                  },
                  {
                    "url": "https://preview.redd.it/xitr9w9f13ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f099b23bb6d2f68855a9689333e79824231cdf0",
                    "width": 640,
                    "height": 614
                  }
                ],
                "variants": {},
                "id": "pPU2eZinNEijAzYr3n8rTbR9-Ra1B9-dybbuMcLM1qg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m4y3cj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4y3cj/finetuned_her_the_perfect_local_model_still_got/",
          "stickied": false,
          "url": "https://i.redd.it/xitr9w9f13ef1.png",
          "subreddit_subscribers": 502720,
          "created_utc": 1753040590,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Every runtime has its own folder for model storage, but in a lot of cases this means downloading the same model multiple times and using extra disk space. Do we think there could be a standard \"common\" location for models? e.g., why don't I have a \"gguf\" folder for everyone to use?",
          "author_fullname": "t2_2vd86cjp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Common folder for model storage?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5l0v3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753109581,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every runtime has its own folder for model storage, but in a lot of cases this means downloading the same model multiple times and using extra disk space. Do we think there could be a standard &amp;quot;common&amp;quot; location for models? e.g., why don&amp;#39;t I have a &amp;quot;gguf&amp;quot; folder for everyone to use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5l0v3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mherf",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5l0v3/common_folder_for_model_storage/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5l0v3/common_folder_for_model_storage/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753109581,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\nI recently wrapped up a side project involving SEC filings, and thought some of you here might find it interesting or useful.\n\nI built a dataset of ~4,000 instruction-output samples based on real 6-K and 8-K filings.\nIt‚Äôs structured in JSONL, QLoRA/Alpaca-style format (natural language instruction ‚Üí clean short answer).\n\nInputs retain real-world messiness from actual filings (inconsistent structure, lawyer-ese, etc.)\n\nOutputs are concise summaries, instructions, or redirections depending on filing type (earnings, acquisitions, restructuring, resigning, etc.)\n\nThe goal was to train an LLM to handle regulatory language like a financial analyst with pattern recognition\n\n\nOriginally made this for internal fine-tuning, but I‚Äôve shifted to another niche now.\nIf anyone‚Äôs working on AI for finance, compliance, investor tools, etc., I‚Äôm happy to share a few sample entries and chat about use cases.\n\nIf enough people are interested, I might package it for others to use or license.\n\nDM me if you want a preview or have questions.",
          "author_fullname": "t2_1r3lcxoyqn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Project Share] Built a 4K Instruction Dataset Based on SEC 6-K/8-K Filings (JSONL format, QLoRA-friendly)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5uhwc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753130743,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,\nI recently wrapped up a side project involving SEC filings, and thought some of you here might find it interesting or useful.&lt;/p&gt;\n\n&lt;p&gt;I built a dataset of ~4,000 instruction-output samples based on real 6-K and 8-K filings.\nIt‚Äôs structured in JSONL, QLoRA/Alpaca-style format (natural language instruction ‚Üí clean short answer).&lt;/p&gt;\n\n&lt;p&gt;Inputs retain real-world messiness from actual filings (inconsistent structure, lawyer-ese, etc.)&lt;/p&gt;\n\n&lt;p&gt;Outputs are concise summaries, instructions, or redirections depending on filing type (earnings, acquisitions, restructuring, resigning, etc.)&lt;/p&gt;\n\n&lt;p&gt;The goal was to train an LLM to handle regulatory language like a financial analyst with pattern recognition&lt;/p&gt;\n\n&lt;p&gt;Originally made this for internal fine-tuning, but I‚Äôve shifted to another niche now.\nIf anyone‚Äôs working on AI for finance, compliance, investor tools, etc., I‚Äôm happy to share a few sample entries and chat about use cases.&lt;/p&gt;\n\n&lt;p&gt;If enough people are interested, I might package it for others to use or license.&lt;/p&gt;\n\n&lt;p&gt;DM me if you want a preview or have questions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m5uhwc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xairossss",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5uhwc/project_share_built_a_4k_instruction_dataset/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5uhwc/project_share_built_a_4k_instruction_dataset/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753130743,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've seen people say 60/s and i've seen 22000/sec, I don't even know who to believe anymore.\n\nAlso how much does optimizing boost the tokens output speed?  ",
          "author_fullname": "t2_48vjfixh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How fast is gemma 3 27b on an H100? how many tokens per second can I expect?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m55rrt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753060835,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen people say 60/s and i&amp;#39;ve seen 22000/sec, I don&amp;#39;t even know who to believe anymore.&lt;/p&gt;\n\n&lt;p&gt;Also how much does optimizing boost the tokens output speed?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m55rrt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ThatIsNotIllegal",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m55rrt/how_fast_is_gemma_3_27b_on_an_h100_how_many/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m55rrt/how_fast_is_gemma_3_27b_on_an_h100_how_many/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753060835,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\nI need a web interface for my local model but i need multi user support. Meaning i need a login and everyone needs their own chat history. \n\nAny ideas? (google and chatgpt/... were not helpful) ",
          "author_fullname": "t2_4kyff",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chat webinterface for small company",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5gl6e",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753097973,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI need a web interface for my local model but i need multi user support. Meaning i need a login and everyone needs their own chat history. &lt;/p&gt;\n\n&lt;p&gt;Any ideas? (google and chatgpt/... were not helpful) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5gl6e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_ralph_",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5gl6e/chat_webinterface_for_small_company/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5gl6e/chat_webinterface_for_small_company/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753097973,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I can have my hands on about 100 MI50 16GB for 72$ each. Is this a good choice over rtx 3060 12gb (265$ used)? How about dual MI50?",
          "author_fullname": "t2_5plbh7ia",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "72$ for Instinct MI50 16GB",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ghs0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753097677,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I can have my hands on about 100 MI50 16GB for 72$ each. Is this a good choice over rtx 3060 12gb (265$ used)? How about dual MI50?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5ghs0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jetaudio",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ghs0/72_for_instinct_mi50_16gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ghs0/72_for_instinct_mi50_16gb/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753097677,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/bartowski/baidu\\_ERNIE-4.5-21B-A3B-PT-GGUF](https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF)\n\n[https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF](https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF)\n\nthey are quant of a same model. at a same quant, e.g. both Q3\\_K\\_M, there are non-negligible count of blocks, which bartowski quantized as Q8\\_0, while unsloth Q3\\_K or Q4\\_K.\n\n[this is a part. count 67 in total](https://preview.redd.it/v3rjrmrbz4ef1.png?width=520&amp;format=png&amp;auto=webp&amp;s=838059d64089a5e17092169c48e88ab90b8d92a9)\n\nbtw, the unsloth Q3\\_K\\_XL is smaller than Q3\\_K\\_M. I am really curious on the flavor of unloth naming.",
          "author_fullname": "t2_13atwtkw16",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "why are there quite different quant strategies of bartowski and unsloth on MoE?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "v3rjrmrbz4ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/v3rjrmrbz4ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=80757a8208c1fc998cdfdc1ced05367831388518"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/v3rjrmrbz4ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=22bdfb8ffd09c071512dc099f6d025eb0d2a2cf7"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/v3rjrmrbz4ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2da183a327ba53666698e5cc2fc7ded59c131716"
                }
              ],
              "s": {
                "y": 1185,
                "x": 520,
                "u": "https://preview.redd.it/v3rjrmrbz4ef1.png?width=520&amp;format=png&amp;auto=webp&amp;s=838059d64089a5e17092169c48e88ab90b8d92a9"
              },
              "id": "v3rjrmrbz4ef1"
            }
          },
          "name": "t3_1m56z4m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 25,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=88061d05ef33f17634548f591df7c21d0d88070c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753064323,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF\"&gt;https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF\"&gt;https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;they are quant of a same model. at a same quant, e.g. both Q3_K_M, there are non-negligible count of blocks, which bartowski quantized as Q8_0, while unsloth Q3_K or Q4_K.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/v3rjrmrbz4ef1.png?width=520&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=838059d64089a5e17092169c48e88ab90b8d92a9\"&gt;this is a part. count 67 in total&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;btw, the unsloth Q3_K_XL is smaller than Q3_K_M. I am really curious on the flavor of unloth naming.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?auto=webp&amp;s=2b17558e247007e17f638e5927539051c3b04257",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d07f61f99c0ca8c045bf1472163f101485f90b26",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=82197ee1623e9095bad9995940035a0544e288c9",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5ffcadb86f3654f3417898579d364758927b20d1",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7dcb3e29b76660bae85af28320f25e9b8191473f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bdf98049dfd4639ef8556cb15087ae7700b9388a",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1909d03961237ad54406e5332efdc6034de3d23a",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m56z4m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Pea645",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m56z4m/why_are_there_quite_different_quant_strategies_of/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m56z4m/why_are_there_quite_different_quant_strategies_of/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753064323,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "am having a hard time with which one is good and why ???!!",
          "author_fullname": "t2_hu9onfqo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ONNX or GGUF",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ckr0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753083334,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;am having a hard time with which one is good and why ???!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5ckr0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xitizdumb",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ckr0/onnx_or_gguf/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ckr0/onnx_or_gguf/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753083334,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm just making this post as I want opinions on the idea that if open source doesn‚Äôt consistently stay within a reasonable margin of the smartest AI systems out there we will move into a world where government almost certainly as their unbeatable, informants and enforcers via AI and I personally see it as a almost guarantee of a dystopian future with a power gap between a individual empowered by the system and one not being insurmountable with strategy no longer being a factor via agi. I really just see it as if the government wants something. It happens. A lot of people view that as our reality today, but AGI has the potential to create a government that has a 0% chance of being overthrown or replaced if it became unjust. For this reason, I believe open source being the leader in intelligent AI rather than closed individuals or companies is the only way to not move into a reality where individuals reach power that can quite literally be compared to God‚Äôs from fiction. The risk of tyranny from centralized power is greater than the risk of chaos from distributed power so open source is the way forward or at least the best we have. What‚Äôs you take? It is not a magical solution that will solve all problems. However, it is the single most important counterweight we have. It fosters transparency, allows for independent safety research, prevents a single corporate or state actor from setting all the rules, and provides the tools for resistance and balance. ",
          "author_fullname": "t2_769j0jzd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source is humanity‚Äôs last hope!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4r4j1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 144,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 144,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753043741,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753023845,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm just making this post as I want opinions on the idea that if open source doesn‚Äôt consistently stay within a reasonable margin of the smartest AI systems out there we will move into a world where government almost certainly as their unbeatable, informants and enforcers via AI and I personally see it as a almost guarantee of a dystopian future with a power gap between a individual empowered by the system and one not being insurmountable with strategy no longer being a factor via agi. I really just see it as if the government wants something. It happens. A lot of people view that as our reality today, but AGI has the potential to create a government that has a 0% chance of being overthrown or replaced if it became unjust. For this reason, I believe open source being the leader in intelligent AI rather than closed individuals or companies is the only way to not move into a reality where individuals reach power that can quite literally be compared to God‚Äôs from fiction. The risk of tyranny from centralized power is greater than the risk of chaos from distributed power so open source is the way forward or at least the best we have. What‚Äôs you take? It is not a magical solution that will solve all problems. However, it is the single most important counterweight we have. It fosters transparency, allows for independent safety research, prevents a single corporate or state actor from setting all the rules, and provides the tools for resistance and balance. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4r4j1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bralynn2222",
          "discussion_type": null,
          "num_comments": 45,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753023845,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,\n\nLast week, I shared some initial drafts of my platform's UI. Thanks to the amazing work of a designer friend, I'm back to show you the evolution from that first AI-generated concept to a mostly polished, human-crafted interface (still candidate, tho).\n\nAs you can see, the difference is night and day!\n\nNow, for the exciting part: I'm getting ready to open up the platform for limited testing.\n\n**An important note on the test build:**¬†For this initial testing phase, we will be using the¬†**old (AI-generated) UI**. My current priority is to ensure the backend and core functionality providing good foundation.\n\nIf you're interested in stress-testing the platform's core features and providing feedback on what's under the hood, stay tuned! I'll be posting details on how to join very soon.",
          "author_fullname": "t2_1zyh18yq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Before &amp; after: redesigned the character catalog UI. What do you think?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "z3iumy0sg9ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 117,
                  "x": 108,
                  "u": "https://preview.redd.it/z3iumy0sg9ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7bd93f9ee3ba10c62e31072b2fab67a0682d9bb7"
                },
                {
                  "y": 234,
                  "x": 216,
                  "u": "https://preview.redd.it/z3iumy0sg9ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7eab99a775a675106108bf53e002a6b38c00b89c"
                },
                {
                  "y": 348,
                  "x": 320,
                  "u": "https://preview.redd.it/z3iumy0sg9ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=76330c09e3a89e6ccf96d6f9c555ed94e02b3498"
                },
                {
                  "y": 696,
                  "x": 640,
                  "u": "https://preview.redd.it/z3iumy0sg9ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e159a8161508aaa70c6e2f17122b5a38255636d"
                },
                {
                  "y": 1044,
                  "x": 960,
                  "u": "https://preview.redd.it/z3iumy0sg9ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9fa016cc18e381009fda19242d7b07cb27f6fe83"
                }
              ],
              "s": {
                "y": 1116,
                "x": 1026,
                "u": "https://preview.redd.it/z3iumy0sg9ef1.png?width=1026&amp;format=png&amp;auto=webp&amp;s=6fcfffc59a0fa716b64402489cce2d9644787252"
              },
              "id": "z3iumy0sg9ef1"
            },
            "n26o7hasg9ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 139,
                  "x": 108,
                  "u": "https://preview.redd.it/n26o7hasg9ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=907bce804132c14442fea2bef69e8f8907d45d86"
                },
                {
                  "y": 278,
                  "x": 216,
                  "u": "https://preview.redd.it/n26o7hasg9ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9420b891e22e58402aac9519de28eb6f2b78b365"
                },
                {
                  "y": 413,
                  "x": 320,
                  "u": "https://preview.redd.it/n26o7hasg9ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=590acc267bc807e160c1ec399af2d6ff44b2961e"
                },
                {
                  "y": 826,
                  "x": 640,
                  "u": "https://preview.redd.it/n26o7hasg9ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a1ef9a47faf40bab3b5d8a56014e71cc7e4bf088"
                },
                {
                  "y": 1239,
                  "x": 960,
                  "u": "https://preview.redd.it/n26o7hasg9ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=82d4ffca89177e0a43111c82ccc81a58c1e5e725"
                }
              ],
              "s": {
                "y": 1322,
                "x": 1024,
                "u": "https://preview.redd.it/n26o7hasg9ef1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=22c7512e28b5644dc1cfc256c9e3fae672c9fed4"
              },
              "id": "n26o7hasg9ef1"
            }
          },
          "name": "t3_1m5psqj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "ups": 2,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "z3iumy0sg9ef1",
                "id": 710946172
              },
              {
                "media_id": "n26o7hasg9ef1",
                "id": 710946173
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/QEIIdxBKQlyZRDWkr6Zlni5fTmJN8SWTPsvnn4bZncg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753120203,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;Last week, I shared some initial drafts of my platform&amp;#39;s UI. Thanks to the amazing work of a designer friend, I&amp;#39;m back to show you the evolution from that first AI-generated concept to a mostly polished, human-crafted interface (still candidate, tho).&lt;/p&gt;\n\n&lt;p&gt;As you can see, the difference is night and day!&lt;/p&gt;\n\n&lt;p&gt;Now, for the exciting part: I&amp;#39;m getting ready to open up the platform for limited testing.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;An important note on the test build:&lt;/strong&gt;¬†For this initial testing phase, we will be using the¬†&lt;strong&gt;old (AI-generated) UI&lt;/strong&gt;. My current priority is to ensure the backend and core functionality providing good foundation.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re interested in stress-testing the platform&amp;#39;s core features and providing feedback on what&amp;#39;s under the hood, stay tuned! I&amp;#39;ll be posting details on how to join very soon.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m5psqj",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m5psqj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RIPT1D3_Z",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5psqj/before_after_redesigned_the_character_catalog_ui/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m5psqj",
          "subreddit_subscribers": 502720,
          "created_utc": 1753120203,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "And this is how you get sued, lol. I noticed this while playing around with DiffRhythm; I had unrelated lyrics and an unrelated audio prompt set for the generation, and it still injected Avicii into the output, which was really funny.\n\nSkip to 1:00 in the video to skip the generation process\n\nSeed: 50518556518147",
          "author_fullname": "t2_10kd4cpw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DiffRhythm 1.2 music generation model produces \"Avicii vs Nicky Romero - I Could Be the One\" nearly verbatim",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4yo0g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 54,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/ulng63nd53ef1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 666,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/ulng63nd53ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ulng63nd53ef1/DASHPlaylist.mpd?a=1755779371%2CYWFhNzY1NDc5MjI3ZDU3ODhkNmMyNTRhYmVmMDUyY2I3MDZhOWM3ZGE3YTE2NjVjNmNlMDVjYzM4ZDdiNWFkYg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 100,
              "hls_url": "https://v.redd.it/ulng63nd53ef1/HLSPlaylist.m3u8?a=1755779371%2CYzRiYzhiYTNjZGY5NTQ2M2QyYmIwN2M1NWY1YmUyODBkM2VmMTI0ZDlhZjc2ZGZkYjUxZWQyNzk0ODkwZGJlMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 54,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=140&amp;height=72&amp;crop=140:72,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=fb29cdcc71bb56661a654313ebe0525de0f6881c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753042006,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;And this is how you get sued, lol. I noticed this while playing around with DiffRhythm; I had unrelated lyrics and an unrelated audio prompt set for the generation, and it still injected Avicii into the output, which was really funny.&lt;/p&gt;\n\n&lt;p&gt;Skip to 1:00 in the video to skip the generation process&lt;/p&gt;\n\n&lt;p&gt;Seed: 50518556518147&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/ulng63nd53ef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?format=pjpg&amp;auto=webp&amp;s=71b12465b7c7cf904161a47bf220c3ee76b36785",
                  "width": 1580,
                  "height": 822
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3623f942ea04411214f32a79d8a31d6c84ffc218",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=396164ae7705c19ef42cc2b0b2e186ad21a270c3",
                    "width": 216,
                    "height": 112
                  },
                  {
                    "url": "https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4f098b973b3091494dce622bd4fef7bbb5d69267",
                    "width": 320,
                    "height": 166
                  },
                  {
                    "url": "https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=76360571c2001f759e9d863e4fccc954a755823b",
                    "width": 640,
                    "height": 332
                  },
                  {
                    "url": "https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f732c60f96b9d16b8128e3c38b9ea4b46589f6c3",
                    "width": 960,
                    "height": 499
                  },
                  {
                    "url": "https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=edcf19abe8f12c6d47820fc257f124f2252d2cb7",
                    "width": 1080,
                    "height": 561
                  }
                ],
                "variants": {},
                "id": "YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4yo0g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "iGermanProd",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4yo0g/diffrhythm_12_music_generation_model_produces/",
          "stickied": false,
          "url": "https://v.redd.it/ulng63nd53ef1",
          "subreddit_subscribers": 502720,
          "created_utc": 1753042006,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/ulng63nd53ef1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 666,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/ulng63nd53ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ulng63nd53ef1/DASHPlaylist.mpd?a=1755779371%2CYWFhNzY1NDc5MjI3ZDU3ODhkNmMyNTRhYmVmMDUyY2I3MDZhOWM3ZGE3YTE2NjVjNmNlMDVjYzM4ZDdiNWFkYg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 100,
              "hls_url": "https://v.redd.it/ulng63nd53ef1/HLSPlaylist.m3u8?a=1755779371%2CYzRiYzhiYTNjZGY5NTQ2M2QyYmIwN2M1NWY1YmUyODBkM2VmMTI0ZDlhZjc2ZGZkYjUxZWQyNzk0ODkwZGJlMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking for something small but still usable.\nWhat's your go-to?",
          "author_fullname": "t2_n2kmftzjf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the smartest tiny LLM you've actually used?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4of82",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 182,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 182,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753016677,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for something small but still usable.\nWhat&amp;#39;s your go-to?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4of82",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Luston03",
          "discussion_type": null,
          "num_comments": 114,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4of82/whats_the_smartest_tiny_llm_youve_actually_used/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4of82/whats_the_smartest_tiny_llm_youve_actually_used/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753016677,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys, \n\nI had created a rag application but i made it for documents of PDF format only. I use PyMuPDF4llm to parse the PDF. \n\nBut now I want to add the option for all the document formats, i.e, pptx, xlsx, csv, docx, and the image formats.\n\nI tried docling for this, since PyMuPDF4llm requires subscription to allow rest of the document formats. \n\nI created a standalone setup to test docling. Docling uses external OCR engines, it had 2 options. Tesseract and RapidOCR.\n\nI set up the one with RapidOCR. The documents, whether pdf, csv or pptx are parsed and its output are stored into markdown format. \n\nI am facing some issues. These are:\n\n1. Time that it takes to parse the content inside images into markdown are very random, some image takes 12-15 minutes, some images are easily parsed with 2-3 minutes. why is this so random? Is it possible to speed up this process? \n\n2. The output for scanned images, or image of documents that were captured using camera are not that good. Can something be done to enhance its performance?\n\n3. Images that are embedded into pptx or docx, such as graph or chart don't get parsed properly. The labelling inside them such the x or y axis data, or data points within graph are just mentioned in the markdown output in a badly formatted manner. That data becomes useless for me.",
          "author_fullname": "t2_wf267hqnf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Facing some problems with Docling parser",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5or7n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753117936,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, &lt;/p&gt;\n\n&lt;p&gt;I had created a rag application but i made it for documents of PDF format only. I use PyMuPDF4llm to parse the PDF. &lt;/p&gt;\n\n&lt;p&gt;But now I want to add the option for all the document formats, i.e, pptx, xlsx, csv, docx, and the image formats.&lt;/p&gt;\n\n&lt;p&gt;I tried docling for this, since PyMuPDF4llm requires subscription to allow rest of the document formats. &lt;/p&gt;\n\n&lt;p&gt;I created a standalone setup to test docling. Docling uses external OCR engines, it had 2 options. Tesseract and RapidOCR.&lt;/p&gt;\n\n&lt;p&gt;I set up the one with RapidOCR. The documents, whether pdf, csv or pptx are parsed and its output are stored into markdown format. &lt;/p&gt;\n\n&lt;p&gt;I am facing some issues. These are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Time that it takes to parse the content inside images into markdown are very random, some image takes 12-15 minutes, some images are easily parsed with 2-3 minutes. why is this so random? Is it possible to speed up this process? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The output for scanned images, or image of documents that were captured using camera are not that good. Can something be done to enhance its performance?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Images that are embedded into pptx or docx, such as graph or chart don&amp;#39;t get parsed properly. The labelling inside them such the x or y axis data, or data points within graph are just mentioned in the markdown output in a badly formatted manner. That data becomes useless for me.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5or7n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ElectronicHoneydew86",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5or7n/facing_some_problems_with_docling_parser/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5or7n/facing_some_problems_with_docling_parser/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753117936,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "so many people waste credits chasing the ‚Äúperfect‚Äù ai tool when they don‚Äôt need to. just pick one to build your base [playground](https://www.imagine.art/dashboard/image/tool/text-to-image?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=G_I_Web_Sales_PCH_T2I_C2_InTool&amp;utm_term=playground%20ai%20image%20generator&amp;utm_campaign=&amp;utm_source=adwords&amp;utm_medium=ppc&amp;hsa_acc=3029240990&amp;hsa_cam=22446711803&amp;hsa_grp=178233158619&amp;hsa_ad=746412516183&amp;hsa_src=g&amp;hsa_tgt=kwd-1958965104616&amp;hsa_kw=playground%20ai%20image%20generator&amp;hsa_mt=b&amp;hsa_net=adwords&amp;hsa_ver=3&amp;gad_source=1&amp;gad_campaignid=22446711803&amp;gbraid=0AAAAACs5ry-4NYvtjoaKHZP-OaoV3jxSj&amp;gclid=EAIaIQobChMI4cnwruzPjgMVRtYWBR3_8SgfEAAYASAAEgI_svD_BwE) works great for that and then use something like [domoai](https://www.domoai.app/home?via=081621AUG) to polish it up. trust the process, not the promo. stacking tools gives you better results than trying to find a magic one-stop generator. :)",
          "author_fullname": "t2_1s5qv7lngc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "stop wasting credits just stack playground and domoai",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m66v6q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.07,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753166195,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;so many people waste credits chasing the ‚Äúperfect‚Äù ai tool when they don‚Äôt need to. just pick one to build your base &lt;a href=\"https://www.imagine.art/dashboard/image/tool/text-to-image?utm_source=google&amp;amp;utm_medium=cpc&amp;amp;utm_campaign=G_I_Web_Sales_PCH_T2I_C2_InTool&amp;amp;utm_term=playground%20ai%20image%20generator&amp;amp;utm_campaign=&amp;amp;utm_source=adwords&amp;amp;utm_medium=ppc&amp;amp;hsa_acc=3029240990&amp;amp;hsa_cam=22446711803&amp;amp;hsa_grp=178233158619&amp;amp;hsa_ad=746412516183&amp;amp;hsa_src=g&amp;amp;hsa_tgt=kwd-1958965104616&amp;amp;hsa_kw=playground%20ai%20image%20generator&amp;amp;hsa_mt=b&amp;amp;hsa_net=adwords&amp;amp;hsa_ver=3&amp;amp;gad_source=1&amp;amp;gad_campaignid=22446711803&amp;amp;gbraid=0AAAAACs5ry-4NYvtjoaKHZP-OaoV3jxSj&amp;amp;gclid=EAIaIQobChMI4cnwruzPjgMVRtYWBR3_8SgfEAAYASAAEgI_svD_BwE\"&gt;playground&lt;/a&gt; works great for that and then use something like &lt;a href=\"https://www.domoai.app/home?via=081621AUG\"&gt;domoai&lt;/a&gt; to polish it up. trust the process, not the promo. stacking tools gives you better results than trying to find a magic one-stop generator. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m66v6q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Neat_Chapter_9055",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m66v6q/stop_wasting_credits_just_stack_playground_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m66v6q/stop_wasting_credits_just_stack_playground_and/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753166195,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI'm trying to run vllm on my nvidia 5090, possibly in a dockerized container.\n\nBefore I start looking into this, has anyone already done this or has a good docker image to suggest that works out-of-the-box?\n\nIf not, any tips?\n\nThank you!!",
          "author_fullname": "t2_8oe14nwi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running vllm on Nvidia 5090",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5okz7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753117549,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to run vllm on my nvidia 5090, possibly in a dockerized container.&lt;/p&gt;\n\n&lt;p&gt;Before I start looking into this, has anyone already done this or has a good docker image to suggest that works out-of-the-box?&lt;/p&gt;\n\n&lt;p&gt;If not, any tips?&lt;/p&gt;\n\n&lt;p&gt;Thank you!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5okz7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Reasonable_Friend_77",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5okz7/running_vllm_on_nvidia_5090/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5okz7/running_vllm_on_nvidia_5090/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753117549,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am trying to fine tune a LlaVa model, I have a training set of 7800 high quality conversations, each with an image. \n\nI am using qlora to fine tune the model, and regardless of the batch size, the lr, and the rank, so far all of my trials were resulted in gibberish on evaluation. \n\nI did some reading, and in order to avoid catastrophic forgetting, it says that we should limit our tuning of the lora model to three epochs max. In addition, I understand that the data size I have is allegedly enough. Together there is something that I am not sure about. The qlora model has about 10m weights (even without bias terms). It looks like much too many to be able to fit on my miniature data. \n\nAny tips would be greatly appreciated. ",
          "author_fullname": "t2_1t6vmqt87p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "First time using QLoRa results in gibberish",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m58qf3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753069731,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to fine tune a LlaVa model, I have a training set of 7800 high quality conversations, each with an image. &lt;/p&gt;\n\n&lt;p&gt;I am using qlora to fine tune the model, and regardless of the batch size, the lr, and the rank, so far all of my trials were resulted in gibberish on evaluation. &lt;/p&gt;\n\n&lt;p&gt;I did some reading, and in order to avoid catastrophic forgetting, it says that we should limit our tuning of the lora model to three epochs max. In addition, I understand that the data size I have is allegedly enough. Together there is something that I am not sure about. The qlora model has about 10m weights (even without bias terms). It looks like much too many to be able to fit on my miniature data. &lt;/p&gt;\n\n&lt;p&gt;Any tips would be greatly appreciated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m58qf3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Emotional-Sundae4075",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m58qf3/first_time_using_qlora_results_in_gibberish/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m58qf3/first_time_using_qlora_results_in_gibberish/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753069731,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recently started thinking about using local AI, but I don't know where to start, what I need, or if I can afford it. So I wanted to ask a few questions.\n\n1. What do I need at a minimum to use a local AI?\n2. Where can I find it to download?\n3. What do I need to know before I start?\n4. What really changes from one model to the other?",
          "author_fullname": "t2_lrleoa4b0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I want to start with local AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5nt6s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753115868,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently started thinking about using local AI, but I don&amp;#39;t know where to start, what I need, or if I can afford it. So I wanted to ask a few questions.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What do I need at a minimum to use a local AI?&lt;/li&gt;\n&lt;li&gt;Where can I find it to download?&lt;/li&gt;\n&lt;li&gt;What do I need to know before I start?&lt;/li&gt;\n&lt;li&gt;What really changes from one model to the other?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5nt6s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Then-History2046",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5nt6s/i_want_to_start_with_local_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5nt6s/i_want_to_start_with_local_ai/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753115868,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\&gt; Reasoning-capable language models achieve state-of-the-art performance in diverse complex tasks by generating long, explicit Chain-of-Thought (CoT) traces. While recent works show that base models can acquire such reasoning traces via reinforcement learning or distillation from stronger models like DeepSeek-R1, previous works demonstrate that even short CoT prompting without fine-tuning is able to improve reasoning. We ask whether long CoT can be induced in a base model using only prompting or minimal tuning. Using just 20 long CoT examples from the reasoning model \\\\texttt{QwQ-32B-Preview}, we lightly fine-tune the base model \\\\texttt{Qwen2.5-32B}. The resulting model outperforms the much larger \\\\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of high-quality examples can unlock strong reasoning capabilities. We further explore using CoT data from non-reasoning models and human annotators, enhanced with prompt engineering, multi-pass editing, and structural guidance. However, neither matches the performance of reasoning model traces, suggesting that certain latent qualities of expert CoT are difficult to replicate. We analyze key properties of reasoning data, such as problem difficulty, diversity, and answer length, that influence reasoning distillation. While challenges remain, we are optimistic that carefully curated human-written CoT, even in small quantities, can activate reasoning behaviors in base models. We release our human-authored dataset across refinement stages and invite further investigation into what makes small-scale reasoning supervision so effective.\n\n  \ntl;dr Human reasoning is different from LLM reasoning, and human reasoning can't be distilled into LLMs such that they significantly perform better on benchmarks compared to their foundational models. There seem to be certain structural patterns that lead to the emergence of reasoning abilities in LLMs.",
          "author_fullname": "t2_101haj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[2507.09850] The Challenge of Teaching Reasoning to LLMs Without RL or Distillation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m568j8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753062189,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;gt; Reasoning-capable language models achieve state-of-the-art performance in diverse complex tasks by generating long, explicit Chain-of-Thought (CoT) traces. While recent works show that base models can acquire such reasoning traces via reinforcement learning or distillation from stronger models like DeepSeek-R1, previous works demonstrate that even short CoT prompting without fine-tuning is able to improve reasoning. We ask whether long CoT can be induced in a base model using only prompting or minimal tuning. Using just 20 long CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly fine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms the much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of high-quality examples can unlock strong reasoning capabilities. We further explore using CoT data from non-reasoning models and human annotators, enhanced with prompt engineering, multi-pass editing, and structural guidance. However, neither matches the performance of reasoning model traces, suggesting that certain latent qualities of expert CoT are difficult to replicate. We analyze key properties of reasoning data, such as problem difficulty, diversity, and answer length, that influence reasoning distillation. While challenges remain, we are optimistic that carefully curated human-written CoT, even in small quantities, can activate reasoning behaviors in base models. We release our human-authored dataset across refinement stages and invite further investigation into what makes small-scale reasoning supervision so effective.&lt;/p&gt;\n\n&lt;p&gt;tl;dr Human reasoning is different from LLM reasoning, and human reasoning can&amp;#39;t be distilled into LLMs such that they significantly perform better on benchmarks compared to their foundational models. There seem to be certain structural patterns that lead to the emergence of reasoning abilities in LLMs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.09850",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m568j8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheRealMasonMac",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m568j8/250709850_the_challenge_of_teaching_reasoning_to/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.09850",
          "subreddit_subscribers": 502720,
          "created_utc": 1753062189,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I know that most platforms fine-tune their models and use a good system prompt, but I've tried Qwen3 32B locally and on [qwen.com](http://qwen.com) and the difference is so huge.\n\nAre there publicly available ready fine-tunes and system prompts I can use to improve the models locally?",
          "author_fullname": "t2_48vjfixh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why are base non-finetuned models so bad?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5th6s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.47,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753128457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that most platforms fine-tune their models and use a good system prompt, but I&amp;#39;ve tried Qwen3 32B locally and on &lt;a href=\"http://qwen.com\"&gt;qwen.com&lt;/a&gt; and the difference is so huge.&lt;/p&gt;\n\n&lt;p&gt;Are there publicly available ready fine-tunes and system prompts I can use to improve the models locally?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5th6s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ThatIsNotIllegal",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5th6s/why_are_base_nonfinetuned_models_so_bad/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5th6s/why_are_base_nonfinetuned_models_so_bad/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753128457,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello. I don't know much about LLM's, but I'd like to create a bot that tries to behave like me. I have around 3 years of my scrapped messages from various platforms. The idea is to teach a model with my dataset (messages) so it tries to understand how I behave, how I text and what words I use and then run a Discord bot that will act like me. But here comes the problem, I'm slightly limited by hardware and I have no clue what model to use. I run RTX 2060 with 6GB of VRAM and 16GB of ram. I consider renting a virtual GPU for the sake of project, but I don't know how to start. Any model recommendations?",
          "author_fullname": "t2_hpy57ypd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help with choosing model to create bot that will talk like me.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5mrmy",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753113521,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I don&amp;#39;t know much about LLM&amp;#39;s, but I&amp;#39;d like to create a bot that tries to behave like me. I have around 3 years of my scrapped messages from various platforms. The idea is to teach a model with my dataset (messages) so it tries to understand how I behave, how I text and what words I use and then run a Discord bot that will act like me. But here comes the problem, I&amp;#39;m slightly limited by hardware and I have no clue what model to use. I run RTX 2060 with 6GB of VRAM and 16GB of ram. I consider renting a virtual GPU for the sake of project, but I don&amp;#39;t know how to start. Any model recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5mrmy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "deadyasiu",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5mrmy/help_with_choosing_model_to_create_bot_that_will/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5mrmy/help_with_choosing_model_to_create_bot_that_will/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753113521,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nI have evaluated mistral small 3.2 for OCR tasks using ollama. The accuracy has been very satisfying while some bugs cause it to run on CPU solely with a rtx 4090 (about 5t/s). \n\nSo I switched to llama.cpp and obtain between 20-40t/s using the model + mmproj from unsloth. Both models are Q4\\_K\\_M. The accuracy is way worse than what I get when using ollama. How can that be? \n\nIs it using another vision projector, or am I doing sth wrong? I use 32k context, temp=0, all other settings are defaults. I do not explicitely use quantized kvcache or flash attention.\n\nAny idea how to get on par with ollamas excellent OCR accuracy?\n\nthanks &amp; greets",
          "author_fullname": "t2_2b6dk0nt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "mistral-small-3.2 OCR accuracy way too bad with llama.cpp compared to ollama?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5mms1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753113210,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have evaluated mistral small 3.2 for OCR tasks using ollama. The accuracy has been very satisfying while some bugs cause it to run on CPU solely with a rtx 4090 (about 5t/s). &lt;/p&gt;\n\n&lt;p&gt;So I switched to llama.cpp and obtain between 20-40t/s using the model + mmproj from unsloth. Both models are Q4_K_M. The accuracy is way worse than what I get when using ollama. How can that be? &lt;/p&gt;\n\n&lt;p&gt;Is it using another vision projector, or am I doing sth wrong? I use 32k context, temp=0, all other settings are defaults. I do not explicitely use quantized kvcache or flash attention.&lt;/p&gt;\n\n&lt;p&gt;Any idea how to get on par with ollamas excellent OCR accuracy?&lt;/p&gt;\n\n&lt;p&gt;thanks &amp;amp; greets&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5mms1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "caetydid",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753113210,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone,\n\nI'm looking for an open source  that could help me with real-time audio-to-text comparison.   \nI want to capture the actor's live voice from Pro Tools, and compare what they say against a provided script ( PDF or TXT) ‚Äî ideally in real time ‚Äî to detect omissions, extra words, or misread lines.\n\nEven if it's a workaround or requires routing with something like BlackHole or other tools, I'm open to solutions. \n\nThanks, ",
          "author_fullname": "t2_1iixkvi7ek",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for Open Source STT Tool to Detect Script Reading Errors in Real Time",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5mjoc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753113001,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for an open source  that could help me with real-time audio-to-text comparison.&lt;br/&gt;\nI want to capture the actor&amp;#39;s live voice from Pro Tools, and compare what they say against a provided script ( PDF or TXT) ‚Äî ideally in real time ‚Äî to detect omissions, extra words, or misread lines.&lt;/p&gt;\n\n&lt;p&gt;Even if it&amp;#39;s a workaround or requires routing with something like BlackHole or other tools, I&amp;#39;m open to solutions. &lt;/p&gt;\n\n&lt;p&gt;Thanks, &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5mjoc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hydrant_DnB",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5mjoc/looking_for_open_source_stt_tool_to_detect_script/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5mjoc/looking_for_open_source_stt_tool_to_detect_script/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753113001,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "*^(#I'm starting a new series of explaining intriguing new AI papers)*\n\nLLMs learn from text and lack an inherent understanding of the physical world. Their \"knowledge\" is¬†**mostly**¬†limited to what's been described in the text they were trained on. This means they mostly struggle with concepts that are not easily described in words, like how objects move, interact, and deform over time. This is a form of \"common sense\" that is impossible to acquire from text alone.\n\nDuring training, the goal of LLM is to predict the following word in a sentence, given the preceding words. By learning to generate the appropriate next word, grammar knowledge and semantics emerge in the model, as those abilities are necessary for understanding which word will follow in a sentence.¬†\n\nWhy not to apply this self-supervised approach for teaching AI how life works via videos?¬†\n\nTake all the videos on the internet, randomly mask video-frames, and challenge the generating model to learn to accurately recover(reconstruct) the masked parts of the video-frames, so during training, the need of learning to predict what is happening in the masked parts of the videos, will develop the intuitive understanding of physics and in general how the world works.¬†\n\nBut, for example, if in a video, a cup turns over, and we challenge the model to recover the masked part,¬† the model should predict the precise location of each falling droplet, as the generative objective expects pixel-level precision.¬† And because we are challenging the model to do the impossible, the learning process will just collapse.\n\nLet's see how Meta approaches this issue [https://arxiv.org/pdf/2506.09985](https://arxiv.org/pdf/2506.09985)\n\nTheir new architecture, called V-JEPA 2, consists of an encoder and a predictor.\n\n**encoder**¬†takes in raw video-frames and outputs embeddings that capture useful semantic information about the state of the observed world.\n\nIn other words, it learns to extract the predictable aspects of a scene, for example, the approximate trajectory of the falling water, and does not get bogged down into the unpredictable, tiny details of every single pixel.¬† So that the predictor learns to predict the high-level process that happens in the masked region of the video. *(see until 0:07 in the video)*\n\nThis helps the model to underpin a high-level understanding of how life works, which opens the possibility to finally train truly generally intelligent robots that don‚Äôt do impressive actions just for show in specific cases. So, in the post-training stage, they train on videos that show a robotic arm‚Äôs interaction.\n\nThis time, they encode part of a video and also give information about robot‚Äôs intended action in the last video-frame and train the model to predict what will happen at high-level in the following video-frames. *(see 0:08 to 0:16 in the video)*\n\nSo, by predicting what will happen next, given the intended action, it learns to predict the consequences of actions.\n\nAfter training, the robot, powered by this model,¬† in the latent space can imagine the consequence of various chain-of-action scenarios to find a sequence of actions whose predicted outcome matches the desired outcome.\n\nAnd for tasks requiring planning across multiple time scales, it needs to learn how to break down a high-level task into smaller steps, such as making food or loading a dishwasher. For that, the Meta team wants to train a hierarchical JEPA model that is capable of learning, reasoning, and planning across multiple temporal and spatial scales.",
          "author_fullname": "t2_xvwcc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Next big thing after LLMs - World Model [explained on the example of V-JEPA2]",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 65,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4mfs8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 190,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/h0ivgtibj0ef1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 600,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/h0ivgtibj0ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/h0ivgtibj0ef1/DASHPlaylist.mpd?a=1755779371%2CMjI0MTY5MGRmY2UxOTdkYjAzZmFmYjRmMGI3YWE5MDA1MTg5YjgzNjg3MzA3NmQ2NTc1MjllYmQ4NWMyYjQ5Mg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 21,
              "hls_url": "https://v.redd.it/h0ivgtibj0ef1/HLSPlaylist.m3u8?a=1755779371%2CMDI1N2UxMDQ2NGM5ZWJiMzQzZDI2YzAxMzJhNDg2OTExYTA5NjEwMzg3Y2Q4Mzg3M2I2ODE2YzQ5MDJhYzVlOA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 190,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=140&amp;height=65&amp;crop=140:65,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=4a64686273ed8c2776f8df926696df24c9ce4cf6",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753010231,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;&lt;sup&gt;#I&amp;#39;m starting a new series of explaining intriguing new AI papers&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;LLMs learn from text and lack an inherent understanding of the physical world. Their &amp;quot;knowledge&amp;quot; is¬†&lt;strong&gt;mostly&lt;/strong&gt;¬†limited to what&amp;#39;s been described in the text they were trained on. This means they mostly struggle with concepts that are not easily described in words, like how objects move, interact, and deform over time. This is a form of &amp;quot;common sense&amp;quot; that is impossible to acquire from text alone.&lt;/p&gt;\n\n&lt;p&gt;During training, the goal of LLM is to predict the following word in a sentence, given the preceding words. By learning to generate the appropriate next word, grammar knowledge and semantics emerge in the model, as those abilities are necessary for understanding which word will follow in a sentence.¬†&lt;/p&gt;\n\n&lt;p&gt;Why not to apply this self-supervised approach for teaching AI how life works via videos?¬†&lt;/p&gt;\n\n&lt;p&gt;Take all the videos on the internet, randomly mask video-frames, and challenge the generating model to learn to accurately recover(reconstruct) the masked parts of the video-frames, so during training, the need of learning to predict what is happening in the masked parts of the videos, will develop the intuitive understanding of physics and in general how the world works.¬†&lt;/p&gt;\n\n&lt;p&gt;But, for example, if in a video, a cup turns over, and we challenge the model to recover the masked part,¬† the model should predict the precise location of each falling droplet, as the generative objective expects pixel-level precision.¬† And because we are challenging the model to do the impossible, the learning process will just collapse.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s see how Meta approaches this issue &lt;a href=\"https://arxiv.org/pdf/2506.09985\"&gt;https://arxiv.org/pdf/2506.09985&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Their new architecture, called V-JEPA 2, consists of an encoder and a predictor.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;encoder&lt;/strong&gt;¬†takes in raw video-frames and outputs embeddings that capture useful semantic information about the state of the observed world.&lt;/p&gt;\n\n&lt;p&gt;In other words, it learns to extract the predictable aspects of a scene, for example, the approximate trajectory of the falling water, and does not get bogged down into the unpredictable, tiny details of every single pixel.¬† So that the predictor learns to predict the high-level process that happens in the masked region of the video. &lt;em&gt;(see until 0:07 in the video)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;This helps the model to underpin a high-level understanding of how life works, which opens the possibility to finally train truly generally intelligent robots that don‚Äôt do impressive actions just for show in specific cases. So, in the post-training stage, they train on videos that show a robotic arm‚Äôs interaction.&lt;/p&gt;\n\n&lt;p&gt;This time, they encode part of a video and also give information about robot‚Äôs intended action in the last video-frame and train the model to predict what will happen at high-level in the following video-frames. &lt;em&gt;(see 0:08 to 0:16 in the video)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;So, by predicting what will happen next, given the intended action, it learns to predict the consequences of actions.&lt;/p&gt;\n\n&lt;p&gt;After training, the robot, powered by this model,¬† in the latent space can imagine the consequence of various chain-of-action scenarios to find a sequence of actions whose predicted outcome matches the desired outcome.&lt;/p&gt;\n\n&lt;p&gt;And for tasks requiring planning across multiple time scales, it needs to learn how to break down a high-level task into smaller steps, such as making food or loading a dishwasher. For that, the Meta team wants to train a hierarchical JEPA model that is capable of learning, reasoning, and planning across multiple temporal and spatial scales.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/h0ivgtibj0ef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?format=pjpg&amp;auto=webp&amp;s=6a8ec9b4980d9a40f31e6b1a7c7113057d70d42c",
                  "width": 1920,
                  "height": 900
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bd457375042fde995b2dbbbc85910bf0ffe7d88b",
                    "width": 108,
                    "height": 50
                  },
                  {
                    "url": "https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=506b500241f56e40e0a1feebc362f80c5440bc81",
                    "width": 216,
                    "height": 101
                  },
                  {
                    "url": "https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7f2f1446c77d923e99ffd8c175c55641539c9bb9",
                    "width": 320,
                    "height": 150
                  },
                  {
                    "url": "https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=60a24c15686917cbed627c6f9aa1cd0d68bc6c1d",
                    "width": 640,
                    "height": 300
                  },
                  {
                    "url": "https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4a674fb74efa796fcaa16cc6a4f6d5158ed3c8ab",
                    "width": 960,
                    "height": 450
                  },
                  {
                    "url": "https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e547aa9ee3bf8f5210ba9ff0685c84ce1099e4d7",
                    "width": 1080,
                    "height": 506
                  }
                ],
                "variants": {},
                "id": "bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m4mfs8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VR-Person",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/",
          "stickied": false,
          "url": "https://v.redd.it/h0ivgtibj0ef1",
          "subreddit_subscribers": 502720,
          "created_utc": 1753010231,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/h0ivgtibj0ef1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 600,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/h0ivgtibj0ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/h0ivgtibj0ef1/DASHPlaylist.mpd?a=1755779371%2CMjI0MTY5MGRmY2UxOTdkYjAzZmFmYjRmMGI3YWE5MDA1MTg5YjgzNjg3MzA3NmQ2NTc1MjllYmQ4NWMyYjQ5Mg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 21,
              "hls_url": "https://v.redd.it/h0ivgtibj0ef1/HLSPlaylist.m3u8?a=1755779371%2CMDI1N2UxMDQ2NGM5ZWJiMzQzZDI2YzAxMzJhNDg2OTExYTA5NjEwMzg3Y2Q4Mzg3M2I2ODE2YzQ5MDJhYzVlOA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Alright so basically - I want to run qwen3 235b MoE. I dont wanna pay 235b MoE money tho. So far I've been eyeing grabbing an old dell xeon workstation, slapping in lots of RAM &amp; two mi50 cards &amp; calling it a day. Would that work? probably i guess, hell you'd even get good performance out of that running 32b models which do the job for most cases. but i want real crackhead technology. completely out of the box shit. the funnier in its sheer absurdity/cheaper/faster the better. let's hear what you guys can think of ",
          "author_fullname": "t2_9y0b4xlc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the most crackhead garbage local LLM setup you can think of?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4u7j6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 57,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 57,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753031293,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Alright so basically - I want to run qwen3 235b MoE. I dont wanna pay 235b MoE money tho. So far I&amp;#39;ve been eyeing grabbing an old dell xeon workstation, slapping in lots of RAM &amp;amp; two mi50 cards &amp;amp; calling it a day. Would that work? probably i guess, hell you&amp;#39;d even get good performance out of that running 32b models which do the job for most cases. but i want real crackhead technology. completely out of the box shit. the funnier in its sheer absurdity/cheaper/faster the better. let&amp;#39;s hear what you guys can think of &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4u7j6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "caraccidentGAMING",
          "discussion_type": null,
          "num_comments": 61,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4u7j6/whats_the_most_crackhead_garbage_local_llm_setup/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4u7j6/whats_the_most_crackhead_garbage_local_llm_setup/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753031293,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "People have started throwing the terms ethical and ethics around with respect and I'm not sure how to read those terms. Is a more ethical model one which was trained using \"less\" electricity with something made on a raspberry pi approaching \"peak\" ethicalness? Are the inputs to a model more important? Less? How do both matter? Something else?",
          "author_fullname": "t2_w2zszpg3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What makes a model ethical?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m59xzv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753073699,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;People have started throwing the terms ethical and ethics around with respect and I&amp;#39;m not sure how to read those terms. Is a more ethical model one which was trained using &amp;quot;less&amp;quot; electricity with something made on a raspberry pi approaching &amp;quot;peak&amp;quot; ethicalness? Are the inputs to a model more important? Less? How do both matter? Something else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m59xzv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "KnownDairyAcolyte",
          "discussion_type": null,
          "num_comments": 60,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m59xzv/what_makes_a_model_ethical/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m59xzv/what_makes_a_model_ethical/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753073699,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I need it to be as similar as possible with my voice, so people on Youtube won't notice if I'm using my voice or a TTS.\n\nAlso I have only a nvidia GTX 1660 Super with 6 GB of ram, so I don't want to clone it every time I have a text, just clone at a time with the best, leave it for a couple of hours, and hen use it each time I need it.\n\nI also saw some that only let you do 300 characters a t a time, which is too slow, because I usually have 1000 - 2000 words.\n\nSo is something that you can reccomand? Even if is not more than 5-10 $ a month, but have available over 10 hours each month will be good for me.\n\nAlso if it can also use the Romanian language, it would be even better, but is ok only with English.",
          "author_fullname": "t2_izlog",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What free TTS is the best to clone my voice for reading large portions of text?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5kmxl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753108666,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need it to be as similar as possible with my voice, so people on Youtube won&amp;#39;t notice if I&amp;#39;m using my voice or a TTS.&lt;/p&gt;\n\n&lt;p&gt;Also I have only a nvidia GTX 1660 Super with 6 GB of ram, so I don&amp;#39;t want to clone it every time I have a text, just clone at a time with the best, leave it for a couple of hours, and hen use it each time I need it.&lt;/p&gt;\n\n&lt;p&gt;I also saw some that only let you do 300 characters a t a time, which is too slow, because I usually have 1000 - 2000 words.&lt;/p&gt;\n\n&lt;p&gt;So is something that you can reccomand? Even if is not more than 5-10 $ a month, but have available over 10 hours each month will be good for me.&lt;/p&gt;\n\n&lt;p&gt;Also if it can also use the Romanian language, it would be even better, but is ok only with English.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5kmxl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shaggy98",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5kmxl/what_free_tts_is_the_best_to_clone_my_voice_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5kmxl/what_free_tts_is_the_best_to_clone_my_voice_for/",
          "subreddit_subscribers": 502720,
          "created_utc": 1753108666,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}