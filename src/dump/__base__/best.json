{
  "kind": "Listing",
  "data": {
    "after": "t3_1lrsi1e",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nLast year I posted about 2x MI60 performance. Since then, I bought more cards and PCIE riser cables to build a rack with 8x AMD MI50 32GB cards. My motherboard (Asus rog dark hero viii with AMD 5950x CPU and 96GB 3200Mhz RAM) had stability issues with 8x MI50 (does not boot), so I connected four (or sometimes six) of those cards. I bought these cards on eBay when one seller sold them for around $150 (I started seeing MI50 32GB cards again on eBay).\n\nI connected 4x MI50 cards using ASUS Hyper M.2 x16 Gen5 Card (PCIE4.0 x16 to 4xM.2 card then I used M.2 to PCIE4.0 cables to connect 4 GPUs) through the first PCIE4.0 x16 slot on the motherboard that supports 4x4 bifurcation. I set the PCIE to use PCIE3.0 so that I don't get occasional freezing issues in my system. Each card was running at PCIE3.0 x4 (later I also tested 2x MI50s with PCIE4.0 x8 speed and did not see any PP/TG speed difference).\n\nI am using 1.2A blower fans to cool these cards which are a bit noisy at max speed but I adjusted their speeds to be acceptable.\n\nI have tested both llama.cpp (ROCm 6.3.4 and vulkan backend) and vLLM v0.9.2 in Ubuntu 24.04.02. Below are some results.\n\nNote that MI50/60 cards do not have matrix or tensor cores and that is why their Prompt Processing (PP) speed is not great. But Text Generation (TG) speeds are great!\n\nLlama.cpp (build: 247e5c6e (5606)) with ROCm 6.3.4. All of the runs use one MI50 (I will note the ones that use 2x or 4x MI50 in the model column). Note that MI50/60 cards perform best with Q4\\_0 and Q4\\_1 quantizations (that is why I ran larger models with those Quants).\n\n|Model|size|test|t/s|\n|:-|:-|:-|:-|\n|qwen3 0.6B Q8\\_0|604.15 MiB|pp1024|3014.18 ± 1.71|\n|qwen3 0.6B Q8\\_0|604.15 MiB|tg128|191.63 ± 0.38|\n|llama 7B Q4\\_0|3.56 GiB|pp512|1289.11 ± 0.62|\n|llama 7B Q4\\_0|3.56 GiB|tg128|91.46 ± 0.13|\n|qwen3 8B Q8\\_0|8.11 GiB|pp512|357.71 ± 0.04|\n|qwen3 8B Q8\\_0|8.11 GiB|tg128|48.09 ± 0.04|\n|qwen2 14B Q8\\_0|14.62 GiB|pp512|249.45 ± 0.08|\n|qwen2 14B Q8\\_0|14.62 GiB|tg128|29.24 ± 0.03|\n|qwen2 32B Q4\\_0|17.42 GiB|pp512|300.02 ± 0.52|\n|qwen2 32B Q4\\_0|17.42 GiB|tg128|20.39 ± 0.37|\n|qwen2 70B Q5\\_K - Medium|50.70 GiB|pp512|48.92 ± 0.02|\n|qwen2 70B Q5\\_K - Medium|50.70 GiB|tg128|9.05 ± 0.10|\n|qwen2vl 70B Q4\\_1 (4x MI50 row split)|42.55 GiB|pp512|56.33 ± 0.09|\n|qwen2vl 70B Q4\\_1 (4x MI50 row split)|42.55 GiB|tg128|16.00 ± 0.01|\n|qwen3moe 30B.A3B Q4\\_1|17.87 GiB|pp1024|1023.81 ± 3.76|\n|qwen3moe 30B.A3B Q4\\_1|17.87 GiB|tg128|63.87 ± 0.06|\n|qwen3 32B Q4\\_1 (2x MI50)|19.21 GiB|pp1024|238.17 ± 0.30|\n|qwen3 32B Q4\\_1 (2x MI50)|19.21 GiB|tg128|25.17 ± 0.01|\n|qwen3moe 235B.A22B Q4\\_1 (5x MI50)|137.11 GiB|pp1024|202.50 ± 0.32|\n|qwen3moe 235B.A22B Q4\\_1 (5x MI50) (4x mi50 with some expert offloading should give around 16t/s)|137.11 GiB|tg128|19.17 ± 0.04|\n\nPP is not great but TG is very good for most use cases. \n\nBy the way, I also tested Deepseek R1 IQ2-XXS (although it was running with 6x MI50) and I was getting \\~9 t/s for TG with a few experts offloaded to CPU RAM.\n\nNow, let's look at vllm (version 0.9.2.dev1+g5273453b6. Fork used: https://github.com/nlzy/vllm-gfx906).\n\nAWQ and GPTQ quants are supported. For gptq models, desc\\_act=false quants are used to get a better performance. Max concurrency is set to 1.\n\n|Model|Output token throughput (tok/s) (256)|Prompt processing  t/s (4096)|\n|:-|:-|:-|\n|Mistral-Large-Instruct-2407-AWQ 123B (4x MI50)|19.68|80|\n|Qwen2.5-72B-Instruct-GPTQ-Int4 (2x MI50)|19.76|130|\n|Qwen2.5-72B-Instruct-GPTQ-Int4 (4x MI50)|25.96|130|\n|Llama-3.3-70B-Instruct-AWQ (4x MI50)|27.26|130|\n|Qwen3-32B-GPTQ-Int8 (4x MI50)|32.3|230|\n|Qwen3-32B-autoround-4bit-gptq (4x MI50)|38.55|230|\n|gemma-3-27b-it-int4-awq (4x MI50)|36.96|350|\n\n  \nTensor parallelism (TP) gives MI50s extra performance in Text Generation (TG). Overall, great performance for the price. And I am sure we will not get 128GB VRAM with such TG speeds any time soon for \\~$600.\n\nPower consumption is around 900W for the system when using vllm with TP during text generation. Llama.cpp does not use TP so I did not see it using above 500W. Each GPU runs at around 18W when idle.",
          "author_fullname": "t2_3zy7pnf1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "128GB VRAM for ~$600. Qwen3 MOE 235B.A22B reaching 20 t/s. 4x AMD MI50 32GB.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lspzn3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 47,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 47,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751767150,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Last year I posted about 2x MI60 performance. Since then, I bought more cards and PCIE riser cables to build a rack with 8x AMD MI50 32GB cards. My motherboard (Asus rog dark hero viii with AMD 5950x CPU and 96GB 3200Mhz RAM) had stability issues with 8x MI50 (does not boot), so I connected four (or sometimes six) of those cards. I bought these cards on eBay when one seller sold them for around $150 (I started seeing MI50 32GB cards again on eBay).&lt;/p&gt;\n\n&lt;p&gt;I connected 4x MI50 cards using ASUS Hyper M.2 x16 Gen5 Card (PCIE4.0 x16 to 4xM.2 card then I used M.2 to PCIE4.0 cables to connect 4 GPUs) through the first PCIE4.0 x16 slot on the motherboard that supports 4x4 bifurcation. I set the PCIE to use PCIE3.0 so that I don&amp;#39;t get occasional freezing issues in my system. Each card was running at PCIE3.0 x4 (later I also tested 2x MI50s with PCIE4.0 x8 speed and did not see any PP/TG speed difference).&lt;/p&gt;\n\n&lt;p&gt;I am using 1.2A blower fans to cool these cards which are a bit noisy at max speed but I adjusted their speeds to be acceptable.&lt;/p&gt;\n\n&lt;p&gt;I have tested both llama.cpp (ROCm 6.3.4 and vulkan backend) and vLLM v0.9.2 in Ubuntu 24.04.02. Below are some results.&lt;/p&gt;\n\n&lt;p&gt;Note that MI50/60 cards do not have matrix or tensor cores and that is why their Prompt Processing (PP) speed is not great. But Text Generation (TG) speeds are great!&lt;/p&gt;\n\n&lt;p&gt;Llama.cpp (build: 247e5c6e (5606)) with ROCm 6.3.4. All of the runs use one MI50 (I will note the ones that use 2x or 4x MI50 in the model column). Note that MI50/60 cards perform best with Q4_0 and Q4_1 quantizations (that is why I ran larger models with those Quants).&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;size&lt;/th&gt;\n&lt;th align=\"left\"&gt;test&lt;/th&gt;\n&lt;th align=\"left\"&gt;t/s&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3 0.6B Q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;604.15 MiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp1024&lt;/td&gt;\n&lt;td align=\"left\"&gt;3014.18 ± 1.71&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3 0.6B Q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;604.15 MiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;191.63 ± 0.38&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;llama 7B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.56 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"left\"&gt;1289.11 ± 0.62&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;llama 7B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.56 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;91.46 ± 0.13&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3 8B Q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.11 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"left\"&gt;357.71 ± 0.04&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3 8B Q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.11 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;48.09 ± 0.04&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen2 14B Q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.62 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"left\"&gt;249.45 ± 0.08&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen2 14B Q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.62 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;29.24 ± 0.03&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen2 32B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.42 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"left\"&gt;300.02 ± 0.52&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen2 32B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.42 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.39 ± 0.37&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen2 70B Q5_K - Medium&lt;/td&gt;\n&lt;td align=\"left\"&gt;50.70 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"left\"&gt;48.92 ± 0.02&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen2 70B Q5_K - Medium&lt;/td&gt;\n&lt;td align=\"left\"&gt;50.70 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.05 ± 0.10&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen2vl 70B Q4_1 (4x MI50 row split)&lt;/td&gt;\n&lt;td align=\"left\"&gt;42.55 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"left\"&gt;56.33 ± 0.09&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen2vl 70B Q4_1 (4x MI50 row split)&lt;/td&gt;\n&lt;td align=\"left\"&gt;42.55 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;16.00 ± 0.01&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.87 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp1024&lt;/td&gt;\n&lt;td align=\"left\"&gt;1023.81 ± 3.76&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.87 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;63.87 ± 0.06&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3 32B Q4_1 (2x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.21 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp1024&lt;/td&gt;\n&lt;td align=\"left\"&gt;238.17 ± 0.30&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3 32B Q4_1 (2x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.21 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;25.17 ± 0.01&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3moe 235B.A22B Q4_1 (5x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;137.11 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp1024&lt;/td&gt;\n&lt;td align=\"left\"&gt;202.50 ± 0.32&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3moe 235B.A22B Q4_1 (5x MI50) (4x mi50 with some expert offloading should give around 16t/s)&lt;/td&gt;\n&lt;td align=\"left\"&gt;137.11 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.17 ± 0.04&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;PP is not great but TG is very good for most use cases. &lt;/p&gt;\n\n&lt;p&gt;By the way, I also tested Deepseek R1 IQ2-XXS (although it was running with 6x MI50) and I was getting ~9 t/s for TG with a few experts offloaded to CPU RAM.&lt;/p&gt;\n\n&lt;p&gt;Now, let&amp;#39;s look at vllm (version 0.9.2.dev1+g5273453b6. Fork used: &lt;a href=\"https://github.com/nlzy/vllm-gfx906\"&gt;https://github.com/nlzy/vllm-gfx906&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;AWQ and GPTQ quants are supported. For gptq models, desc_act=false quants are used to get a better performance. Max concurrency is set to 1.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Output token throughput (tok/s) (256)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Prompt processing  t/s (4096)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mistral-Large-Instruct-2407-AWQ 123B (4x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.68&lt;/td&gt;\n&lt;td align=\"left\"&gt;80&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen2.5-72B-Instruct-GPTQ-Int4 (2x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.76&lt;/td&gt;\n&lt;td align=\"left\"&gt;130&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen2.5-72B-Instruct-GPTQ-Int4 (4x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;25.96&lt;/td&gt;\n&lt;td align=\"left\"&gt;130&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama-3.3-70B-Instruct-AWQ (4x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;27.26&lt;/td&gt;\n&lt;td align=\"left\"&gt;130&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3-32B-GPTQ-Int8 (4x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;32.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;230&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3-32B-autoround-4bit-gptq (4x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;38.55&lt;/td&gt;\n&lt;td align=\"left\"&gt;230&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;gemma-3-27b-it-int4-awq (4x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;36.96&lt;/td&gt;\n&lt;td align=\"left\"&gt;350&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Tensor parallelism (TP) gives MI50s extra performance in Text Generation (TG). Overall, great performance for the price. And I am sure we will not get 128GB VRAM with such TG speeds any time soon for ~$600.&lt;/p&gt;\n\n&lt;p&gt;Power consumption is around 900W for the system when using vllm with TP during text generation. Llama.cpp does not use TP so I did not see it using above 500W. Each GPU runs at around 18W when idle.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?auto=webp&amp;s=8c340ab5ae3eebd3a1f3a8e634fa1bb0cf891fee",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e4c9f1d82654452ab9abf4c2dfaa69dd9495bbf",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=87b01812b7fbad8b7970e973412d609dc1ebcd54",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3dfcdc7e43c5456819743e8f71d12c77ef8db87a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6166a63e8cec75d08489356905b0d102369f198e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d1c7db1463f9c600c11ea411ad404650ada2e07e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b47148bbb6b2e8e0e6fb909a8a087abbf287326e",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lspzn3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MLDataScientist",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751767150,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Building a PC was always one of those \"someday\" projects I never got around to. As a long-time Mac user, I honestly never had a real need for it. That all changed when I stumbled into the world of local AI. Suddenly, my 16GB Mac wasn't just slow, it was a hard bottleneck.\n\nSo, I started mapping out what this new machine needed to be:\n\n\\- **32GB VRAM as the baseline.** I'm really bullish on the future of MoE models and think 32-64gigs of VRAM should hold quite well.  \n\\- **128GB of RAM as the baseline.** Essential for wrangling the large datasets that come with the territory.  \n\\- **A clean, consumer-desk look.** I don't want a rugged, noisy server rack.  \n\\- **AI inference as the main job,** but I didn't want a one-trick pony. It still needed to be a decent all-rounder for daily tasks and, of course, some gaming.  \n\\- **Room to grow.** I wanted a foundation I could build on later.  \n\\- And the big one: **Keep it under $1500.**\n\nA new Mac with these specs would cost a fortune and be a dead end for upgrades. New NVIDIA cards? Forget about it, way too expensive. I looked at used 3090s, but they were still going for about $1000 where I am, and that was a definite no-no for my budget.\n\nJust as I was about to give up, I discovered the AMD MI50. The price-to-performance was incredible, and I started getting excited. Sure, the raw power isn't record-breaking, but the idea of running massive models and getting such insane value for my money was a huge draw.\n\nBut here was the catch: these are server cards. Even though they have a display port, it doesn't actually work. That would have killed my \"all-rounder\" requirement.\n\nI started digging deep, trying to find a workaround. That's when I hit a wall. Everywhere I looked, the consensus was the same: cross-flashing the VBIOS on these cards to enable the display port was a dead end for the 32GB version. It was largely declared impossible...\n\n...until the kind-hearted u/Accurate_Ad4323 from China stepped in to confirm it was possible. They even told me I could get the 32GB MI50s for as cheap as $130 from China, and that some people there had even programmed custom VBIOSes specifically for these 32GB cards. With all these pieces of crucial info, I was sold.\n\nI still had my doubts. Was this custom VBIOS stable? Would it mess with AI performance? There was practically no info out there about this on the 32GB cards, only the 16GB ones. Could I really trust a random stranger's advice? And with ROCm's reputation for being a bit tricky, I didn't want to make my life even harder.\n\nIn the end, I decided to pull the trigger. Worst-case scenario? I'd have 64GB of HBM2 memory for AI work for about $300, just with no display output. I decided to treat a working display as a bonus.\n\nI found a reliable seller on Alibaba who specialized in server gear and was selling the MI50 for $137. I browsed their store and found some other lucrative deals, formulating my build list right there.\n\nHere’s what I ordered from them:\n\n\\- Supermicro X11DPI-N -&gt; $320  \n\\- Dual Xeon 6148 CPUs -&gt; 27 \\* 2 = $54  \n\\- 2x CPU Coolers -&gt; $62  \n\\- 2x MI50 32GB GPUs -&gt; $137 \\* 2 = $274  \n\\- 4x 32GB DDR4 2666hz ECC RDIMM RAM sticks -&gt; $124  \n\\- 10x 120mm RGB fans -&gt; $32  \n\\- 6x 140mm RGB fans -&gt; $27  \n\\- 2x custom cooling shrouded fans for MI50s -&gt; $14  \n\\- Shipping + Duties -&gt; $187\n\nI know people get skeptical about Alibaba, but in my opinion, you're safe as long as you find the right seller, use a reliable freight forwarder, and always buy through Trade Assurance.\n\nWhen the parts arrived, one of the Xeon CPUs was DOA. It took some back-and-forth, but the seller was great and sent a replacement for free once they were convinced (I offered to cover the shipping on it, which is included in that $187 cost).\n\nI also bought these peripherals brand-new:\n\n\\- Phanteks Enthoo Pro 2 Server Edition -&gt; $200  \n\\- ProLab 1200W 80Plus Gold PSU -&gt; $100  \n\\- 2TB NVMe SSD (For Ubuntu) -&gt; $100  \n\\- 1TB 2.5 SSD (For Windows) -&gt; $50\n\nAll in, I spent exactly **$1544.**\n\nNow for the two final hurdles:\n\n1. **Assembling everything without breaking it!** As a first-timer, it took me about three very careful days, but I'm so proud of how it turned out.\n2. **Testing that custom VBIOS. Did I get the \"bonus\"?** After downloading the VBIOS, finding the right version of amdvbflash to force-flash, and installing the community NimeZ drivers... it actually works!!!\n\nNow, to answer the questions I had for myself about the VBIOS cross-flash:\n\n**Is it stable?** Totally. It acts just like a regular graphics card from boot-up. The only weird quirk is on Windows: if I set \"VGA Priority\" to the GPU in the BIOS, the NimeZ drivers get corrupted. A quick reinstall and switching the priority back to \"Onboard\" fixes it. This doesn't happen at all in Ubuntu with ROCm.\n\n**Does the flash hurt AI performance?** Surprisingly, no! It performs identically. The VBIOS is based on a Radeon Pro VII, and I've seen zero difference. If anything weird pops up, I'll be sure to update.\n\n**Can it game?** Yes! Performance is like a Radeon VII but with a ridiculous 32GB of VRAM. It comfortably handles anything I throw at it in 1080p at max settings and 60fps.\n\nI ended up with 64GB of versatile VRAM for under $300, and thanks to the Supermicro board, I have a clear upgrade path to 4TB of RAM and Xeon Platinum CPUs down the line. (if needed)\n\nNow, I'll end this off with a couple pictures of the build and some benchmarks.\n\nhttps://preview.redd.it/fev4jvhbn3bf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=23c596ba1d922e7bf231b8dacb4add0c9b8d5790\n\nhttps://preview.redd.it/n6idruhbn3bf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=b9a883ded03b7bb26cf5e291562beff40f68ca71\n\n(The build is still a work-in-progress with regards to cable management :facepalm)\n\n**Benchmarks:**\n\n**llama.cpp:**\n\n**A power limit of 150W was imposed on both GPUs for all these tests.**\n\n**Qwen3-30B-A3B-128K-UD-Q4\\_K\\_XL:**\n\nbuild/bin/llama-bench --model models/Downloads/Qwen3-30B-A3B-128K-UD-Q4\\_K\\_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap\n\n`| model | size | params | backend | ngl | test | t/s |`\n\n`| ------------------------------ | --------: | ------: | ------- | --: | ----: | ------------: |`\n\n`| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 99 | pp512 | 472.40 ± 2.44 |`\n\n`| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 99 | tg128 | 49.40 ± 0.07 |`\n\n**Magistral-Small-2506-UD-Q4\\_K\\_XL:**\n\nbuild/bin/llama-bench --model models/Downloads/Magistral-Small-2506-UD-Q4\\_K\\_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap\n\n`| model | size | params | backend | ngl | test | t/s |`\n\n`| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |`\n\n`| llama 13B Q4_K - Medium | 13.50 GiB | 23.57 B | ROCm | 99 | pp512 | 130.75 ± 0.09 |`\n\n`| llama 13B Q4_K - Medium | 13.50 GiB | 23.57 B | ROCm | 99 | tg128 | 20.96 ± 0.09 |`\n\n**gemma-3-27b-it-Q4\\_K\\_M:**\n\nbuild/bin/llama-bench --model models/Downloads/gemma-3-27b-it-Q4\\_K\\_M.gguf -ngl 99 --threads 40 --flash-attn --no-mmap\n\n`| model | size | params | backend | ngl | test | t/s |`\n\n`| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |`\n\n`| gemma3 27B Q4_K - Medium | 15.40 GiB | 27.01 B | ROCm | 99 | pp512 | 110.88 ± 3.01 |`\n\n`| gemma3 27B Q4_K - Medium | 15.40 GiB | 27.01 B | ROCm | 99 | tg128 | 17.98 ± 0.02 |`\n\n**Qwen3-32B-Q4\\_K\\_M:**\n\nbuild/bin/llama-bench --model models/Downloads/Qwen3-32B-Q4\\_K\\_M.gguf -ngl 99 --threads 40 --flash-attn --no-mmap\n\n`| model | size | params | backend | ngl | test | t/s |`\n\n`| ----------------------- | --------: | ------: | ------- | --: | ----: | -----------: |`\n\n`| qwen3 32B Q4_K - Medium | 18.40 GiB | 32.76 B | ROCm | 99 | pp512 | 91.72 ± 0.03 |`\n\n`| qwen3 32B Q4_K - Medium | 18.40 GiB | 32.76 B | ROCm | 99 | tg128 | 16.12 ± 0.01 |`\n\n**Llama-3.3-70B-Instruct-UD-Q4\\_K\\_XL:**\n\nbuild/bin/llama-bench --model models/Downloads/Llama-3.3-70B-Instruct-UD-Q4\\_K\\_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap\n\n`| model | size | params | backend | ngl | test | t/s |`\n\n`| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |`\n\n`| llama 70B Q4_K - Medium | 39.73 GiB | 70.55 B | ROCm | 99 | pp512 | 42.49 ± 0.05 |`\n\n`| llama 70B Q4_K - Medium | 39.73 GiB | 70.55 B | ROCm | 99 | tg128 | 7.70 ± 0.01 |`\n\n**Qwen3-235B-A22B-128K-UD-Q2\\_K\\_XL:**\n\nbuild/bin/llama-bench --model models/Downloads/Qwen3-235B-A22B-128K-GGUF/Qwen3-235B-A22B-128K-UD-Q2\\_K\\_XL-00001-of-00002.gguf -ot '(4-7+).ffn\\_.\\_exps.=CPU' -ngl 99 --threads 40 --flash-attn --no-mmap\n\n`| model | size | params | backend | ngl | ot | test | t/s |`\n\n`| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------------- | --------------: | -------------------: |`\n\n`| qwen3moe 235B.A22B Q2_K - Medium | 81.96 GiB | 235.09 B | ROCm | 99 | (4-7+).ffn_._exps.=CPU | pp512 | 29.80 ± 0.15 |`\n\n`| qwen3moe 235B.A22B Q2_K - Medium | 81.96 GiB | 235.09 B | ROCm | 99 | (4-7+).ffn_._exps.=CPU | tg128 | 7.45 ± 0.09 |`\n\nI'm aware of the severe multi-GPU performance bottleneck with llama.cpp. Just started messing with vLLM, exLlamav2 and MLC-LLM. Will update results here once I get them up and running properly.\n\n**Furmark scores post VBIOS flash and NimeZ drivers on Windows:**\n\nhttps://preview.redd.it/isqld5nwn3bf1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=c184e61a0ba5272ba7ffd63fbca1690286810f8f\n\nhttps://preview.redd.it/muxix6nwn3bf1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=355644c658940db5c30be46cc702e63c795b2764\n\nOverall, this whole experience has been an adventure, but it's been overwhelmingly positive. I thought I'd share it for anyone else thinking about a similar build.\n\nEdit:  \nNoticed a lot of requests to post the seller. Here you go: [https://www.alibaba.com/product-detail/Best-Price-Graphics-Cards-MI50-32GB\\_1601432581416.html](https://www.alibaba.com/product-detail/Best-Price-Graphics-Cards-MI50-32GB_1601432581416.html)",
          "author_fullname": "t2_xg2jtdg74",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Successfully Built My First PC for AI (Sourcing Parts from Alibaba - Under $1500!)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "fev4jvhbn3bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/fev4jvhbn3bf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0cb53b670e9300ae2b50074de2b8d3dc66a9fe87"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/fev4jvhbn3bf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=51c55df38925fd8e5035205796df0579bdeb151a"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/fev4jvhbn3bf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a80d52a276702ac9d77640d9bad77ca78e31e34"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/fev4jvhbn3bf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b7d12fa0c8bc1d2eb10c9b53791aecf0436b5244"
                },
                {
                  "y": 1280,
                  "x": 960,
                  "u": "https://preview.redd.it/fev4jvhbn3bf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=87e20f461b5eec4dfab0f593184f9388a2b8ff4f"
                },
                {
                  "y": 1440,
                  "x": 1080,
                  "u": "https://preview.redd.it/fev4jvhbn3bf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=471ae28347a731a4a36bfb79254a6accb46173c7"
                }
              ],
              "s": {
                "y": 4032,
                "x": 3024,
                "u": "https://preview.redd.it/fev4jvhbn3bf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=23c596ba1d922e7bf231b8dacb4add0c9b8d5790"
              },
              "id": "fev4jvhbn3bf1"
            },
            "n6idruhbn3bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/n6idruhbn3bf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=767fc70468637b42fc552ed9d235a923f05ff0a0"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/n6idruhbn3bf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=84307816e868bb27b7fa02ed5b85329ec9df0407"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/n6idruhbn3bf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a5eb864627b1ae955a8268976b4ecd531b53bf5c"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/n6idruhbn3bf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6bc4fba42ce896fa0055d37bd9cba425adb5e165"
                },
                {
                  "y": 1280,
                  "x": 960,
                  "u": "https://preview.redd.it/n6idruhbn3bf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1692c1bece8efa78cadcb51da30eb79d827033b1"
                },
                {
                  "y": 1440,
                  "x": 1080,
                  "u": "https://preview.redd.it/n6idruhbn3bf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7a47400bc8de85ca0360049c6e29f7cb93d8e629"
                }
              ],
              "s": {
                "y": 4032,
                "x": 3024,
                "u": "https://preview.redd.it/n6idruhbn3bf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=b9a883ded03b7bb26cf5e291562beff40f68ca71"
              },
              "id": "n6idruhbn3bf1"
            },
            "isqld5nwn3bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/isqld5nwn3bf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5748b64b5ba78c86aae4285fc588436d8f81742c"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/isqld5nwn3bf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7e3b3cd10872cf1549a7b35b41f8996ed0825843"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/isqld5nwn3bf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d7e9270eb1cbb10537d007f43745e028b629c89"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/isqld5nwn3bf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b33ea43cbc62e3a698a0c1182ba53c283868438f"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/isqld5nwn3bf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=370a54cdae2b14affceb8311fc135c836eddeace"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/isqld5nwn3bf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0ef07848e931b66eae2c8dba1494196f2e0a289b"
                }
              ],
              "s": {
                "y": 3024,
                "x": 4032,
                "u": "https://preview.redd.it/isqld5nwn3bf1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=c184e61a0ba5272ba7ffd63fbca1690286810f8f"
              },
              "id": "isqld5nwn3bf1"
            },
            "muxix6nwn3bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/muxix6nwn3bf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c7826369c69a8f918bbc5d94aee6b055cdf9e749"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/muxix6nwn3bf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=32c29605876a631d5d1dcd649dd015890659b2b2"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/muxix6nwn3bf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e043d7730616fe9e50abd9bb67a25c734ccb9174"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/muxix6nwn3bf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a18d4a7b1e058b3585ff069f100539ed52b4b964"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/muxix6nwn3bf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a9c9b18fb6633140ab5763bb4fc56bcc9b25c07"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/muxix6nwn3bf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02d1747cdc8ab2217c6a8ac0ff6a04af3097efc8"
                }
              ],
              "s": {
                "y": 3024,
                "x": 4032,
                "u": "https://preview.redd.it/muxix6nwn3bf1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=355644c658940db5c30be46cc702e63c795b2764"
              },
              "id": "muxix6nwn3bf1"
            }
          },
          "name": "t3_1lsgtvy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 168,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 168,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/qkqp42ZUQM3ud5lixeAGPwFgRk03v_VKVBUbGROr91s.jpeg?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=b008aeddc56c8fcc997e1dfd569cf4b419fe9e6a",
          "edited": 1751767433,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751740750,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Building a PC was always one of those &amp;quot;someday&amp;quot; projects I never got around to. As a long-time Mac user, I honestly never had a real need for it. That all changed when I stumbled into the world of local AI. Suddenly, my 16GB Mac wasn&amp;#39;t just slow, it was a hard bottleneck.&lt;/p&gt;\n\n&lt;p&gt;So, I started mapping out what this new machine needed to be:&lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;32GB VRAM as the baseline.&lt;/strong&gt; I&amp;#39;m really bullish on the future of MoE models and think 32-64gigs of VRAM should hold quite well.&lt;br/&gt;\n- &lt;strong&gt;128GB of RAM as the baseline.&lt;/strong&gt; Essential for wrangling the large datasets that come with the territory.&lt;br/&gt;\n- &lt;strong&gt;A clean, consumer-desk look.&lt;/strong&gt; I don&amp;#39;t want a rugged, noisy server rack.&lt;br/&gt;\n- &lt;strong&gt;AI inference as the main job,&lt;/strong&gt; but I didn&amp;#39;t want a one-trick pony. It still needed to be a decent all-rounder for daily tasks and, of course, some gaming.&lt;br/&gt;\n- &lt;strong&gt;Room to grow.&lt;/strong&gt; I wanted a foundation I could build on later.&lt;br/&gt;\n- And the big one: &lt;strong&gt;Keep it under $1500.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A new Mac with these specs would cost a fortune and be a dead end for upgrades. New NVIDIA cards? Forget about it, way too expensive. I looked at used 3090s, but they were still going for about $1000 where I am, and that was a definite no-no for my budget.&lt;/p&gt;\n\n&lt;p&gt;Just as I was about to give up, I discovered the AMD MI50. The price-to-performance was incredible, and I started getting excited. Sure, the raw power isn&amp;#39;t record-breaking, but the idea of running massive models and getting such insane value for my money was a huge draw.&lt;/p&gt;\n\n&lt;p&gt;But here was the catch: these are server cards. Even though they have a display port, it doesn&amp;#39;t actually work. That would have killed my &amp;quot;all-rounder&amp;quot; requirement.&lt;/p&gt;\n\n&lt;p&gt;I started digging deep, trying to find a workaround. That&amp;#39;s when I hit a wall. Everywhere I looked, the consensus was the same: cross-flashing the VBIOS on these cards to enable the display port was a dead end for the 32GB version. It was largely declared impossible...&lt;/p&gt;\n\n&lt;p&gt;...until the kind-hearted &lt;a href=\"/u/Accurate_Ad4323\"&gt;u/Accurate_Ad4323&lt;/a&gt; from China stepped in to confirm it was possible. They even told me I could get the 32GB MI50s for as cheap as $130 from China, and that some people there had even programmed custom VBIOSes specifically for these 32GB cards. With all these pieces of crucial info, I was sold.&lt;/p&gt;\n\n&lt;p&gt;I still had my doubts. Was this custom VBIOS stable? Would it mess with AI performance? There was practically no info out there about this on the 32GB cards, only the 16GB ones. Could I really trust a random stranger&amp;#39;s advice? And with ROCm&amp;#39;s reputation for being a bit tricky, I didn&amp;#39;t want to make my life even harder.&lt;/p&gt;\n\n&lt;p&gt;In the end, I decided to pull the trigger. Worst-case scenario? I&amp;#39;d have 64GB of HBM2 memory for AI work for about $300, just with no display output. I decided to treat a working display as a bonus.&lt;/p&gt;\n\n&lt;p&gt;I found a reliable seller on Alibaba who specialized in server gear and was selling the MI50 for $137. I browsed their store and found some other lucrative deals, formulating my build list right there.&lt;/p&gt;\n\n&lt;p&gt;Here’s what I ordered from them:&lt;/p&gt;\n\n&lt;p&gt;- Supermicro X11DPI-N -&amp;gt; $320&lt;br/&gt;\n- Dual Xeon 6148 CPUs -&amp;gt; 27 * 2 = $54&lt;br/&gt;\n- 2x CPU Coolers -&amp;gt; $62&lt;br/&gt;\n- 2x MI50 32GB GPUs -&amp;gt; $137 * 2 = $274&lt;br/&gt;\n- 4x 32GB DDR4 2666hz ECC RDIMM RAM sticks -&amp;gt; $124&lt;br/&gt;\n- 10x 120mm RGB fans -&amp;gt; $32&lt;br/&gt;\n- 6x 140mm RGB fans -&amp;gt; $27&lt;br/&gt;\n- 2x custom cooling shrouded fans for MI50s -&amp;gt; $14&lt;br/&gt;\n- Shipping + Duties -&amp;gt; $187&lt;/p&gt;\n\n&lt;p&gt;I know people get skeptical about Alibaba, but in my opinion, you&amp;#39;re safe as long as you find the right seller, use a reliable freight forwarder, and always buy through Trade Assurance.&lt;/p&gt;\n\n&lt;p&gt;When the parts arrived, one of the Xeon CPUs was DOA. It took some back-and-forth, but the seller was great and sent a replacement for free once they were convinced (I offered to cover the shipping on it, which is included in that $187 cost).&lt;/p&gt;\n\n&lt;p&gt;I also bought these peripherals brand-new:&lt;/p&gt;\n\n&lt;p&gt;- Phanteks Enthoo Pro 2 Server Edition -&amp;gt; $200&lt;br/&gt;\n- ProLab 1200W 80Plus Gold PSU -&amp;gt; $100&lt;br/&gt;\n- 2TB NVMe SSD (For Ubuntu) -&amp;gt; $100&lt;br/&gt;\n- 1TB 2.5 SSD (For Windows) -&amp;gt; $50&lt;/p&gt;\n\n&lt;p&gt;All in, I spent exactly &lt;strong&gt;$1544.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Now for the two final hurdles:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Assembling everything without breaking it!&lt;/strong&gt; As a first-timer, it took me about three very careful days, but I&amp;#39;m so proud of how it turned out.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Testing that custom VBIOS. Did I get the &amp;quot;bonus&amp;quot;?&lt;/strong&gt; After downloading the VBIOS, finding the right version of amdvbflash to force-flash, and installing the community NimeZ drivers... it actually works!!!&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Now, to answer the questions I had for myself about the VBIOS cross-flash:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is it stable?&lt;/strong&gt; Totally. It acts just like a regular graphics card from boot-up. The only weird quirk is on Windows: if I set &amp;quot;VGA Priority&amp;quot; to the GPU in the BIOS, the NimeZ drivers get corrupted. A quick reinstall and switching the priority back to &amp;quot;Onboard&amp;quot; fixes it. This doesn&amp;#39;t happen at all in Ubuntu with ROCm.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Does the flash hurt AI performance?&lt;/strong&gt; Surprisingly, no! It performs identically. The VBIOS is based on a Radeon Pro VII, and I&amp;#39;ve seen zero difference. If anything weird pops up, I&amp;#39;ll be sure to update.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Can it game?&lt;/strong&gt; Yes! Performance is like a Radeon VII but with a ridiculous 32GB of VRAM. It comfortably handles anything I throw at it in 1080p at max settings and 60fps.&lt;/p&gt;\n\n&lt;p&gt;I ended up with 64GB of versatile VRAM for under $300, and thanks to the Supermicro board, I have a clear upgrade path to 4TB of RAM and Xeon Platinum CPUs down the line. (if needed)&lt;/p&gt;\n\n&lt;p&gt;Now, I&amp;#39;ll end this off with a couple pictures of the build and some benchmarks.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fev4jvhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23c596ba1d922e7bf231b8dacb4add0c9b8d5790\"&gt;https://preview.redd.it/fev4jvhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23c596ba1d922e7bf231b8dacb4add0c9b8d5790&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/n6idruhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b9a883ded03b7bb26cf5e291562beff40f68ca71\"&gt;https://preview.redd.it/n6idruhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b9a883ded03b7bb26cf5e291562beff40f68ca71&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;(The build is still a work-in-progress with regards to cable management :facepalm)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Benchmarks:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;llama.cpp:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;A power limit of 150W was imposed on both GPUs for all these tests.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-128K-UD-Q4_K_XL:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;build/bin/llama-bench --model models/Downloads/Qwen3-30B-A3B-128K-UD-Q4_K_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| ------------------------------ | --------: | ------: | ------- | --: | ----: | ------------: |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 99 | pp512 | 472.40 ± 2.44 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 99 | tg128 | 49.40 ± 0.07 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Magistral-Small-2506-UD-Q4_K_XL:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;build/bin/llama-bench --model models/Downloads/Magistral-Small-2506-UD-Q4_K_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| llama 13B Q4_K - Medium | 13.50 GiB | 23.57 B | ROCm | 99 | pp512 | 130.75 ± 0.09 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| llama 13B Q4_K - Medium | 13.50 GiB | 23.57 B | ROCm | 99 | tg128 | 20.96 ± 0.09 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;gemma-3-27b-it-Q4_K_M:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;build/bin/llama-bench --model models/Downloads/gemma-3-27b-it-Q4_K_M.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| gemma3 27B Q4_K - Medium | 15.40 GiB | 27.01 B | ROCm | 99 | pp512 | 110.88 ± 3.01 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| gemma3 27B Q4_K - Medium | 15.40 GiB | 27.01 B | ROCm | 99 | tg128 | 17.98 ± 0.02 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3-32B-Q4_K_M:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;build/bin/llama-bench --model models/Downloads/Qwen3-32B-Q4_K_M.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| ----------------------- | --------: | ------: | ------- | --: | ----: | -----------: |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| qwen3 32B Q4_K - Medium | 18.40 GiB | 32.76 B | ROCm | 99 | pp512 | 91.72 ± 0.03 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| qwen3 32B Q4_K - Medium | 18.40 GiB | 32.76 B | ROCm | 99 | tg128 | 16.12 ± 0.01 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Llama-3.3-70B-Instruct-UD-Q4_K_XL:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;build/bin/llama-bench --model models/Downloads/Llama-3.3-70B-Instruct-UD-Q4_K_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| llama 70B Q4_K - Medium | 39.73 GiB | 70.55 B | ROCm | 99 | pp512 | 42.49 ± 0.05 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| llama 70B Q4_K - Medium | 39.73 GiB | 70.55 B | ROCm | 99 | tg128 | 7.70 ± 0.01 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-128K-UD-Q2_K_XL:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;build/bin/llama-bench --model models/Downloads/Qwen3-235B-A22B-128K-GGUF/Qwen3-235B-A22B-128K-UD-Q2_K_XL-00001-of-00002.gguf -ot &amp;#39;(4-7+).ffn_._exps.=CPU&amp;#39; -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | ot | test | t/s |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------------- | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| qwen3moe 235B.A22B Q2_K - Medium | 81.96 GiB | 235.09 B | ROCm | 99 | (4-7+).ffn_._exps.=CPU | pp512 | 29.80 ± 0.15 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| qwen3moe 235B.A22B Q2_K - Medium | 81.96 GiB | 235.09 B | ROCm | 99 | (4-7+).ffn_._exps.=CPU | tg128 | 7.45 ± 0.09 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware of the severe multi-GPU performance bottleneck with llama.cpp. Just started messing with vLLM, exLlamav2 and MLC-LLM. Will update results here once I get them up and running properly.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Furmark scores post VBIOS flash and NimeZ drivers on Windows:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/isqld5nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c184e61a0ba5272ba7ffd63fbca1690286810f8f\"&gt;https://preview.redd.it/isqld5nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c184e61a0ba5272ba7ffd63fbca1690286810f8f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/muxix6nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=355644c658940db5c30be46cc702e63c795b2764\"&gt;https://preview.redd.it/muxix6nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=355644c658940db5c30be46cc702e63c795b2764&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Overall, this whole experience has been an adventure, but it&amp;#39;s been overwhelmingly positive. I thought I&amp;#39;d share it for anyone else thinking about a similar build.&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;br/&gt;\nNoticed a lot of requests to post the seller. Here you go: &lt;a href=\"https://www.alibaba.com/product-detail/Best-Price-Graphics-Cards-MI50-32GB_1601432581416.html\"&gt;https://www.alibaba.com/product-detail/Best-Price-Graphics-Cards-MI50-32GB_1601432581416.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qkqp42ZUQM3ud5lixeAGPwFgRk03v_VKVBUbGROr91s.jpeg?auto=webp&amp;s=e44dca6fa481ac23f63b69284bd0e06e39e13139",
                  "width": 800,
                  "height": 800
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qkqp42ZUQM3ud5lixeAGPwFgRk03v_VKVBUbGROr91s.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9bff86ba77f876ea113af80eda2c7c1173584b76",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/qkqp42ZUQM3ud5lixeAGPwFgRk03v_VKVBUbGROr91s.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d6db19ea6059623c7bcfb87881c8cec7be1f0b73",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/qkqp42ZUQM3ud5lixeAGPwFgRk03v_VKVBUbGROr91s.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b1c75555b8422f8af4b5cc9d7073d3199a679e14",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/qkqp42ZUQM3ud5lixeAGPwFgRk03v_VKVBUbGROr91s.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=66839b47c62157c843227bea591a16264454d62a",
                    "width": 640,
                    "height": 640
                  }
                ],
                "variants": {},
                "id": "qkqp42ZUQM3ud5lixeAGPwFgRk03v_VKVBUbGROr91s"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsgtvy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lowkey_LokiSN",
          "discussion_type": null,
          "num_comments": 79,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751740750,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking for the best open-source LLM for local use, focused on programming. I have a 2 RTX 5090.\n\nIs Codestral 22B still the best choice for local code related tasks (code completion, refactoring, understanding context etc.), or are there better alternatives now like DeepSeek-Coder V2, StarCoder2, or WizardCoder?\n\nLooking for models that run locally (preferably via GGUF with llama.cpp or LM Studio) and give good real-world coding performance – not just benchmark wins. C/C++, python and Js.\n\nThanks in advance.",
          "author_fullname": "t2_77k54e2b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is Codestral 22B still the best open LLM for local coding on 32–64 GB VRAM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsmtzr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 46,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 46,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751757237,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for the best open-source LLM for local use, focused on programming. I have a 2 RTX 5090.&lt;/p&gt;\n\n&lt;p&gt;Is Codestral 22B still the best choice for local code related tasks (code completion, refactoring, understanding context etc.), or are there better alternatives now like DeepSeek-Coder V2, StarCoder2, or WizardCoder?&lt;/p&gt;\n\n&lt;p&gt;Looking for models that run locally (preferably via GGUF with llama.cpp or LM Studio) and give good real-world coding performance – not just benchmark wins. C/C++, python and Js.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsmtzr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "One-Stress-6734",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751757237,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Why are they so expensive, has anybody here ever tested them?\nHow many rtx 5090s are needed to match it's performance?\nWhat llm can we run entirely on one h100 with as much RAM as required?\n\nNaive questions but I am very confused\n",
          "author_fullname": "t2_1p0cxl3avi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Should I buy an appartment or 4 H100s",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lskb8k",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 48,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 48,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751750079,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why are they so expensive, has anybody here ever tested them?\nHow many rtx 5090s are needed to match it&amp;#39;s performance?\nWhat llm can we run entirely on one h100 with as much RAM as required?&lt;/p&gt;\n\n&lt;p&gt;Naive questions but I am very confused&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lskb8k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "InfiniteEjaculation",
          "discussion_type": null,
          "num_comments": 59,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lskb8k/should_i_buy_an_appartment_or_4_h100s/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lskb8k/should_i_buy_an_appartment_or_4_h100s/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751750079,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Its been years since local models started gaining traction and hobbyist experiment at home with cheaper hardware like multi 3090s and old DDR4 servers. But none of these solutions have been good enough, with multi-GPUs not having enough ram for large models such as DeepSeek and old server not having usable speeds.\n\nWhen can we expect hardware that will finally let us run large LLMs with decent speeds at home without spending 100k?",
          "author_fullname": "t2_z326l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "When Should We Expect Affordable Hardware That Will Run Large LLMs With Usable Speed?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsbhzs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 147,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 147,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751726687,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Its been years since local models started gaining traction and hobbyist experiment at home with cheaper hardware like multi 3090s and old DDR4 servers. But none of these solutions have been good enough, with multi-GPUs not having enough ram for large models such as DeepSeek and old server not having usable speeds.&lt;/p&gt;\n\n&lt;p&gt;When can we expect hardware that will finally let us run large LLMs with decent speeds at home without spending 100k?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsbhzs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "spiritxfly",
          "discussion_type": null,
          "num_comments": 167,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsbhzs/when_should_we_expect_affordable_hardware_that/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsbhzs/when_should_we_expect_affordable_hardware_that/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751726687,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here's Llama-4-Maverick-17B-128E-Instruct on a oneplus 13, which used UFS 4.0 storage. Any phone will work, as long as the RAM size is sufficient for context and repeating layers. (8-12gb)\n\n\n\nHere's the command used: \n\n`./llama-cli -m Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_M-00001-of-00003.gguf -t 6 -p \"hi\" -c 2048`\n\n\n\n\\- Why llama maverick can run on a phone at 2 T/s: The big pool of experts are only in every odd layer, and a majority of the model is loaded into RAM. Therefore, you could think of it as loading mostly a 17 billion model with an annoying piece that slows down what should have been average 17B Q4-Q2 speeds.\n\n\n\n[https://imgur.com/a/QwkaFHf](https://imgur.com/a/QwkaFHf)\n\n picture shows the model layers as seen on huggingface tensor viewer: \n\n\\- Green: in RAM\n\n\\- Red: read from DISC\n\n\n\n\n\n\n\nOther MOEs will have less impressive results due to a difference in architecture.\n\n\n\n\n\nGreater results can be obtained by increasing the quantity of Q4\\_0 tensors for repeating layers in place of other types IQ4\\_XS, Q6\\_K, Q4\\_K, Q3\\_K, Q2\\_K, etc. as many phones use a preferred backend for Increasing token generation and prompt processing. For example, this particular phone when using the special Q4\\_0 type will upscale activations to int8 instead of float16, which barely affects accuracy, and doubles prompt processing. You may have to run experiments for your own device. \n\n",
          "author_fullname": "t2_el5pibmej",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama-4-Maverick 402B on a oneplus 13",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsdjnb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 105,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/tletuj5ov2bf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 1280,
              "width": 582,
              "scrubber_media_url": "https://v.redd.it/tletuj5ov2bf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/tletuj5ov2bf1/DASHPlaylist.mpd?a=1754366592%2CNzdmYTNkMzgxZTc4ZWVkNmI1NDU0N2FlZDQxNDliYzliNmFhN2Q4NDA5MGZiN2ViZTI3Zjk4NDJkYmI3ZThhMw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 234,
              "hls_url": "https://v.redd.it/tletuj5ov2bf1/HLSPlaylist.m3u8?a=1754366592%2CMDhjZjE5OGQ0MTQ4NzQyNDJlOTgzOTliYjZlNjVkMmI1ZTM5ZmM3NTMzZDcwN2I0YWJhMmQzZTYyYzdjZjRhZQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 105,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=09b14e42a33fa63a19ebe35436a4b813db77c3a7",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751732149,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s Llama-4-Maverick-17B-128E-Instruct on a oneplus 13, which used UFS 4.0 storage. Any phone will work, as long as the RAM size is sufficient for context and repeating layers. (8-12gb)&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the command used: &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;./llama-cli -m Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_M-00001-of-00003.gguf -t 6 -p &amp;quot;hi&amp;quot; -c 2048&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;- Why llama maverick can run on a phone at 2 T/s: The big pool of experts are only in every odd layer, and a majority of the model is loaded into RAM. Therefore, you could think of it as loading mostly a 17 billion model with an annoying piece that slows down what should have been average 17B Q4-Q2 speeds.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/QwkaFHf\"&gt;https://imgur.com/a/QwkaFHf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;picture shows the model layers as seen on huggingface tensor viewer: &lt;/p&gt;\n\n&lt;p&gt;- Green: in RAM&lt;/p&gt;\n\n&lt;p&gt;- Red: read from DISC&lt;/p&gt;\n\n&lt;p&gt;Other MOEs will have less impressive results due to a difference in architecture.&lt;/p&gt;\n\n&lt;p&gt;Greater results can be obtained by increasing the quantity of Q4_0 tensors for repeating layers in place of other types IQ4_XS, Q6_K, Q4_K, Q3_K, Q2_K, etc. as many phones use a preferred backend for Increasing token generation and prompt processing. For example, this particular phone when using the special Q4_0 type will upscale activations to int8 instead of float16, which barely affects accuracy, and doubles prompt processing. You may have to run experiments for your own device. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/tletuj5ov2bf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG.png?format=pjpg&amp;auto=webp&amp;s=b41bda758b2febb62700c5d3b5b0fabc7b9a45cd",
                  "width": 720,
                  "height": 1584
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=66fb299c22385809a3328d96e048aabe44be27d7",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=79ef95dcb07005cfeb57c4a164385a30bd2d2390",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://external-preview.redd.it/aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5b81d45b1bfd42e0fdada50eae685867397eef4b",
                    "width": 320,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=efbb69eda5aea86a93bc498cc10e5357c102fe44",
                    "width": 640,
                    "height": 1280
                  }
                ],
                "variants": {},
                "id": "aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lsdjnb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Aaaaaaaaaeeeee",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsdjnb/llama4maverick_402b_on_a_oneplus_13/",
          "stickied": false,
          "url": "https://v.redd.it/tletuj5ov2bf1",
          "subreddit_subscribers": 494897,
          "created_utc": 1751732149,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/tletuj5ov2bf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 1280,
              "width": 582,
              "scrubber_media_url": "https://v.redd.it/tletuj5ov2bf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/tletuj5ov2bf1/DASHPlaylist.mpd?a=1754366592%2CNzdmYTNkMzgxZTc4ZWVkNmI1NDU0N2FlZDQxNDliYzliNmFhN2Q4NDA5MGZiN2ViZTI3Zjk4NDJkYmI3ZThhMw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 234,
              "hls_url": "https://v.redd.it/tletuj5ov2bf1/HLSPlaylist.m3u8?a=1754366592%2CMDhjZjE5OGQ0MTQ4NzQyNDJlOTgzOTliYjZlNjVkMmI1ZTM5ZmM3NTMzZDcwN2I0YWJhMmQzZTYyYzdjZjRhZQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\&gt; ollama run phi4-mini-reasoning:3.8b\n\n\\&gt;&gt;&gt; hello\n\n&lt;think&gt;\n\nOkay, let's see what the user needs here. The message says \"You will be given a problem.\" but\n\nthen it just has \"hello\". Hmm, maybe there was a typo or the problem didn't get sent\n\ncorrectly. Let me check again.\n\n\n\nWait, the user's name is Phi, an AI math expert from Microsoft. Then there's a greeting\n\n\"hello\" and that's it. So perhaps the actual problem wasn't included properly. Maybe the user\n\nintended to send a math problem but only wrote \"hello\". I should prompt them to provide the\n\nproblem they need help with. But according to the instructions, I need to first think through\n\nsolving the problem before giving the answer. However, there's no problem here.\n\n\n\nAlternatively, maybe \"hello\" is the problem itself? That seems unlikely. Maybe it's a test or\n\na mistake. Let me consider if \"hello\" could be part of a math question. For example,\n\nconverting letters to numbers using ASCII values. Let's try that.\n\n\n\nH in ASCII is 72, E is 69, L is 76, another L is 76, and O is 79. So adding those up: 72 + 69\n\n\\+ 76 + 76 + 79. Let me calculate that. 72 + 69 is 141. 141 + 76 is 217. 217 + 76 is 293. 293\n\n\\+ 79 is 372. So the total would be 372. But I'm not sure if that's what the user wants. They\n\nmight have intended a different approach.\n\n\n\nAlternatively, maybe it's a cipher problem, like Caesar cipher where each letter is shifted.\n\nFor example, shifting letters by a certain number. But without more context, it's hard to\n\ntell.\n\n\n\nSince the user mentioned being an AI math expert and the initial instruction was about\n\nsolving a problem, but the input only has \"hello\", I think the best course is to ask for\n\nclarification. However, since the task requires providing an answer within a box, maybe I\n\nshould state that the problem is missing or assume a common problem. Wait, perhaps the user\n\nintended to send a problem but it didn't go through, so responding by pointing out the\n\nmissing problem would be better. But according to the example given in the query, sometimes\n\neven if there's confusion, you have to proceed.\n\n\n\nAlternatively, maybe \"hello\" is a greeting and the actual problem is forthcoming. But the\n\nstructure of the query shows that after \"You will be given a problem,\" the user input is\n\n\"hello\". So perhaps there was an error in submission. In that case, my answer should note\n\nthat the problem seems missing and ask for it. But since I need to provide a boxed answer,\n\nmaybe there's a default response. Hmm.\n\n\n\nAlternatively, if this is a test of handling errors, then the correct response would be to\n\ninform the user that no problem was detected and request the problem statement. However,\n\ngiven the setup here, perhaps the assistant is supposed to generate a sample problem and\n\nsolution? But that's speculative.\n\n\n\nWait, looking back at the original instructions: \"You will be given a problem. Please reason\n\nstep by step, and put your final answer within \\\\boxed{}.\" The user's input after that is\n\n\"hello\". So maybe the actual problem was meant to follow \"hello\" but didn't. Therefore, my\n\nconclusion is that there's a missing problem here. Since I need to provide an answer, perhaps\n\nI should state that the problem is missing and ask for it, but in the boxed answer? But the\n\nexample shows mathematical answers in boxes. Alternatively, maybe it's a trick question where\n\n\"hello\" translates to something else.\n\n\n\nIf we consider \"hello\" as a hexadecimal number, H isn't valid hex (only 0-9 and A-F). So\n\nthat's not possible. If using base64 decoding, but again, letters are allowed. But without\n\nmore context, this is all guesswork.\n\n\n\nGiven that the user is an AI math expert, maybe they intended to send a problem but it didn't\n\ncome through. Since I need to follow the instructions strictly, perhaps I should answer based\n\non \"hello\" being the input. If we take \"hello\" as a string and compute something related to\n\nits letters, like the sum of their positions in the alphabet: H=8, E=5, L=12, L=12, O=15. So\n\n8+5+12+12+15 = 52. Maybe that's the answer? \\\\boxed{52}\n\n\n\nAlternatively, product: 8\\*5\\*12\\*12\\*15. Let's compute that: 8\\*5=40, 40\\*12=480, 480\\*12=5760,\n\n5760\\*15=86400. That's a big number, but maybe not likely.\n\n\n\nAlternatively, count the letters: \"hello\" has 5 letters. So \\\\boxed{5}. But that's too simple.\n\n\n\n\n\nAnother angle: \"HELLO\" in different languages? Not sure. Or Morse code: H is comma, E is dot\n\nslash, L are double dash, O is hollow exclamation. But converting that to numbers or\n\nsomething else... Maybe the total dashes and dots. H (comma) has 3 symbols, but maybe not\n\nrelevant.\n\n\n\nAlternatively, if it's a calculus problem, like finding the derivative of \"hello\", which\n\ndoesn't make sense.\n\n\n\nGiven all this uncertainty, perhaps the safest assumption is that the user intended to send a\n\nmath problem but it didn't come through correctly. Therefore, my answer should indicate that\n\nthe problem is missing. However, since I need to provide a boxed answer as per instructions,\n\nand there's no way to know, maybe I'll go with the sum of the alphabetical positions:\n\n8+5+12+12+15=52. So \\\\boxed{52}.\n\n&lt;/think&gt;\n\n\n\nThe problem statement appears to be incomplete or incorrectly submitted. However, if we\n\ninterpret \"hello\" by converting each letter to its position in the alphabet (H=8, E=5, L=12,\n\nO=15) and summing them:\n\n\n\n\\\\\\[\n\n8 + 5 + 12 + 12 + 15 = 52\n\n\\\\\\]\n\n\n\n\\\\boxed{52}",
          "author_fullname": "t2_ltqoxikt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "All i said was hello lol",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsofwq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751762144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;gt; ollama run phi4-mini-reasoning:3.8b&lt;/p&gt;\n\n&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; hello&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;think&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;Okay, let&amp;#39;s see what the user needs here. The message says &amp;quot;You will be given a problem.&amp;quot; but&lt;/p&gt;\n\n&lt;p&gt;then it just has &amp;quot;hello&amp;quot;. Hmm, maybe there was a typo or the problem didn&amp;#39;t get sent&lt;/p&gt;\n\n&lt;p&gt;correctly. Let me check again.&lt;/p&gt;\n\n&lt;p&gt;Wait, the user&amp;#39;s name is Phi, an AI math expert from Microsoft. Then there&amp;#39;s a greeting&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;hello&amp;quot; and that&amp;#39;s it. So perhaps the actual problem wasn&amp;#39;t included properly. Maybe the user&lt;/p&gt;\n\n&lt;p&gt;intended to send a math problem but only wrote &amp;quot;hello&amp;quot;. I should prompt them to provide the&lt;/p&gt;\n\n&lt;p&gt;problem they need help with. But according to the instructions, I need to first think through&lt;/p&gt;\n\n&lt;p&gt;solving the problem before giving the answer. However, there&amp;#39;s no problem here.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, maybe &amp;quot;hello&amp;quot; is the problem itself? That seems unlikely. Maybe it&amp;#39;s a test or&lt;/p&gt;\n\n&lt;p&gt;a mistake. Let me consider if &amp;quot;hello&amp;quot; could be part of a math question. For example,&lt;/p&gt;\n\n&lt;p&gt;converting letters to numbers using ASCII values. Let&amp;#39;s try that.&lt;/p&gt;\n\n&lt;p&gt;H in ASCII is 72, E is 69, L is 76, another L is 76, and O is 79. So adding those up: 72 + 69&lt;/p&gt;\n\n&lt;p&gt;+ 76 + 76 + 79. Let me calculate that. 72 + 69 is 141. 141 + 76 is 217. 217 + 76 is 293. 293&lt;/p&gt;\n\n&lt;p&gt;+ 79 is 372. So the total would be 372. But I&amp;#39;m not sure if that&amp;#39;s what the user wants. They&lt;/p&gt;\n\n&lt;p&gt;might have intended a different approach.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, maybe it&amp;#39;s a cipher problem, like Caesar cipher where each letter is shifted.&lt;/p&gt;\n\n&lt;p&gt;For example, shifting letters by a certain number. But without more context, it&amp;#39;s hard to&lt;/p&gt;\n\n&lt;p&gt;tell.&lt;/p&gt;\n\n&lt;p&gt;Since the user mentioned being an AI math expert and the initial instruction was about&lt;/p&gt;\n\n&lt;p&gt;solving a problem, but the input only has &amp;quot;hello&amp;quot;, I think the best course is to ask for&lt;/p&gt;\n\n&lt;p&gt;clarification. However, since the task requires providing an answer within a box, maybe I&lt;/p&gt;\n\n&lt;p&gt;should state that the problem is missing or assume a common problem. Wait, perhaps the user&lt;/p&gt;\n\n&lt;p&gt;intended to send a problem but it didn&amp;#39;t go through, so responding by pointing out the&lt;/p&gt;\n\n&lt;p&gt;missing problem would be better. But according to the example given in the query, sometimes&lt;/p&gt;\n\n&lt;p&gt;even if there&amp;#39;s confusion, you have to proceed.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, maybe &amp;quot;hello&amp;quot; is a greeting and the actual problem is forthcoming. But the&lt;/p&gt;\n\n&lt;p&gt;structure of the query shows that after &amp;quot;You will be given a problem,&amp;quot; the user input is&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;hello&amp;quot;. So perhaps there was an error in submission. In that case, my answer should note&lt;/p&gt;\n\n&lt;p&gt;that the problem seems missing and ask for it. But since I need to provide a boxed answer,&lt;/p&gt;\n\n&lt;p&gt;maybe there&amp;#39;s a default response. Hmm.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, if this is a test of handling errors, then the correct response would be to&lt;/p&gt;\n\n&lt;p&gt;inform the user that no problem was detected and request the problem statement. However,&lt;/p&gt;\n\n&lt;p&gt;given the setup here, perhaps the assistant is supposed to generate a sample problem and&lt;/p&gt;\n\n&lt;p&gt;solution? But that&amp;#39;s speculative.&lt;/p&gt;\n\n&lt;p&gt;Wait, looking back at the original instructions: &amp;quot;You will be given a problem. Please reason&lt;/p&gt;\n\n&lt;p&gt;step by step, and put your final answer within \\boxed{}.&amp;quot; The user&amp;#39;s input after that is&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;hello&amp;quot;. So maybe the actual problem was meant to follow &amp;quot;hello&amp;quot; but didn&amp;#39;t. Therefore, my&lt;/p&gt;\n\n&lt;p&gt;conclusion is that there&amp;#39;s a missing problem here. Since I need to provide an answer, perhaps&lt;/p&gt;\n\n&lt;p&gt;I should state that the problem is missing and ask for it, but in the boxed answer? But the&lt;/p&gt;\n\n&lt;p&gt;example shows mathematical answers in boxes. Alternatively, maybe it&amp;#39;s a trick question where&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;hello&amp;quot; translates to something else.&lt;/p&gt;\n\n&lt;p&gt;If we consider &amp;quot;hello&amp;quot; as a hexadecimal number, H isn&amp;#39;t valid hex (only 0-9 and A-F). So&lt;/p&gt;\n\n&lt;p&gt;that&amp;#39;s not possible. If using base64 decoding, but again, letters are allowed. But without&lt;/p&gt;\n\n&lt;p&gt;more context, this is all guesswork.&lt;/p&gt;\n\n&lt;p&gt;Given that the user is an AI math expert, maybe they intended to send a problem but it didn&amp;#39;t&lt;/p&gt;\n\n&lt;p&gt;come through. Since I need to follow the instructions strictly, perhaps I should answer based&lt;/p&gt;\n\n&lt;p&gt;on &amp;quot;hello&amp;quot; being the input. If we take &amp;quot;hello&amp;quot; as a string and compute something related to&lt;/p&gt;\n\n&lt;p&gt;its letters, like the sum of their positions in the alphabet: H=8, E=5, L=12, L=12, O=15. So&lt;/p&gt;\n\n&lt;p&gt;8+5+12+12+15 = 52. Maybe that&amp;#39;s the answer? \\boxed{52}&lt;/p&gt;\n\n&lt;p&gt;Alternatively, product: 8*5*12*12*15. Let&amp;#39;s compute that: 8*5=40, 40*12=480, 480*12=5760,&lt;/p&gt;\n\n&lt;p&gt;5760*15=86400. That&amp;#39;s a big number, but maybe not likely.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, count the letters: &amp;quot;hello&amp;quot; has 5 letters. So \\boxed{5}. But that&amp;#39;s too simple.&lt;/p&gt;\n\n&lt;p&gt;Another angle: &amp;quot;HELLO&amp;quot; in different languages? Not sure. Or Morse code: H is comma, E is dot&lt;/p&gt;\n\n&lt;p&gt;slash, L are double dash, O is hollow exclamation. But converting that to numbers or&lt;/p&gt;\n\n&lt;p&gt;something else... Maybe the total dashes and dots. H (comma) has 3 symbols, but maybe not&lt;/p&gt;\n\n&lt;p&gt;relevant.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, if it&amp;#39;s a calculus problem, like finding the derivative of &amp;quot;hello&amp;quot;, which&lt;/p&gt;\n\n&lt;p&gt;doesn&amp;#39;t make sense.&lt;/p&gt;\n\n&lt;p&gt;Given all this uncertainty, perhaps the safest assumption is that the user intended to send a&lt;/p&gt;\n\n&lt;p&gt;math problem but it didn&amp;#39;t come through correctly. Therefore, my answer should indicate that&lt;/p&gt;\n\n&lt;p&gt;the problem is missing. However, since I need to provide a boxed answer as per instructions,&lt;/p&gt;\n\n&lt;p&gt;and there&amp;#39;s no way to know, maybe I&amp;#39;ll go with the sum of the alphabetical positions:&lt;/p&gt;\n\n&lt;p&gt;8+5+12+12+15=52. So \\boxed{52}.&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;The problem statement appears to be incomplete or incorrectly submitted. However, if we&lt;/p&gt;\n\n&lt;p&gt;interpret &amp;quot;hello&amp;quot; by converting each letter to its position in the alphabet (H=8, E=5, L=12,&lt;/p&gt;\n\n&lt;p&gt;O=15) and summing them:&lt;/p&gt;\n\n&lt;p&gt;\\[&lt;/p&gt;\n\n&lt;p&gt;8 + 5 + 12 + 12 + 15 = 52&lt;/p&gt;\n\n&lt;p&gt;\\]&lt;/p&gt;\n\n&lt;p&gt;\\boxed{52}&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lsofwq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "numinouslymusing",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsofwq/all_i_said_was_hello_lol/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsofwq/all_i_said_was_hello_lol/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751762144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello all,\n\nI present to you **Impish\\_LLAMA\\_4B**, one of the most powerful roleplay \\\\ adventure finetunes at its size category.\n\nTL;DR:\n\n* An **incredibly powerful** roleplay model for the size. It has **sovl !**\n* Does **Adventure** very well for such size!\n* Characters have **agency**, and might surprise you! [See the examples in the logs](https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B#roleplay-examples-this-character-is-availbe-here) 🙂\n* Roleplay &amp; Assistant data used plenty of **16K** examples.\n* **Very responsive**, feels 'in the moment', kicks **far above** its weight. You might forget it's a **4B** if you squint.\n* Based on a lot of the data in [Impish\\_Magic\\_24B](https://huggingface.co/SicariusSicariiStuff/Impish_Magic_24B)\n* **Super long context** as well as context attention for **4B**, personally tested for up to **16K**.\n* Can run on **Raspberry Pi 5** with ease.\n* Trained on over **400m tokens** with highlly currated data that was tested on countless models beforehand. And some new stuff, as always.\n* Very decent assistant.\n* Mostly **uncensored** while retaining plenty of intelligence.\n* Less **positivity** &amp; **uncensored**, [Negative\\_LLAMA\\_70B](https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B) style of data, adjusted for **4B**, with serious upgrades. Training data contains combat scenarios. And it **shows**!\n* Trained on **extended 4chan dataset** to add humanity, quirkiness, and naturally— less positivity, and the inclination to... argue 🙃\n* **Short length** response (1-3 paragraphs, usually 1-2). CAI Style.\n\nCheck out the model card for more details &amp; character cards for Roleplay \\\\ Adventure:\n\n[https://huggingface.co/SicariusSicariiStuff/Impish\\_LLAMA\\_4B](https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B)\n\nAlso, currently hosting it on Horde at an extremely high availability, likely less than 2 seconds queue, even under maximum load (\\~**3600** tokens per second, **96 threads**)\n\n[Horde](https://preview.redd.it/ga4ihkf1q0bf1.png?width=1086&amp;format=png&amp;auto=webp&amp;s=d387a56cd2c4029a1f36db3df13c627e6d9f11cd)\n\n\\~3600 tokens per second, 96 threads)Would love some feedback! :)",
          "author_fullname": "t2_ik8czvp65",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Powerful 4B Nemotron based finetune",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ga4ihkf1q0bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 13,
                  "x": 108,
                  "u": "https://preview.redd.it/ga4ihkf1q0bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fe9c07c5aca94fec68bd82ec08897fbba3685f73"
                },
                {
                  "y": 27,
                  "x": 216,
                  "u": "https://preview.redd.it/ga4ihkf1q0bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=55ad7e703ab87be0976513907e8f3332aed2c757"
                },
                {
                  "y": 40,
                  "x": 320,
                  "u": "https://preview.redd.it/ga4ihkf1q0bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=174a5aeb5083723c5a53e0ad354cfb98bbc788a2"
                },
                {
                  "y": 81,
                  "x": 640,
                  "u": "https://preview.redd.it/ga4ihkf1q0bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec349f287cfaee9b714f2c2ddefa2f68c02f4f70"
                },
                {
                  "y": 121,
                  "x": 960,
                  "u": "https://preview.redd.it/ga4ihkf1q0bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=908e4a0d614b4ed29d46ecc2bdb4c09374d78e05"
                },
                {
                  "y": 137,
                  "x": 1080,
                  "u": "https://preview.redd.it/ga4ihkf1q0bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=782da816bbb449615bf022ffc6a3acc461126bb3"
                }
              ],
              "s": {
                "y": 138,
                "x": 1086,
                "u": "https://preview.redd.it/ga4ihkf1q0bf1.png?width=1086&amp;format=png&amp;auto=webp&amp;s=d387a56cd2c4029a1f36db3df13c627e6d9f11cd"
              },
              "id": "ga4ihkf1q0bf1"
            }
          },
          "name": "t3_1ls5b89",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 139,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 139,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ec8383a655a2e03811781fd06b6cb761f8247773",
          "edited": 1751705598,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751705018,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I present to you &lt;strong&gt;Impish_LLAMA_4B&lt;/strong&gt;, one of the most powerful roleplay \\ adventure finetunes at its size category.&lt;/p&gt;\n\n&lt;p&gt;TL;DR:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;An &lt;strong&gt;incredibly powerful&lt;/strong&gt; roleplay model for the size. It has &lt;strong&gt;sovl !&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Does &lt;strong&gt;Adventure&lt;/strong&gt; very well for such size!&lt;/li&gt;\n&lt;li&gt;Characters have &lt;strong&gt;agency&lt;/strong&gt;, and might surprise you! &lt;a href=\"https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B#roleplay-examples-this-character-is-availbe-here\"&gt;See the examples in the logs&lt;/a&gt; 🙂&lt;/li&gt;\n&lt;li&gt;Roleplay &amp;amp; Assistant data used plenty of &lt;strong&gt;16K&lt;/strong&gt; examples.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Very responsive&lt;/strong&gt;, feels &amp;#39;in the moment&amp;#39;, kicks &lt;strong&gt;far above&lt;/strong&gt; its weight. You might forget it&amp;#39;s a &lt;strong&gt;4B&lt;/strong&gt; if you squint.&lt;/li&gt;\n&lt;li&gt;Based on a lot of the data in &lt;a href=\"https://huggingface.co/SicariusSicariiStuff/Impish_Magic_24B\"&gt;Impish_Magic_24B&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Super long context&lt;/strong&gt; as well as context attention for &lt;strong&gt;4B&lt;/strong&gt;, personally tested for up to &lt;strong&gt;16K&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;Can run on &lt;strong&gt;Raspberry Pi 5&lt;/strong&gt; with ease.&lt;/li&gt;\n&lt;li&gt;Trained on over &lt;strong&gt;400m tokens&lt;/strong&gt; with highlly currated data that was tested on countless models beforehand. And some new stuff, as always.&lt;/li&gt;\n&lt;li&gt;Very decent assistant.&lt;/li&gt;\n&lt;li&gt;Mostly &lt;strong&gt;uncensored&lt;/strong&gt; while retaining plenty of intelligence.&lt;/li&gt;\n&lt;li&gt;Less &lt;strong&gt;positivity&lt;/strong&gt; &amp;amp; &lt;strong&gt;uncensored&lt;/strong&gt;, &lt;a href=\"https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B\"&gt;Negative_LLAMA_70B&lt;/a&gt; style of data, adjusted for &lt;strong&gt;4B&lt;/strong&gt;, with serious upgrades. Training data contains combat scenarios. And it &lt;strong&gt;shows&lt;/strong&gt;!&lt;/li&gt;\n&lt;li&gt;Trained on &lt;strong&gt;extended 4chan dataset&lt;/strong&gt; to add humanity, quirkiness, and naturally— less positivity, and the inclination to... argue 🙃&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Short length&lt;/strong&gt; response (1-3 paragraphs, usually 1-2). CAI Style.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Check out the model card for more details &amp;amp; character cards for Roleplay \\ Adventure:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B\"&gt;https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Also, currently hosting it on Horde at an extremely high availability, likely less than 2 seconds queue, even under maximum load (~&lt;strong&gt;3600&lt;/strong&gt; tokens per second, &lt;strong&gt;96 threads&lt;/strong&gt;)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ga4ihkf1q0bf1.png?width=1086&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d387a56cd2c4029a1f36db3df13c627e6d9f11cd\"&gt;Horde&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;~3600 tokens per second, 96 threads)Would love some feedback! :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?auto=webp&amp;s=e0ddfe5f9c048e7d84958224fc2086f57be2ac3b",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=eb145a5dc8675fac7f239771cdb889ea5c13d23f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3defe65e0259d3bc1f66e6715d355bed1c5b9032",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf7987ab30b7378084d356fe44b809efff4d294e",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d882c129728e2bb772cd8f145ea68d43d0c6637",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bed7323290921e159214218f6c1a4e9d9b531c3b",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=46ca457684af7e9d7797c792fb61454125c1e6ee",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1ls5b89",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sicarius_The_First",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751705018,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yesterday, I finished evaluating my Android agent model, deki, on two separate benchmarks: Android Control and Android World. For both benchmarks I used a subset of the dataset without fine-tuning. The results show that image description models like deki enables large LLMs (like GPT-4o, GPT-4.1, and Gemini 2.5) to become State-of-the-Art on Android AI agent benchmarks using only vision capabilities, without relying on Accessibility Trees, on both single-step and multi-step tasks.\n\ndeki is a model that understands what’s on your screen and creates a description of the UI screenshot with all coordinates/sizes/attributes. All the code is open sourced. ML, Backend, Android, code updates for benchmarks and also evaluation logs.\n\nAll the code/information is available on GitHub: [https://github.com/RasulOs/deki](https://github.com/RasulOs/deki)\n\nI have also uploaded the model to Hugging Face:  \nSpace: [orasul/deki](https://huggingface.co/spaces/orasul/deki)  \n(Check the analyze-and-get-yolo endpoint)\n\nModel: [orasul/deki-yolo](https://huggingface.co/orasul/deki-yolo)",
          "author_fullname": "t2_dlk476nn6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Open-sourced image description models (Object detection, OCR, Image processing, CNN) make LLMs SOTA in AI agentic benchmarks like Android World and Android Control",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 92,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "h3d7d54lx3bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 92,
                  "x": 108,
                  "u": "https://preview.redd.it/h3d7d54lx3bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4e1e5d552b5048937f14f6fd537e8fbb4b0d150"
                },
                {
                  "y": 184,
                  "x": 216,
                  "u": "https://preview.redd.it/h3d7d54lx3bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=99c1e9b34fec66220de01042567f3fc3e0c6a259"
                },
                {
                  "y": 274,
                  "x": 320,
                  "u": "https://preview.redd.it/h3d7d54lx3bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2613b7141cb698ec6225fc2468221fb83e01b05c"
                },
                {
                  "y": 548,
                  "x": 640,
                  "u": "https://preview.redd.it/h3d7d54lx3bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=85620d052f7150005812abb8a13a2e5d1d114565"
                },
                {
                  "y": 822,
                  "x": 960,
                  "u": "https://preview.redd.it/h3d7d54lx3bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=91c13d5b1213142a0c0f6c94c56dfc419e2ff681"
                },
                {
                  "y": 924,
                  "x": 1080,
                  "u": "https://preview.redd.it/h3d7d54lx3bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0b9764f7c6678970b734b4a15de0c1f169495106"
                }
              ],
              "s": {
                "y": 3571,
                "x": 4170,
                "u": "https://preview.redd.it/h3d7d54lx3bf1.png?width=4170&amp;format=png&amp;auto=webp&amp;s=954c7b87399c485ba49db571b812da2074217da3"
              },
              "id": "h3d7d54lx3bf1"
            },
            "fdza644lx3bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/fdza644lx3bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d4d6593a8646a51a547feb3205684e71abf2db72"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/fdza644lx3bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab9e21f032465ac2f1b78d73bc7e60ef4292cf17"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/fdza644lx3bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=324dc7197628490cd411502598a0f4d63b2b3940"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/fdza644lx3bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8acf1705fcc3361cb92b8594e6da9e651189c421"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/fdza644lx3bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6005de9e5b453e8976adca33f0eb49616b0edc8f"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/fdza644lx3bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4771d4a434c50d01d7e2d7531554bc52ecd7064d"
                }
              ],
              "s": {
                "y": 2368,
                "x": 3571,
                "u": "https://preview.redd.it/fdza644lx3bf1.png?width=3571&amp;format=png&amp;auto=webp&amp;s=841429e64ec9a831262483c0b7479886fd01029f"
              },
              "id": "fdza644lx3bf1"
            },
            "0hm1zxpmx3bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 55,
                  "x": 108,
                  "u": "https://preview.redd.it/0hm1zxpmx3bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1905e1e3c4a5669198f11b544de3bae463692a72"
                },
                {
                  "y": 111,
                  "x": 216,
                  "u": "https://preview.redd.it/0hm1zxpmx3bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=78a297ef8333986019408fd6b72f1830a22911da"
                },
                {
                  "y": 164,
                  "x": 320,
                  "u": "https://preview.redd.it/0hm1zxpmx3bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=63de894cbcd59bb3e4281500136b499d2a2ff125"
                },
                {
                  "y": 329,
                  "x": 640,
                  "u": "https://preview.redd.it/0hm1zxpmx3bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b1df195ce5438559971c054504be62e57dc0eaa4"
                },
                {
                  "y": 493,
                  "x": 960,
                  "u": "https://preview.redd.it/0hm1zxpmx3bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0b238eb03338824287ccc0dbcbf06acfc69ee819"
                },
                {
                  "y": 555,
                  "x": 1080,
                  "u": "https://preview.redd.it/0hm1zxpmx3bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3c508e356a58da9829f51954ad7a2aa96e1088e9"
                }
              ],
              "s": {
                "y": 1636,
                "x": 3180,
                "u": "https://preview.redd.it/0hm1zxpmx3bf1.png?width=3180&amp;format=png&amp;auto=webp&amp;s=66b70eca5ea3d910e05c01580bc20f0a7aea73b9"
              },
              "id": "0hm1zxpmx3bf1"
            }
          },
          "name": "t3_1lsi0gj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 16,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "fdza644lx3bf1",
                "id": 699412800
              },
              {
                "media_id": "h3d7d54lx3bf1",
                "id": 699412801
              },
              {
                "media_id": "0hm1zxpmx3bf1",
                "id": 699412802
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/RwkhySplb6CWm6KR-CbkXw8jpoNWLPRz3r_cADWabdw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751743870,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yesterday, I finished evaluating my Android agent model, deki, on two separate benchmarks: Android Control and Android World. For both benchmarks I used a subset of the dataset without fine-tuning. The results show that image description models like deki enables large LLMs (like GPT-4o, GPT-4.1, and Gemini 2.5) to become State-of-the-Art on Android AI agent benchmarks using only vision capabilities, without relying on Accessibility Trees, on both single-step and multi-step tasks.&lt;/p&gt;\n\n&lt;p&gt;deki is a model that understands what’s on your screen and creates a description of the UI screenshot with all coordinates/sizes/attributes. All the code is open sourced. ML, Backend, Android, code updates for benchmarks and also evaluation logs.&lt;/p&gt;\n\n&lt;p&gt;All the code/information is available on GitHub: &lt;a href=\"https://github.com/RasulOs/deki\"&gt;https://github.com/RasulOs/deki&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I have also uploaded the model to Hugging Face:&lt;br/&gt;\nSpace: &lt;a href=\"https://huggingface.co/spaces/orasul/deki\"&gt;orasul/deki&lt;/a&gt;&lt;br/&gt;\n(Check the analyze-and-get-yolo endpoint)&lt;/p&gt;\n\n&lt;p&gt;Model: &lt;a href=\"https://huggingface.co/orasul/deki-yolo\"&gt;orasul/deki-yolo&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lsi0gj",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsi0gj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Old_Mathematician107",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsi0gj/opensourced_image_description_models_object/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lsi0gj",
          "subreddit_subscribers": 494897,
          "created_utc": 1751743870,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3lnwo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Impact of PCIe 5.0 Bandwidth on GPU Content Creation Performance",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls70r2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 52,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 52,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=6fba0fd765a16b9b2064046509d9b2e66373edd6",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751712185,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "pugetsystems.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.pugetsystems.com/labs/articles/impact-of-pcie-5-0-bandwidth-on-gpu-content-creation-performance/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?auto=webp&amp;s=f1630bd3dfb2226fa410a38649e7d40447a81866",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=41313f8558078bc1cb1b64de29b33e20187ee0b2",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=028bbe9db629fd24048f079e684f025b3f56072b",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5d7b9e566a80ee05583c3ef0b1342f3dda9d636",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=86ebf97301bc00e90b9b236ebf2a2bb13dae2a1a",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=08be7ce0643e4022cce3ddea140b9601987b4e49",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6e3c8cc9b20478e822baff4323bcf8a63d838755",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1ls70r2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "d5dq",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls70r2/impact_of_pcie_50_bandwidth_on_gpu_content/",
          "stickied": false,
          "url": "https://www.pugetsystems.com/labs/articles/impact-of-pcie-5-0-bandwidth-on-gpu-content-creation-performance/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751712185,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Most RAG explainers jump into theories and scary infra diagrams. Here’s the tiny end-to-end demo that can easy to understand for me:\n\nSuppose we have a documentation like this: \"Boil an egg. Poach an egg. How to change a tire\"\n\n# Step 1: Chunk\n\n    S0: \"Boil an egg\"\n    S1: \"Poach an egg\"\n    S2: \"How to change a tire\"\n\n# Step 2: Embed\n\nAfter the words “Boil an egg” pass through a pretrained transformer, the model compresses its hidden states into a single 4-dimensional vector; each value is just one coordinate of that learned “meaning point” in vector space.\n\nToy demo values:\n\n    V0 = [ 0.90, 0.10, 0.00, 0.10]   # “Boil an egg”\n    V1 = [ 0.88, 0.12, 0.00, 0.09]   # “Poach an egg”\n    V2 = [-0.20, 0.40, 0.80, 0.10]   # “How to change a tire”\n\n(Real models spit out 384-D to 3072-D vectors; 4-D keeps the math readable.)\n\n# Step 3: Normalize\n\nPut every vector on the unit sphere:\n\n    # Normalised (unit-length) vectors\n    V0̂ = [ 0.988, 0.110, 0.000, 0.110]   # 0.988² + 0.110² + 0.000² + 0.110² ≈ 1.000 → 1\n    V1̂ = [ 0.986, 0.134, 0.000, 0.101]   # 0.986² + 0.134² + 0.000² + 0.101² ≈ 1.000 → 1\n    V2̂ = [-0.217, 0.434, 0.868, 0.108]   # (-0.217)² + 0.434² + 0.868² + 0.108² ≈ 1.001 → 1\n\n# Step 4: Index\n\nDrop V0\\^,V1\\^,V2\\^ into a similarity index (FAISS, Qdrant, etc.).  \nKeep a side map `{0:S0, 1:S1, 2:S2}` so IDs can turn back into text later.\n\n# Step 5: Similarity Search\n\n**User asks**  \n“Best way to cook an egg?”\n\nWe embed this sentence and normalize it as well, which gives us something like:\n\n    Vi^ = [0.989, 0.086, 0.000, 0.118]\n\nThen we need to find the vector that’s *closest* to this one.  \nThe most common way is cosine similarity — often written as:\n\n    cos(θ) = (A ⋅ B) / (‖A‖ × ‖B‖)\n\nBut since we already normalized all vectors,  \n‖A‖ = ‖B‖ = 1 → so the formula becomes just:\n\n    cos(θ) = A ⋅ B\n\nThis means we just need to calculate the **dot product** between the user input vector and each stored vector.  \nIf two vectors are exactly the same, dot product = 1.  \nSo we sort by which ones have values closest to 1 - higher = more similar.\n\nLet’s calculate the scores (example, not real)\n\n    Vi^ ⋅ V0̂ = (0.989)(0.988) + (0.086)(0.110) + (0)(0) + (0.118)(0.110)\n            ≈ 0.977 + 0.009 + 0 + 0.013 = 0.999\n    \n    Vi^ ⋅ V1̂ = (0.989)(0.986) + (0.086)(0.134) + (0)(0) + (0.118)(0.101)\n            ≈ 0.975 + 0.012 + 0 + 0.012 = 0.999\n    \n    Vi^ ⋅ V2̂ = (0.989)(-0.217) + (0.086)(0.434) + (0)(0.868) + (0.118)(0.108)\n            ≈ -0.214 + 0.037 + 0 + 0.013 = -0.164\n    \n\nSo we find that sentence 0 (“Boil an egg”) and sentence 1 (“Poach an egg”)  \nare both very close to the user input.\n\nWe **retrieve those two as context**, and pass them to the LLM.  \nNow the LLM has relevant info to answer accurately, instead of guessing.",
          "author_fullname": "t2_1pnlpczpqa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How RAG actually works — a toy example with real math",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrsx20",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 579,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 579,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751661855,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most RAG explainers jump into theories and scary infra diagrams. Here’s the tiny end-to-end demo that can easy to understand for me:&lt;/p&gt;\n\n&lt;p&gt;Suppose we have a documentation like this: &amp;quot;Boil an egg. Poach an egg. How to change a tire&amp;quot;&lt;/p&gt;\n\n&lt;h1&gt;Step 1: Chunk&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;S0: &amp;quot;Boil an egg&amp;quot;\nS1: &amp;quot;Poach an egg&amp;quot;\nS2: &amp;quot;How to change a tire&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Step 2: Embed&lt;/h1&gt;\n\n&lt;p&gt;After the words “Boil an egg” pass through a pretrained transformer, the model compresses its hidden states into a single 4-dimensional vector; each value is just one coordinate of that learned “meaning point” in vector space.&lt;/p&gt;\n\n&lt;p&gt;Toy demo values:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;V0 = [ 0.90, 0.10, 0.00, 0.10]   # “Boil an egg”\nV1 = [ 0.88, 0.12, 0.00, 0.09]   # “Poach an egg”\nV2 = [-0.20, 0.40, 0.80, 0.10]   # “How to change a tire”\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;(Real models spit out 384-D to 3072-D vectors; 4-D keeps the math readable.)&lt;/p&gt;\n\n&lt;h1&gt;Step 3: Normalize&lt;/h1&gt;\n\n&lt;p&gt;Put every vector on the unit sphere:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# Normalised (unit-length) vectors\nV0̂ = [ 0.988, 0.110, 0.000, 0.110]   # 0.988² + 0.110² + 0.000² + 0.110² ≈ 1.000 → 1\nV1̂ = [ 0.986, 0.134, 0.000, 0.101]   # 0.986² + 0.134² + 0.000² + 0.101² ≈ 1.000 → 1\nV2̂ = [-0.217, 0.434, 0.868, 0.108]   # (-0.217)² + 0.434² + 0.868² + 0.108² ≈ 1.001 → 1\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Step 4: Index&lt;/h1&gt;\n\n&lt;p&gt;Drop V0^,V1^,V2^ into a similarity index (FAISS, Qdrant, etc.).&lt;br/&gt;\nKeep a side map &lt;code&gt;{0:S0, 1:S1, 2:S2}&lt;/code&gt; so IDs can turn back into text later.&lt;/p&gt;\n\n&lt;h1&gt;Step 5: Similarity Search&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;User asks&lt;/strong&gt;&lt;br/&gt;\n“Best way to cook an egg?”&lt;/p&gt;\n\n&lt;p&gt;We embed this sentence and normalize it as well, which gives us something like:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Vi^ = [0.989, 0.086, 0.000, 0.118]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then we need to find the vector that’s &lt;em&gt;closest&lt;/em&gt; to this one.&lt;br/&gt;\nThe most common way is cosine similarity — often written as:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;cos(θ) = (A ⋅ B) / (‖A‖ × ‖B‖)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;But since we already normalized all vectors,&lt;br/&gt;\n‖A‖ = ‖B‖ = 1 → so the formula becomes just:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;cos(θ) = A ⋅ B\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This means we just need to calculate the &lt;strong&gt;dot product&lt;/strong&gt; between the user input vector and each stored vector.&lt;br/&gt;\nIf two vectors are exactly the same, dot product = 1.&lt;br/&gt;\nSo we sort by which ones have values closest to 1 - higher = more similar.&lt;/p&gt;\n\n&lt;p&gt;Let’s calculate the scores (example, not real)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Vi^ ⋅ V0̂ = (0.989)(0.988) + (0.086)(0.110) + (0)(0) + (0.118)(0.110)\n        ≈ 0.977 + 0.009 + 0 + 0.013 = 0.999\n\nVi^ ⋅ V1̂ = (0.989)(0.986) + (0.086)(0.134) + (0)(0) + (0.118)(0.101)\n        ≈ 0.975 + 0.012 + 0 + 0.012 = 0.999\n\nVi^ ⋅ V2̂ = (0.989)(-0.217) + (0.086)(0.434) + (0)(0.868) + (0.118)(0.108)\n        ≈ -0.214 + 0.037 + 0 + 0.013 = -0.164\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So we find that sentence 0 (“Boil an egg”) and sentence 1 (“Poach an egg”)&lt;br/&gt;\nare both very close to the user input.&lt;/p&gt;\n\n&lt;p&gt;We &lt;strong&gt;retrieve those two as context&lt;/strong&gt;, and pass them to the LLM.&lt;br/&gt;\nNow the LLM has relevant info to answer accurately, instead of guessing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lrsx20",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Main-Fisherman-2075",
          "discussion_type": null,
          "num_comments": 60,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrsx20/how_rag_actually_works_a_toy_example_with_real/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrsx20/how_rag_actually_works_a_toy_example_with_real/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751661855,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi.\n\nI create Android application for locally running AI models on smartphone\n\nI am interested in your opinion.\n\n[https://play.google.com/store/apps/details?id=com.romankryvolapov.offlineailauncher](https://play.google.com/store/apps/details?id=com.romankryvolapov.offlineailauncher)\n\nhttps://preview.redd.it/cc4u5d7h23bf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=e7d88e77046cf08053899eda10e9fac0a9752cf5",
          "author_fullname": "t2_3qjxph23",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New app for locally running AI models on Android your smartphone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "cc4u5d7h23bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/cc4u5d7h23bf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1e95cfbe66e4d06235f507c322e5bdbb1a486137"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/cc4u5d7h23bf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cab4b3f53a8baaa5ae1a23863e9f1000ead939ec"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/cc4u5d7h23bf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=86a54df6c728461224b28806669598087ba092c5"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/cc4u5d7h23bf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=329ec3f74abebac985366aec79860020d2535471"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/cc4u5d7h23bf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=296441b5e6dc2b8ba74e33a8fbb137c827216655"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/cc4u5d7h23bf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c393a90227facf1440dc8ca53d47acb2bb72018c"
                }
              ],
              "s": {
                "y": 3200,
                "x": 1440,
                "u": "https://preview.redd.it/cc4u5d7h23bf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=e7d88e77046cf08053899eda10e9fac0a9752cf5"
              },
              "id": "cc4u5d7h23bf1"
            }
          },
          "name": "t3_1lsdxc2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/mzou-cvKbo89yySFKfh6cxlVrw7VRIQEkdJHPKwwKng.jpeg?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=114ac5e21930254990653d99f465d0f0b70db196",
          "edited": 1751736773,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751733155,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi.&lt;/p&gt;\n\n&lt;p&gt;I create Android application for locally running AI models on smartphone&lt;/p&gt;\n\n&lt;p&gt;I am interested in your opinion.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://play.google.com/store/apps/details?id=com.romankryvolapov.offlineailauncher\"&gt;https://play.google.com/store/apps/details?id=com.romankryvolapov.offlineailauncher&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cc4u5d7h23bf1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d88e77046cf08053899eda10e9fac0a9752cf5\"&gt;https://preview.redd.it/cc4u5d7h23bf1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d88e77046cf08053899eda10e9fac0a9752cf5&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/mzou-cvKbo89yySFKfh6cxlVrw7VRIQEkdJHPKwwKng.jpeg?auto=webp&amp;s=603294677425600238aebad8643a28f98a37ac18",
                  "width": 512,
                  "height": 512
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/mzou-cvKbo89yySFKfh6cxlVrw7VRIQEkdJHPKwwKng.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=50a2358f9b890643e86778f4cba80f28e095ae0b",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/mzou-cvKbo89yySFKfh6cxlVrw7VRIQEkdJHPKwwKng.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fd696e0f18267befba2f7df8f0a9badcfdda702f",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/mzou-cvKbo89yySFKfh6cxlVrw7VRIQEkdJHPKwwKng.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dba60f6210ef2ca39ebd034795d1371f991f9f7d",
                    "width": 320,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "mzou-cvKbo89yySFKfh6cxlVrw7VRIQEkdJHPKwwKng"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsdxc2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RomanKryvolapov",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsdxc2/new_app_for_locally_running_ai_models_on_android/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsdxc2/new_app_for_locally_running_ai_models_on_android/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751733155,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As title states I tried to find the way to use Jan AI with ollama available local models but I didn't found the working way.\n\nAfter lot of trial and error I found working way forwared and document in a blog post\n\n[Jan.AI with Ollama (working solution)](https://developers.knowivate.com/@kheersagar/jan-ai-with-ollama-working-solution)",
          "author_fullname": "t2_1b8utegv8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Jan.AI with Ollama (working solution)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsoflk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751762120,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As title states I tried to find the way to use Jan AI with ollama available local models but I didn&amp;#39;t found the working way.&lt;/p&gt;\n\n&lt;p&gt;After lot of trial and error I found working way forwared and document in a blog post&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://developers.knowivate.com/@kheersagar/jan-ai-with-ollama-working-solution\"&gt;Jan.AI with Ollama (working solution)&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/91QifmmkYXf4kont-M3pku_SgKmkEq-VLJGmQTJzr_I.png?auto=webp&amp;s=afb36909de3fdf0851321ac502e2cce712b98c60",
                  "width": 500,
                  "height": 500
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/91QifmmkYXf4kont-M3pku_SgKmkEq-VLJGmQTJzr_I.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3c478b3adde7eeada968ed24f96e70509dcbda47",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/91QifmmkYXf4kont-M3pku_SgKmkEq-VLJGmQTJzr_I.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=41118f6fe36315cf0dbd19388a1a22c9d4d07793",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/91QifmmkYXf4kont-M3pku_SgKmkEq-VLJGmQTJzr_I.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2d6edb2be32708b5dd197c38934d1d63c949e3a9",
                    "width": 320,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "91QifmmkYXf4kont-M3pku_SgKmkEq-VLJGmQTJzr_I"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lsoflk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "InsideResolve4517",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751762120,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have developed the web app and chrome extension to summarize the long reddit threads discussion using chatgpt, it helps user to analyize thread discussions and sentiments of the discussion.",
          "author_fullname": "t2_7azucao8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I created this tool I named ReddSummary.com – just paste a link and boom you got the summary",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 63,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsfmcj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/DoYaP55WOHORdN07iAI3p-62cotQb-fwucdWaCd1d5w.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751737600,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have developed the web app and chrome extension to summarize the long reddit threads discussion using chatgpt, it helps user to analyize thread discussions and sentiments of the discussion.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/2exxosoue3bf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/2exxosoue3bf1.png?auto=webp&amp;s=1cda7f051de15b6e90aa4909e2aa41b713769f20",
                  "width": 2546,
                  "height": 1156
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/2exxosoue3bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1fd832cbd0013c8cbdcec784df645584bcc52a5d",
                    "width": 108,
                    "height": 49
                  },
                  {
                    "url": "https://preview.redd.it/2exxosoue3bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=85fbce9bb96ceac0b7288b66741e746c2e211765",
                    "width": 216,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/2exxosoue3bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa1e1c0c6dacbd464f6a80a23c5f63d703ec0db4",
                    "width": 320,
                    "height": 145
                  },
                  {
                    "url": "https://preview.redd.it/2exxosoue3bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1c1d1e13fd17ff4381b38e36072d089c11c07e48",
                    "width": 640,
                    "height": 290
                  },
                  {
                    "url": "https://preview.redd.it/2exxosoue3bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b555565ac5d5919934305538ff3e4a858e9348e0",
                    "width": 960,
                    "height": 435
                  },
                  {
                    "url": "https://preview.redd.it/2exxosoue3bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d66d21b5f93bfbece51c5fe6e6c40464cc7e91e6",
                    "width": 1080,
                    "height": 490
                  }
                ],
                "variants": {},
                "id": "q_udZco6iPYKDtVF7n70XwNt8-iEUXvFctKoNCOAC5Q"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lsfmcj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Himanshu507",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsfmcj/i_created_this_tool_i_named_reddsummarycom_just/",
          "stickied": false,
          "url": "https://i.redd.it/2exxosoue3bf1.png",
          "subreddit_subscribers": 494897,
          "created_utc": 1751737600,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm genuinely struggling with everything out there in terms of making me smile and general joke quality. If there is such a model, at what settings should it run? (temp/top\\_k etc). ",
          "author_fullname": "t2_nvmilgf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which open source LLM has the most genuine sense of humor?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls9jvu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751721047,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m genuinely struggling with everything out there in terms of making me smile and general joke quality. If there is such a model, at what settings should it run? (temp/top_k etc). &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ls9jvu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "UltrMgns",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls9jvu/which_open_source_llm_has_the_most_genuine_sense/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls9jvu/which_open_source_llm_has_the_most_genuine_sense/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751721047,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Trying to clean up audio voice profiles for chatterbox ai. Would like to run an AI to clean up isolate and clean up vocals. Tried a few premium online tools and myEdit ai works the best but don’t want to use a premium tool. Extra bonus if it can do other common audio tasks.",
          "author_fullname": "t2_4j40jjpb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local LLM for Audio Cleanup",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsju4i",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751748817,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to clean up audio voice profiles for chatterbox ai. Would like to run an AI to clean up isolate and clean up vocals. Tried a few premium online tools and myEdit ai works the best but don’t want to use a premium tool. Extra bonus if it can do other common audio tasks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsju4i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AnonTheGreat12345",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsju4i/local_llm_for_audio_cleanup/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsju4i/local_llm_for_audio_cleanup/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751748817,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m doing self-funded AI research and recently got access to 2× NVIDIA A100 SXM4 GPUs. I want to build a quiet, stable node at home to run local models and training workloads — no cloud.\n\nHas anyone here actually built a DIY system with A100 SXM4s (not PCIe)? If so:\n What HGX carrier board or server chassis did you use?\n How did you handle power + cooling safely at home?\n Any tips on finding used baseboards or reference systems?\n\nI’m not working for any company — just serious about doing advanced AI work locally and learning by building. Happy to share progress once it’s working.\n\nThanks in advance — would love any help or photos from others doing the same.",
          "author_fullname": "t2_rysc7jpo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone built a home 2× A100 SXM4 node?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsflii",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751737534,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m doing self-funded AI research and recently got access to 2× NVIDIA A100 SXM4 GPUs. I want to build a quiet, stable node at home to run local models and training workloads — no cloud.&lt;/p&gt;\n\n&lt;p&gt;Has anyone here actually built a DIY system with A100 SXM4s (not PCIe)? If so:\n What HGX carrier board or server chassis did you use?\n How did you handle power + cooling safely at home?\n Any tips on finding used baseboards or reference systems?&lt;/p&gt;\n\n&lt;p&gt;I’m not working for any company — just serious about doing advanced AI work locally and learning by building. Happy to share progress once it’s working.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance — would love any help or photos from others doing the same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsflii",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun_Nefariousness228",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsflii/anyone_built_a_home_2_a100_sxm4_node/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsflii/anyone_built_a_home_2_a100_sxm4_node/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751737534,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Qwen3-8B model using Winogrande as benchmark.  \nDWQ and 5bit rule! \n\n🥇 dwq – 68.82%  \n🥈 5bit – 68.51%  \n🥉 6bit – 68.35%  \nbf16 – 67.64%  \ndynamic – 67.56%  \n8bit – 67.56%  \n4bit – 66.30%  \n3bit – 63.85%\n\nhttps://preview.redd.it/95nyy1fby1bf1.png?width=1979&amp;format=png&amp;auto=webp&amp;s=d6402294cedb1bdfc338ea34983203e7118188a3\n\n",
          "author_fullname": "t2_sjt9gvtpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Apple MLX Quantizations Royal Rumble 🔥",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 83,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "95nyy1fby1bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/95nyy1fby1bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2798636b99eaca27c41a9593f8951555bd9519f5"
                },
                {
                  "y": 128,
                  "x": 216,
                  "u": "https://preview.redd.it/95nyy1fby1bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a57ed516e1790497910eefb4c735e6e77ac1f8de"
                },
                {
                  "y": 190,
                  "x": 320,
                  "u": "https://preview.redd.it/95nyy1fby1bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce4dadab624e142cf9d7d7db23388916f76dd9eb"
                },
                {
                  "y": 381,
                  "x": 640,
                  "u": "https://preview.redd.it/95nyy1fby1bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0ca2b93d49f1ae546be5c061c56c6e0626766369"
                },
                {
                  "y": 572,
                  "x": 960,
                  "u": "https://preview.redd.it/95nyy1fby1bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=76964504cdc351c1e0bf249a2fa3dc41b6054cc0"
                },
                {
                  "y": 643,
                  "x": 1080,
                  "u": "https://preview.redd.it/95nyy1fby1bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b35c161ebc60802ca9e3179e1dfeb362709eac86"
                }
              ],
              "s": {
                "y": 1180,
                "x": 1979,
                "u": "https://preview.redd.it/95nyy1fby1bf1.png?width=1979&amp;format=png&amp;auto=webp&amp;s=d6402294cedb1bdfc338ea34983203e7118188a3"
              },
              "id": "95nyy1fby1bf1"
            }
          },
          "name": "t3_1ls95oj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/-1Fa5pMoUdufyX7EbInbdiYRv8uh6lx6DYYAentaN_0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751719836,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3-8B model using Winogrande as benchmark.&lt;br/&gt;\nDWQ and 5bit rule! &lt;/p&gt;\n\n&lt;p&gt;🥇 dwq – 68.82%&lt;br/&gt;\n🥈 5bit – 68.51%&lt;br/&gt;\n🥉 6bit – 68.35%&lt;br/&gt;\nbf16 – 67.64%&lt;br/&gt;\ndynamic – 67.56%&lt;br/&gt;\n8bit – 67.56%&lt;br/&gt;\n4bit – 66.30%&lt;br/&gt;\n3bit – 63.85%&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/95nyy1fby1bf1.png?width=1979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6402294cedb1bdfc338ea34983203e7118188a3\"&gt;https://preview.redd.it/95nyy1fby1bf1.png?width=1979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6402294cedb1bdfc338ea34983203e7118188a3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ls95oj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ifioravanti",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls95oj/apple_mlx_quantizations_royal_rumble/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls95oj/apple_mlx_quantizations_royal_rumble/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751719836,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey yall I made a new open-source tool.\n\nIt's an app that **creates training data for AI models from your text and PDFs**.\n\nIt uses AI like Gemini, Claude, and OpenAI to make good question-answer sets that you can use to make your own AI smarter. The data comes out ready for different models.\n\nSuper simple, super useful, and it's all open source!",
          "author_fullname": "t2_5gpifn7q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source tool for generating training datasets from text files and pdf for fine-tuning language models.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls3gho",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 43,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 43,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1751697395,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey yall I made a new open-source tool.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s an app that &lt;strong&gt;creates training data for AI models from your text and PDFs&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;It uses AI like Gemini, Claude, and OpenAI to make good question-answer sets that you can use to make your own AI smarter. The data comes out ready for different models.&lt;/p&gt;\n\n&lt;p&gt;Super simple, super useful, and it&amp;#39;s all open source!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/MonkWarrior08/Dataset_Generator_for_Fine-tuning?tab=readme-ov-file",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ls3gho",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Idonotknow101",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls3gho/open_source_tool_for_generating_training_datasets/",
          "stickied": false,
          "url": "https://github.com/MonkWarrior08/Dataset_Generator_for_Fine-tuning?tab=readme-ov-file",
          "subreddit_subscribers": 494897,
          "created_utc": 1751697395,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I can’t code/program (at least not yet). \n\nIs anyone building tools/abilities to use a FOSS LLM like Llama to integrate with the family tree software GRAMPS? \n\nI’m thinking you could talk to Llama (ie 3.1 or 3.3) in plain English information about family members, relationships, events, locations, etc and Llama automatically inputs the data into GRAMPS? \n\nThanks 🙏 \n",
          "author_fullname": "t2_z1u2be88b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama &amp; GRAMPS",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lsqr9n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751769743,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I can’t code/program (at least not yet). &lt;/p&gt;\n\n&lt;p&gt;Is anyone building tools/abilities to use a FOSS LLM like Llama to integrate with the family tree software GRAMPS? &lt;/p&gt;\n\n&lt;p&gt;I’m thinking you could talk to Llama (ie 3.1 or 3.3) in plain English information about family members, relationships, events, locations, etc and Llama automatically inputs the data into GRAMPS? &lt;/p&gt;\n\n&lt;p&gt;Thanks 🙏 &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsqr9n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AdCompetitive6193",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsqr9n/llama_gramps/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsqr9n/llama_gramps/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751769743,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone. I am author of Hyprnote(https://github.com/fastrepl/hyprnote) - privacy-first notepad for meetings. We regularly test out the AI models we use in various devices to make sure it runs well.\n\nWhen testing MacBook, Qwen3 1.7B is used, and for Windows, Qwen3 0.6B is used. (All Q4 KM)\n\n[b5828](https://github.com/ggml-org/llama.cpp/tree/b5828)(newer)  .. [b5162](https://github.com/ggml-org/llama.cpp/tree/b5162)(older)\n\nThinking of writing lot longer blog post with lots of numbers &amp; what I learned during the experiment. Please let me know if that is something you guys are interested in.\n\n|Device|OS|SoC|RAM|Compute|Prefill Tok/s|Gen Tok/s|Median Load (ms)|Prefill RAM (MB)|Gen RAM (MB)|Load RAM (MB)|SHA|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|MacBook Pro 14-inch|macOS 15.3.2|Apple M2 Pro|16GB|Metal|615.20|21.69|362.52|2332.28|2337.67|2089.56|b5828|\n||||||571.85|21.43|372.32|2341.77|2347.05|2102.27|b5162|\n|HP EliteBook 660 16-inch G11|Windows 11.24H2|Intel Core Ultra 7 155U|32GB|Vulkan|162.52|14.05|1533.99|3719.23|3641.65|3535.43|b5828|\n||||||148.52|12.89|2487.26|3719.96|3642.34|3535.24|b5162|",
          "author_fullname": "t2_lgebhlu22",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Got some real numbers how llama.cpp got FASTER over last 3-months",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrz5uy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 86,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 86,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751696718,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751681311,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. I am author of Hyprnote(&lt;a href=\"https://github.com/fastrepl/hyprnote\"&gt;https://github.com/fastrepl/hyprnote&lt;/a&gt;) - privacy-first notepad for meetings. We regularly test out the AI models we use in various devices to make sure it runs well.&lt;/p&gt;\n\n&lt;p&gt;When testing MacBook, Qwen3 1.7B is used, and for Windows, Qwen3 0.6B is used. (All Q4 KM)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/tree/b5828\"&gt;b5828&lt;/a&gt;(newer)  .. &lt;a href=\"https://github.com/ggml-org/llama.cpp/tree/b5162\"&gt;b5162&lt;/a&gt;(older)&lt;/p&gt;\n\n&lt;p&gt;Thinking of writing lot longer blog post with lots of numbers &amp;amp; what I learned during the experiment. Please let me know if that is something you guys are interested in.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Device&lt;/th&gt;\n&lt;th align=\"left\"&gt;OS&lt;/th&gt;\n&lt;th align=\"left\"&gt;SoC&lt;/th&gt;\n&lt;th align=\"left\"&gt;RAM&lt;/th&gt;\n&lt;th align=\"left\"&gt;Compute&lt;/th&gt;\n&lt;th align=\"left\"&gt;Prefill Tok/s&lt;/th&gt;\n&lt;th align=\"left\"&gt;Gen Tok/s&lt;/th&gt;\n&lt;th align=\"left\"&gt;Median Load (ms)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Prefill RAM (MB)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Gen RAM (MB)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Load RAM (MB)&lt;/th&gt;\n&lt;th align=\"left\"&gt;SHA&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;MacBook Pro 14-inch&lt;/td&gt;\n&lt;td align=\"left\"&gt;macOS 15.3.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apple M2 Pro&lt;/td&gt;\n&lt;td align=\"left\"&gt;16GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;Metal&lt;/td&gt;\n&lt;td align=\"left\"&gt;615.20&lt;/td&gt;\n&lt;td align=\"left\"&gt;21.69&lt;/td&gt;\n&lt;td align=\"left\"&gt;362.52&lt;/td&gt;\n&lt;td align=\"left\"&gt;2332.28&lt;/td&gt;\n&lt;td align=\"left\"&gt;2337.67&lt;/td&gt;\n&lt;td align=\"left\"&gt;2089.56&lt;/td&gt;\n&lt;td align=\"left\"&gt;b5828&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;571.85&lt;/td&gt;\n&lt;td align=\"left\"&gt;21.43&lt;/td&gt;\n&lt;td align=\"left\"&gt;372.32&lt;/td&gt;\n&lt;td align=\"left\"&gt;2341.77&lt;/td&gt;\n&lt;td align=\"left\"&gt;2347.05&lt;/td&gt;\n&lt;td align=\"left\"&gt;2102.27&lt;/td&gt;\n&lt;td align=\"left\"&gt;b5162&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;HP EliteBook 660 16-inch G11&lt;/td&gt;\n&lt;td align=\"left\"&gt;Windows 11.24H2&lt;/td&gt;\n&lt;td align=\"left\"&gt;Intel Core Ultra 7 155U&lt;/td&gt;\n&lt;td align=\"left\"&gt;32GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;162.52&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.05&lt;/td&gt;\n&lt;td align=\"left\"&gt;1533.99&lt;/td&gt;\n&lt;td align=\"left\"&gt;3719.23&lt;/td&gt;\n&lt;td align=\"left\"&gt;3641.65&lt;/td&gt;\n&lt;td align=\"left\"&gt;3535.43&lt;/td&gt;\n&lt;td align=\"left\"&gt;b5828&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;148.52&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.89&lt;/td&gt;\n&lt;td align=\"left\"&gt;2487.26&lt;/td&gt;\n&lt;td align=\"left\"&gt;3719.96&lt;/td&gt;\n&lt;td align=\"left\"&gt;3642.34&lt;/td&gt;\n&lt;td align=\"left\"&gt;3535.24&lt;/td&gt;\n&lt;td align=\"left\"&gt;b5162&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wUf3Yu2e5X0htYmKODWf6bn_xezHqWptQE969HmhAbI.png?auto=webp&amp;s=17ee6f66f22e0cdee718e7c56430ab72410c37af",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wUf3Yu2e5X0htYmKODWf6bn_xezHqWptQE969HmhAbI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec7da19ac0d910c3d450387c36d5eef2bfd4ab9f",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/wUf3Yu2e5X0htYmKODWf6bn_xezHqWptQE969HmhAbI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d47c3e18c8cc7d921297a211ef8a297d73d4864",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/wUf3Yu2e5X0htYmKODWf6bn_xezHqWptQE969HmhAbI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2eff33c76a6c81497a6908f1357ebcebcdcb4b67",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/wUf3Yu2e5X0htYmKODWf6bn_xezHqWptQE969HmhAbI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3da9c4718cb1c1eb81ac8490a86de0117459de96",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/wUf3Yu2e5X0htYmKODWf6bn_xezHqWptQE969HmhAbI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b947dac2e37c9309baa6a23b33b50d1ae442faf",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/wUf3Yu2e5X0htYmKODWf6bn_xezHqWptQE969HmhAbI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ed50d79e9fed615f1b2e276d3583f302af58f869",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "wUf3Yu2e5X0htYmKODWf6bn_xezHqWptQE969HmhAbI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lrz5uy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AggressiveHunt2300",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrz5uy/got_some_real_numbers_how_llamacpp_got_faster/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrz5uy/got_some_real_numbers_how_llamacpp_got_faster/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751681311,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3coetveu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a platform to collect &amp; solve real-world AI automation use cases – would love your feedback!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lsq6xi",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1751767813,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "aisolutionscamp.io",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://aisolutionscamp.io",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsq6xi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "disappead",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsq6xi/i_built_a_platform_to_collect_solve_realworld_ai/",
          "stickied": false,
          "url": "https://aisolutionscamp.io",
          "subreddit_subscribers": 494897,
          "created_utc": 1751767813,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a ryzen AI 7h CPU (with 50 TOPS NPU) with 64gb DDR5 RAM or an RTX5070 with 8gb DDR7. Should I run inference off of GPU or CPU for better performance?",
          "author_fullname": "t2_wrb36r6lc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Larger model on CPU or small model on GPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lsq2m3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751767413,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a ryzen AI 7h CPU (with 50 TOPS NPU) with 64gb DDR5 RAM or an RTX5070 with 8gb DDR7. Should I run inference off of GPU or CPU for better performance?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsq2m3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Professional_582",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751767413,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just read the [FinLLM technical report](https://aveni.ai/wp-content/uploads/2025/05/Aveni-Detect-Combined-Case-Study.pdf) from Aveni Labs. It’s a 7B parameter language model built specifically for UK financial services, trained with regulatory alignment and fine-tuned for tasks like compliance monitoring, adviser QA, and KYC review.\n\nKey points that stood out:\n\n* Outperforms GPT-4o mini, Gemini 1.5 Flash, and LLaMA-based models on financial domain tasks like tabular data analysis, multi-turn customer dialogue, long-context reasoning, and document QA\n* Built using a filtering pipeline called Finance Classifier 2.0 that selects high-quality, in-domain training data (regulatory guidance, advice transcripts, etc.)\n* Open 1B and 7B variants designed for fine-tuning and secure deployment in VPC or on-prem environments\n* Optimized for agentic RAG setups where traceability and source-grounding are required\n* Benchmarked using their own dataset, AveniBench, which focuses on real FS tasks like consumer vulnerability detection and conduct risk spotting\n\nThey are also working on a 30B version, but the current 7B model is already matching or beating much larger models in this domain.\n\nAnyone else here working on small or mid-scale domain-specific models in regulated industries? Curious how others are handling fine-tuning and evaluation for high-risk applications.",
          "author_fullname": "t2_1sutxb3mam",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Aveni Labs releases FinLLM technical report: a 7B domain-specific model for financial services outperforming some frontier LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls8c2s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751717081,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just read the &lt;a href=\"https://aveni.ai/wp-content/uploads/2025/05/Aveni-Detect-Combined-Case-Study.pdf\"&gt;FinLLM technical report&lt;/a&gt; from Aveni Labs. It’s a 7B parameter language model built specifically for UK financial services, trained with regulatory alignment and fine-tuned for tasks like compliance monitoring, adviser QA, and KYC review.&lt;/p&gt;\n\n&lt;p&gt;Key points that stood out:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Outperforms GPT-4o mini, Gemini 1.5 Flash, and LLaMA-based models on financial domain tasks like tabular data analysis, multi-turn customer dialogue, long-context reasoning, and document QA&lt;/li&gt;\n&lt;li&gt;Built using a filtering pipeline called Finance Classifier 2.0 that selects high-quality, in-domain training data (regulatory guidance, advice transcripts, etc.)&lt;/li&gt;\n&lt;li&gt;Open 1B and 7B variants designed for fine-tuning and secure deployment in VPC or on-prem environments&lt;/li&gt;\n&lt;li&gt;Optimized for agentic RAG setups where traceability and source-grounding are required&lt;/li&gt;\n&lt;li&gt;Benchmarked using their own dataset, AveniBench, which focuses on real FS tasks like consumer vulnerability detection and conduct risk spotting&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;They are also working on a 30B version, but the current 7B model is already matching or beating much larger models in this domain.&lt;/p&gt;\n\n&lt;p&gt;Anyone else here working on small or mid-scale domain-specific models in regulated industries? Curious how others are handling fine-tuning and evaluation for high-risk applications.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1ls8c2s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Cryptographer9361",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls8c2s/aveni_labs_releases_finllm_technical_report_a_7b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls8c2s/aveni_labs_releases_finllm_technical_report_a_7b/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751717081,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm a beginner developer who just completed my first AI project. In past, I almost dedicated to traditional frontend, backend and toolchain development and know a little knowledges about AI. Recently, I'm working for a toolchain project of myself and compositing its documents. An idea suddenly emerges, I could utilize MCP to told AI project's details and make agent help me coding. After communicating with GPT, I decided to adopt the following technology stacks:\n\n* **Backend**: FastAPI + Python\n* **Vector DB**: ChromaDB (with memory fallback)\n* **Embeddings**: Sentence Transformers\n* **LLM**: Local Qwen2.5-7B via Ollama\n* **Architecture**: RAG (Retrieval-Augmented Generation) \n\nBefore vectoring document, I decided to split chunks from every document instead of directly adopting, considering that the model token requirment is limited and documents contains lots markdown and markdown involves lots subtiltle like h2, h3, h4. Approximately spending half hours, I finished this target and successed vectoring documents and chunks. But according to results from test units, outcomes based on similarity pattern looks so bad. Because some keywords don't explicitly present on original text and result in unavaliable information matched. Then I read about multi-round retrieval. The idea: do a broad search first, then refine it. It actually worked better! Not perfect, but definitely an improvement. \n\nWhen tasks were above finished, I start to call local LLMs through ollama. The development of later story is better smoth than data preprocess. With the prompts that match the context of the input information, splice in the input problem, and the large model quickly gives me the answer I want. But the practice of MCP is terrible for me. GPT gives me lots dirty codes which include tedious access chain using any type, invalid function signature and incorrect parameters pass. What's worst, it's no support MCP integration for Cursor IDE I often use. Therefore, AI told me calling function by HTTP is fine compared to MCP. Ultimately, I had to give up call the knowledge base by MCP method.",
          "author_fullname": "t2_bg90uypls",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a RAG-powered knowledge base for docs of my project using FastAPI + Ollama. Here's what I learned.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsox8o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751763680,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a beginner developer who just completed my first AI project. In past, I almost dedicated to traditional frontend, backend and toolchain development and know a little knowledges about AI. Recently, I&amp;#39;m working for a toolchain project of myself and compositing its documents. An idea suddenly emerges, I could utilize MCP to told AI project&amp;#39;s details and make agent help me coding. After communicating with GPT, I decided to adopt the following technology stacks:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: FastAPI + Python&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Vector DB&lt;/strong&gt;: ChromaDB (with memory fallback)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Embeddings&lt;/strong&gt;: Sentence Transformers&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;LLM&lt;/strong&gt;: Local Qwen2.5-7B via Ollama&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;: RAG (Retrieval-Augmented Generation) &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Before vectoring document, I decided to split chunks from every document instead of directly adopting, considering that the model token requirment is limited and documents contains lots markdown and markdown involves lots subtiltle like h2, h3, h4. Approximately spending half hours, I finished this target and successed vectoring documents and chunks. But according to results from test units, outcomes based on similarity pattern looks so bad. Because some keywords don&amp;#39;t explicitly present on original text and result in unavaliable information matched. Then I read about multi-round retrieval. The idea: do a broad search first, then refine it. It actually worked better! Not perfect, but definitely an improvement. &lt;/p&gt;\n\n&lt;p&gt;When tasks were above finished, I start to call local LLMs through ollama. The development of later story is better smoth than data preprocess. With the prompts that match the context of the input information, splice in the input problem, and the large model quickly gives me the answer I want. But the practice of MCP is terrible for me. GPT gives me lots dirty codes which include tedious access chain using any type, invalid function signature and incorrect parameters pass. What&amp;#39;s worst, it&amp;#39;s no support MCP integration for Cursor IDE I often use. Therefore, AI told me calling function by HTTP is fine compared to MCP. Ultimately, I had to give up call the knowledge base by MCP method.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsox8o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ansurfen",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsox8o/i_built_a_ragpowered_knowledge_base_for_docs_of/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsox8o/i_built_a_ragpowered_knowledge_base_for_docs_of/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751763680,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying to configure a workstation that I can do some AI dev work, in particular, RAG qualitative and quantitative analysis. I also need a system that I can use to prep many unstructured documents like pdfs and powerpoints, mostly marketing material for ingestion.\n\nI'm not quite sure as to how robust a system I should be spec'ing out and would like your opinion and comments. I've been using ChatGPT and Claude quite a bit for RAG but for the sake of my clients, I want to conduct all this locally on my on system.\n\nAlso, not sure if I should use Windows 11 with WSL2 or native Ubuntu. I would like to use this system as a business computer as well for regular biz apps, but if Windows 11 with WSL2 will significantly impact performance on my AI work, then maybe I should go with native Ubuntu.\n\nWhat do you think? I don't really want to spend over $22k...\n\nhttps://preview.redd.it/ik0iupjjq3bf1.jpg?width=895&amp;format=pjpg&amp;auto=webp&amp;s=a63883ed5c0546e15227c752bd22f6b3bff1939a\n\n",
          "author_fullname": "t2_1mkh7x2yxn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI desktop configuration recommendations for RAG and LLM training",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 133,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ik0iupjjq3bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 103,
                  "x": 108,
                  "u": "https://preview.redd.it/ik0iupjjq3bf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9df6be116a8aa6722ad339ce3e3742b56cc011d8"
                },
                {
                  "y": 206,
                  "x": 216,
                  "u": "https://preview.redd.it/ik0iupjjq3bf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e64fecaedf5d8560f96f1d9c081d3434b71f4944"
                },
                {
                  "y": 306,
                  "x": 320,
                  "u": "https://preview.redd.it/ik0iupjjq3bf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=860ed423296aad54466987e5105545cd11cf209f"
                },
                {
                  "y": 612,
                  "x": 640,
                  "u": "https://preview.redd.it/ik0iupjjq3bf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c86f88f6c5c78981b1240d8b576880b68f5cfbf9"
                }
              ],
              "s": {
                "y": 856,
                "x": 895,
                "u": "https://preview.redd.it/ik0iupjjq3bf1.jpg?width=895&amp;format=pjpg&amp;auto=webp&amp;s=a63883ed5c0546e15227c752bd22f6b3bff1939a"
              },
              "id": "ik0iupjjq3bf1"
            }
          },
          "name": "t3_1lsh4a8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ICNGm2ETJXIpzJmqXpJCo-cHdVJelyzwCI7CAp8OSxs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751741507,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to configure a workstation that I can do some AI dev work, in particular, RAG qualitative and quantitative analysis. I also need a system that I can use to prep many unstructured documents like pdfs and powerpoints, mostly marketing material for ingestion.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not quite sure as to how robust a system I should be spec&amp;#39;ing out and would like your opinion and comments. I&amp;#39;ve been using ChatGPT and Claude quite a bit for RAG but for the sake of my clients, I want to conduct all this locally on my on system.&lt;/p&gt;\n\n&lt;p&gt;Also, not sure if I should use Windows 11 with WSL2 or native Ubuntu. I would like to use this system as a business computer as well for regular biz apps, but if Windows 11 with WSL2 will significantly impact performance on my AI work, then maybe I should go with native Ubuntu.&lt;/p&gt;\n\n&lt;p&gt;What do you think? I don&amp;#39;t really want to spend over $22k...&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ik0iupjjq3bf1.jpg?width=895&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a63883ed5c0546e15227c752bd22f6b3bff1939a\"&gt;https://preview.redd.it/ik0iupjjq3bf1.jpg?width=895&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a63883ed5c0546e15227c752bd22f6b3bff1939a&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsh4a8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Square-Onion-1825",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsh4a8/ai_desktop_configuration_recommendations_for_rag/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsh4a8/ai_desktop_configuration_recommendations_for_rag/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751741507,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone! 👋\n\nHere is a major update to my Generative AI Project Template : \n⸻\n\n🚀 Highlights\n\t•\tFrontend built with NiceGUI for a robust, clean and interactive UI\n\n\t•\tBackend powered by FastAPI for high-performance API endpoints\n\n\t•\tComplete settings and environment management\n\n\t•\tPre-configured Docker Compose setup for containerization\n\n\t•\tOut-of-the-box CI/CD pipeline (GitHub Actions)\n\t\n      •\tAuto-generated documentation (OpenAPI/Swagger)\n\n\t•\tAnd much more—all wired together for a smooth dev experience!\n\n⸻\n\n🔗 Check it out on GitHub\n\n[Generative AI Project Template](https://github.com/AmineDjeghri/generative-ai-project-template)\n",
          "author_fullname": "t2_rvz5twf1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "(Updated) All‑in‑One Generative AI Template: Frontend, Backend, Docker, Docs &amp; CI/CD + Ollama for local LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsjy83",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751749120,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! 👋&lt;/p&gt;\n\n&lt;p&gt;Here is a major update to my Generative AI Project Template : \n⸻&lt;/p&gt;\n\n&lt;p&gt;🚀 Highlights\n    • Frontend built with NiceGUI for a robust, clean and interactive UI&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;• Backend powered by FastAPI for high-performance API endpoints\n\n• Complete settings and environment management\n\n• Pre-configured Docker Compose setup for containerization\n\n• Out-of-the-box CI/CD pipeline (GitHub Actions)\n\n  •   Auto-generated documentation (OpenAPI/Swagger)\n\n• And much more—all wired together for a smooth dev experience!\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;⸻&lt;/p&gt;\n\n&lt;p&gt;🔗 Check it out on GitHub&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/AmineDjeghri/generative-ai-project-template\"&gt;Generative AI Project Template&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Y56C7zAB3QPMW6uvJvAl6DX3xNJEv6Tcypn7-Km01Kc.png?auto=webp&amp;s=6f1c7352c9d5eb8c258dea3b3c56f46c02d36944",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Y56C7zAB3QPMW6uvJvAl6DX3xNJEv6Tcypn7-Km01Kc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed7bbfda0d4b9d48fc23ba78743f2000e5fa0193",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Y56C7zAB3QPMW6uvJvAl6DX3xNJEv6Tcypn7-Km01Kc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a1e7db4c8c6064aeb668d394bbeb4b737d6fe304",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Y56C7zAB3QPMW6uvJvAl6DX3xNJEv6Tcypn7-Km01Kc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=704ca7426127c0eed9ae899b5266f792fdbd8e8e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Y56C7zAB3QPMW6uvJvAl6DX3xNJEv6Tcypn7-Km01Kc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=75b5dbb7a45837e1ec3a478454adf241e1a5f59c",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Y56C7zAB3QPMW6uvJvAl6DX3xNJEv6Tcypn7-Km01Kc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f7ed9245e5c4c45ae2da0b05a5bd9212c992b6f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Y56C7zAB3QPMW6uvJvAl6DX3xNJEv6Tcypn7-Km01Kc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a60be7cc9646ea19534f7a6d090c808aa1e72232",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Y56C7zAB3QPMW6uvJvAl6DX3xNJEv6Tcypn7-Km01Kc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lsjy83",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aminedjeghri",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsjy83/updated_allinone_generative_ai_template_frontend/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsjy83/updated_allinone_generative_ai_template_frontend/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751749120,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Up to 8x B60 pro, 24GB VRAM 456 GB/s apiece. Price point unknown",
          "author_fullname": "t2_4ty73",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Intel Project Battlematrix",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lso57g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1751761200,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "intel.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Up to 8x B60 pro, 24GB VRAM 456 GB/s apiece. Price point unknown&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.intel.com/content/www/us/en/developer/articles/technical/introduction-project-battlematrix.html",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lso57g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "evil0sheep",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lso57g/intel_project_battlematrix/",
          "stickied": false,
          "url": "https://www.intel.com/content/www/us/en/developer/articles/technical/introduction-project-battlematrix.html",
          "subreddit_subscribers": 494897,
          "created_utc": 1751761200,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I apologize if this is the Nth time something like this was posted, but I am just at my wit's end. As the title says, I need help setting up an uncensored local LLM for the purpose of running / DMing a single player text-based RPG adventure. I have tried online services like Kobold AI Lite, etc. but I always encounter issues with them (AI deciding my actions on my behalf even after numerous corrections, AI forgetting important details just after they occurred, etc.), perhaps due to my lack of knowledge and experience in this field.\n\nTo preface, I'm basically a boomer when it comes to AI related things. This all started when I tried a mobile app called Everweave and I was hooked immediately. Unfortunately, the monthly limit and monetization scheme is not something I am inclined to participate in. After trying online services and finding them unsatisfactory (see reasons above), I instead decided to try hosting an LLM that does the same, locally. I tried to search online and watch videos, but there is only so much I can \"learn\" if I couldn't even understand the terminologies being used.  I really did try to take this on by myself and be independent but my brain just could not absorb this new paradigm.\n\nSo far what I had done is download LM Studio and search for LLMs that would fit my intended purpose and that works with the limitations of my machine (R7 4700G 3.6 GHz, 24 GB RAM, RX 6600 8 GB VRAM). Chat GPT suggested I use Mythomist 7b and Mythomax L2 13b, so I tried both. I also wrote a long, detailed system prompt to tell it exactly what I want it to do, but the issues tend to persist.\n\nSo my question is, can anyone who has done the same and found it without any issues, tell me exactly what I should do? Explain it to me like I'm 5, because with all these new emerging fields I'm pretty much a child.\n\nThank you!",
          "author_fullname": "t2_ro3h94zr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help setting up an uncensored local LLM for a text-based RPG adventure / DMing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsfpi0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751737829,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I apologize if this is the Nth time something like this was posted, but I am just at my wit&amp;#39;s end. As the title says, I need help setting up an uncensored local LLM for the purpose of running / DMing a single player text-based RPG adventure. I have tried online services like Kobold AI Lite, etc. but I always encounter issues with them (AI deciding my actions on my behalf even after numerous corrections, AI forgetting important details just after they occurred, etc.), perhaps due to my lack of knowledge and experience in this field.&lt;/p&gt;\n\n&lt;p&gt;To preface, I&amp;#39;m basically a boomer when it comes to AI related things. This all started when I tried a mobile app called Everweave and I was hooked immediately. Unfortunately, the monthly limit and monetization scheme is not something I am inclined to participate in. After trying online services and finding them unsatisfactory (see reasons above), I instead decided to try hosting an LLM that does the same, locally. I tried to search online and watch videos, but there is only so much I can &amp;quot;learn&amp;quot; if I couldn&amp;#39;t even understand the terminologies being used.  I really did try to take this on by myself and be independent but my brain just could not absorb this new paradigm.&lt;/p&gt;\n\n&lt;p&gt;So far what I had done is download LM Studio and search for LLMs that would fit my intended purpose and that works with the limitations of my machine (R7 4700G 3.6 GHz, 24 GB RAM, RX 6600 8 GB VRAM). Chat GPT suggested I use Mythomist 7b and Mythomax L2 13b, so I tried both. I also wrote a long, detailed system prompt to tell it exactly what I want it to do, but the issues tend to persist.&lt;/p&gt;\n\n&lt;p&gt;So my question is, can anyone who has done the same and found it without any issues, tell me exactly what I should do? Explain it to me like I&amp;#39;m 5, because with all these new emerging fields I&amp;#39;m pretty much a child.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsfpi0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tac7878",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsfpi0/help_setting_up_an_uncensored_local_llm_for_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsfpi0/help_setting_up_an_uncensored_local_llm_for_a/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751737829,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am planning to build a pc for LLM Research not very big models but at least 3-7b model training and inference on 13-30b models. \n\nI am planning to build a 5070ti 16gb and probably add another 5070ti after a month. \n\n\nAny suggestions around the RAM, do i really need a top notch cpu ?? ",
          "author_fullname": "t2_9lieougz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "PC build for LLM research",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsiffa",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751744998,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am planning to build a pc for LLM Research not very big models but at least 3-7b model training and inference on 13-30b models. &lt;/p&gt;\n\n&lt;p&gt;I am planning to build a 5070ti 16gb and probably add another 5070ti after a month. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions around the RAM, do i really need a top notch cpu ?? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsiffa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Financial_Web530",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsiffa/pc_build_for_llm_research/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsiffa/pc_build_for_llm_research/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751744998,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "About to pull the trigger on this one I can't believe how cheap it is. ",
          "author_fullname": "t2_p9oua",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Great price on a 5090",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lri12r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 553,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 553,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "https://b.thumbs.redditmedia.com/cwYO-UP68Tz8WUEPVuh4c_NrTX_wV92NhJNuhyDisPM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751633609,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;About to pull the trigger on this one I can&amp;#39;t believe how cheap it is. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/1en1lic1uuaf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/1en1lic1uuaf1.jpeg?auto=webp&amp;s=963851520a2b3619b0143cf0a76376bba48f2ab2",
                  "width": 1080,
                  "height": 1281
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/1en1lic1uuaf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f0892aac54334c15c1614bc8d67b8f98944cf56b",
                    "width": 108,
                    "height": 128
                  },
                  {
                    "url": "https://preview.redd.it/1en1lic1uuaf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d86f0c95b86af0a79c1f9d116d4a0a5e0135d639",
                    "width": 216,
                    "height": 256
                  },
                  {
                    "url": "https://preview.redd.it/1en1lic1uuaf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1a66f4e2ea3b819884d70c7a516efe360765ec91",
                    "width": 320,
                    "height": 379
                  },
                  {
                    "url": "https://preview.redd.it/1en1lic1uuaf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7df49758108c1feb283e4286654e01dbd232a219",
                    "width": 640,
                    "height": 759
                  },
                  {
                    "url": "https://preview.redd.it/1en1lic1uuaf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=16b2f9df579521d947f35cdfa2860538990f3af9",
                    "width": 960,
                    "height": 1138
                  },
                  {
                    "url": "https://preview.redd.it/1en1lic1uuaf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=748f059c23bb57e735b69e2c97fea559109df4de",
                    "width": 1080,
                    "height": 1281
                  }
                ],
                "variants": {},
                "id": "1HrB-tINYXxp73JOlsfUGwhn301V7cOf0KBJiJsdefc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lri12r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "psdwizzard",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lri12r/great_price_on_a_5090/",
          "stickied": false,
          "url": "https://i.redd.it/1en1lic1uuaf1.jpeg",
          "subreddit_subscribers": 494897,
          "created_utc": 1751633609,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "From the HF repo:\n\n\"OCRFlux is a multimodal large language model based toolkit for converting PDFs and images into clean, readable, plain Markdown text. It aims to push the current state-of-the-art to a significantly higher level.\"\n\nClaims to beat other models like olmOCR and Nanonets-OCR-s by a substantial margin.\nRead online that it can also merge content spanning multiple pages such as long tables. There's also a docker container with the full toolkit and a github repo. What are your thoughts on this?",
          "author_fullname": "t2_ckear9l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OCRFlux-3B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrsf6x",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 134,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 134,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/x9gxRnW-oFgiJds7kCEygtLLuK_ZzX-0pJcvDDyr2xk.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=20825673dd2b3d549727ad4016d1a7bc7e357ce6",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751660480,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From the HF repo:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;OCRFlux is a multimodal large language model based toolkit for converting PDFs and images into clean, readable, plain Markdown text. It aims to push the current state-of-the-art to a significantly higher level.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Claims to beat other models like olmOCR and Nanonets-OCR-s by a substantial margin.\nRead online that it can also merge content spanning multiple pages such as long tables. There&amp;#39;s also a docker container with the full toolkit and a github repo. What are your thoughts on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/ChatDOC/OCRFlux-3B",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/x9gxRnW-oFgiJds7kCEygtLLuK_ZzX-0pJcvDDyr2xk.png?auto=webp&amp;s=afb14939b4aa3f51a91e57f99a792335507fe4d1",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/x9gxRnW-oFgiJds7kCEygtLLuK_ZzX-0pJcvDDyr2xk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=14e38f0f603f7da8b5de8711620b4650bf1d4210",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/x9gxRnW-oFgiJds7kCEygtLLuK_ZzX-0pJcvDDyr2xk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ae495b95698f88324c2de5bee59f634ce1f520a",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/x9gxRnW-oFgiJds7kCEygtLLuK_ZzX-0pJcvDDyr2xk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7feed7bbbd225ed5c6548fef6d4e4e80c88cfc2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/x9gxRnW-oFgiJds7kCEygtLLuK_ZzX-0pJcvDDyr2xk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5dcd5f194d87db7612ad57b0c05d46dc6f0cfae3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/x9gxRnW-oFgiJds7kCEygtLLuK_ZzX-0pJcvDDyr2xk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=732395c8a70b5a91dddd0d853d90eeecc4237725",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/x9gxRnW-oFgiJds7kCEygtLLuK_ZzX-0pJcvDDyr2xk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2b0d6dc48e73c54a93bbcfea33cfa0364bd60840",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "x9gxRnW-oFgiJds7kCEygtLLuK_ZzX-0pJcvDDyr2xk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lrsf6x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "k-en",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrsf6x/ocrflux3b/",
          "stickied": false,
          "url": "https://huggingface.co/ChatDOC/OCRFlux-3B",
          "subreddit_subscribers": 494897,
          "created_utc": 1751660480,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is it beneficial for LLM inference? I have MSI Afterburner, wondering if there's any settings that would be beneficial for my 3060 ¯\\\\\\_(ツ)\\_/¯ It's not something I've seen discussed, so I'm *assuming* not, just figured I'd ask. Thanks!",
          "author_fullname": "t2_d3dy8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPU overclocking?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsm1yb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751754951,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it beneficial for LLM inference? I have MSI Afterburner, wondering if there&amp;#39;s any settings that would be beneficial for my 3060 ¯\\_(ツ)_/¯ It&amp;#39;s not something I&amp;#39;ve seen discussed, so I&amp;#39;m &lt;em&gt;assuming&lt;/em&gt; not, just figured I&amp;#39;d ask. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsm1yb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "wpg4665",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsm1yb/gpu_overclocking/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsm1yb/gpu_overclocking/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751754951,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking forward to the GGUF quants to give it a shot. Would love if the awesome Unsloth team did their magic here, too.\n\n  \n[https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking](https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking)",
          "author_fullname": "t2_jip8rft4d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "THUDM/GLM-4.1V-9B-Thinking looks impressive",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 60,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrss4u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 117,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 117,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/idh4Gmb5EiyF75my7tifZeUzHzbypwJivniL4vcgyZw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751661469,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking forward to the GGUF quants to give it a shot. Would love if the awesome Unsloth team did their magic here, too.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking\"&gt;https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/62vkwepq4xaf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/62vkwepq4xaf1.jpeg?auto=webp&amp;s=47b4a0ac5b3ff7c55707c305f18aa5050819fad3",
                  "width": 1880,
                  "height": 817
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/62vkwepq4xaf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e48334db0b608a0149e13ef26625c23ab6d950f5",
                    "width": 108,
                    "height": 46
                  },
                  {
                    "url": "https://preview.redd.it/62vkwepq4xaf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7a0adffe54dca86e2f243f886bd70bfb4adeec7e",
                    "width": 216,
                    "height": 93
                  },
                  {
                    "url": "https://preview.redd.it/62vkwepq4xaf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a3cf3c7b47dec2fa728dffaaca1e20fbe7fd48b",
                    "width": 320,
                    "height": 139
                  },
                  {
                    "url": "https://preview.redd.it/62vkwepq4xaf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ded30c7e3562f37aa41502883d7aa8b656c68551",
                    "width": 640,
                    "height": 278
                  },
                  {
                    "url": "https://preview.redd.it/62vkwepq4xaf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4bba18cd703124b505c4f466cea3f48dab329314",
                    "width": 960,
                    "height": 417
                  },
                  {
                    "url": "https://preview.redd.it/62vkwepq4xaf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f949903dcbbd4917c219dad41bf2c369c7c664bf",
                    "width": 1080,
                    "height": 469
                  }
                ],
                "variants": {},
                "id": "xwCchr8Lt12drvPmOfSw-f2wfP9Iz1_9fFJuW2C6gvw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lrss4u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ConfidentTrifle7247",
          "discussion_type": null,
          "num_comments": 39,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/",
          "stickied": false,
          "url": "https://i.redd.it/62vkwepq4xaf1.jpeg",
          "subreddit_subscribers": 494897,
          "created_utc": 1751661469,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have an AMD build acting as a home server. Ryzen 5600G, 32GB RAM. I want a card with all the VRAM I can get, but I don't want to spend a lot. What are my options? I'm pretty new to all this. \n\nI see that MI50 cards are going for relatively cheap. Is that still a good option? 32GB is probably more than enough. I do NOT need video output at all. I have a 5600G, and this server is headless anyway. I guess my questions are: \n\n* What's the best way to get at least 32GB of VRAM for not Nvidia prices? I know not to just buy a gaming card, but I'm not sure what to look for and I've never bought from somewhere like Ali Express. \n* If I find a great deal, should I get two cards to double my VRAM? Cards don't really have LSI-like crossover anymore, so I feel like this would bottleneck me. \n* How much should I expect to spend per card? Again, I don't need video out. I'm fine with a data center card with no ports. \n* Is my 5600G good enough? All the work should happen on the GPU, so I'd guess I'm fine here. I'm aware I should get more system memory. \n\nThanks.",
          "author_fullname": "t2_10xj3s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Options for a lot of VRAM for local Ollama server?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lslglw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751753263,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an AMD build acting as a home server. Ryzen 5600G, 32GB RAM. I want a card with all the VRAM I can get, but I don&amp;#39;t want to spend a lot. What are my options? I&amp;#39;m pretty new to all this. &lt;/p&gt;\n\n&lt;p&gt;I see that MI50 cards are going for relatively cheap. Is that still a good option? 32GB is probably more than enough. I do NOT need video output at all. I have a 5600G, and this server is headless anyway. I guess my questions are: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What&amp;#39;s the best way to get at least 32GB of VRAM for not Nvidia prices? I know not to just buy a gaming card, but I&amp;#39;m not sure what to look for and I&amp;#39;ve never bought from somewhere like Ali Express. &lt;/li&gt;\n&lt;li&gt;If I find a great deal, should I get two cards to double my VRAM? Cards don&amp;#39;t really have LSI-like crossover anymore, so I feel like this would bottleneck me. &lt;/li&gt;\n&lt;li&gt;How much should I expect to spend per card? Again, I don&amp;#39;t need video out. I&amp;#39;m fine with a data center card with no ports. &lt;/li&gt;\n&lt;li&gt;Is my 5600G good enough? All the work should happen on the GPU, so I&amp;#39;d guess I&amp;#39;m fine here. I&amp;#39;m aware I should get more system memory. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lslglw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mehgcap",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lslglw/options_for_a_lot_of_vram_for_local_ollama_server/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lslglw/options_for_a_lot_of_vram_for_local_ollama_server/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751753263,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone, \n\nRecently got myself a brand new M4 Max 128Gb ram Mac Studio. \n\nI saw some old posts about the best models to use with this computer, but I am wondering if that has changed throughout the months/years. \n\nCurrently, what is the best model and settings to use with this machine? \n\nCheers! ",
          "author_fullname": "t2_151bnd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best model at the moment for 128GB M4 Max",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrzrmd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751683492,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;\n\n&lt;p&gt;Recently got myself a brand new M4 Max 128Gb ram Mac Studio. &lt;/p&gt;\n\n&lt;p&gt;I saw some old posts about the best models to use with this computer, but I am wondering if that has changed throughout the months/years. &lt;/p&gt;\n\n&lt;p&gt;Currently, what is the best model and settings to use with this machine? &lt;/p&gt;\n\n&lt;p&gt;Cheers! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lrzrmd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xx_DarDoAzuL_xX",
          "discussion_type": null,
          "num_comments": 39,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrzrmd/best_model_at_the_moment_for_128gb_m4_max/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrzrmd/best_model_at_the_moment_for_128gb_m4_max/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751683492,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Inspired by the awesome work presented by Kathleen Kenealy on ViT benchmarks in PyTorch DDP and Jax TPUs by Google DeepMind, I developed this intensive article on the solid foundations to transformers, Vision Transformers, and Distributed Learning,  and to say I learnt a lot would be an understatement. After a few revisions (extending and including Jax sharded parallelism), I will transform it into a book. The article starts off with the interesting reference to Dr Mihai Nica’s interesting “A random variable is not random, and it’s not a variable\", kicking off the article’s explorations of human language transformation to machine readable computationally crunchable tokens and embeddings, using rich animations to then redirect us to building Llama2 from the core, basing it as the ‘equilibrium in the model space map’, a phrase meaning a solid understanding of Llama2 architecture could easily be mapped to any SOTA LLM variant with few iterations. I spin a fast inference as I document Modal’s awesome magic gpu pipelining without ssh. I then show the major transformations from Llama2 to ViT, coauthored by the infamous Lucas Beyer &amp; co. I then narrow to the four variants of ViTs benchmarked by DeepMind where I explore the architectures by further referencing the paper “Scaling ViTs”. The final section then explores parallelism, starting from Open-MPI in C, building programs in peer-to-peer and collective communications before then finally building data parallelism in DDP and exploring helix editor, tmux, ssh tunneling on RunPod to run distributed training. I then ultimately explore Fully Sharded Data Parallel and the transformations to the training pipeline!\n\nThe Article:https://drive.google.com/file/d/1CPwbWaJ\\_NiBZJ6NbHDlPBFYe9hf36Y0q/view?usp=sharing\n\nI built this article, standing on the shoulders of giants, people who never stopped building and enjoying open-source, and I appreciate the much you share on X, r/LocalLLaMA, and GPU MODE, led by Mark Saroufim &amp; co on YouTube! Your expertise has motivated me to learn a whole lot more by being curious!\n\nIf you feel I can thrive well in your collaborative team, working towards impactful research, I am currently open to work starting this Fall, open to relocation, open to internships with return offers available. Currently based in Massachusetts. Please do reach out, and please share with your networks, I really do appreciate!",
          "author_fullname": "t2_yy5sq4dor",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "From The Foundations of Transformers to Scaling Vision Transformers",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsjm9l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751754061,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751748220,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Inspired by the awesome work presented by Kathleen Kenealy on ViT benchmarks in PyTorch DDP and Jax TPUs by Google DeepMind, I developed this intensive article on the solid foundations to transformers, Vision Transformers, and Distributed Learning,  and to say I learnt a lot would be an understatement. After a few revisions (extending and including Jax sharded parallelism), I will transform it into a book. The article starts off with the interesting reference to Dr Mihai Nica’s interesting “A random variable is not random, and it’s not a variable&amp;quot;, kicking off the article’s explorations of human language transformation to machine readable computationally crunchable tokens and embeddings, using rich animations to then redirect us to building Llama2 from the core, basing it as the ‘equilibrium in the model space map’, a phrase meaning a solid understanding of Llama2 architecture could easily be mapped to any SOTA LLM variant with few iterations. I spin a fast inference as I document Modal’s awesome magic gpu pipelining without ssh. I then show the major transformations from Llama2 to ViT, coauthored by the infamous Lucas Beyer &amp;amp; co. I then narrow to the four variants of ViTs benchmarked by DeepMind where I explore the architectures by further referencing the paper “Scaling ViTs”. The final section then explores parallelism, starting from Open-MPI in C, building programs in peer-to-peer and collective communications before then finally building data parallelism in DDP and exploring helix editor, tmux, ssh tunneling on RunPod to run distributed training. I then ultimately explore Fully Sharded Data Parallel and the transformations to the training pipeline!&lt;/p&gt;\n\n&lt;p&gt;The Article:&lt;a href=\"https://drive.google.com/file/d/1CPwbWaJ%5C_NiBZJ6NbHDlPBFYe9hf36Y0q/view?usp=sharing\"&gt;https://drive.google.com/file/d/1CPwbWaJ\\_NiBZJ6NbHDlPBFYe9hf36Y0q/view?usp=sharing&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I built this article, standing on the shoulders of giants, people who never stopped building and enjoying open-source, and I appreciate the much you share on X, &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;, and GPU MODE, led by Mark Saroufim &amp;amp; co on YouTube! Your expertise has motivated me to learn a whole lot more by being curious!&lt;/p&gt;\n\n&lt;p&gt;If you feel I can thrive well in your collaborative team, working towards impactful research, I am currently open to work starting this Fall, open to relocation, open to internships with return offers available. Currently based in Massachusetts. Please do reach out, and please share with your networks, I really do appreciate!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lsjm9l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Des_goes_Brrr",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsjm9l/from_the_foundations_of_transformers_to_scaling/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsjm9l/from_the_foundations_of_transformers_to_scaling/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751748220,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I would like to make a \"clown-car\" MoE as described by Goddard in https://goddard.blog/posts/clown-moe/ but after initializing the gates as he describes, I would like to perform continued pre-training on *just* the gates, not any of the expert weights.\n\nDo any of the easy-to-use training frameworks like Unsloth support this, or am I having to write some code?",
          "author_fullname": "t2_cpegz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there an easy way to continue pretraining of *just* the gate network of an MoE?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsjc83",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751749178,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751747466,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to make a &amp;quot;clown-car&amp;quot; MoE as described by Goddard in &lt;a href=\"https://goddard.blog/posts/clown-moe/\"&gt;https://goddard.blog/posts/clown-moe/&lt;/a&gt; but after initializing the gates as he describes, I would like to perform continued pre-training on &lt;em&gt;just&lt;/em&gt; the gates, not any of the expert weights.&lt;/p&gt;\n\n&lt;p&gt;Do any of the easy-to-use training frameworks like Unsloth support this, or am I having to write some code?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsjc83",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ttkciar",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lsjc83/is_there_an_easy_way_to_continue_pretraining_of/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsjc83/is_there_an_easy_way_to_continue_pretraining_of/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751747466,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a desktop on my LAN that I'm using for inference. I start ./llama-server on that desktop, and then submit queries using curl. However, when I submit queries using the \"prompt\" field, I get replies back that look like foundation model completions, rather than instruct completions. I assume this is because something is going wrong with the template, so my question is really about how to properly set up the template with llama-server. I know this is a basic question but I haven't been able to find a working recipe... any help/insights/guidance/links appreciated...\n\nHere are my commands:\n\n    # On the host:\n    % ./llama-server --jinja -t 30 -m $MODELS/Qwen3-8B-Q4_K_M.gguf --host $HOST_IP --port 11434 --prio 3 --n-gpu-layers 20 --no-webui\n\n    # On the client:\n    % curl --request POST --url http://$HOST_IP:11434/completion --header \"Content-Type: application/json\" --data '{\"prompt\": \"What is the capital of Italy?\", \"n_predict\": 100}'  | jq -r '.content'\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100  2082  100  2021  100    61    226      6  0:00:10  0:00:08  0:00:02   429\n     How many states are there in the United States? What is the largest planet in our solar system? What is the chemical symbol for water? What is the square root of 64? What is the main function of the liver in the human body? What is the most common language spoken in Brazil? What is the smallest prime number? What is the formula for calculating the area of a circle? What is the capital of France? What is the process by which plants make their own food using sunlight",
          "author_fullname": "t2_61b8b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama server completion not working correctly",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lshxep",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751743641,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a desktop on my LAN that I&amp;#39;m using for inference. I start ./llama-server on that desktop, and then submit queries using curl. However, when I submit queries using the &amp;quot;prompt&amp;quot; field, I get replies back that look like foundation model completions, rather than instruct completions. I assume this is because something is going wrong with the template, so my question is really about how to properly set up the template with llama-server. I know this is a basic question but I haven&amp;#39;t been able to find a working recipe... any help/insights/guidance/links appreciated...&lt;/p&gt;\n\n&lt;p&gt;Here are my commands:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# On the host:\n% ./llama-server --jinja -t 30 -m $MODELS/Qwen3-8B-Q4_K_M.gguf --host $HOST_IP --port 11434 --prio 3 --n-gpu-layers 20 --no-webui\n\n# On the client:\n% curl --request POST --url http://$HOST_IP:11434/completion --header &amp;quot;Content-Type: application/json&amp;quot; --data &amp;#39;{&amp;quot;prompt&amp;quot;: &amp;quot;What is the capital of Italy?&amp;quot;, &amp;quot;n_predict&amp;quot;: 100}&amp;#39;  | jq -r &amp;#39;.content&amp;#39;\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  2082  100  2021  100    61    226      6  0:00:10  0:00:08  0:00:02   429\n How many states are there in the United States? What is the largest planet in our solar system? What is the chemical symbol for water? What is the square root of 64? What is the main function of the liver in the human body? What is the most common language spoken in Brazil? What is the smallest prime number? What is the formula for calculating the area of a circle? What is the capital of France? What is the process by which plants make their own food using sunlight\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lshxep",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "claytonkb",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lshxep/llama_server_completion_not_working_correctly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lshxep/llama_server_completion_not_working_correctly/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751743641,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I know this is locallama but what is the SoTA speech to speech model right now? We've been testing with gemini 2.5 audio native preview at work and while it still has some issues, it's looking real good. Ive been limited to Gemini cause we got free GCP credits to play with at work.",
          "author_fullname": "t2_raq6z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SoTA Audio native models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsfv8c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751738248,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this is locallama but what is the SoTA speech to speech model right now? We&amp;#39;ve been testing with gemini 2.5 audio native preview at work and while it still has some issues, it&amp;#39;s looking real good. Ive been limited to Gemini cause we got free GCP credits to play with at work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsfv8c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Theboyscampus",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsfv8c/sota_audio_native_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsfv8c/sota_audio_native_models/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751738248,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m looking to build a budget experimentation machine for inference and perhaps training some multimodal models and such. I saw that there are lots of refurbished K80s available on eBay for quite cheap that appear to be in ok condition. I’m wondering what kind of backbone I would need to support say 4 or even 8x of them. Has anyone heard of similar builds?",
          "author_fullname": "t2_4jwyuisr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What motherboard for 4xK80s?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsfj67",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751737358,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m looking to build a budget experimentation machine for inference and perhaps training some multimodal models and such. I saw that there are lots of refurbished K80s available on eBay for quite cheap that appear to be in ok condition. I’m wondering what kind of backbone I would need to support say 4 or even 8x of them. Has anyone heard of similar builds?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsfj67",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "itsacommon",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsfj67/what_motherboard_for_4xk80s/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsfj67/what_motherboard_for_4xk80s/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751737358,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Update from 5 july 2025:  \nI've resolved this issue with ollama for AMD and replacing ROCm libraries.  \n  \nHello!  \nI'm wandering if it possible to use iGPU for inference in Windows despite the dGPU is online and connected to the Display.  \nThe whole idea that I can use idling iGPU for the AI tasks (small 7b models).  \nThe MUX switch itself is not limiting the iGPU for the general tasks (not related to the video rendering, right?).  \nI've a modern laptop with a ryzen 7840hs and MUX switch for the dGPU - RTX4060.  \nI know, that I can do opposite - run a display on the iGPU and use dGPU for the AI inference.\n\nHow to:\n\n* Download [https://github.com/likelovewant/ollama-for-amd](https://github.com/likelovewant/ollama-for-amd)\n* Download modified rocm libs for 780m (gfx1103): [https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU](https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU)\n* Replace rocm libs in the ollama (follow instructions on the ollama-for-amd project)\n* Enjoy!\n\n`total duration: 1m1.7299746s`  \n`load duration: 28.6558ms`  \n`prompt eval count: 15 token(s)`  \n`prompt eval duration: 169.7987ms`  \n`prompt eval rate: 88.34 tokens/s`  \n`eval count: 583 token(s)`  \n`eval duration: 1m1.5301253s`  \n`eval rate: 9.48 tokens/s`\n\nhttps://preview.redd.it/2ijxnzigy3bf1.png?width=1125&amp;format=png&amp;auto=webp&amp;s=e5322a6e67ef4ef7f6c5f6cfacf54b0eb76e30c0",
          "author_fullname": "t2_82tuxz2q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Utilize iGPU (AMD Radeon 780m) even if the dGPU is running via MUX switch",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "2ijxnzigy3bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 136,
                  "x": 108,
                  "u": "https://preview.redd.it/2ijxnzigy3bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=db8fcdefae698047e7c43fcc9cf0e3d460378013"
                },
                {
                  "y": 272,
                  "x": 216,
                  "u": "https://preview.redd.it/2ijxnzigy3bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=000b44c9ef03747383d0662cd10b7bb90f292a29"
                },
                {
                  "y": 404,
                  "x": 320,
                  "u": "https://preview.redd.it/2ijxnzigy3bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bfde51021f455a19bcca98a6e3da497cbcd881ec"
                },
                {
                  "y": 808,
                  "x": 640,
                  "u": "https://preview.redd.it/2ijxnzigy3bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=26b0bd685ae4fd3a78f1bec4824e9b91e7d58869"
                },
                {
                  "y": 1212,
                  "x": 960,
                  "u": "https://preview.redd.it/2ijxnzigy3bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ca075af8a1307dbbaab832d19afb2872e7d24a78"
                },
                {
                  "y": 1364,
                  "x": 1080,
                  "u": "https://preview.redd.it/2ijxnzigy3bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4d10ae0766d23448af456f26ebcf29c540230bb1"
                }
              ],
              "s": {
                "y": 1421,
                "x": 1125,
                "u": "https://preview.redd.it/2ijxnzigy3bf1.png?width=1125&amp;format=png&amp;auto=webp&amp;s=e5322a6e67ef4ef7f6c5f6cfacf54b0eb76e30c0"
              },
              "id": "2ijxnzigy3bf1"
            }
          },
          "name": "t3_1lsaczg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=25025ca1608212effce373a7407b8ef848304be7",
          "edited": 1751744505,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751723484,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Update from 5 july 2025:&lt;br/&gt;\nI&amp;#39;ve resolved this issue with ollama for AMD and replacing ROCm libraries.  &lt;/p&gt;\n\n&lt;p&gt;Hello!&lt;br/&gt;\nI&amp;#39;m wandering if it possible to use iGPU for inference in Windows despite the dGPU is online and connected to the Display.&lt;br/&gt;\nThe whole idea that I can use idling iGPU for the AI tasks (small 7b models).&lt;br/&gt;\nThe MUX switch itself is not limiting the iGPU for the general tasks (not related to the video rendering, right?).&lt;br/&gt;\nI&amp;#39;ve a modern laptop with a ryzen 7840hs and MUX switch for the dGPU - RTX4060.&lt;br/&gt;\nI know, that I can do opposite - run a display on the iGPU and use dGPU for the AI inference.&lt;/p&gt;\n\n&lt;p&gt;How to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Download &lt;a href=\"https://github.com/likelovewant/ollama-for-amd\"&gt;https://github.com/likelovewant/ollama-for-amd&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Download modified rocm libs for 780m (gfx1103): &lt;a href=\"https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU\"&gt;https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Replace rocm libs in the ollama (follow instructions on the ollama-for-amd project)&lt;/li&gt;\n&lt;li&gt;Enjoy!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;code&gt;total duration: 1m1.7299746s&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;load duration: 28.6558ms&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;prompt eval count: 15 token(s)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;prompt eval duration: 169.7987ms&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;prompt eval rate: 88.34 tokens/s&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;eval count: 583 token(s)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;eval duration: 1m1.5301253s&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;eval rate: 9.48 tokens/s&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2ijxnzigy3bf1.png?width=1125&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e5322a6e67ef4ef7f6c5f6cfacf54b0eb76e30c0\"&gt;https://preview.redd.it/2ijxnzigy3bf1.png?width=1125&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e5322a6e67ef4ef7f6c5f6cfacf54b0eb76e30c0&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc.png?auto=webp&amp;s=c7ceaa88750addb21e920c5e11469eb8aed66305",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5960d476ea4194f61592e054c2304a93e90b6cf",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1fc656388c7bbc7b08b8e4e6ead8cf2913340dce",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ef8e46ee8bae87a4a35328d16c351d686a3df088",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=62d44de2eb2d27863a6c83b4cc777b0791accde7",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0df28c9f3cd3c033dadb5940a59cf4592aafb6ee",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18dbbdd4cd0ca14cd965e3eedb8e130806093f7f",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsaczg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "panther_ra",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751723484,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What's the current state of multi GPU use in local UIs? For example, GPUs such as 2x RX570/580/GTX1060, GTX1650, etc... I ask for future reference of the possibility of having twice VRam amount or an increase since some of these can still be found for half the price of a RTX.\n\nIn case it's possible, pairing AMD GPU with Nvidia one is a bad idea? And if pairing a ~8gb Nvidia with an RTX to hit nearly 20gb or more?",
          "author_fullname": "t2_eljq22kg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Multi GPUs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls7vmb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751715462,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the current state of multi GPU use in local UIs? For example, GPUs such as 2x RX570/580/GTX1060, GTX1650, etc... I ask for future reference of the possibility of having twice VRam amount or an increase since some of these can still be found for half the price of a RTX.&lt;/p&gt;\n\n&lt;p&gt;In case it&amp;#39;s possible, pairing AMD GPU with Nvidia one is a bad idea? And if pairing a ~8gb Nvidia with an RTX to hit nearly 20gb or more?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ls7vmb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WEREWOLF_BX13",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751715462,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "over the last couple of years we have seen LLMs become super duper popular and some of them are small enough to run on consumer level hardware, but in most cases we are talking about pre-trained models that can be used only in inference mode without considering the full training phase. Something that i was cuorious about tho is what kind of performance i could get if i did everything, including the full training without using other tools like lora or quantization, on my own everyday machine so i made a script that does exactly that, the script contains also a file (config.py) that can be used to tune the hyperparameters of the architecture so that anyone running it can easily set them to have the largest model as possible with their hardware (in my case with the model in the script and with a 12gb 3060 i can train about 50M params, 300M with smaller batch and mixed precision)\nhere is the repo https://github.com/samas69420/transformino , to run the code the only thing you'll need is a dataset in the form of a csv file with a column containing the text that will be used for training (tweets, sentences from a book etc), the project also have a very low number of dependencies to make it more easy to run (you'll need only pytorch, pandas and tokenizers), every kind of feedback would be appreciated ",
          "author_fullname": "t2_hto9ceix",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "i made a script to train your own transformer model on a custom dataset on your machine",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrqoul",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 60,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 60,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751655857,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;over the last couple of years we have seen LLMs become super duper popular and some of them are small enough to run on consumer level hardware, but in most cases we are talking about pre-trained models that can be used only in inference mode without considering the full training phase. Something that i was cuorious about tho is what kind of performance i could get if i did everything, including the full training without using other tools like lora or quantization, on my own everyday machine so i made a script that does exactly that, the script contains also a file (config.py) that can be used to tune the hyperparameters of the architecture so that anyone running it can easily set them to have the largest model as possible with their hardware (in my case with the model in the script and with a 12gb 3060 i can train about 50M params, 300M with smaller batch and mixed precision)\nhere is the repo &lt;a href=\"https://github.com/samas69420/transformino\"&gt;https://github.com/samas69420/transformino&lt;/a&gt; , to run the code the only thing you&amp;#39;ll need is a dataset in the form of a csv file with a column containing the text that will be used for training (tweets, sentences from a book etc), the project also have a very low number of dependencies to make it more easy to run (you&amp;#39;ll need only pytorch, pandas and tokenizers), every kind of feedback would be appreciated &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/BVtEt6cZ0osDH-48KskOkgP07Gr7jhgYOk0LZe_LbvY.png?auto=webp&amp;s=4e22124998d1dfc17c2401aefc49c230762915d2",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/BVtEt6cZ0osDH-48KskOkgP07Gr7jhgYOk0LZe_LbvY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=172adefafb8f644efa0ce3d9f1b5a82f3a2f5ad3",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/BVtEt6cZ0osDH-48KskOkgP07Gr7jhgYOk0LZe_LbvY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8cdb45f9a6579cee9d199e639654dff09c54e85f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/BVtEt6cZ0osDH-48KskOkgP07Gr7jhgYOk0LZe_LbvY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=48e6714f2d1c199368cdf6fc0a4c5588ce26d3dd",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/BVtEt6cZ0osDH-48KskOkgP07Gr7jhgYOk0LZe_LbvY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7a12b0c49f0e8cbbd4134054d1314cf1ff1a6527",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/BVtEt6cZ0osDH-48KskOkgP07Gr7jhgYOk0LZe_LbvY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=749ea1f3397f07a0b2a913e3de8b61571e01dfde",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/BVtEt6cZ0osDH-48KskOkgP07Gr7jhgYOk0LZe_LbvY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f4287bd0f8d0fa234d958fe2c9d27d512bcd291d",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "BVtEt6cZ0osDH-48KskOkgP07Gr7jhgYOk0LZe_LbvY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lrqoul",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "samas69420",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrqoul/i_made_a_script_to_train_your_own_transformer/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrqoul/i_made_a_script_to_train_your_own_transformer/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751655857,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've worked about 7 years in software development companies, and it's \"easy\" to be a software/backend/web developer because we use tools/frameworks/libs that are mature and battle-tested.\n\nProblem with Django? Update it, the bug was probably fixed ages ago.\n\nWith LLMs it's an absolute clusterfuck. You just bought an RTX 5090? Boom, you have to recompile everything to make it work with SM_120. And I'm skipping the hellish Ubuntu installation part with cursed headers just to get it running in degraded mode.\n\nExample from last week: vLLM implemented Dual Chunked Attention for Qwen 7B/14B 1M, THE ONLY (open weight) model that seriously handles long context.\n\n1. Unmerged bugfix that makes it **UNUSABLE** https://github.com/vllm-project/vllm/pull/19084\n2. FP8 wasn't working, I had to make the PR myself https://github.com/vllm-project/vllm/pull/19420  \n3. Some guy broke Dual Chunk attention because of CUDA kernel and division by zero, had to write another PR https://github.com/vllm-project/vllm/pull/20488\n\nHoly shit, I spend more time at the office hammering away at libraries than actually working on the project that's supposed to use these libraries.\n\nAm I going crazy or do you guys also notice this is a COMPLETE SHITSHOW????\n\nAnd I'm not even talking about the nightmare of having to use virtualized GPUs with NVIDIA GRID drivers that you can't download yourself and that EXPLODE at the slightest conflict: \n\n`driver versions &lt;----&gt; torch version &lt;-----&gt; vLLM version`\n\n\n\nIt's driving me insane.\n\nI don't understand how Ggerganov can keep working on llama.cpp every single day with no break and not turn INSANE.",
          "author_fullname": "t2_152zyn72n4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone else feel like working with LLM libs is like navigating a minefield ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrjy15",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 134,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 134,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751638923,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve worked about 7 years in software development companies, and it&amp;#39;s &amp;quot;easy&amp;quot; to be a software/backend/web developer because we use tools/frameworks/libs that are mature and battle-tested.&lt;/p&gt;\n\n&lt;p&gt;Problem with Django? Update it, the bug was probably fixed ages ago.&lt;/p&gt;\n\n&lt;p&gt;With LLMs it&amp;#39;s an absolute clusterfuck. You just bought an RTX 5090? Boom, you have to recompile everything to make it work with SM_120. And I&amp;#39;m skipping the hellish Ubuntu installation part with cursed headers just to get it running in degraded mode.&lt;/p&gt;\n\n&lt;p&gt;Example from last week: vLLM implemented Dual Chunked Attention for Qwen 7B/14B 1M, THE ONLY (open weight) model that seriously handles long context.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Unmerged bugfix that makes it &lt;strong&gt;UNUSABLE&lt;/strong&gt; &lt;a href=\"https://github.com/vllm-project/vllm/pull/19084\"&gt;https://github.com/vllm-project/vllm/pull/19084&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;FP8 wasn&amp;#39;t working, I had to make the PR myself &lt;a href=\"https://github.com/vllm-project/vllm/pull/19420\"&gt;https://github.com/vllm-project/vllm/pull/19420&lt;/a&gt;&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Some guy broke Dual Chunk attention because of CUDA kernel and division by zero, had to write another PR &lt;a href=\"https://github.com/vllm-project/vllm/pull/20488\"&gt;https://github.com/vllm-project/vllm/pull/20488&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Holy shit, I spend more time at the office hammering away at libraries than actually working on the project that&amp;#39;s supposed to use these libraries.&lt;/p&gt;\n\n&lt;p&gt;Am I going crazy or do you guys also notice this is a COMPLETE SHITSHOW????&lt;/p&gt;\n\n&lt;p&gt;And I&amp;#39;m not even talking about the nightmare of having to use virtualized GPUs with NVIDIA GRID drivers that you can&amp;#39;t download yourself and that EXPLODE at the slightest conflict: &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;driver versions &amp;lt;----&amp;gt; torch version &amp;lt;-----&amp;gt; vLLM version&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s driving me insane.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t understand how Ggerganov can keep working on llama.cpp every single day with no break and not turn INSANE.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/L1dGEw1UUf0GHnpmf5uDpElgs8er0s6PxenyNFO_HEs.png?auto=webp&amp;s=d5a316e649c7301ee83fc9d79f029748b4b9c2ac",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/L1dGEw1UUf0GHnpmf5uDpElgs8er0s6PxenyNFO_HEs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4129a80937cfe58e8199cc33db87b61d13901e85",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/L1dGEw1UUf0GHnpmf5uDpElgs8er0s6PxenyNFO_HEs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ebcf0f7824053e06fbffb388aac640d0a5b5594d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/L1dGEw1UUf0GHnpmf5uDpElgs8er0s6PxenyNFO_HEs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa4d4757f190695ea42fdb147abc8832a8641249",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/L1dGEw1UUf0GHnpmf5uDpElgs8er0s6PxenyNFO_HEs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=92338135c3a069a62034d2d9464d3910f7b05699",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/L1dGEw1UUf0GHnpmf5uDpElgs8er0s6PxenyNFO_HEs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8e9ef1e665e533340d9acd1ec25e7f92fdbf8648",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/L1dGEw1UUf0GHnpmf5uDpElgs8er0s6PxenyNFO_HEs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a4a3e58f7cad83a6b34458387ae70bc5ad29c244",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "L1dGEw1UUf0GHnpmf5uDpElgs8er0s6PxenyNFO_HEs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lrjy15",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LinkSea8324",
          "discussion_type": null,
          "num_comments": 42,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751638923,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_152zyn72n4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "llama : add high-throughput mode by ggerganov · Pull Request #14363 · ggml-org/llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrmxn7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bbbdbf",
          "ups": 85,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 85,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=39e27648161efabef59abd39787fd6f8222c7969",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751646416,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14363",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?auto=webp&amp;s=b075ebe4728db7369f2baf8fe5f07feb026d677f",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2f754516aec2c21e4f5375196c6d7bba0b657d3",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f50c6c3d42adc6f5e7c7537d79d13142c6f96a5e",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2ee740e6978d6728572a5a26bb4282a2e0bcf4ea",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b6fff29b1956872ed77766a108a404db46b43026",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=20807a8fd9c08388e57b320daad60921fe93ecce",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1fb818ec45c4aa13e22ce4fcf051a5dddea654c9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lrmxn7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LinkSea8324",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14363",
          "subreddit_subscribers": 494897,
          "created_utc": 1751646416,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Edit 2:**\n\nAs my original question is causing too much confusion, let me rephrase it:\n\nHow much time (in days, weeks, months or years) did it take you (given your own skillset that you had at the beginning) from the moment you started to learn about LLM until you felt comfortable to self-host a model?\n\n\n\nPlease just ignore the original text. I am really just interested in a time estimate and not details of a solution. The \"Please consider everything needed...\" was intended that you think about what you would do and estimate how long it would take, but the intention was not to get a detailed plan.\n\nSorry for the inconvenience...\n\n\n\n~~Please imagine the following:~~\n\n* ~~You are a Software Developer in a medium sized company, let's say 500 employees with all of them doing the same kind of work (will become relevant later), except from you. You have no experience at all with machine learning or LLM. Everything is completely new for you. You have of course heard of it, you used ChatGPT, but you have never worked with anything in the field of AI before. You are a complete AI newbie.~~\n* ~~Your boss gave you the task to host an opensource LLM on-premise in the company, including a Chat app that is connected to it. You know nothing about possible opensource chat apps yet either and have to research everything from scratch.~~\n\n~~I would like to know what would you would estimate, how much time would this person have to spend until there is a running on-premise open-source LLM running in that company and the Chat functionality is available for all 500 users (all of them white collar who exclusively work at the computer).~~\n\n~~Please consider everything needed to achieve this that comes to your mind, like researching how to achieve that, reading blog posts, reading reddit :) , watching youtube videos, watching courses, conducting experiments, writing code, also: researching what model would suit the need, defining the hardware to be purchased, finding a Chat Tool that can run locally, install the tool, run tests, bring it to production.~~\n\n**~~Note:~~** ~~during the whole process the person~~ **~~is allowed to use tools like ChatGPT~~** ~~to help with this task.~~\n\n~~Please also make an estimate how much of the working time have to be spent to maintain it, after it is in production.~~\n\n**~~Why am I asking this question ?~~**\n\n~~Because I think, that the skills that we have are highly under estimated and are not appreciated enough. I hope that these results will not only help me, but also others here when it comes to discussions with your employer or also when it comes to just get a feeling on how much time you already spent in your local LLM journey, or what ever... I consider this a really valuable info to have for all of us.~~\n\n**~~Edit 1:~~**\n\n~~My question is not about how to implement this, but your estimated time effort to learn this and bring this to production, is it weeks, months, years?~~",
          "author_fullname": "t2_4g2lejyp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the necessary time effort to learn to self-host an LLM and chat app on-premise in a mid size company?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsivf4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.53,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751749568,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751746201,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Edit 2:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;As my original question is causing too much confusion, let me rephrase it:&lt;/p&gt;\n\n&lt;p&gt;How much time (in days, weeks, months or years) did it take you (given your own skillset that you had at the beginning) from the moment you started to learn about LLM until you felt comfortable to self-host a model?&lt;/p&gt;\n\n&lt;p&gt;Please just ignore the original text. I am really just interested in a time estimate and not details of a solution. The &amp;quot;Please consider everything needed...&amp;quot; was intended that you think about what you would do and estimate how long it would take, but the intention was not to get a detailed plan.&lt;/p&gt;\n\n&lt;p&gt;Sorry for the inconvenience...&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;Please imagine the following:&lt;/del&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;del&gt;You are a Software Developer in a medium sized company, let&amp;#39;s say 500 employees with all of them doing the same kind of work (will become relevant later), except from you. You have no experience at all with machine learning or LLM. Everything is completely new for you. You have of course heard of it, you used ChatGPT, but you have never worked with anything in the field of AI before. You are a complete AI newbie.&lt;/del&gt;&lt;/li&gt;\n&lt;li&gt;&lt;del&gt;Your boss gave you the task to host an opensource LLM on-premise in the company, including a Chat app that is connected to it. You know nothing about possible opensource chat apps yet either and have to research everything from scratch.&lt;/del&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;del&gt;I would like to know what would you would estimate, how much time would this person have to spend until there is a running on-premise open-source LLM running in that company and the Chat functionality is available for all 500 users (all of them white collar who exclusively work at the computer).&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;Please consider everything needed to achieve this that comes to your mind, like researching how to achieve that, reading blog posts, reading reddit :) , watching youtube videos, watching courses, conducting experiments, writing code, also: researching what model would suit the need, defining the hardware to be purchased, finding a Chat Tool that can run locally, install the tool, run tests, bring it to production.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;del&gt;Note:&lt;/del&gt;&lt;/strong&gt; &lt;del&gt;during the whole process the person&lt;/del&gt; &lt;strong&gt;&lt;del&gt;is allowed to use tools like ChatGPT&lt;/del&gt;&lt;/strong&gt; &lt;del&gt;to help with this task.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;Please also make an estimate how much of the working time have to be spent to maintain it, after it is in production.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;del&gt;Why am I asking this question ?&lt;/del&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;Because I think, that the skills that we have are highly under estimated and are not appreciated enough. I hope that these results will not only help me, but also others here when it comes to discussions with your employer or also when it comes to just get a feeling on how much time you already spent in your local LLM journey, or what ever... I consider this a really valuable info to have for all of us.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;del&gt;Edit 1:&lt;/del&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;My question is not about how to implement this, but your estimated time effort to learn this and bring this to production, is it weeks, months, years?&lt;/del&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsivf4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent_Hour_301",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsivf4/what_is_the_necessary_time_effort_to_learn_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsivf4/what_is_the_necessary_time_effort_to_learn_to/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751746201,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Title says it all, are there any models that work like gpt image 1 that I can run on an AMD GPU or on RAM?\n",
          "author_fullname": "t2_30tb5dmf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any autoregressive image gen models I can run locally on a 9070 XT/RAM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls8sk9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751718625,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title says it all, are there any models that work like gpt image 1 that I can run on an AMD GPU or on RAM?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ls8sk9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jojokingxp",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls8sk9/are_there_any_autoregressive_image_gen_models_i/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls8sk9/are_there_any_autoregressive_image_gen_models_i/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751718625,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Based on the this [benchmark for coding and UI/UX](https://www.designarena.ai/leaderboard), the Llama models are absolutely horrendous when it comes to build websites, apps, and other kinds of user interfaces. \n\nHow is Llama this bad and Meta so behind on AI compared to everyone else? No wonder they're trying to poach every top AI researcher out there. \n\n[Llama Examples](https://www.designarena.ai/models/llama-4-scout)",
          "author_fullname": "t2_15wmlp183l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "How and why is Llama so behind the other models at coding and UI/UX? Who is even using it?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 74,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "uh0dpa2rsxaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/uh0dpa2rsxaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=eebaebedefb7e6d4e8ff5df1affbd89ae578bf8a"
                },
                {
                  "y": 125,
                  "x": 216,
                  "u": "https://preview.redd.it/uh0dpa2rsxaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=66951caca8dfec2a4ca9db0ac6c26d344c4cbabf"
                },
                {
                  "y": 186,
                  "x": 320,
                  "u": "https://preview.redd.it/uh0dpa2rsxaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba2f50c343073de41862a2096f915bfd3fb6563c"
                },
                {
                  "y": 372,
                  "x": 640,
                  "u": "https://preview.redd.it/uh0dpa2rsxaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1fe85bccfa08dd07358d039a3aa1a45726b94800"
                },
                {
                  "y": 558,
                  "x": 960,
                  "u": "https://preview.redd.it/uh0dpa2rsxaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a15f7ace8eb7191fcf63f515802cee1d796c8d8f"
                },
                {
                  "y": 627,
                  "x": 1080,
                  "u": "https://preview.redd.it/uh0dpa2rsxaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=24609a96b4ac923e07198a1c59f740532d7c73b8"
                }
              ],
              "s": {
                "y": 1480,
                "x": 2546,
                "u": "https://preview.redd.it/uh0dpa2rsxaf1.png?width=2546&amp;format=png&amp;auto=webp&amp;s=e3cb26cf0342dafcd6eeb977bd85a8593aa31eb6"
              },
              "id": "uh0dpa2rsxaf1"
            },
            "ds1hszgcsxaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 57,
                  "x": 108,
                  "u": "https://preview.redd.it/ds1hszgcsxaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3cb9c84690474a26bbf87a9bfb1bd998b8970eca"
                },
                {
                  "y": 114,
                  "x": 216,
                  "u": "https://preview.redd.it/ds1hszgcsxaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=28e294c8871af2545ca0260bb16bd2ff24e49b0f"
                },
                {
                  "y": 170,
                  "x": 320,
                  "u": "https://preview.redd.it/ds1hszgcsxaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1e9139ca024158104ada0bc31ba3a8a76e068556"
                },
                {
                  "y": 340,
                  "x": 640,
                  "u": "https://preview.redd.it/ds1hszgcsxaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=22ef93d523e44b3613360f466e305ef60cc4482c"
                },
                {
                  "y": 510,
                  "x": 960,
                  "u": "https://preview.redd.it/ds1hszgcsxaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9dd93ea41aed35e1e594fc7d97a90744a229e8d8"
                },
                {
                  "y": 574,
                  "x": 1080,
                  "u": "https://preview.redd.it/ds1hszgcsxaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=86aa10a177a70ed19f43406337ecaf7bc483a8c9"
                }
              ],
              "s": {
                "y": 1396,
                "x": 2626,
                "u": "https://preview.redd.it/ds1hszgcsxaf1.png?width=2626&amp;format=png&amp;auto=webp&amp;s=641ae6f2f1ad5b242d23fef71ccef731b86d8dd5"
              },
              "id": "ds1hszgcsxaf1"
            }
          },
          "name": "t3_1lrvlsx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "ups": 25,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "ds1hszgcsxaf1",
                "id": 698864361
              },
              {
                "media_id": "uh0dpa2rsxaf1",
                "id": 698864362
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ky0nsIz_RJXchcUklbRQzW-JFzxABDUvpQ9m5bMNp2o.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751669552,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Based on the this &lt;a href=\"https://www.designarena.ai/leaderboard\"&gt;benchmark for coding and UI/UX&lt;/a&gt;, the Llama models are absolutely horrendous when it comes to build websites, apps, and other kinds of user interfaces. &lt;/p&gt;\n\n&lt;p&gt;How is Llama this bad and Meta so behind on AI compared to everyone else? No wonder they&amp;#39;re trying to poach every top AI researcher out there. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.designarena.ai/models/llama-4-scout\"&gt;Llama Examples&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lrvlsx",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lrvlsx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "idwiw_wiw",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrvlsx/how_and_why_is_llama_so_behind_the_other_models/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lrvlsx",
          "subreddit_subscribers": 494897,
          "created_utc": 1751669552,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "From time to time, often months between it. I start a roleplay with a local LLM and when I do this I chat for a while. And since two years I run every time into the same issue: After a while the roleplay turned into a \"how do I fix the LLM from repeating itself too much\" or into a \"Post an answer, wait for the LLM answer, edit the answer more and more\" game.\n\nI really hate this crap. I want to have fun and not want to always closely looking what the LLM answers and compare it the previous answer so that the LLM never tend to go down this stupid repeating rabbit hole...\n\nOne idea for a solution that I have would be to use the LLM answer an let it check that one with another prompt itself, let it compare with maybe the last 10 LLM answers before that one and let it rephrase the answer when some phrases are too similar.\n\nAt least that would be my first quick idea which could work. Even when it would make the answer time even longer. But for that you would need to write your own \"Chatbot\" (well, on that I work from time to time a bit - and such things hold be also back from it).\n\nRun into that problem minutes ago and it ruined my roleplay, again. This time I used Mistral 3.2, but it didn't really matter what LLM I use. It always tend to slowly repeate stuff before you really notice it without analyzing every answer (what already would ruin the RP). It is especially annoying because the first hour or so (depends on the LLM and the settings) it works without any problems and so you can have a lot of fun.\n\nWhat are your experiences when you do longer roleplay or maybe even endless roleplays you continue every time? I love to do this, but that ruins it for me every time.\n\nAnd before anyone comes with that up: no, any setting that should avoid repetion did not fix that problem, It only delays it at best, but it didn't disappear.",
          "author_fullname": "t2_j0e2r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Will this ever be fixed? RP repetition",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls1hd2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751689775,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From time to time, often months between it. I start a roleplay with a local LLM and when I do this I chat for a while. And since two years I run every time into the same issue: After a while the roleplay turned into a &amp;quot;how do I fix the LLM from repeating itself too much&amp;quot; or into a &amp;quot;Post an answer, wait for the LLM answer, edit the answer more and more&amp;quot; game.&lt;/p&gt;\n\n&lt;p&gt;I really hate this crap. I want to have fun and not want to always closely looking what the LLM answers and compare it the previous answer so that the LLM never tend to go down this stupid repeating rabbit hole...&lt;/p&gt;\n\n&lt;p&gt;One idea for a solution that I have would be to use the LLM answer an let it check that one with another prompt itself, let it compare with maybe the last 10 LLM answers before that one and let it rephrase the answer when some phrases are too similar.&lt;/p&gt;\n\n&lt;p&gt;At least that would be my first quick idea which could work. Even when it would make the answer time even longer. But for that you would need to write your own &amp;quot;Chatbot&amp;quot; (well, on that I work from time to time a bit - and such things hold be also back from it).&lt;/p&gt;\n\n&lt;p&gt;Run into that problem minutes ago and it ruined my roleplay, again. This time I used Mistral 3.2, but it didn&amp;#39;t really matter what LLM I use. It always tend to slowly repeate stuff before you really notice it without analyzing every answer (what already would ruin the RP). It is especially annoying because the first hour or so (depends on the LLM and the settings) it works without any problems and so you can have a lot of fun.&lt;/p&gt;\n\n&lt;p&gt;What are your experiences when you do longer roleplay or maybe even endless roleplays you continue every time? I love to do this, but that ruins it for me every time.&lt;/p&gt;\n\n&lt;p&gt;And before anyone comes with that up: no, any setting that should avoid repetion did not fix that problem, It only delays it at best, but it didn&amp;#39;t disappear.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ls1hd2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Blizado",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls1hd2/will_this_ever_be_fixed_rp_repetition/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls1hd2/will_this_ever_be_fixed_rp_repetition/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751689775,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‘m looking for a iOS-App where I can run a local model (e.g. Qwen3-4b) which provides a Ollama like API where I can connect to from other apps.\n\nAs iPhone 16/iPad are quite fast with promt processing and token generation at such small models and very power efficient, I would like to test some use cases.\n\n(If someone know something like this for android, let me know too).",
          "author_fullname": "t2_3nvody",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running GGUF model on iOS with local API",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls66qt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751708701,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‘m looking for a iOS-App where I can run a local model (e.g. Qwen3-4b) which provides a Ollama like API where I can connect to from other apps.&lt;/p&gt;\n\n&lt;p&gt;As iPhone 16/iPad are quite fast with promt processing and token generation at such small models and very power efficient, I would like to test some use cases.&lt;/p&gt;\n\n&lt;p&gt;(If someone know something like this for android, let me know too).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ls66qt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vistalba",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls66qt/running_gguf_model_on_ios_with_local_api/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls66qt/running_gguf_model_on_ios_with_local_api/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751708701,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have an r730XD that I'm looking to convert into an LLM server, mostly just inference, maybe some training in the future, and I'm stuck on deciding on a GPU.\n\nThe two I'm currently considering are the RTX 2000E Ada (16GB) or RTX 3090 (24GB). Both are about the same price.\n\nThe 2000E is much newer, has a higher CUDA version, and much lower power requirements (meaning I don't need to upgrade my PSUs or track down additional power cables, which isn't really a big deal, but makes it slightly easier). Since it's single slot, I could also theoretically add two more down the line and have 48GB VRAM, which sounds appealing. However, the bandwidth is only 224GB/s.\n\nThe 3090 requires me to upgrade the PSUs and get the power cables, and I can only fit one, so a hard limit at 24GB, but at 900+GB/s.\n\nSo do I go for more-and-faster VRAM, with a hard cap on expandability, OR the slower-but-newer card that would allow me to add more VRAM in the future?\n\nI'm like 80% leaning towards the 3090 but since I'm just getting started in this, wanted to see if there was anything I was overlooking. Or if anyone had other card suggestions.",
          "author_fullname": "t2_satfqea",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPU Choice for r730XD",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsck2e",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751729543,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an r730XD that I&amp;#39;m looking to convert into an LLM server, mostly just inference, maybe some training in the future, and I&amp;#39;m stuck on deciding on a GPU.&lt;/p&gt;\n\n&lt;p&gt;The two I&amp;#39;m currently considering are the RTX 2000E Ada (16GB) or RTX 3090 (24GB). Both are about the same price.&lt;/p&gt;\n\n&lt;p&gt;The 2000E is much newer, has a higher CUDA version, and much lower power requirements (meaning I don&amp;#39;t need to upgrade my PSUs or track down additional power cables, which isn&amp;#39;t really a big deal, but makes it slightly easier). Since it&amp;#39;s single slot, I could also theoretically add two more down the line and have 48GB VRAM, which sounds appealing. However, the bandwidth is only 224GB/s.&lt;/p&gt;\n\n&lt;p&gt;The 3090 requires me to upgrade the PSUs and get the power cables, and I can only fit one, so a hard limit at 24GB, but at 900+GB/s.&lt;/p&gt;\n\n&lt;p&gt;So do I go for more-and-faster VRAM, with a hard cap on expandability, OR the slower-but-newer card that would allow me to add more VRAM in the future?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m like 80% leaning towards the 3090 but since I&amp;#39;m just getting started in this, wanted to see if there was anything I was overlooking. Or if anyone had other card suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsck2e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gat0r87",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsck2e/gpu_choice_for_r730xd/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsck2e/gpu_choice_for_r730xd/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751729543,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Build vLLM on  CUDA 12.9, Kernel 6.15.2, NVIDIA 575.64, PyTorch 2.9cu129 Nightly\n\nLet's fucking go!!!!!!!!",
          "author_fullname": "t2_1rig07ocmc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Build vLLM on  CUDA 12.9, Kernel 6.15.2, NVIDIA 575.64, PyTorch 2.9cu129 Nightly",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lshe4q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751742225,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Build vLLM on  CUDA 12.9, Kernel 6.15.2, NVIDIA 575.64, PyTorch 2.9cu129 Nightly&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s fucking go!!!!!!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lshe4q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sorry_Ad191",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lshe4q/build_vllm_on_cuda_129_kernel_6152_nvidia_57564/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lshe4q/build_vllm_on_cuda_129_kernel_6152_nvidia_57564/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751742225,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hopefully I uploaded everything correctly and haven't embarrassed myself..:  \n[https://github.com/mrhappynice/bplus-server](https://github.com/mrhappynice/bplus-server)\n\nMy little toy. Just talk into the mic. hit gen. look at code, is it there?? hit create, page is hosted and live.   \nalso app manager(edit, delete, create llm-ready context) and manual app builder.   \nGemini connection added also, select model. Local through LM Studio(port 1234) should be able to just change url for Ollama etc.. \n\nVoice is through Whisper server port 5752. Piper TTS(cmd line exe) also have browser speech through Web Speech API(ehh..)  \n\nmdChat and pic-chat are special WIP and blocked from the app manager. I'm forgetting about 22 things.  \nHopefully everything is working for ya.  p e a c e   \n",
          "author_fullname": "t2_176lp9bww1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "speech, app studio, hosting - all local and seemless(ish) | my toy: bplus Server",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 98,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls0d8u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/7yuXmSYIYZLs-ovdGvY48xZ_4QN2WDELP3cYhhxo-Ow.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751685672,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hopefully I uploaded everything correctly and haven&amp;#39;t embarrassed myself..:&lt;br/&gt;\n&lt;a href=\"https://github.com/mrhappynice/bplus-server\"&gt;https://github.com/mrhappynice/bplus-server&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;My little toy. Just talk into the mic. hit gen. look at code, is it there?? hit create, page is hosted and live.&lt;br/&gt;\nalso app manager(edit, delete, create llm-ready context) and manual app builder.&lt;br/&gt;\nGemini connection added also, select model. Local through LM Studio(port 1234) should be able to just change url for Ollama etc.. &lt;/p&gt;\n\n&lt;p&gt;Voice is through Whisper server port 5752. Piper TTS(cmd line exe) also have browser speech through Web Speech API(ehh..)  &lt;/p&gt;\n\n&lt;p&gt;mdChat and pic-chat are special WIP and blocked from the app manager. I&amp;#39;m forgetting about 22 things.&lt;br/&gt;\nHopefully everything is working for ya.  p e a c e   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/toexu80x1zaf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/toexu80x1zaf1.jpeg?auto=webp&amp;s=779c2b2c0bdb24b70ff225360eb172f3059651f6",
                  "width": 1328,
                  "height": 938
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/toexu80x1zaf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=13c05e9d01ffd1112614238278e02a8b0c5fa8f7",
                    "width": 108,
                    "height": 76
                  },
                  {
                    "url": "https://preview.redd.it/toexu80x1zaf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e96e58c319716a83c6825120e0f8e593a20b042d",
                    "width": 216,
                    "height": 152
                  },
                  {
                    "url": "https://preview.redd.it/toexu80x1zaf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0b3aae3cb68d2c7f7b28be5224bfab205e0ab7a0",
                    "width": 320,
                    "height": 226
                  },
                  {
                    "url": "https://preview.redd.it/toexu80x1zaf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc03891c2dcbeac4c24ad26bd6c710c99bf02bd7",
                    "width": 640,
                    "height": 452
                  },
                  {
                    "url": "https://preview.redd.it/toexu80x1zaf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ac83a5160d39df8d9dad1841926c9b4b487398c5",
                    "width": 960,
                    "height": 678
                  },
                  {
                    "url": "https://preview.redd.it/toexu80x1zaf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=68e5a1f38a90d8d5f1f288b97264b78ec54eb1b8",
                    "width": 1080,
                    "height": 762
                  }
                ],
                "variants": {},
                "id": "wvcBxMivAlrWftFzT5G_jefDjmIBAZqu_oLu8q9vOPE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ls0d8u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mr_happy_nice",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls0d8u/speech_app_studio_hosting_all_local_and/",
          "stickied": false,
          "url": "https://i.redd.it/toexu80x1zaf1.jpeg",
          "subreddit_subscribers": 494897,
          "created_utc": 1751685672,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all! I'm creating a project that applies Monte Carlo Tree Search to LLM conversations. Instead of just generating the next response, it simulates entire conversation trees to find paths that achieve long-term goals. The initial draft version is up.\n\nGithub: [https://github.com/MVPandey/CAE](https://github.com/MVPandey/CAE)\n\n  \n(Note: This is a Claude-generated mock UI. The payload is real but the UI is simulated :) I'm a terrible frontend dev)\n\nhttps://i.redd.it/dqws3fzgysaf1.gif\n\n  \n\n\n**How it works:**\n\n* Generates multiple response candidates at each conversation state\n* Simulates how conversations might unfold down each branch (using the LLM to predict user responses)\n* Scores each trajectory on metrics like empathy, goal achievement, coherence\n* Uses MCTS with UCB1 to efficiently explore the most promising paths\n* Selects the response that leads to the best expected outcome\n\n**Technical implementation:**\n\n* FastAPI backend with async SQLAlchemy (PostgreSQL)\n* Aggressive parallelization - all branch evaluations run concurrently with asyncio.gather()\n* Works with any OpenAI-compatible endpoint\n* Dual-purpose: works as both a standard chat API and on-demand analysis engine\n* No agentic framework dependencies \n\n**Limitations:**\n\n* Scoring is done by the same LLM that generates responses (obviously bad - not very grounded or reproducible or scientific yet)\n* Branch pruning is naive - just threshold-based instead of something smarter like progressive widening\n* Memory usage grows with tree size - haven't implemented node recycling yet\n* The pgvector embedding code is there but commented out (wanted semantic search over conversation history)\n\nOriginally thought of this to generate preference data for RL training (converting instruct/response datasets to PPO datasets) and refined the idea into code at a hackathon - the system outputs full JSON showing why certain conversation paths outperform others, with rationales and metrics. Been testing on customer support scenarios and therapeutic conversations.\n\nExample output shows the selected response, rejected alternatives, simulated user reactions, and scoring breakdowns. Pretty interesting to see it reason through de-escalation strategies or teaching approaches.\n\nCurious if anyone's tried similar approaches or has ideas for more grounded scoring methods. The LLM-as-judge problem is real here.\n\nAnyway, please let me know any thoughts, criticisms, feedback, etc! :) \n\nI also am not sure what I want this project to evolve into. This is a very crude first approach and IDK what I wanna do for next steps. ",
          "author_fullname": "t2_3fx3fjwm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Created an Open Source Conversation Response Path Exploration System using Monte Carlo Tree Search",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "dqws3fzgysaf1": {
              "status": "valid",
              "e": "AnimatedImage",
              "m": "image/gif",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/dqws3fzgysaf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=681d3e8ada7686c61eb140c06d2970adf715c643"
                },
                {
                  "y": 137,
                  "x": 216,
                  "u": "https://preview.redd.it/dqws3fzgysaf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=501058916599f636f9c6f39e654606a970a31607"
                },
                {
                  "y": 204,
                  "x": 320,
                  "u": "https://preview.redd.it/dqws3fzgysaf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=d6716ae553702d0115496d7b4d8376216ebc6e8a"
                },
                {
                  "y": 408,
                  "x": 640,
                  "u": "https://preview.redd.it/dqws3fzgysaf1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=57b11dd73f09e4d90403510ca044cbf554fc102f"
                }
              ],
              "s": {
                "y": 511,
                "gif": "https://i.redd.it/dqws3fzgysaf1.gif",
                "mp4": "https://preview.redd.it/dqws3fzgysaf1.gif?format=mp4&amp;s=98baaded5b2c4c8b2c39eb68cbe5851b9161debd",
                "x": 800
              },
              "id": "dqws3fzgysaf1"
            }
          },
          "name": "t3_1lrbwmz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 360,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 360,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=b817f104e6c280379522e8c2510286bb2bcf024e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751610972,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all! I&amp;#39;m creating a project that applies Monte Carlo Tree Search to LLM conversations. Instead of just generating the next response, it simulates entire conversation trees to find paths that achieve long-term goals. The initial draft version is up.&lt;/p&gt;\n\n&lt;p&gt;Github: &lt;a href=\"https://github.com/MVPandey/CAE\"&gt;https://github.com/MVPandey/CAE&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;(Note: This is a Claude-generated mock UI. The payload is real but the UI is simulated :) I&amp;#39;m a terrible frontend dev)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.redd.it/dqws3fzgysaf1.gif\"&gt;https://i.redd.it/dqws3fzgysaf1.gif&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Generates multiple response candidates at each conversation state&lt;/li&gt;\n&lt;li&gt;Simulates how conversations might unfold down each branch (using the LLM to predict user responses)&lt;/li&gt;\n&lt;li&gt;Scores each trajectory on metrics like empathy, goal achievement, coherence&lt;/li&gt;\n&lt;li&gt;Uses MCTS with UCB1 to efficiently explore the most promising paths&lt;/li&gt;\n&lt;li&gt;Selects the response that leads to the best expected outcome&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Technical implementation:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;FastAPI backend with async SQLAlchemy (PostgreSQL)&lt;/li&gt;\n&lt;li&gt;Aggressive parallelization - all branch evaluations run concurrently with asyncio.gather()&lt;/li&gt;\n&lt;li&gt;Works with any OpenAI-compatible endpoint&lt;/li&gt;\n&lt;li&gt;Dual-purpose: works as both a standard chat API and on-demand analysis engine&lt;/li&gt;\n&lt;li&gt;No agentic framework dependencies &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Scoring is done by the same LLM that generates responses (obviously bad - not very grounded or reproducible or scientific yet)&lt;/li&gt;\n&lt;li&gt;Branch pruning is naive - just threshold-based instead of something smarter like progressive widening&lt;/li&gt;\n&lt;li&gt;Memory usage grows with tree size - haven&amp;#39;t implemented node recycling yet&lt;/li&gt;\n&lt;li&gt;The pgvector embedding code is there but commented out (wanted semantic search over conversation history)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Originally thought of this to generate preference data for RL training (converting instruct/response datasets to PPO datasets) and refined the idea into code at a hackathon - the system outputs full JSON showing why certain conversation paths outperform others, with rationales and metrics. Been testing on customer support scenarios and therapeutic conversations.&lt;/p&gt;\n\n&lt;p&gt;Example output shows the selected response, rejected alternatives, simulated user reactions, and scoring breakdowns. Pretty interesting to see it reason through de-escalation strategies or teaching approaches.&lt;/p&gt;\n\n&lt;p&gt;Curious if anyone&amp;#39;s tried similar approaches or has ideas for more grounded scoring methods. The LLM-as-judge problem is real here.&lt;/p&gt;\n\n&lt;p&gt;Anyway, please let me know any thoughts, criticisms, feedback, etc! :) &lt;/p&gt;\n\n&lt;p&gt;I also am not sure what I want this project to evolve into. This is a very crude first approach and IDK what I wanna do for next steps. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?auto=webp&amp;s=47a3f76cc722819491300c172d752e4c070bb71e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f2b459033ec7b1c73ba64efd65042a691fe84f0",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f5c8d8980cb42e5790ed42559709e0a5abe078ac",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0235e4b5d4b4902638d04f61b768821c4f6dc438",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8a18a6b5aef4f1d48310d2918ee6ff6f6c5943c2",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a2e375eec31e7ac8f8c340f24ca723dd41a02316",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=059d2324da4c876d11cd8712dde3728119ec0134",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lrbwmz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ManavTheWorld",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751610972,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "# Day 10/50: Building a Small Language Model from Scratch — What is Model Distillation?\n\n*This is one of my favorite topics. I’ve always wanted to run large models (several billion parameters, like DeepSeek 671b) or at least make my smaller models behave as intelligently and powerfully as those massive, high-parameter models. But like many of us, I don’t always have the hardware to run those resource-intensive models. But what if we could transfer the knowledge of a large model to a smaller one? That’s the whole idea of model distillation.*\n\n# What is Model Distillation?\n\n*Model distillation is a technique in which a large, complex model (referred to as the teacher) transfers its knowledge to a smaller, simpler model (referred to as the student). The goal is to make the student model perform almost as well as the teacher, but with fewer resources.*\n\n&gt;***Think of it like this:*** *A PhD professor (teacher model) teaches a high school student (student model) everything they know, without the student having to go through a decade of research.*\n\n# Why Do We Need Model Distillation?\n\n*Large models are:*\n\n* *Expensive to run*\n* *Hard to deploy on edge devices*\n\n*Distillation solves this by:*\n\n* *Lowering memory/compute usage*\n* *Maintaining competitive accuracy*\n\n# How Does Model Distillation Work?\n\n*There are three main components:*\n\n1. ***Teacher Model****: A large, pre-trained model with high performance.*\n2. ***Student Model****: A smaller model, which we aim to train to mimic the teacher.*\n3. ***Soft Targets****: Instead of just learning from the ground-truth labels, the student also learns from the teacher’s probability distribution over classes (logits), which carries extra information*\n\n\n\n*Let me break it down in simple language. In the case of traditional training, the model learns from hard labels. For example, if the correct answer is “Cat,” the label is simply 1 for “Cat” and 0 for everything else.*\n\n*However, in model distillation, the student also learns from the teacher’s soft predictions, which means it not only knows the correct answer but also how confident the teacher is about each possible answer.*\n\n*If you are still unclear about it, let me provide a simpler example.*\n\n*Let’s say the task is image classification.*\n\n***Image:*** *Picture of a cat*\n\n***Hard label (ground truth):***\n\n* *“Cat” → 1*\n* *All other classes → 0*\n\n***Teacher model’s prediction (soft label):***\n\n* *“Cat” → 85%*\n* *“Dog” → 10%*\n* *“Fox” → 4%*\n* *“Rabbit” → 1%*\n\n*Instead of learning only “This is a Cat”, the student model also learns that:*\n\n&gt;*“The teacher is very confident it’s a cat, but it’s also somewhat similar to a dog or a fox.”*\n\n*This additional information helps students learn more nuanced decision boundaries, making them more innovative and generalizable, even with fewer parameters.*\n\n*To sum up, Distillation allows the student to model learning not just what the teacher thinks is correct, but also how confident the teacher is across all options; this is what we call learning from soft targets.*\n\n# Types of Knowledge Distillation\n\n*There is more than one way to pass knowledge from a teacher to a student. Let’s look at the main types:*\n\n***1. Logit-based Distillation (Hinton et al.):***  \n *This is the method introduced by Geoffrey Hinton, the father of deep learning.*  \n*Here, the student doesn’t just learn from the correct label, but from the full output of the teacher (called logits), which contains rich information about how confident the teacher is in each class.*\n\n&gt;*Think of it like learning how the teacher thinks, not just what the final answer is.*\n\n***2. Feature-based Distillation:***  \n *Instead of copying the final output, the student attempts to mimic the intermediate representations (such as hidden layers) of the teacher model.*\n\n&gt;*Imagine learning how the teacher breaks down and analyzes the problem step by step, rather than just their final conclusion.*\n\n*This is useful when you want the student to develop a similar internal understanding to that of the teacher.*\n\n***3. Response-based Distillation:***  \n *This one is more straightforward; the student is trained to match the teacher’s final output, often without worrying about logits or hidden features.*\n\n&gt;*It’s like learning to copy the teacher’s answer sheet during a test — not the most comprehensive learning, but sometimes good enough for quick tasks!*\n\n# Real-World Applications — Why Distillation Matters\n\n***Mobile Devices:***  \n *Want to run BERT or GPT on your phone without needing a cloud GPU? Distilled models make this possible by reducing the size of large models while preserving much of their power.*\n\n***Autonomous Vehicles:***  \n *Edge devices in self-driving cars can’t afford slow, bulky models. Distilled vision models enable faster, real-time decisions without requiring a massive compute stack in the trunk.*\n\n***Chatbots and Virtual Assistants:***  \n *For real-time conversations, low latency is key. Distilled language models offer fast responses while maintaining low memory and compute usage, making them ideal for customer service bots or AI tutors.*\n\n# Limitations and Challenges \n\n***1. Performance Gap:***  \n*Despite the best efforts, a student model may not accurately match the teacher’s performance, especially on complex tasks that require fine-grained reasoning.*\n\n***2. Architecture Mismatch:***  \n *If the student model is too different from the teacher in design, it may struggle to “understand” what the teacher is trying to teach.*\n\n***3. Training Overhead:***  \n *Training a good student model still takes time, data, and effort; it’s not a simple copy-paste job. And sometimes, tuning distillation hyperparameters (such as temperature or alpha) can be tricky.*\n\n# Popular Tools and Frameworks \n\n***Hugging Face:***  \n *Models like DistilBERT are smaller and faster versions of BERT, trained via distillation.*\n\n***TinyML:***  \n *This focuses on deploying distilled models on ultra-low-power device****s,*** *such as microcontrollers, think smartwatches or IoT sensors.*\n\n***OpenVINO / TensorRT:***  \n *These are optimization toolkits by Intel and NVIDIA that pair well with distilled models to extract every last bit of performance from them on CPUs and GPUs.*\n\n# Summary\n\n*I was genuinely amazed when I first learned about model distillation.* \n\n*In my case, I applied model distillation while building a model specifically for the DevOps field. I had a set of DevOps-related questions, but I didn’t have high-quality answers. So, I used GPT-o3 (yes, it did cost me) to generate expert-level responses. Once I had those, I used them to train a smaller model that could perform well without relying on GPT o3 every time. I’ll share the code for this in a future post.*\n\n*Even DeepSeek has mentioned using model distillation as part of their training strategy for smaller models* [https://www.cnbc.com/2025/02/21/deepseek-trained-ai-model-using-distillation-now-a-disruptive-force.html](https://www.cnbc.com/2025/02/21/deepseek-trained-ai-model-using-distillation-now-a-disruptive-force.html)*. It’s a great example of how powerful this technique can be.*\n\n*Distillation initially felt like a complex idea, but I’ve done my best to break it down into simple language.*",
          "author_fullname": "t2_8ht7a116",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Day 10/50: Building a Small Language Model from Scratch - What is Model Distillation?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrv48g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751668085,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Day 10/50: Building a Small Language Model from Scratch — What is Model Distillation?&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;This is one of my favorite topics. I’ve always wanted to run large models (several billion parameters, like DeepSeek 671b) or at least make my smaller models behave as intelligently and powerfully as those massive, high-parameter models. But like many of us, I don’t always have the hardware to run those resource-intensive models. But what if we could transfer the knowledge of a large model to a smaller one? That’s the whole idea of model distillation.&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;What is Model Distillation?&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;Model distillation is a technique in which a large, complex model (referred to as the teacher) transfers its knowledge to a smaller, simpler model (referred to as the student). The goal is to make the student model perform almost as well as the teacher, but with fewer resources.&lt;/em&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Think of it like this:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;A PhD professor (teacher model) teaches a high school student (student model) everything they know, without the student having to go through a decade of research.&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;h1&gt;Why Do We Need Model Distillation?&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;Large models are:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;Expensive to run&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Hard to deploy on edge devices&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;em&gt;Distillation solves this by:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;Lowering memory/compute usage&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Maintaining competitive accuracy&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;How Does Model Distillation Work?&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;There are three main components:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Teacher Model&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: A large, pre-trained model with high performance.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Student Model&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: A smaller model, which we aim to train to mimic the teacher.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Soft Targets&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Instead of just learning from the ground-truth labels, the student also learns from the teacher’s probability distribution over classes (logits), which carries extra information&lt;/em&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;em&gt;Let me break it down in simple language. In the case of traditional training, the model learns from hard labels. For example, if the correct answer is “Cat,” the label is simply 1 for “Cat” and 0 for everything else.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;However, in model distillation, the student also learns from the teacher’s soft predictions, which means it not only knows the correct answer but also how confident the teacher is about each possible answer.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;If you are still unclear about it, let me provide a simpler example.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Let’s say the task is image classification.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Image:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Picture of a cat&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Hard label (ground truth):&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;“Cat” → 1&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;All other classes → 0&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Teacher model’s prediction (soft label):&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;“Cat” → 85%&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;“Dog” → 10%&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;“Fox” → 4%&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;“Rabbit” → 1%&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;em&gt;Instead of learning only “This is a Cat”, the student model also learns that:&lt;/em&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;em&gt;“The teacher is very confident it’s a cat, but it’s also somewhat similar to a dog or a fox.”&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;em&gt;This additional information helps students learn more nuanced decision boundaries, making them more innovative and generalizable, even with fewer parameters.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;To sum up, Distillation allows the student to model learning not just what the teacher thinks is correct, but also how confident the teacher is across all options; this is what we call learning from soft targets.&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;Types of Knowledge Distillation&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;There is more than one way to pass knowledge from a teacher to a student. Let’s look at the main types:&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;1. Logit-based Distillation (Hinton et al.):&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;\n &lt;em&gt;This is the method introduced by Geoffrey Hinton, the father of deep learning.&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;Here, the student doesn’t just learn from the correct label, but from the full output of the teacher (called logits), which contains rich information about how confident the teacher is in each class.&lt;/em&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;em&gt;Think of it like learning how the teacher thinks, not just what the final answer is.&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;2. Feature-based Distillation:&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;\n &lt;em&gt;Instead of copying the final output, the student attempts to mimic the intermediate representations (such as hidden layers) of the teacher model.&lt;/em&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;em&gt;Imagine learning how the teacher breaks down and analyzes the problem step by step, rather than just their final conclusion.&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;em&gt;This is useful when you want the student to develop a similar internal understanding to that of the teacher.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;3. Response-based Distillation:&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;\n &lt;em&gt;This one is more straightforward; the student is trained to match the teacher’s final output, often without worrying about logits or hidden features.&lt;/em&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;em&gt;It’s like learning to copy the teacher’s answer sheet during a test — not the most comprehensive learning, but sometimes good enough for quick tasks!&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;h1&gt;Real-World Applications — Why Distillation Matters&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Mobile Devices:&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;\n &lt;em&gt;Want to run BERT or GPT on your phone without needing a cloud GPU? Distilled models make this possible by reducing the size of large models while preserving much of their power.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Autonomous Vehicles:&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;\n &lt;em&gt;Edge devices in self-driving cars can’t afford slow, bulky models. Distilled vision models enable faster, real-time decisions without requiring a massive compute stack in the trunk.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Chatbots and Virtual Assistants:&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;\n &lt;em&gt;For real-time conversations, low latency is key. Distilled language models offer fast responses while maintaining low memory and compute usage, making them ideal for customer service bots or AI tutors.&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;Limitations and Challenges &lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;1. Performance Gap:&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;\n&lt;em&gt;Despite the best efforts, a student model may not accurately match the teacher’s performance, especially on complex tasks that require fine-grained reasoning.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;2. Architecture Mismatch:&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;\n &lt;em&gt;If the student model is too different from the teacher in design, it may struggle to “understand” what the teacher is trying to teach.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;3. Training Overhead:&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;\n &lt;em&gt;Training a good student model still takes time, data, and effort; it’s not a simple copy-paste job. And sometimes, tuning distillation hyperparameters (such as temperature or alpha) can be tricky.&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;Popular Tools and Frameworks &lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Hugging Face:&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;\n &lt;em&gt;Models like DistilBERT are smaller and faster versions of BERT, trained via distillation.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;TinyML:&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;\n &lt;em&gt;This focuses on deploying distilled models on ultra-low-power device&lt;/em&gt;&lt;strong&gt;&lt;em&gt;s,&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;such as microcontrollers, think smartwatches or IoT sensors.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;OpenVINO / TensorRT:&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;\n &lt;em&gt;These are optimization toolkits by Intel and NVIDIA that pair well with distilled models to extract every last bit of performance from them on CPUs and GPUs.&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;Summary&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;I was genuinely amazed when I first learned about model distillation.&lt;/em&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;In my case, I applied model distillation while building a model specifically for the DevOps field. I had a set of DevOps-related questions, but I didn’t have high-quality answers. So, I used GPT-o3 (yes, it did cost me) to generate expert-level responses. Once I had those, I used them to train a smaller model that could perform well without relying on GPT o3 every time. I’ll share the code for this in a future post.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Even DeepSeek has mentioned using model distillation as part of their training strategy for smaller models&lt;/em&gt; &lt;a href=\"https://www.cnbc.com/2025/02/21/deepseek-trained-ai-model-using-distillation-now-a-disruptive-force.html\"&gt;https://www.cnbc.com/2025/02/21/deepseek-trained-ai-model-using-distillation-now-a-disruptive-force.html&lt;/a&gt;&lt;em&gt;. It’s a great example of how powerful this technique can be.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Distillation initially felt like a complex idea, but I’ve done my best to break it down into simple language.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/gb7m4qcvvylH6NNx0ZMIbw8Owvba0Plr3imkJqRcZso.jpeg?auto=webp&amp;s=341f04e68194816c34b91cdfc3d1dddd1771d758",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/gb7m4qcvvylH6NNx0ZMIbw8Owvba0Plr3imkJqRcZso.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=02c27e5c55a67614952db7dbeb132f79dd07ebdc",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/gb7m4qcvvylH6NNx0ZMIbw8Owvba0Plr3imkJqRcZso.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c2dd488510b71e347975d81bc4bec580796b005",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/gb7m4qcvvylH6NNx0ZMIbw8Owvba0Plr3imkJqRcZso.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fea728b6a1a393a60b9c13b61462594e8c6a44b9",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/gb7m4qcvvylH6NNx0ZMIbw8Owvba0Plr3imkJqRcZso.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=447cd45569e48abb1712ef23c081e58320d929d0",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/gb7m4qcvvylH6NNx0ZMIbw8Owvba0Plr3imkJqRcZso.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=accc27ec612995f2ac5caec6276c2d7dd87a24d1",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/gb7m4qcvvylH6NNx0ZMIbw8Owvba0Plr3imkJqRcZso.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e8d8783d97b387cad9102615143bd956e192d6ff",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "gb7m4qcvvylH6NNx0ZMIbw8Owvba0Plr3imkJqRcZso"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lrv48g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Prashant-Lakhera",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrv48g/day_1050_building_a_small_language_model_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrv48g/day_1050_building_a_small_language_model_from/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751668085,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI’m building a MOE optimized llm inference rig.\n\nMy plans currently are \nGPU: 2x 5090’s (FE’s I got msrp from Best Buy)\nCPU: threadripper 7000 pro series\nMotherboard: trx50 or wrx 90\nMemory: 512gb ddr5\nCase: ideally rack mountable, not sure\n\nMy performance target is a min of 20 t/s generation with DEEPSEEK R1 5028 @q4 with full 128k context\n\nAny suggestions or thoughts? ",
          "author_fullname": "t2_eqtnew30",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building MOE inference Optimized workstation with 2 5090’s",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsipdy",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751745746,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I’m building a MOE optimized llm inference rig.&lt;/p&gt;\n\n&lt;p&gt;My plans currently are \nGPU: 2x 5090’s (FE’s I got msrp from Best Buy)\nCPU: threadripper 7000 pro series\nMotherboard: trx50 or wrx 90\nMemory: 512gb ddr5\nCase: ideally rack mountable, not sure&lt;/p&gt;\n\n&lt;p&gt;My performance target is a min of 20 t/s generation with DEEPSEEK R1 5028 @q4 with full 128k context&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or thoughts? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsipdy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "novel_market_21",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsipdy/building_moe_inference_optimized_workstation_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsipdy/building_moe_inference_optimized_workstation_with/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751745746,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The `~/.cache/huggingface` location is where a lot of stuff gets stored (on Windows it's `$HOME\\.cache\\huggingface`). You could just delete it every so often, but then you'll be re-downloading stuff you use.\n\n**How to:**\n\n1. `uv pip install 'huggingface_hub[cli]'` ([use uv](https://docs.astral.sh/uv/) it's worth it)\n2. Run `huggingface-cli scan-cache`. It'll show you all the model files you have downloaded.\n3. Run `huggingface-cli delete-cache`. This shows you a TUI that lets you select which models to delete.\n\nI recovered several hundred GBs by clearing out model files I hadn't used in a while. I'm sure `google/t5-v1_1-xxl` was worth the 43GB when I was doing something with it, but I'm happy to delete it now and get the space back.",
          "author_fullname": "t2_5rvpo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Run `huggingface-cli scan-cache` occasionally to see what models are taking up space. Then run `huggingface-cli delete-cache` to delete the ones you don't use. (See text post)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrrvva",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 27,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 27,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751659036,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The &lt;code&gt;~/.cache/huggingface&lt;/code&gt; location is where a lot of stuff gets stored (on Windows it&amp;#39;s &lt;code&gt;$HOME\\.cache\\huggingface&lt;/code&gt;). You could just delete it every so often, but then you&amp;#39;ll be re-downloading stuff you use.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How to:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;code&gt;uv pip install &amp;#39;huggingface_hub[cli]&amp;#39;&lt;/code&gt; (&lt;a href=\"https://docs.astral.sh/uv/\"&gt;use uv&lt;/a&gt; it&amp;#39;s worth it)&lt;/li&gt;\n&lt;li&gt;Run &lt;code&gt;huggingface-cli scan-cache&lt;/code&gt;. It&amp;#39;ll show you all the model files you have downloaded.&lt;/li&gt;\n&lt;li&gt;Run &lt;code&gt;huggingface-cli delete-cache&lt;/code&gt;. This shows you a TUI that lets you select which models to delete.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I recovered several hundred GBs by clearing out model files I hadn&amp;#39;t used in a while. I&amp;#39;m sure &lt;code&gt;google/t5-v1_1-xxl&lt;/code&gt; was worth the 43GB when I was doing something with it, but I&amp;#39;m happy to delete it now and get the space back.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lrrvva",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "The_frozen_one",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrrvva/run_huggingfacecli_scancache_occasionally_to_see/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrrvva/run_huggingfacecli_scancache_occasionally_to_see/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751659036,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Many AI models are built for lower CUDA versions, mostly 12.1-12.2 \nWhy wouldn't I just buy 2x3090 that will end up with pretty much same speed with bigger vRAM?",
          "author_fullname": "t2_qrnpi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why 5090 for inference if min CUDA is 12.9",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsm0ua",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.31,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751754862,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Many AI models are built for lower CUDA versions, mostly 12.1-12.2 \nWhy wouldn&amp;#39;t I just buy 2x3090 that will end up with pretty much same speed with bigger vRAM?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsm0ua",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VihmaVillu",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsm0ua/why_5090_for_inference_if_min_cuda_is_129/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsm0ua/why_5090_for_inference_if_min_cuda_is_129/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751754862,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Unlike Claude Code, [Gemini CLI is open source](https://github.com/google-gemini/gemini-cli/tree/main). Wouldn’t it be interesting to fork it and extend it to support other models, similar to what Aider provides?",
          "author_fullname": "t2_k2tavofw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gemini CLI is open source. Could we fork it to be able to use other models ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lroonm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 41,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 41,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751653106,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751650799,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Unlike Claude Code, &lt;a href=\"https://github.com/google-gemini/gemini-cli/tree/main\"&gt;Gemini CLI is open source&lt;/a&gt;. Wouldn’t it be interesting to fork it and extend it to support other models, similar to what Aider provides?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lroonm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SubliminalPoet",
          "discussion_type": null,
          "num_comments": 37,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lroonm/gemini_cli_is_open_source_could_we_fork_it_to_be/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lroonm/gemini_cli_is_open_source_could_we_fork_it_to_be/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751650799,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Fixed title: Asking LLMs for data visualized as plots\n\nHi, I'm looking for an app (e.g. LM Studio) + LLM solution that allows me to visualize LLM-generated data.\n\nI often ask LLM questions that returns some form of numerical data. For example, I might ask \"what's the world's population over time\" or \"what's the population by country in 2000\", which might return me a table with some data. This data is better visualized as a plot (e.g. bar graph).\n\nAre there models that might return plots (which I guess is a form of image)? I am aware of [https://github.com/nyanp/chat2plot](chat2plot), but are there others? Are there ones which can simply plug into a generalist app like LM Studio (afaik, LM Studio doesn't output graphics. Is that true?)?\n\nI'm pretty new to self-hosted local LLMs so pardon me if I'm missing something obvious!",
          "author_fullname": "t2_6czvm5cr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Asking LLMs data visualized as plots",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls663p",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751711905,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751708626,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Fixed title: Asking LLMs for data visualized as plots&lt;/p&gt;\n\n&lt;p&gt;Hi, I&amp;#39;m looking for an app (e.g. LM Studio) + LLM solution that allows me to visualize LLM-generated data.&lt;/p&gt;\n\n&lt;p&gt;I often ask LLM questions that returns some form of numerical data. For example, I might ask &amp;quot;what&amp;#39;s the world&amp;#39;s population over time&amp;quot; or &amp;quot;what&amp;#39;s the population by country in 2000&amp;quot;, which might return me a table with some data. This data is better visualized as a plot (e.g. bar graph).&lt;/p&gt;\n\n&lt;p&gt;Are there models that might return plots (which I guess is a form of image)? I am aware of [&lt;a href=\"https://github.com/nyanp/chat2plot%5D(chat2plot)\"&gt;https://github.com/nyanp/chat2plot](chat2plot)&lt;/a&gt;, but are there others? Are there ones which can simply plug into a generalist app like LM Studio (afaik, LM Studio doesn&amp;#39;t output graphics. Is that true?)?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m pretty new to self-hosted local LLMs so pardon me if I&amp;#39;m missing something obvious!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/EmPlmkUgK-psVbhlsUSnHxl3YPY4gyS7RTnvxPH48b4.png?auto=webp&amp;s=7723e569af1e8ff257ee3db7f222bb80e3ae7678",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/EmPlmkUgK-psVbhlsUSnHxl3YPY4gyS7RTnvxPH48b4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=83f44c8691982ad244b150d65aff459fb5af56ed",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/EmPlmkUgK-psVbhlsUSnHxl3YPY4gyS7RTnvxPH48b4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ba32ca7ee6ca6f824a135a3c17fdb257ffbfb32",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/EmPlmkUgK-psVbhlsUSnHxl3YPY4gyS7RTnvxPH48b4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4a8f6c4e384662e5c474a588b61568eafdfe0fc4",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/EmPlmkUgK-psVbhlsUSnHxl3YPY4gyS7RTnvxPH48b4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e7223e9ff4aeb45d9d61a3c8499d357e4157c31",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/EmPlmkUgK-psVbhlsUSnHxl3YPY4gyS7RTnvxPH48b4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c34d17676d9faf29f0b5eed5020de03d3f79c45d",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/EmPlmkUgK-psVbhlsUSnHxl3YPY4gyS7RTnvxPH48b4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=082296ef5dab26f9655e3f6205d204df6ae67499",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "EmPlmkUgK-psVbhlsUSnHxl3YPY4gyS7RTnvxPH48b4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ls663p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "injeolmi-bingsoo",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls663p/asking_llms_data_visualized_as_plots/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls663p/asking_llms_data_visualized_as_plots/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751708626,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m not entirely certain what it is, people recommend using it sometimes while recommending against it other times.\n\nWhat is NVlink and what’s the difference against just plugging two cards into the motherboard?\n\nDoes it require more hardware? I heard stuff about a bridge? How does that work?\n\nWhat about AMD cards, given it’s called nvlink, I assume it’s only for nvidia, is there an amd version of this?\n\nWhat are the performance differences if I have a system with nvlink and one without but the specs are the same?",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is NVLink?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls3pkv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751698380,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m not entirely certain what it is, people recommend using it sometimes while recommending against it other times.&lt;/p&gt;\n\n&lt;p&gt;What is NVlink and what’s the difference against just plugging two cards into the motherboard?&lt;/p&gt;\n\n&lt;p&gt;Does it require more hardware? I heard stuff about a bridge? How does that work?&lt;/p&gt;\n\n&lt;p&gt;What about AMD cards, given it’s called nvlink, I assume it’s only for nvidia, is there an amd version of this?&lt;/p&gt;\n\n&lt;p&gt;What are the performance differences if I have a system with nvlink and one without but the specs are the same?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ls3pkv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls3pkv/what_is_nvlink/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls3pkv/what_is_nvlink/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751698380,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Need help, I am running a series of full fine-tuning on Llama 2 7B hf with unsloth. For some time, it was working just fine, and then this happened. I didn't notice until after the training was completed. I was sure of the training script because I had previously executed it with a slightly different setting (I modified how many checkpoints to save), and it was running with no problem at all. I ran all the trainings on the same GPU card, RTX A6000.\n\nRun A\n\n[Run A](https://preview.redd.it/1jvncyciv1bf1.png?width=1801&amp;format=png&amp;auto=webp&amp;s=c52ffc1ddf585adfb0690dbffaac99b58017d7c8)\n\nRun B\n\n[Run B](https://preview.redd.it/5myd8r0lv1bf1.png?width=1808&amp;format=png&amp;auto=webp&amp;s=74cda16524182268aa8c358f7e55744774ecd97e)\n\nOn some other models (this one with Gemma), after some time with the same script it returns this error:  \n`/tmp/torchinductor_user/ey/cey6r66b2emihdiuktnmimfzgbacyvafuvx2vlr4kpbmybs2o63r.py:45: unknown: block: [0,0,0], thread: [5,0,0] Assertion \\`index out of bounds: 0 &lt;= tmp8 &lt; ks0\\` failed.`\n\nI suppose that can be what caused the grad norm to become 0 in the llama model? Currently, I have no other clue outside of this.\n\nHere are the parameters that I am using:\n\n                per_device_train_batch_size = 1,\n                gradient_accumulation_steps = 16,\n                learning_rate = 5e-5,\n                lr_scheduler_type = \"linear\",\n                embedding_learning_rate = 1e-5,\n                warmup_ratio = 0.1,\n                epochs = 1,\n                fp16 = not is_bfloat16_supported(),\n                bf16 = is_bfloat16_supported(),\n                optim = \"adamw_8bit\",\n                weight_decay = 0.01,\n                seed = 3407,\n                logging_steps = 1,\n                report_to = \"wandb\",\n                output_dir = output_path,\n                save_strategy=\"steps\",\n                save_steps=total_steps // 10,\n                save_total_limit=11,\n                save_safetensors=True,\n\nThe difference between run A and run B is the number of layers trained. I am training multiple models with each different number of unfrozen layers. So for some reason, the ones with high trainable parameter counts always fail this way. How can I debug this and what might've caused this? Any suggestions/helps would be greatly appreciated! Thank you",
          "author_fullname": "t2_1hv8pwe507",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why do grad norm sink to 0 (at least I think) randomly during unsloth full finetuning?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 42,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "5myd8r0lv1bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 32,
                  "x": 108,
                  "u": "https://preview.redd.it/5myd8r0lv1bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d144ade08b2c96f586214413c11297979ec8d4d"
                },
                {
                  "y": 65,
                  "x": 216,
                  "u": "https://preview.redd.it/5myd8r0lv1bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8bfda56b2d1181a124b4a1dd8749cd0962a070fb"
                },
                {
                  "y": 97,
                  "x": 320,
                  "u": "https://preview.redd.it/5myd8r0lv1bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6c3a7f9a8335b8c1a04f2da73b9dd10be57dddb2"
                },
                {
                  "y": 195,
                  "x": 640,
                  "u": "https://preview.redd.it/5myd8r0lv1bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=422d6eb34940d5d70a88cf024964863027609079"
                },
                {
                  "y": 292,
                  "x": 960,
                  "u": "https://preview.redd.it/5myd8r0lv1bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c87aaefafa376baf550f1c602538410113e2eff"
                },
                {
                  "y": 329,
                  "x": 1080,
                  "u": "https://preview.redd.it/5myd8r0lv1bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7a6f69250b0e96031240201ba666fb27ad9f4f22"
                }
              ],
              "s": {
                "y": 551,
                "x": 1808,
                "u": "https://preview.redd.it/5myd8r0lv1bf1.png?width=1808&amp;format=png&amp;auto=webp&amp;s=74cda16524182268aa8c358f7e55744774ecd97e"
              },
              "id": "5myd8r0lv1bf1"
            },
            "1jvncyciv1bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 32,
                  "x": 108,
                  "u": "https://preview.redd.it/1jvncyciv1bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ddc391d0cb5045ec9b8a02bbabdc84c80f399bb"
                },
                {
                  "y": 65,
                  "x": 216,
                  "u": "https://preview.redd.it/1jvncyciv1bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5ee500470b397366b447b7782723b17a9523f7f"
                },
                {
                  "y": 97,
                  "x": 320,
                  "u": "https://preview.redd.it/1jvncyciv1bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=23533cd9aae59ca64b2508d127fea3b8ed93cd79"
                },
                {
                  "y": 195,
                  "x": 640,
                  "u": "https://preview.redd.it/1jvncyciv1bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=61320140882f8e401cdf9b01be3a1c9099a81435"
                },
                {
                  "y": 293,
                  "x": 960,
                  "u": "https://preview.redd.it/1jvncyciv1bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=94333bdb4b621043395fab513635aac40e82a221"
                },
                {
                  "y": 329,
                  "x": 1080,
                  "u": "https://preview.redd.it/1jvncyciv1bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c232f09941d21e18e262241ac45390eada31dc2b"
                }
              ],
              "s": {
                "y": 550,
                "x": 1801,
                "u": "https://preview.redd.it/1jvncyciv1bf1.png?width=1801&amp;format=png&amp;auto=webp&amp;s=c52ffc1ddf585adfb0690dbffaac99b58017d7c8"
              },
              "id": "1jvncyciv1bf1"
            }
          },
          "name": "t3_1ls91w3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/-WaLKKKHOW63KWzApJwmrCQ0HG8GbQsikPkjj1sphR8.jpg",
          "edited": 1751722984,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751719489,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need help, I am running a series of full fine-tuning on Llama 2 7B hf with unsloth. For some time, it was working just fine, and then this happened. I didn&amp;#39;t notice until after the training was completed. I was sure of the training script because I had previously executed it with a slightly different setting (I modified how many checkpoints to save), and it was running with no problem at all. I ran all the trainings on the same GPU card, RTX A6000.&lt;/p&gt;\n\n&lt;p&gt;Run A&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1jvncyciv1bf1.png?width=1801&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c52ffc1ddf585adfb0690dbffaac99b58017d7c8\"&gt;Run A&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Run B&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5myd8r0lv1bf1.png?width=1808&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=74cda16524182268aa8c358f7e55744774ecd97e\"&gt;Run B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;On some other models (this one with Gemma), after some time with the same script it returns this error:&lt;br/&gt;\n&lt;code&gt;/tmp/torchinductor_user/ey/cey6r66b2emihdiuktnmimfzgbacyvafuvx2vlr4kpbmybs2o63r.py:45: unknown: block: [0,0,0], thread: [5,0,0] Assertion \\&lt;/code&gt;index out of bounds: 0 &amp;lt;= tmp8 &amp;lt; ks0` failed.`&lt;/p&gt;\n\n&lt;p&gt;I suppose that can be what caused the grad norm to become 0 in the llama model? Currently, I have no other clue outside of this.&lt;/p&gt;\n\n&lt;p&gt;Here are the parameters that I am using:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;            per_device_train_batch_size = 1,\n            gradient_accumulation_steps = 16,\n            learning_rate = 5e-5,\n            lr_scheduler_type = &amp;quot;linear&amp;quot;,\n            embedding_learning_rate = 1e-5,\n            warmup_ratio = 0.1,\n            epochs = 1,\n            fp16 = not is_bfloat16_supported(),\n            bf16 = is_bfloat16_supported(),\n            optim = &amp;quot;adamw_8bit&amp;quot;,\n            weight_decay = 0.01,\n            seed = 3407,\n            logging_steps = 1,\n            report_to = &amp;quot;wandb&amp;quot;,\n            output_dir = output_path,\n            save_strategy=&amp;quot;steps&amp;quot;,\n            save_steps=total_steps // 10,\n            save_total_limit=11,\n            save_safetensors=True,\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The difference between run A and run B is the number of layers trained. I am training multiple models with each different number of unfrozen layers. So for some reason, the ones with high trainable parameter counts always fail this way. How can I debug this and what might&amp;#39;ve caused this? Any suggestions/helps would be greatly appreciated! Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ls91w3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Old-Acanthisitta-574",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls91w3/why_do_grad_norm_sink_to_0_at_least_i_think/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls91w3/why_do_grad_norm_sink_to_0_at_least_i_think/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751719489,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "When humanity gets to the point where humanoid robots are advanced enough to do household tasks and be personal companions, do you think their AIs will be local or will they have to be connected to the internet?\n\nHow difficult would it be to fit the gpus or hardware needed to run the best local llms/voice to voice models in a robot? You could have smaller hardware, but I assume the people that spend tens of thousands of dollars on a robot would want the AI to be basically SOTA, since the robot will likely also be used to answer questions they normally ask AIs like chatgpt.",
          "author_fullname": "t2_1j5ctch7o1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Will commercial humanoid robots ever use local AI?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls0r0w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.61,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751700631,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751687088,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When humanity gets to the point where humanoid robots are advanced enough to do household tasks and be personal companions, do you think their AIs will be local or will they have to be connected to the internet?&lt;/p&gt;\n\n&lt;p&gt;How difficult would it be to fit the gpus or hardware needed to run the best local llms/voice to voice models in a robot? You could have smaller hardware, but I assume the people that spend tens of thousands of dollars on a robot would want the AI to be basically SOTA, since the robot will likely also be used to answer questions they normally ask AIs like chatgpt.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ls0r0w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Bristull",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls0r0w/will_commercial_humanoid_robots_ever_use_local_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls0r0w/will_commercial_humanoid_robots_ever_use_local_ai/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751687088,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am getting openbox for $8369 which I guess is a good deal.\n\nMy main concern is the cooling system used here. These machine are made for gaming. I am unable to find more details around the same.  \n",
          "author_fullname": "t2_1a2muq8q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is this a good machine for running local LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 139,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsl0qn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.35,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/4Prpmv5k9ilR84zrkZQoWErvYr1ZQ0cEBTuEbJ1_8jA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751752029,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am getting openbox for $8369 which I guess is a good deal.&lt;/p&gt;\n\n&lt;p&gt;My main concern is the cooling system used here. These machine are made for gaming. I am unable to find more details around the same.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/4jpzeysml4bf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/4jpzeysml4bf1.png?auto=webp&amp;s=674f4fefa02a9b25a520317ef1bf19f3c819a21d",
                  "width": 1036,
                  "height": 1031
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/4jpzeysml4bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a72bd197759ab38dd0e7123700230fafe91d9594",
                    "width": 108,
                    "height": 107
                  },
                  {
                    "url": "https://preview.redd.it/4jpzeysml4bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e645daff891b5cb84ec210f929e87c16bfce1368",
                    "width": 216,
                    "height": 214
                  },
                  {
                    "url": "https://preview.redd.it/4jpzeysml4bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e976953439fb2d6784045c062540a94df3642f42",
                    "width": 320,
                    "height": 318
                  },
                  {
                    "url": "https://preview.redd.it/4jpzeysml4bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=56a76190c33c310dc40b05b508084e82d2a2ddcd",
                    "width": 640,
                    "height": 636
                  },
                  {
                    "url": "https://preview.redd.it/4jpzeysml4bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a34d8e421b61879fa78f40997d220c3666f2261",
                    "width": 960,
                    "height": 955
                  }
                ],
                "variants": {},
                "id": "wREbGXd9aiVb0xMfsjG_Zu1XHWOZ63-rpNoDuvblS-o"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsl0qn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sudocode14",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsl0qn/is_this_a_good_machine_for_running_local_llms/",
          "stickied": false,
          "url": "https://i.redd.it/4jpzeysml4bf1.png",
          "subreddit_subscribers": 494897,
          "created_utc": 1751752029,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "TLDR Personally, I suck at CLI troubleshooting, I realized I will now happily trade away some token speed for a more simple and intuitive UI/UX\n\nI'm very new to Linux as well as local LLMs, finally switched over to Linux just last week from Windows 10. I have basically zero CLI experience.\n\nFew days ago, I started having trouble with Ollama. One night, I was getting 4 t/s with unsloth's Deepseek R1 0528 684b Q4, then the next day 0.15 t/s... Model generation speeds were painfully slow and inconsistent. Many here on the sub suggested that I switch over from ollama to llama.cpp or ik\\_llama.cpp, so I gave both a try.\n\nThe performance difference of llama.cpp / ik\\_llama.cpp over ollama is absolutely nuts. So running unsloth's Deepseek R1-0528 684B at Q4 (with Threadripper, 512gb DDR4 RAM, and dual 3090s), I got:\n\n* **Ollama**: 0.15 t/s - absolutely terrible\n* **llama.cpp** (through LM Studio): \\~4.7 t/s - massive improvement\n* **ik\\_llama.cpp**: \\~7.6 t/s!! 60% faster than LM Studio, and literally FIFTY times faster than ollama\n\nSounds absolutely amazing, BUT there was a huge catch I didn't know at first.\n\nThe learning curve is incredibly steep, especially for a noob like me. I spent WAY more time troubleshooting errors, crashes, scouring online, GH, [r/llocalllama](https://www.reddit.com/r/llocalllama/), asking other users, and hunting for obscure fixes than time actually using the models. I actually copied someone else's ik\\_llama.cpp build set up and server run command to use Deepseek 0528, and it ran smoothly. But the moment I try to run any other model, even 20b, 30b or 70b parametermodel, things quickly went downhill. Memory failures, crashes, cryptic error logs. Many hours spent looking for solutions online, or asking CGPT / Deepseek for insight. Sometimes getting lucky with a solution, and other times just giving up altogether. Also hard to optimize for different models with my hardware, as I have no idea what the dozens of flags, commands, and parameters do even after reading the llama-server --help stuff.\n\nI realized one important thing that's obvious now but didn't think of earlier. What works for one user doesn't always scale to other users (or noobs like me lol). While many suggested ik\\_llama.cpp, there's not always blanket solution that fits all. Perhaps not everyone needs to move to the absolute fastest backend. There's also a ton of great advice out there or troubleshooting tips, but some of it is definitely geared toward power users that understand things like what happens and why it happens when randomparameter=1, when to turn various parameters off, flag this, tensor that, re-build with this flag, CUDA that, offload this here, don't offload that thing in this specific situation. Reading some of the CLI help I found felt like reading another language, felt super lost.\n\nOn the flip side, LM Studio was genuinely plug and play. Felt very intuitive, stable, and it just worked out of the box. I didn't encounter any crashes, or error logs to navigate. Practically zero command line stuff after install. Downloading, loading, and swapping models is SO easy in LMS. Front end + back end packaged together. Sure, it's not the fastest, but right now I will take the usability and speed hit over hours of troubleshooting chaos.\n\nFor now, I'm probably going to daily drive LM Studio, while slowly working through the steep CLI learning curve on the side. Not an LM Studio ad btw lol. Hopefully one day I can earn my CLI blue belt lol. Thanks for letting me rant.",
          "author_fullname": "t2_vct0oav1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you guys balance speed versus ease and usability?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrtv8u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751664501,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR Personally, I suck at CLI troubleshooting, I realized I will now happily trade away some token speed for a more simple and intuitive UI/UX&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m very new to Linux as well as local LLMs, finally switched over to Linux just last week from Windows 10. I have basically zero CLI experience.&lt;/p&gt;\n\n&lt;p&gt;Few days ago, I started having trouble with Ollama. One night, I was getting 4 t/s with unsloth&amp;#39;s Deepseek R1 0528 684b Q4, then the next day 0.15 t/s... Model generation speeds were painfully slow and inconsistent. Many here on the sub suggested that I switch over from ollama to llama.cpp or ik_llama.cpp, so I gave both a try.&lt;/p&gt;\n\n&lt;p&gt;The performance difference of llama.cpp / ik_llama.cpp over ollama is absolutely nuts. So running unsloth&amp;#39;s Deepseek R1-0528 684B at Q4 (with Threadripper, 512gb DDR4 RAM, and dual 3090s), I got:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: 0.15 t/s - absolutely terrible&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;llama.cpp&lt;/strong&gt; (through LM Studio): ~4.7 t/s - massive improvement&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;ik_llama.cpp&lt;/strong&gt;: ~7.6 t/s!! 60% faster than LM Studio, and literally FIFTY times faster than ollama&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Sounds absolutely amazing, BUT there was a huge catch I didn&amp;#39;t know at first.&lt;/p&gt;\n\n&lt;p&gt;The learning curve is incredibly steep, especially for a noob like me. I spent WAY more time troubleshooting errors, crashes, scouring online, GH, &lt;a href=\"https://www.reddit.com/r/llocalllama/\"&gt;r/llocalllama&lt;/a&gt;, asking other users, and hunting for obscure fixes than time actually using the models. I actually copied someone else&amp;#39;s ik_llama.cpp build set up and server run command to use Deepseek 0528, and it ran smoothly. But the moment I try to run any other model, even 20b, 30b or 70b parametermodel, things quickly went downhill. Memory failures, crashes, cryptic error logs. Many hours spent looking for solutions online, or asking CGPT / Deepseek for insight. Sometimes getting lucky with a solution, and other times just giving up altogether. Also hard to optimize for different models with my hardware, as I have no idea what the dozens of flags, commands, and parameters do even after reading the llama-server --help stuff.&lt;/p&gt;\n\n&lt;p&gt;I realized one important thing that&amp;#39;s obvious now but didn&amp;#39;t think of earlier. What works for one user doesn&amp;#39;t always scale to other users (or noobs like me lol). While many suggested ik_llama.cpp, there&amp;#39;s not always blanket solution that fits all. Perhaps not everyone needs to move to the absolute fastest backend. There&amp;#39;s also a ton of great advice out there or troubleshooting tips, but some of it is definitely geared toward power users that understand things like what happens and why it happens when randomparameter=1, when to turn various parameters off, flag this, tensor that, re-build with this flag, CUDA that, offload this here, don&amp;#39;t offload that thing in this specific situation. Reading some of the CLI help I found felt like reading another language, felt super lost.&lt;/p&gt;\n\n&lt;p&gt;On the flip side, LM Studio was genuinely plug and play. Felt very intuitive, stable, and it just worked out of the box. I didn&amp;#39;t encounter any crashes, or error logs to navigate. Practically zero command line stuff after install. Downloading, loading, and swapping models is SO easy in LMS. Front end + back end packaged together. Sure, it&amp;#39;s not the fastest, but right now I will take the usability and speed hit over hours of troubleshooting chaos.&lt;/p&gt;\n\n&lt;p&gt;For now, I&amp;#39;m probably going to daily drive LM Studio, while slowly working through the steep CLI learning curve on the side. Not an LM Studio ad btw lol. Hopefully one day I can earn my CLI blue belt lol. Thanks for letting me rant.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lrtv8u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sourpatchgrownadults",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrtv8u/how_do_you_guys_balance_speed_versus_ease_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrtv8u/how_do_you_guys_balance_speed_versus_ease_and/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751664501,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Exploring an idea, potentially to expand a collection of data from Meshtastic nodes, but looking to keep it really simple/see what is possible. \n\nI don't know if it's going to be like an abridged version of the Farmers Almanac, but I'm curious if there's AI tools that can evaluate offgrid meteorological readings like temp, humidity, pressure, and calculate dewpoint, rain/storms, tornado risk, snow, etc. \n\n",
          "author_fullname": "t2_12koak",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any models with weather forecast automation?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrzmk8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751682987,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Exploring an idea, potentially to expand a collection of data from Meshtastic nodes, but looking to keep it really simple/see what is possible. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know if it&amp;#39;s going to be like an abridged version of the Farmers Almanac, but I&amp;#39;m curious if there&amp;#39;s AI tools that can evaluate offgrid meteorological readings like temp, humidity, pressure, and calculate dewpoint, rain/storms, tornado risk, snow, etc. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lrzmk8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "techtornado",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrzmk8/any_models_with_weather_forecast_automation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrzmk8/any_models_with_weather_forecast_automation/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751682987,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone know what the difference in tps would be for 64g mini pro vs 64g Studio since the studio has more gpu cores, but is it a meaningful difference for tps.  I'm getting 5.4 tps on 70b on the mini. Curious if it's worth going to the studio ",
          "author_fullname": "t2_1gzvdilba8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "M4 Mini pro Vs M4 Studio",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrz52e",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751681234,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone know what the difference in tps would be for 64g mini pro vs 64g Studio since the studio has more gpu cores, but is it a meaningful difference for tps.  I&amp;#39;m getting 5.4 tps on 70b on the mini. Curious if it&amp;#39;s worth going to the studio &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lrz52e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AlgorithmicMuse",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751681234,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The model Kwai Keye VL 8B is available on Huggingface with Apache 2.0 license. It has been built by Kuaishou (1st time I hear of them) on top of Qwen 3 8B and combines it with SigLIP-400M.\n\nTheir paper is truly a gem as they detail their pretraining and post-training methodology exhaustively. Haven't tested it yet, but their evaluation seems pretty solid.",
          "author_fullname": "t2_6mscsckt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kwai Keye VL 8B - Very promising new VL model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrlags",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1751642298,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The model Kwai Keye VL 8B is available on Huggingface with Apache 2.0 license. It has been built by Kuaishou (1st time I hear of them) on top of Qwen 3 8B and combines it with SigLIP-400M.&lt;/p&gt;\n\n&lt;p&gt;Their paper is truly a gem as they detail their pretraining and post-training methodology exhaustively. Haven&amp;#39;t tested it yet, but their evaluation seems pretty solid.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.01949",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lrlags",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pol_phil",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrlags/kwai_keye_vl_8b_very_promising_new_vl_model/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.01949",
          "subreddit_subscribers": 494897,
          "created_utc": 1751642298,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The Model Context Protocol has faced a lot of criticism due to its security vulnerabilities. Anthropic recently released a new Spec Update (`MCP v2025-06-18`) and I have been reviewing it, especially around security. Here are the important changes you should know.\n\n1. MCP servers are classified as OAuth 2.0 Resource Servers.\n2. Clients must include a `resource` parameter (RFC 8707) when requesting tokens, this explicitly binds each access token to a specific MCP server.\n3. Structured JSON tool output is now supported (`structuredContent`).\n4. Servers can now ask users for input mid-session by sending an \\`elicitation/create\\` request with a message and a JSON schema.\n5. “Security Considerations” have been added to prevent token theft, PKCE, redirect URIs, confused deputy issues.\n6. Newly added Security best practices page addresses threats like token passthrough, confused deputy, session hijacking, proxy misuse with concrete countermeasures.\n7. All HTTP requests now must include the `MCP-Protocol-Version` header. If the header is missing and the version can’t be inferred, servers should default to `2025-03-26` for backward compatibility.\n8. New `resource_link` type lets tools point to URIs instead of inlining everything. The client can then subscribe to or fetch this URI as needed.\n9. They removed JSON-RPC batching (not backward compatible). If your SDK or application was sending multiple JSON-RPC calls in a single batch request (an array), it will now break as MCP servers will reject it starting with version `2025-06-18`.\n\nIn the PR (#416), I found “no compelling use cases” for actually removing it. Official JSON-RPC documentation explicitly says a client `MAY send an Array` of requests and the server `SHOULD respond with an Array` of results. MCP’s new rule essentially forbids that.\n\nDetailed writeup: [here](https://forgecode.dev/blog/mcp-spec-updates/)\n\nWhat's your experience? Are you satisfied with the changes or still upset with the security risks?",
          "author_fullname": "t2_1hro18widg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MCP 2025-06-18 Spec Update: Security, Structured Output &amp; Elicitation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrgomi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 69,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 69,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1751629369,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "forgecode.dev",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Model Context Protocol has faced a lot of criticism due to its security vulnerabilities. Anthropic recently released a new Spec Update (&lt;code&gt;MCP v2025-06-18&lt;/code&gt;) and I have been reviewing it, especially around security. Here are the important changes you should know.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;MCP servers are classified as OAuth 2.0 Resource Servers.&lt;/li&gt;\n&lt;li&gt;Clients must include a &lt;code&gt;resource&lt;/code&gt; parameter (RFC 8707) when requesting tokens, this explicitly binds each access token to a specific MCP server.&lt;/li&gt;\n&lt;li&gt;Structured JSON tool output is now supported (&lt;code&gt;structuredContent&lt;/code&gt;).&lt;/li&gt;\n&lt;li&gt;Servers can now ask users for input mid-session by sending an `elicitation/create` request with a message and a JSON schema.&lt;/li&gt;\n&lt;li&gt;“Security Considerations” have been added to prevent token theft, PKCE, redirect URIs, confused deputy issues.&lt;/li&gt;\n&lt;li&gt;Newly added Security best practices page addresses threats like token passthrough, confused deputy, session hijacking, proxy misuse with concrete countermeasures.&lt;/li&gt;\n&lt;li&gt;All HTTP requests now must include the &lt;code&gt;MCP-Protocol-Version&lt;/code&gt; header. If the header is missing and the version can’t be inferred, servers should default to &lt;code&gt;2025-03-26&lt;/code&gt; for backward compatibility.&lt;/li&gt;\n&lt;li&gt;New &lt;code&gt;resource_link&lt;/code&gt; type lets tools point to URIs instead of inlining everything. The client can then subscribe to or fetch this URI as needed.&lt;/li&gt;\n&lt;li&gt;They removed JSON-RPC batching (not backward compatible). If your SDK or application was sending multiple JSON-RPC calls in a single batch request (an array), it will now break as MCP servers will reject it starting with version &lt;code&gt;2025-06-18&lt;/code&gt;.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;In the PR (#416), I found “no compelling use cases” for actually removing it. Official JSON-RPC documentation explicitly says a client &lt;code&gt;MAY send an Array&lt;/code&gt; of requests and the server &lt;code&gt;SHOULD respond with an Array&lt;/code&gt; of results. MCP’s new rule essentially forbids that.&lt;/p&gt;\n\n&lt;p&gt;Detailed writeup: &lt;a href=\"https://forgecode.dev/blog/mcp-spec-updates/\"&gt;here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s your experience? Are you satisfied with the changes or still upset with the security risks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://forgecode.dev/blog/mcp-spec-updates/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lrgomi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "anmolbaranwal",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrgomi/mcp_20250618_spec_update_security_structured/",
          "stickied": false,
          "url": "https://forgecode.dev/blog/mcp-spec-updates/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751629369,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My LLM server, [https://generativa.rapport.tec.br](https://generativa.rapport.tec.br), my goal is to set up LLM servers for companies and freelancers who demand confidentiality in their documents, thus allowing a secure and personalized RAG.",
          "author_fullname": "t2_jrhu3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "My LLM Server",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsl6p6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.17,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751752493,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My LLM server, &lt;a href=\"https://generativa.rapport.tec.br\"&gt;https://generativa.rapport.tec.br&lt;/a&gt;, my goal is to set up LLM servers for companies and freelancers who demand confidentiality in their documents, thus allowing a secure and personalized RAG.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lsl6p6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CarlosDelfino",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsl6p6/my_llm_server/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsl6p6/my_llm_server/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751752493,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am looking for an open‑source AI tool that can run locally on my computer (CPU only, no GPU) and process videos and images with the following functionality:\n\n1. The tool should take a video or image as input and output the same video/image with these options for blurring:\n   * Blur the entire body of all men.\n   * Blur the entire body of all women.\n   * Blur the entire bodies of both men and women.\n   * Always blur the entire bodies of anyone whose gender is ambiguous or unrecognized, regardless of the above options, to avoid misclassification.\n2. The rest of the video or image should remain completely untouched and retain original quality. For videos, the audio must be preserved exactly.\n3. The tool should be a command‑line program.\n4. It must run on a typical computer with CPU only (no GPU required).\n5. I plan to process one video or image at a time.\n6. I understand processing may take time, but ideally it would run as fast as possible, aiming for under about 2 minutes for a 10‑minute video if feasible.\n\nMy main priorities are:\n\n* Ease of use.\n* Reliable gender detection (with ambiguous people always blurred automatically).\n* Running fully locally without complicated setup or programming skills.\n\nTo be clear, I want the tool to blur the entire body of the targeted people (not just faces, but full bodies) while leaving everything else intact.\n\nDoes such a tool already exist? If not, are there open‑source components I could combine to build this? Explain clearly what I would need to do.",
          "author_fullname": "t2_jd7vr28ul",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for open-source tool to blur entire bodies by gender in videos/images",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsazjq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.42,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751725268,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for an open‑source AI tool that can run locally on my computer (CPU only, no GPU) and process videos and images with the following functionality:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The tool should take a video or image as input and output the same video/image with these options for blurring:\n\n&lt;ul&gt;\n&lt;li&gt;Blur the entire body of all men.&lt;/li&gt;\n&lt;li&gt;Blur the entire body of all women.&lt;/li&gt;\n&lt;li&gt;Blur the entire bodies of both men and women.&lt;/li&gt;\n&lt;li&gt;Always blur the entire bodies of anyone whose gender is ambiguous or unrecognized, regardless of the above options, to avoid misclassification.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;The rest of the video or image should remain completely untouched and retain original quality. For videos, the audio must be preserved exactly.&lt;/li&gt;\n&lt;li&gt;The tool should be a command‑line program.&lt;/li&gt;\n&lt;li&gt;It must run on a typical computer with CPU only (no GPU required).&lt;/li&gt;\n&lt;li&gt;I plan to process one video or image at a time.&lt;/li&gt;\n&lt;li&gt;I understand processing may take time, but ideally it would run as fast as possible, aiming for under about 2 minutes for a 10‑minute video if feasible.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My main priorities are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ease of use.&lt;/li&gt;\n&lt;li&gt;Reliable gender detection (with ambiguous people always blurred automatically).&lt;/li&gt;\n&lt;li&gt;Running fully locally without complicated setup or programming skills.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To be clear, I want the tool to blur the entire body of the targeted people (not just faces, but full bodies) while leaving everything else intact.&lt;/p&gt;\n\n&lt;p&gt;Does such a tool already exist? If not, are there open‑source components I could combine to build this? Explain clearly what I would need to do.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsazjq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DayOk2",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsazjq/looking_for_opensource_tool_to_blur_entire_bodies/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsazjq/looking_for_opensource_tool_to_blur_entire_bodies/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751725268,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all, I’m an AI/LLM enthusiast coming from a mobile dev background (iOS, Swift). I’ve been building a local inference engine, tailored for Metal-first, real-time inference on iOS (iPhone + iPad).\n\nI’ve been benchmarking on iPhone 16 and hitting what seem to be high token/s rates for 4-bit quantized models.\n\nCurrent Benchmarks (iPhone 16 Plus, all 4-bit):\n\nModel Size\t- Token/s Range\n0.5B–1.7B\t- 30–64 tok/s\n2B\t- 20–48 tok/s\n3B\t- 15–30 tok/s\n4B\t- 7–16 tok/s\n7B\t- often crashes due to RAM, 5–12 tok/s max\n\nI haven’t seen any PrivateLLM, MLC-LLM, or llama.cpp shipping these numbers with live UI streaming, so I’d love validation:\n\t1.\tiPhone 16 / 15 Pro users willing to test, can you reproduce these numbers on A17/A18?\n\t2. If you’ve profiled PrivateLLM or MLC at 2-3 B, please drop raw tok/s + device specs.\n\nHappy to share build structure and testing info if helpful. \nThanks!",
          "author_fullname": "t2_oislx9iru",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "30-60tok/s on 4bit local LLM, iPhone 16.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 40,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lreu44",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 80,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 80,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/FRoKmGK7Jsk92A_ter7XKx0dDaEhWFB3h70lzXi7gfQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751622653,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I’m an AI/LLM enthusiast coming from a mobile dev background (iOS, Swift). I’ve been building a local inference engine, tailored for Metal-first, real-time inference on iOS (iPhone + iPad).&lt;/p&gt;\n\n&lt;p&gt;I’ve been benchmarking on iPhone 16 and hitting what seem to be high token/s rates for 4-bit quantized models.&lt;/p&gt;\n\n&lt;p&gt;Current Benchmarks (iPhone 16 Plus, all 4-bit):&lt;/p&gt;\n\n&lt;p&gt;Model Size  - Token/s Range\n0.5B–1.7B - 30–64 tok/s\n2B  - 20–48 tok/s\n3B  - 15–30 tok/s\n4B  - 7–16 tok/s\n7B  - often crashes due to RAM, 5–12 tok/s max&lt;/p&gt;\n\n&lt;p&gt;I haven’t seen any PrivateLLM, MLC-LLM, or llama.cpp shipping these numbers with live UI streaming, so I’d love validation:\n    1.  iPhone 16 / 15 Pro users willing to test, can you reproduce these numbers on A17/A18?\n    2. If you’ve profiled PrivateLLM or MLC at 2-3 B, please drop raw tok/s + device specs.&lt;/p&gt;\n\n&lt;p&gt;Happy to share build structure and testing info if helpful. \nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/1pi871kgxtaf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/1pi871kgxtaf1.jpeg?auto=webp&amp;s=1ca1ec7ec43d159eb569e75c989fbdf91df20d73",
                  "width": 1290,
                  "height": 371
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/1pi871kgxtaf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c20d57ad84da56626439ad72013e3790e5404e9a",
                    "width": 108,
                    "height": 31
                  },
                  {
                    "url": "https://preview.redd.it/1pi871kgxtaf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d23acd97cfd9aace41c89c646afb41bae4cd88fb",
                    "width": 216,
                    "height": 62
                  },
                  {
                    "url": "https://preview.redd.it/1pi871kgxtaf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db72cd6339207be4048c8d028fd85f1746f070c9",
                    "width": 320,
                    "height": 92
                  },
                  {
                    "url": "https://preview.redd.it/1pi871kgxtaf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b45a353c7618f09017647270bb57fb0e37b72933",
                    "width": 640,
                    "height": 184
                  },
                  {
                    "url": "https://preview.redd.it/1pi871kgxtaf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b8e155a93e84406f67ffcf63950de31118d071da",
                    "width": 960,
                    "height": 276
                  },
                  {
                    "url": "https://preview.redd.it/1pi871kgxtaf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c9ab8faee6174a2821f47df0c04acc65bfadfce3",
                    "width": 1080,
                    "height": 310
                  }
                ],
                "variants": {},
                "id": "uQNJZpTH3DR_345eUwYV66__t-z4DFq2QoGmMo8Fras"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lreu44",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Specific_Opinion_573",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lreu44/3060toks_on_4bit_local_llm_iphone_16/",
          "stickied": false,
          "url": "https://i.redd.it/1pi871kgxtaf1.jpeg",
          "subreddit_subscribers": 494897,
          "created_utc": 1751622653,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm seeking an advice from the community about best of use of my rig -&gt; i9/32GB/3090+4070\n\nI need to host local models for code assistance, and routine automation with N8N. All 8B models are quite useless, and I want to run something decent (if possible). What models and what runtime could I use to get maximum from 3090+4070 combinations?  \nI tried vllmcomressor to run 70B models, but no luck yet.",
          "author_fullname": "t2_16gba1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Advise needed on runtime and Model for my HW",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls5x6q",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751724540,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751707556,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m seeking an advice from the community about best of use of my rig -&amp;gt; i9/32GB/3090+4070&lt;/p&gt;\n\n&lt;p&gt;I need to host local models for code assistance, and routine automation with N8N. All 8B models are quite useless, and I want to run something decent (if possible). What models and what runtime could I use to get maximum from 3090+4070 combinations?&lt;br/&gt;\nI tried vllmcomressor to run 70B models, but no luck yet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ls5x6q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mancubus77",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls5x6q/advise_needed_on_runtime_and_model_for_my_hw/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls5x6q/advise_needed_on_runtime_and_model_for_my_hw/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751707556,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am upgrading my system which will have a 5090.  Would adding my old 3090 be any benefit or would it slow down the 5090 too much?  Inference only.  I'd like to get large context window on high quant of 32B, potentially using 70B.",
          "author_fullname": "t2_ijzb7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "5090 w/ 3090?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls5pbt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751706639,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am upgrading my system which will have a 5090.  Would adding my old 3090 be any benefit or would it slow down the 5090 too much?  Inference only.  I&amp;#39;d like to get large context window on high quant of 32B, potentially using 70B.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ls5pbt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MidnightProgrammer",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls5pbt/5090_w_3090/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls5pbt/5090_w_3090/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751706639,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am recently using my computer to write product reviews based on product images and text descriptions of items, im looking to maximize my hardware as well as generally play around with the largest models that I can run. Im looking to learn and explore as well as use this for practical applications like review writing. I also do a lot of image generation but my understanding is that the system ram is largely irrelevant with this.\n\nMy hardware is:\n\nRTX 5090\n\n9950X3D\n\n192GB RAM (currently 64GB 6000 Mhz CL28 but the order is placed for the 192GB of RAM)\n\nI am hoping and praying I can get this RAM to run at 6000 Mhz CL30 but not holding my breath, I have 2 x kits coming in, it would be 80GB/s bandwidth if I could get it running at the EXPO profile. \n\n[https://www.newegg.com/g-skill-flare-x5-96gb-ddr5-6000-cas-latency-cl30-desktop-memory-white/p/N82E16820374683?Item=N82E16820374683](https://www.newegg.com/g-skill-flare-x5-96gb-ddr5-6000-cas-latency-cl30-desktop-memory-white/p/N82E16820374683?Item=N82E16820374683)\n\nI am reading that I can run Mixture-of-Expert (MoE) models on this kind of hardware like Qwen3-235B-A22B.\n\nHas anyone else here ran a setup like this and can provide any feedback on what kind of models I can/should run on hardware like this? I know the RAM speed could be problematic but im sure i'll get it running at a decent speed.",
          "author_fullname": "t2_i2ox1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "9950X3D + RTX 5090 + 192 GB RAM , reasonable?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsiov1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.3,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751745704,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am recently using my computer to write product reviews based on product images and text descriptions of items, im looking to maximize my hardware as well as generally play around with the largest models that I can run. Im looking to learn and explore as well as use this for practical applications like review writing. I also do a lot of image generation but my understanding is that the system ram is largely irrelevant with this.&lt;/p&gt;\n\n&lt;p&gt;My hardware is:&lt;/p&gt;\n\n&lt;p&gt;RTX 5090&lt;/p&gt;\n\n&lt;p&gt;9950X3D&lt;/p&gt;\n\n&lt;p&gt;192GB RAM (currently 64GB 6000 Mhz CL28 but the order is placed for the 192GB of RAM)&lt;/p&gt;\n\n&lt;p&gt;I am hoping and praying I can get this RAM to run at 6000 Mhz CL30 but not holding my breath, I have 2 x kits coming in, it would be 80GB/s bandwidth if I could get it running at the EXPO profile. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.newegg.com/g-skill-flare-x5-96gb-ddr5-6000-cas-latency-cl30-desktop-memory-white/p/N82E16820374683?Item=N82E16820374683\"&gt;https://www.newegg.com/g-skill-flare-x5-96gb-ddr5-6000-cas-latency-cl30-desktop-memory-white/p/N82E16820374683?Item=N82E16820374683&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I am reading that I can run Mixture-of-Expert (MoE) models on this kind of hardware like Qwen3-235B-A22B.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else here ran a setup like this and can provide any feedback on what kind of models I can/should run on hardware like this? I know the RAM speed could be problematic but im sure i&amp;#39;ll get it running at a decent speed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsiov1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "juggarjew",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsiov1/9950x3d_rtx_5090_192_gb_ram_reasonable/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsiov1/9950x3d_rtx_5090_192_gb_ram_reasonable/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751745704,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So, I want to finetune any model good or bad, into a youtuber persona\nMy idea is i will download youtube videos of that youtuber and generate transcript and POFF! I have the youtuber data, now i just need train the model on that data\n\nMy idea is Gemini have gems, can that be useful? If not, can i achieve my goal for free? Btw, i have gemini advanced subscription \n\nP.S, I am not a technical person, i can write python code, but thats it, so think of me as dumb, and then read the question again",
          "author_fullname": "t2_3hpgagyv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Finetuning a youtuber persona without expensive hardware or buying expensive cloud computing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsevb1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751735592,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I want to finetune any model good or bad, into a youtuber persona\nMy idea is i will download youtube videos of that youtuber and generate transcript and POFF! I have the youtuber data, now i just need train the model on that data&lt;/p&gt;\n\n&lt;p&gt;My idea is Gemini have gems, can that be useful? If not, can i achieve my goal for free? Btw, i have gemini advanced subscription &lt;/p&gt;\n\n&lt;p&gt;P.S, I am not a technical person, i can write python code, but thats it, so think of me as dumb, and then read the question again&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsevb1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Khushalgogia",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsevb1/finetuning_a_youtuber_persona_without_expensive/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsevb1/finetuning_a_youtuber_persona_without_expensive/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751735592,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey All\n\nRight now building a customer service agent with crewai and using tools to access enterprise data. Using self hosted LLMs (qwen30b/llama3.3:70b). \n\nWhat i see is the agent blurting out information which are not available from the tools. Example: Address of your branch in NYC? It just makes up some address and returns. \n\nPrompt has instructions to depend on tools. But i want to ground the responses with only the information available from tools. How do i go about this?\n\nSaw some hallucination detection libraries like opik. But more interested on how to prevent it",
          "author_fullname": "t2_bvk1o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any thoughts on preventing hallucination in agents with tools",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls4kp1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751701898,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey All&lt;/p&gt;\n\n&lt;p&gt;Right now building a customer service agent with crewai and using tools to access enterprise data. Using self hosted LLMs (qwen30b/llama3.3:70b). &lt;/p&gt;\n\n&lt;p&gt;What i see is the agent blurting out information which are not available from the tools. Example: Address of your branch in NYC? It just makes up some address and returns. &lt;/p&gt;\n\n&lt;p&gt;Prompt has instructions to depend on tools. But i want to ground the responses with only the information available from tools. How do i go about this?&lt;/p&gt;\n\n&lt;p&gt;Saw some hallucination detection libraries like opik. But more interested on how to prevent it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ls4kp1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dnivra26",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls4kp1/any_thoughts_on_preventing_hallucination_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls4kp1/any_thoughts_on_preventing_hallucination_in/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751701898,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looks like AWS Bedrock doesn’t have all the Qwen3 models available in their catalog. Anyone successfully load Qwen3-30B-A3B (the MOE variant) on Bedrock through their custom model feature?",
          "author_fullname": "t2_57qj45ls",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 on AWS Bedrock",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrvvkk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751670364,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looks like AWS Bedrock doesn’t have all the Qwen3 models available in their catalog. Anyone successfully load Qwen3-30B-A3B (the MOE variant) on Bedrock through their custom model feature?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lrvvkk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "International_Quail8",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrvvkk/qwen3_on_aws_bedrock/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrvvkk/qwen3_on_aws_bedrock/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751670364,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have thousands upon thousands of photos on various drives in my home. It would likely take the rest of my life to organize it all. What would be amazing is a piece of software or a collection of tools working together that could label and tag all of it. Essential feature would be for me to be like \"this photo here is wh33t\", this photo here \"this is wh33t's best friend\", and then the system would be able to identify wh33t and wh33t's best friend in all of the photos and all of that information would go into some kind of frontend tool that makes browsing it all straight forward, I would even settle for the photos going into tidy organized directories.\n\nI feel like such a thing might exist already but I thought I'd ask here for personal recommendations and I presume at the heart of this system would be a neural network.",
          "author_fullname": "t2_b74xb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does anyone here know of such a system that could easily be trained to recognize objects or people in photos?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls0vb7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751689153,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751687520,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have thousands upon thousands of photos on various drives in my home. It would likely take the rest of my life to organize it all. What would be amazing is a piece of software or a collection of tools working together that could label and tag all of it. Essential feature would be for me to be like &amp;quot;this photo here is wh33t&amp;quot;, this photo here &amp;quot;this is wh33t&amp;#39;s best friend&amp;quot;, and then the system would be able to identify wh33t and wh33t&amp;#39;s best friend in all of the photos and all of that information would go into some kind of frontend tool that makes browsing it all straight forward, I would even settle for the photos going into tidy organized directories.&lt;/p&gt;\n\n&lt;p&gt;I feel like such a thing might exist already but I thought I&amp;#39;d ask here for personal recommendations and I presume at the heart of this system would be a neural network.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ls0vb7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "wh33t",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls0vb7/does_anyone_here_know_of_such_a_system_that_could/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls0vb7/does_anyone_here_know_of_such_a_system_that_could/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751687520,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been exploring LLMs for a while and have been using Ollama and python to just do some formatting, standardisation and conversions of some private files. Beyond this I use Claude to help me with complex excel functions or to help me collate lists of all podcasts with Richard Thaler, for example. \n\nI'm curious about MCPs and want to know how users here are using AI in their PERSONAL LIVES. \n\nI'm so exhausted by all the posts about vibe coding, hardware and model comparisons because they're all for people who view AI very differently than I do. \n\nI'm more curious about personal usage because I'm not keen on using AI to sort my emails as most people on YouTube do with AI agents and such. I mean, let me try and protect my data while I still can. \n\nIt could be as simple as using Image OCR to LLM to make an excel sheet of all the different sneakers you own. \n\n\n",
          "author_fullname": "t2_7ls4c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How are the casual users here using LLMs or/and MCPs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrlmco",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751643112,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been exploring LLMs for a while and have been using Ollama and python to just do some formatting, standardisation and conversions of some private files. Beyond this I use Claude to help me with complex excel functions or to help me collate lists of all podcasts with Richard Thaler, for example. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious about MCPs and want to know how users here are using AI in their PERSONAL LIVES. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m so exhausted by all the posts about vibe coding, hardware and model comparisons because they&amp;#39;re all for people who view AI very differently than I do. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m more curious about personal usage because I&amp;#39;m not keen on using AI to sort my emails as most people on YouTube do with AI agents and such. I mean, let me try and protect my data while I still can. &lt;/p&gt;\n\n&lt;p&gt;It could be as simple as using Image OCR to LLM to make an excel sheet of all the different sneakers you own. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lrlmco",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "man_eating_chicken",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrlmco/how_are_the_casual_users_here_using_llms_orand/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrlmco/how_are_the_casual_users_here_using_llms_orand/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751643112,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was kinda curious if instead of moondream and smolvlm there's more stuff out there?",
          "author_fullname": "t2_ecsmrwz0h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Smallest VLM that currently exists and what's the minimum spec y'all have gotten them to work on?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrtrmw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751664216,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was kinda curious if instead of moondream and smolvlm there&amp;#39;s more stuff out there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lrtrmw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "combo-user",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrtrmw/smallest_vlm_that_currently_exists_and_whats_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrtrmw/smallest_vlm_that_currently_exists_and_whats_the/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751664216,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Need to automatically sort 10k+ images into categories (flat-lay clothing vs people wearing clothes). Looking for the best local VLM approach.",
          "author_fullname": "t2_536w88ry",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Local VLM for Automated Image Classification? (10k+ Images)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls3rw2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751698642,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need to automatically sort 10k+ images into categories (flat-lay clothing vs people wearing clothes). Looking for the best local VLM approach.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ls3rw2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "survior2k",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls3rw2/best_local_vlm_for_automated_image_classification/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls3rw2/best_local_vlm_for_automated_image_classification/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751698642,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm working on a LLM-Project for my CS Degree where I need to run a models locally, because of sensitive data. My current Desktop PC is quite old now (Windows, i5-6600K, 16GB RAM, GTX 1060 6GB) and only capable of running small models, so I want to upgrade it anyway. I saw a few people reccomending Apples ARM for the job, but they are very expensive. I am looking at \n\n**Mac Studio M4 Max**\n\n* Apple M4 Max\n* 16 Core CPU\n* 40 Core GPU\n* 16 Core NE\n* 546 GB/s memory bandwidth \n* 128 GB RAM\n* 1TB SSD\n* MacOS\n\nIn the Edu-Store they sell in my country it for **4,160€**.\n\nI found another alternative: Framework. I knew they build nice Laptops, but one might also preorder their new Desktops (Charge 11 is estimated to ship in Q3). \n\n**Framework Desktop Max+ 395**\n\n* AMD Ryzen AI Max+ 395\n* 16 Core CPU\n* 40 Core GPU\n* 265 GB/s memory bandwidth \n* 128 GB RAM\n* 1TB SSD\n* Fedora\n\nSo with the (on paper) equivalent configuration I arrive at **2,570€**\n\nThat is a lot of money saved! Plus I would be running Linux instead of MacOS. I like not being boxed in an ecosystem. The replacement parts are much cheaper. The only downside would be a few programs like Lightroom are not availabe on Linux (I would cancel my subscription, wich also saves money). Gaming on this thing might also be better.\n\nHas anybody expierence with this System for LLMs? Would this be a good alternative? What benefit am I getting in the Max version and is it worth the premium price?\n\nEdit: fixed CPU core count, added memory bandwidth\n\nEdit2:more Information on the use case: the input prompt will be relativly large (tranacripts of conversations enriched by RAG from a data base of domain specific literarure) and the output small (reccomendations and best practices)",
          "author_fullname": "t2_50jqg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Apple M4 Max or AMD Ryzen AI Max+ 395 (Framwork Desktop)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lre3x9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 50,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 50,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751646388,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751619753,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a LLM-Project for my CS Degree where I need to run a models locally, because of sensitive data. My current Desktop PC is quite old now (Windows, i5-6600K, 16GB RAM, GTX 1060 6GB) and only capable of running small models, so I want to upgrade it anyway. I saw a few people reccomending Apples ARM for the job, but they are very expensive. I am looking at &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Mac Studio M4 Max&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Apple M4 Max&lt;/li&gt;\n&lt;li&gt;16 Core CPU&lt;/li&gt;\n&lt;li&gt;40 Core GPU&lt;/li&gt;\n&lt;li&gt;16 Core NE&lt;/li&gt;\n&lt;li&gt;546 GB/s memory bandwidth &lt;/li&gt;\n&lt;li&gt;128 GB RAM&lt;/li&gt;\n&lt;li&gt;1TB SSD&lt;/li&gt;\n&lt;li&gt;MacOS&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In the Edu-Store they sell in my country it for &lt;strong&gt;4,160€&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;I found another alternative: Framework. I knew they build nice Laptops, but one might also preorder their new Desktops (Charge 11 is estimated to ship in Q3). &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Framework Desktop Max+ 395&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AMD Ryzen AI Max+ 395&lt;/li&gt;\n&lt;li&gt;16 Core CPU&lt;/li&gt;\n&lt;li&gt;40 Core GPU&lt;/li&gt;\n&lt;li&gt;265 GB/s memory bandwidth &lt;/li&gt;\n&lt;li&gt;128 GB RAM&lt;/li&gt;\n&lt;li&gt;1TB SSD&lt;/li&gt;\n&lt;li&gt;Fedora&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So with the (on paper) equivalent configuration I arrive at &lt;strong&gt;2,570€&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;That is a lot of money saved! Plus I would be running Linux instead of MacOS. I like not being boxed in an ecosystem. The replacement parts are much cheaper. The only downside would be a few programs like Lightroom are not availabe on Linux (I would cancel my subscription, wich also saves money). Gaming on this thing might also be better.&lt;/p&gt;\n\n&lt;p&gt;Has anybody expierence with this System for LLMs? Would this be a good alternative? What benefit am I getting in the Max version and is it worth the premium price?&lt;/p&gt;\n\n&lt;p&gt;Edit: fixed CPU core count, added memory bandwidth&lt;/p&gt;\n\n&lt;p&gt;Edit2:more Information on the use case: the input prompt will be relativly large (tranacripts of conversations enriched by RAG from a data base of domain specific literarure) and the output small (reccomendations and best practices)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lre3x9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zeltbrennt",
          "discussion_type": null,
          "num_comments": 108,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lre3x9/apple_m4_max_or_amd_ryzen_ai_max_395_framwork/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lre3x9/apple_m4_max_or_amd_ryzen_ai_max_395_framwork/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751619753,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Title. I wonder if there is any collections/rankings for open-to-use LLMs in the area of generating dataset. As far as I know (please correct me if I'm wrong):\n- ChatGPT disallows \"using ChatGPT to build a competitive model against itself\". Though the terms is quite vague, it wouldn't be safe to assume that they're \"open AI\" (pun intended).\n- DeepSeek allows for the use case, but they require us to note where exactly their LLM was used. Good, isn't it?\n- Llama also allows for the use case, but they require models that inherited their data to be named after them (maybe I misremembered, could be \"your fine-tuned llama model must also be named llama\").\n\nThat's all folks. Hopefully I can get some valuable suggestions!\n\nEdit: Found this useful link.\nhttps://github.com/eugeneyan/open-llms",
          "author_fullname": "t2_42ba8hq5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "License-friendly LLMs for generating synthetic datasets",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrzom4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751700226,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751683192,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title. I wonder if there is any collections/rankings for open-to-use LLMs in the area of generating dataset. As far as I know (please correct me if I&amp;#39;m wrong):\n- ChatGPT disallows &amp;quot;using ChatGPT to build a competitive model against itself&amp;quot;. Though the terms is quite vague, it wouldn&amp;#39;t be safe to assume that they&amp;#39;re &amp;quot;open AI&amp;quot; (pun intended).\n- DeepSeek allows for the use case, but they require us to note where exactly their LLM was used. Good, isn&amp;#39;t it?\n- Llama also allows for the use case, but they require models that inherited their data to be named after them (maybe I misremembered, could be &amp;quot;your fine-tuned llama model must also be named llama&amp;quot;).&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s all folks. Hopefully I can get some valuable suggestions!&lt;/p&gt;\n\n&lt;p&gt;Edit: Found this useful link.\n&lt;a href=\"https://github.com/eugeneyan/open-llms\"&gt;https://github.com/eugeneyan/open-llms&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/5HE7KGF_L1EksV4d9v3Dw9DfwLTMriQo2T312gJwv3o.png?auto=webp&amp;s=520466ad7a3c4073d001b3a28918f2559a6af892",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/5HE7KGF_L1EksV4d9v3Dw9DfwLTMriQo2T312gJwv3o.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c854207e0586cf8b3235769c68e916f5c8c84aec",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/5HE7KGF_L1EksV4d9v3Dw9DfwLTMriQo2T312gJwv3o.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2dd3401a5fb7174d2e6054b541afeeca78efe0e7",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/5HE7KGF_L1EksV4d9v3Dw9DfwLTMriQo2T312gJwv3o.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b8a6e7d7febaa7e19ec4441f4c1187736170d17",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/5HE7KGF_L1EksV4d9v3Dw9DfwLTMriQo2T312gJwv3o.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3a3927f95304dddc9f72eab4d1e5b1ff6be532b7",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/5HE7KGF_L1EksV4d9v3Dw9DfwLTMriQo2T312gJwv3o.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b55d494067e868a8c2095b6ceff8fc271f6082ed",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/5HE7KGF_L1EksV4d9v3Dw9DfwLTMriQo2T312gJwv3o.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fb34443b4d882d5f30403e26a8f0147cb8c1d06f",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "5HE7KGF_L1EksV4d9v3Dw9DfwLTMriQo2T312gJwv3o"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lrzom4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blankboy2022",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrzom4/licensefriendly_llms_for_generating_synthetic/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrzom4/licensefriendly_llms_for_generating_synthetic/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751683192,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Managed to get unmute to work with llama-server API, (thanks to Gemini 2.5 flash). This modified `llm_utils.py` goes into unmute/llm (note, it might make vLLM not work, haven't tested):\n\n[https://gist.github.com/jepjoo/7ab6da43c3e51923eeaf278eac47c9c9](https://gist.github.com/jepjoo/7ab6da43c3e51923eeaf278eac47c9c9)\n\nRun llama-server with --port 8000 (or change settings in docker-compose.yml)\n\nCan fit all unmute parts + Mistral 24B IQ4\\_XS or Gemma 3 27B IQ3\\_M into 24GB.\n\nTips:\n\nSystem prompt can be edited to your liking, it's in `unmute/llm/system_prompt.py`\n\nCharacters' prompts can be edited and a different voice can be selected for them by editing `voices.yaml`\n\nThere's over a 100 voices, they are somewhere in the depths of the docker filesystem in .safetensors format, so I just downloaded them all from here in .wav format to be able to listen to them: [https://huggingface.co/kyutai/tts-voices/tree/main](https://huggingface.co/kyutai/tts-voices/tree/main)\n\nTo switch to a different voice, just edit the `path_on_server` like for example the first charater: `path_on_server: unmute-prod-website/p329_022.wav` \\-&gt; `path_on_server: expresso/ex04-ex03_fast_001_channel2_25s.wav`\n\nAfter you update the `llm_utils.py` or edit those other files you gotta:\n\ndocker compose up -d --build backend\n\n  \nPS. I'm running on Windows, things could be much smoother on Linux and the llm\\_utils.py fix might be unnecessary, dunno.",
          "author_fullname": "t2_12aeph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Unmute + Llama.cpp server",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrjkx3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751639662,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751637955,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Managed to get unmute to work with llama-server API, (thanks to Gemini 2.5 flash). This modified &lt;code&gt;llm_utils.py&lt;/code&gt; goes into unmute/llm (note, it might make vLLM not work, haven&amp;#39;t tested):&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://gist.github.com/jepjoo/7ab6da43c3e51923eeaf278eac47c9c9\"&gt;https://gist.github.com/jepjoo/7ab6da43c3e51923eeaf278eac47c9c9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Run llama-server with --port 8000 (or change settings in docker-compose.yml)&lt;/p&gt;\n\n&lt;p&gt;Can fit all unmute parts + Mistral 24B IQ4_XS or Gemma 3 27B IQ3_M into 24GB.&lt;/p&gt;\n\n&lt;p&gt;Tips:&lt;/p&gt;\n\n&lt;p&gt;System prompt can be edited to your liking, it&amp;#39;s in &lt;code&gt;unmute/llm/system_prompt.py&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Characters&amp;#39; prompts can be edited and a different voice can be selected for them by editing &lt;code&gt;voices.yaml&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s over a 100 voices, they are somewhere in the depths of the docker filesystem in .safetensors format, so I just downloaded them all from here in .wav format to be able to listen to them: &lt;a href=\"https://huggingface.co/kyutai/tts-voices/tree/main\"&gt;https://huggingface.co/kyutai/tts-voices/tree/main&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;To switch to a different voice, just edit the &lt;code&gt;path_on_server&lt;/code&gt; like for example the first charater: &lt;code&gt;path_on_server: unmute-prod-website/p329_022.wav&lt;/code&gt; -&amp;gt; &lt;code&gt;path_on_server: expresso/ex04-ex03_fast_001_channel2_25s.wav&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;After you update the &lt;code&gt;llm_utils.py&lt;/code&gt; or edit those other files you gotta:&lt;/p&gt;\n\n&lt;p&gt;docker compose up -d --build backend&lt;/p&gt;\n\n&lt;p&gt;PS. I&amp;#39;m running on Windows, things could be much smoother on Linux and the llm_utils.py fix might be unnecessary, dunno.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?auto=webp&amp;s=c7cbcc7517e2406e2326e7a1eb6bdb9022c27fda",
                  "width": 1280,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=796041decb8c1250cbc2f301331b72f7385b477d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2e3562243f324d16bc6d9dd09adb1da4e0b100b5",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=564e5f4bb6808064a14eb3965a6911671c3c9807",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f53460a90493497883ab4cacbbb58e2acb464c4",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7a4f79362039959fa37eab208ae001245ccfe6e3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=912f966e123e94e32e7975fe8aebac89450a6b98",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lrjkx3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rerri",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrjkx3/unmute_llamacpp_server/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrjkx3/unmute_llamacpp_server/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751637955,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What are your locally hosted killer apps at the moment. What do you show to wow your friends and boss?\n\nI just got asked by a friend since he has been tasked to install a local ai chat but wants to wow his boss and I also realized I have been stuck in the 'helps coding' and 'helps writing' corner for a while.",
          "author_fullname": "t2_hhdppe9r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are some locally hosted Killer Apps?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrjg7t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751637614,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are your locally hosted killer apps at the moment. What do you show to wow your friends and boss?&lt;/p&gt;\n\n&lt;p&gt;I just got asked by a friend since he has been tasked to install a local ai chat but wants to wow his boss and I also realized I have been stuck in the &amp;#39;helps coding&amp;#39; and &amp;#39;helps writing&amp;#39; corner for a while.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lrjg7t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AdOne8437",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrjg7t/what_are_some_locally_hosted_killer_apps/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrjg7t/what_are_some_locally_hosted_killer_apps/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751637614,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am currently working on something related to social media data and wanna test a censored and uncensored models result on same data. \n\nShare models and if you used them, how good they are. ",
          "author_fullname": "t2_hmolzecg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Finding Uncensored models for some social media project",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsd9t4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.18,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751731439,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working on something related to social media data and wanna test a censored and uncensored models result on same data. &lt;/p&gt;\n\n&lt;p&gt;Share models and if you used them, how good they are. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsd9t4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Unlikely-Chicken3286",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsd9t4/finding_uncensored_models_for_some_social_media/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsd9t4/finding_uncensored_models_for_some_social_media/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751731439,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1ipy2mlwcz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A project to bring CUDA to non-Nvidia GPUs is making major progress",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqvovt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 649,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 649,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=f8513a13ec05e844f00af50b1129d68270d1a0e3",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751564116,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "tomshardware.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.tomshardware.com/software/a-project-to-bring-cuda-to-non-nvidia-gpus-is-making-major-progress-zluda-update-now-has-two-full-time-developers-working-on-32-bit-physx-support-and-llms-amongst-other-things",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?auto=webp&amp;s=89597e27a33fccf11c3d8cb4bba4680fd3482911",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e37ec99fec8c2fe6a73b6748741209a5f1f5c4a",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f2192a2dee3ef234ceea5f28dcbbe00f2badf7a6",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=53021c4c074ef2e8cc80c4a45e8ac25404af8e4f",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0b1a092718ebc960a0b1d79e4d2f8bc6ea6c934f",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d00e1c1ada20cf48e84e3b3fdba5ba81608a97ff",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=19e499d6f1f0b3a98a8c99312501005dacb10260",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lqvovt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OwnWitness2836",
          "discussion_type": null,
          "num_comments": 86,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/",
          "stickied": false,
          "url": "https://www.tomshardware.com/software/a-project-to-bring-cuda-to-non-nvidia-gpus-is-making-major-progress-zluda-update-now-has-two-full-time-developers-working-on-32-bit-physx-support-and-llms-amongst-other-things",
          "subreddit_subscribers": 494897,
          "created_utc": 1751564116,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been enjoying Ollama for the ability to have an easy web interface to download models with and that I can make API calls to a single endpoint and Port while specifying different models that I want used. As far as I understand it, llama.cpp requires one running instance per model, and obviously different ports. I'm enjoying being able to be lazy without needing to SSH to my server and manually manage model download or server instances, but most importantly to query multiple models on a single endpoint and port. Am I giving all that up by moving directly to llama.cpp?\n\nThanks! Just want to make sure before I decide to stick with Ollama.",
          "author_fullname": "t2_fvs8r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Am I correct that to run multiple models with Llama.cpp I need multiple instances on multiple ports?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrqj68",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751655462,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been enjoying Ollama for the ability to have an easy web interface to download models with and that I can make API calls to a single endpoint and Port while specifying different models that I want used. As far as I understand it, llama.cpp requires one running instance per model, and obviously different ports. I&amp;#39;m enjoying being able to be lazy without needing to SSH to my server and manually manage model download or server instances, but most importantly to query multiple models on a single endpoint and port. Am I giving all that up by moving directly to llama.cpp?&lt;/p&gt;\n\n&lt;p&gt;Thanks! Just want to make sure before I decide to stick with Ollama.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lrqj68",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CharlesStross",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrqj68/am_i_correct_that_to_run_multiple_models_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrqj68/am_i_correct_that_to_run_multiple_models_with/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751655462,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone,\n\nSo I've been working on what was initially meant to be a Claude Code clone for arbitrary LLMs over the past two weeks, [cli-agent](https://github.com/amranu/cli-agent). It has support for various APIs as well as ollama, so I felt posting here is as good idea as any.\n\nThe project has access to all the tools Claude Code does, such as arbitrary llm subagent support through the task tool, as well as the recently added hooks feature. I -also- recently added the ability to customize roles for your agents and subagents. This allows for some pretty dynamic behaviour changes. Because of this role feature, I was able to add the /deep-research command which allows a pseudo-deep-research with your chosen LLM. This launches 3-5 \"researcher\" role subagents to investigate the topic and report back, and then launches a \"summarizer\" role subagent to put everything together into a report. It's a pretty powerful feature! Very token hungry though. Finally, it has MCP client -and- server support. Allowing you to hook up your local LLMs to MCP servers and allowing you to make your local LLMs available over MCP through it's local mcp_server.py script. Tools -are- accessible to the LLMs over MCP.\n\nThe project has just made it recently to v1.2.5, so I figured I'd post it here for you all to try out. I'm especially curious if you guys find a good local LLM combination for the deep-research feature. Also, this project is only a couple weeks old, so it's still quite buggy in some places. Still, the more eyes looking at it the better I say. Cheers!",
          "author_fullname": "t2_3yvyd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "cli-agent - An agentic framework for arbitrary LLMs - now with hooks, roles, and deep research!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrq827",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751669487,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751654651,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;ve been working on what was initially meant to be a Claude Code clone for arbitrary LLMs over the past two weeks, &lt;a href=\"https://github.com/amranu/cli-agent\"&gt;cli-agent&lt;/a&gt;. It has support for various APIs as well as ollama, so I felt posting here is as good idea as any.&lt;/p&gt;\n\n&lt;p&gt;The project has access to all the tools Claude Code does, such as arbitrary llm subagent support through the task tool, as well as the recently added hooks feature. I -also- recently added the ability to customize roles for your agents and subagents. This allows for some pretty dynamic behaviour changes. Because of this role feature, I was able to add the /deep-research command which allows a pseudo-deep-research with your chosen LLM. This launches 3-5 &amp;quot;researcher&amp;quot; role subagents to investigate the topic and report back, and then launches a &amp;quot;summarizer&amp;quot; role subagent to put everything together into a report. It&amp;#39;s a pretty powerful feature! Very token hungry though. Finally, it has MCP client -and- server support. Allowing you to hook up your local LLMs to MCP servers and allowing you to make your local LLMs available over MCP through it&amp;#39;s local mcp_server.py script. Tools -are- accessible to the LLMs over MCP.&lt;/p&gt;\n\n&lt;p&gt;The project has just made it recently to v1.2.5, so I figured I&amp;#39;d post it here for you all to try out. I&amp;#39;m especially curious if you guys find a good local LLM combination for the deep-research feature. Also, this project is only a couple weeks old, so it&amp;#39;s still quite buggy in some places. Still, the more eyes looking at it the better I say. Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bdLJtiVMPiAMuYWA26Aedkjth-mZiCG-flDZsN3QbGM.png?auto=webp&amp;s=ac0fd875035bc92edcaa4758d5593c6268142ebc",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bdLJtiVMPiAMuYWA26Aedkjth-mZiCG-flDZsN3QbGM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=031f2ce9bc32e6bd693f22f934b2c517eff29015",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/bdLJtiVMPiAMuYWA26Aedkjth-mZiCG-flDZsN3QbGM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7ced23d5225477f7489a2994ab614ae3c874ba45",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/bdLJtiVMPiAMuYWA26Aedkjth-mZiCG-flDZsN3QbGM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=183485137cce6a09c37cf24a141e3fc5a4bbb177",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/bdLJtiVMPiAMuYWA26Aedkjth-mZiCG-flDZsN3QbGM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1bebfc2de478cfa85fe8e13513a730d925aebd39",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/bdLJtiVMPiAMuYWA26Aedkjth-mZiCG-flDZsN3QbGM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=71e49e7d689f50adf7a88ef4b7e1556a5ce4992a",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/bdLJtiVMPiAMuYWA26Aedkjth-mZiCG-flDZsN3QbGM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ebdc9795638a23b3f6f75de98a330a07b768a06a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "bdLJtiVMPiAMuYWA26Aedkjth-mZiCG-flDZsN3QbGM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lrq827",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "amranu",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrq827/cliagent_an_agentic_framework_for_arbitrary_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrq827/cliagent_an_agentic_framework_for_arbitrary_llms/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751654651,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt; In an ongoing effort to improve the usability of AI vector database searches within retrieval-augmented generation (RAG) systems by optimizing the use of solid-state drives (SSDs), KIOXIA today announced an update to its KIOXIA AiSAQ™[1] (All-in-Storage ANNS with Product Quantization) software. This new open-source release introduces flexible controls allowing system architects to define the balance point between search performance and the number of vectors, which are opposing factors in the fixed capacity of SSD storage in the system. The resulting benefit enables architects of RAG systems to fine tune the optimal balance of specific workloads and their requirements, without any hardware modifications.\n\n- https://europe.kioxia.com/en-europe/business/news/2025/20250703-1.html\n- https://github.com/kioxia-jp/aisaq-diskann",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "KIOXIA AiSAQ software advances AI RAG with new version of vector search library",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lru0fv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/oQ8m1Ysy-iCvuMSa9jsgbDq5mZ905x6TLSsHOyOWaUo.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=6459af39ea27b51c4a7a6d79740e2afd5d271370",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751664905,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "europe.kioxia.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;In an ongoing effort to improve the usability of AI vector database searches within retrieval-augmented generation (RAG) systems by optimizing the use of solid-state drives (SSDs), KIOXIA today announced an update to its KIOXIA AiSAQ™[1] (All-in-Storage ANNS with Product Quantization) software. This new open-source release introduces flexible controls allowing system architects to define the balance point between search performance and the number of vectors, which are opposing factors in the fixed capacity of SSD storage in the system. The resulting benefit enables architects of RAG systems to fine tune the optimal balance of specific workloads and their requirements, without any hardware modifications.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://europe.kioxia.com/en-europe/business/news/2025/20250703-1.html\"&gt;https://europe.kioxia.com/en-europe/business/news/2025/20250703-1.html&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/kioxia-jp/aisaq-diskann\"&gt;https://github.com/kioxia-jp/aisaq-diskann&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://europe.kioxia.com/en-europe/business/news/2025/20250703-1.html",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oQ8m1Ysy-iCvuMSa9jsgbDq5mZ905x6TLSsHOyOWaUo.png?auto=webp&amp;s=1d48dd32d166988e75d692c64f3977d48b31b1bf",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oQ8m1Ysy-iCvuMSa9jsgbDq5mZ905x6TLSsHOyOWaUo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fe07491456a1986f569aa8d4fad62110341814de",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/oQ8m1Ysy-iCvuMSa9jsgbDq5mZ905x6TLSsHOyOWaUo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ccd3771d473ecc51cb8bacd5adba4b1b409bb1c6",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/oQ8m1Ysy-iCvuMSa9jsgbDq5mZ905x6TLSsHOyOWaUo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab82940073475fab4dd3a26916862109cc40d303",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/oQ8m1Ysy-iCvuMSa9jsgbDq5mZ905x6TLSsHOyOWaUo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b06a60efe1607c44571dc9784c3fc65d72ac984e",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/oQ8m1Ysy-iCvuMSa9jsgbDq5mZ905x6TLSsHOyOWaUo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7eea7a314ae00f7b985adb01e67c91f893723942",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/oQ8m1Ysy-iCvuMSa9jsgbDq5mZ905x6TLSsHOyOWaUo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4ee6d9d8d59eaf9f755ffb1954e6a7fde822a18",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "oQ8m1Ysy-iCvuMSa9jsgbDq5mZ905x6TLSsHOyOWaUo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lru0fv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lru0fv/kioxia_aisaq_software_advances_ai_rag_with_new/",
          "stickied": false,
          "url": "https://europe.kioxia.com/en-europe/business/news/2025/20250703-1.html",
          "subreddit_subscribers": 494897,
          "created_utc": 1751664905,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have an archive of several thousand maintenance documents.   They are all very structured and similar but not identical.   They cover 5 major classes of big industrial equipment.    For a single class there may be 20 or more specific builds but not every build in a class is identical.    Sometimes we want information about a whole class, and sometimes we want information about a specific build.\n\nI've had very good luck using an LLM with a well engineered prompt and defined JSON schema.   And basically I'm getting the answers I want, but not fast enough.   These may take 20 seconds each.    \n\nRight now I just do all these in a loop, one at a time and I'm wondering if there is a way to configure the server for better performance.    I have *plenty* of both CPU and GPU resources.   I want to better understand things like continuous batching, kv cache optimizing, threads and anything else that can improve performance when the prompts are nearly the same thing over and over.\n\n",
          "author_fullname": "t2_7osuk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama.cpp and continuous batching for performance",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lroopr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751650803,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an archive of several thousand maintenance documents.   They are all very structured and similar but not identical.   They cover 5 major classes of big industrial equipment.    For a single class there may be 20 or more specific builds but not every build in a class is identical.    Sometimes we want information about a whole class, and sometimes we want information about a specific build.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve had very good luck using an LLM with a well engineered prompt and defined JSON schema.   And basically I&amp;#39;m getting the answers I want, but not fast enough.   These may take 20 seconds each.    &lt;/p&gt;\n\n&lt;p&gt;Right now I just do all these in a loop, one at a time and I&amp;#39;m wondering if there is a way to configure the server for better performance.    I have &lt;em&gt;plenty&lt;/em&gt; of both CPU and GPU resources.   I want to better understand things like continuous batching, kv cache optimizing, threads and anything else that can improve performance when the prompts are nearly the same thing over and over.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lroopr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Simusid",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lroopr/llamacpp_and_continuous_batching_for_performance/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lroopr/llamacpp_and_continuous_batching_for_performance/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751650803,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/46c2vbkrkpaf1.png?width=680&amp;format=png&amp;auto=webp&amp;s=a074dd8ac462e5276d42be14dd98e4b1700f67fd\n\nKyutai has open-sourced Kyutai TTS — a new real-time text-to-speech model that’s packed with features and ready to shake things up in the world of TTS.\n\nIt’s super fast, starting to generate audio in just \\~220ms after getting the first bit of text. Unlike most “streaming” TTS models out there, it doesn’t need the whole text upfront — it works as you type or as an LLM generates text, making it perfect for live interactions.\n\nYou can also clone voices with just 10 seconds of audio.\n\nAnd yes — it handles long sentences or paragraphs without breaking a sweat, going well beyond the usual 30-second limit most models struggle with.\n\nGithub: [https://github.com/kyutai-labs/delayed-streams-modeling/](https://github.com/kyutai-labs/delayed-streams-modeling/)  \nHuggingface: [https://huggingface.co/kyutai/tts-1.6b-en\\_fr](https://huggingface.co/kyutai/tts-1.6b-en_fr)  \n[https://kyutai.org/next/tts](https://kyutai.org/next/tts)",
          "author_fullname": "t2_i7c050twt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kyutai TTS is here: Real-time, voice-cloning, ultra-low-latency TTS, Robust Longform generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "46c2vbkrkpaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/46c2vbkrkpaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=567c7ed50c38b860696b50ffc7a438e206c34e56"
                },
                {
                  "y": 137,
                  "x": 216,
                  "u": "https://preview.redd.it/46c2vbkrkpaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c47bb3849aee309a099207ab2bef2a8cfe0dc17"
                },
                {
                  "y": 204,
                  "x": 320,
                  "u": "https://preview.redd.it/46c2vbkrkpaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=84846ddbff6f90572edd1fd2db9afa1a31694c63"
                },
                {
                  "y": 408,
                  "x": 640,
                  "u": "https://preview.redd.it/46c2vbkrkpaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f4c20800461bdf3a93c3a5a7eea010188095b72"
                }
              ],
              "s": {
                "y": 434,
                "x": 680,
                "u": "https://preview.redd.it/46c2vbkrkpaf1.png?width=680&amp;format=png&amp;auto=webp&amp;s=a074dd8ac462e5276d42be14dd98e4b1700f67fd"
              },
              "id": "46c2vbkrkpaf1"
            }
          },
          "name": "t3_1lqycp0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "ups": 325,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 325,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=cf22588a4104646fb55cc5544239962172f00b39",
          "edited": 1751576263,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751570457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/46c2vbkrkpaf1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a074dd8ac462e5276d42be14dd98e4b1700f67fd\"&gt;https://preview.redd.it/46c2vbkrkpaf1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a074dd8ac462e5276d42be14dd98e4b1700f67fd&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Kyutai has open-sourced Kyutai TTS — a new real-time text-to-speech model that’s packed with features and ready to shake things up in the world of TTS.&lt;/p&gt;\n\n&lt;p&gt;It’s super fast, starting to generate audio in just ~220ms after getting the first bit of text. Unlike most “streaming” TTS models out there, it doesn’t need the whole text upfront — it works as you type or as an LLM generates text, making it perfect for live interactions.&lt;/p&gt;\n\n&lt;p&gt;You can also clone voices with just 10 seconds of audio.&lt;/p&gt;\n\n&lt;p&gt;And yes — it handles long sentences or paragraphs without breaking a sweat, going well beyond the usual 30-second limit most models struggle with.&lt;/p&gt;\n\n&lt;p&gt;Github: &lt;a href=\"https://github.com/kyutai-labs/delayed-streams-modeling/\"&gt;https://github.com/kyutai-labs/delayed-streams-modeling/&lt;/a&gt;&lt;br/&gt;\nHuggingface: &lt;a href=\"https://huggingface.co/kyutai/tts-1.6b-en_fr\"&gt;https://huggingface.co/kyutai/tts-1.6b-en_fr&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://kyutai.org/next/tts\"&gt;https://kyutai.org/next/tts&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?auto=webp&amp;s=b800d3098a23bd5487ff84657bbf23c3edce1a95",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d634539e40735ae87eaf416790acc915ab14a5d7",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ebb7977c292bcaee1601466f75036ff97c874fec",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cc4e8862bcb38d4b54d29ef6fb1974138bef3917",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=abb529167e09a3692724b342df0121216749b7bd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e0a9aef1bc9fcb240220a2587a9b8b2335900c90",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e643baded43564e9eb4ac25aa8ac4425eefe1736",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lqycp0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pheonis2",
          "discussion_type": null,
          "num_comments": 76,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751570457,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Obviously I have heard about this memed tweet, but I just saw that he said it is „unethical” … how do they even dare to talk about ethics? Icant, its so sad that the company that started AI revolution is OAI",
          "author_fullname": "t2_y2qkg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Unethical",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsdnin",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.16,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/tdKDSmgJyJgBRjh3BFJxvGgc49JtZV9A0bBp3hAAeCI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751732429,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Obviously I have heard about this memed tweet, but I just saw that he said it is „unethical” … how do they even dare to talk about ethics? Icant, its so sad that the company that started AI revolution is OAI&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/jquinonero/status/1940926946705395943?s=46",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Mr6r0zW37m2NxYrQHzvcRoXtyemAYa2PoklKZO-79BQ.jpg?auto=webp&amp;s=7fec7f5c190d928081b0d8200c78735d03e00e0c",
                  "width": 200,
                  "height": 200
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Mr6r0zW37m2NxYrQHzvcRoXtyemAYa2PoklKZO-79BQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=36f51b1de6a5fde62dc977e8af18e795bb6d414c",
                    "width": 108,
                    "height": 108
                  }
                ],
                "variants": {},
                "id": "wpHkx4IBY4f2SvtmsnhVr4jXBiXjBh0wFp1GAD4Ofq8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsdnin",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "martinmazur",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsdnin/unethical/",
          "stickied": false,
          "url": "https://x.com/jquinonero/status/1940926946705395943?s=46",
          "subreddit_subscribers": 494897,
          "created_utc": 1751732429,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is there any Gemma3 based model with reasoning (GRPO) implemented for ollama? Thanks!",
          "author_fullname": "t2_4q7j68j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gemma 3 Reasoning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrjrvg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751638470,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any Gemma3 based model with reasoning (GRPO) implemented for ollama? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lrjrvg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dazz9",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrjrvg/gemma_3_reasoning/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrjrvg/gemma_3_reasoning/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751638470,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I got these warnings:\n\n        /home/user/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:262: UserWarning: \n            Found GPU0 NVIDIA GeForce GT 1030 which is of cuda capability 6.1.\n            PyTorch no longer supports this GPU because it is too old.\n            The minimum cuda capability supported by this library is 7.5.\n            \n          warnings.warn(\n        /home/user/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:287: UserWarning: \n        NVIDIA GeForce GT 1030 with CUDA capability sm_61 is not compatible with the current PyTorch installation.\n        The current PyTorch install supports CUDA capabilities sm_75 sm_80 sm_86 sm_90 sm_100 sm_120 compute_120.\n        If you want to use the NVIDIA GeForce GT 1030 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n\nAnd then crash with this error:\n\n    RuntimeError: CUDA error: no kernel image is available for execution on the device\n    CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n    For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n    Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nI tried the 2.7.0 with both cuda 12.6 and 12.8 and they both gave me this error. So I should stick with 2.6.0?",
          "author_fullname": "t2_s6sfw4yy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "pytorch 2.7.x no longer supports Pascal architecture?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrerwe",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751622411,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got these warnings:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    /home/user/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:262: UserWarning: \n        Found GPU0 NVIDIA GeForce GT 1030 which is of cuda capability 6.1.\n        PyTorch no longer supports this GPU because it is too old.\n        The minimum cuda capability supported by this library is 7.5.\n\n      warnings.warn(\n    /home/user/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:287: UserWarning: \n    NVIDIA GeForce GT 1030 with CUDA capability sm_61 is not compatible with the current PyTorch installation.\n    The current PyTorch install supports CUDA capabilities sm_75 sm_80 sm_86 sm_90 sm_100 sm_120 compute_120.\n    If you want to use the NVIDIA GeForce GT 1030 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And then crash with this error:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I tried the 2.7.0 with both cuda 12.6 and 12.8 and they both gave me this error. So I should stick with 2.6.0?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/akOX9n901M19sZspfWwfi0njVQhgKCdPXxQXMrrTCpM.png?auto=webp&amp;s=a4a4b27e25fe7be94d2bb6716d4153232d9083e2",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/akOX9n901M19sZspfWwfi0njVQhgKCdPXxQXMrrTCpM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=20375fdf4c207a82be22152fe6cce0f4a088a374",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/akOX9n901M19sZspfWwfi0njVQhgKCdPXxQXMrrTCpM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d8eb7c64539a062375e955f937dd9515807f9d44",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/akOX9n901M19sZspfWwfi0njVQhgKCdPXxQXMrrTCpM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fe288d2ab95f74e0f439844cdb0cb9111ae2f5bf",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/akOX9n901M19sZspfWwfi0njVQhgKCdPXxQXMrrTCpM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b05c39653087936a3c8e8d31032d726939a7e51e",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/akOX9n901M19sZspfWwfi0njVQhgKCdPXxQXMrrTCpM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e73fdbba2bcee51a9b87195a767a96dcb134fb4e",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/akOX9n901M19sZspfWwfi0njVQhgKCdPXxQXMrrTCpM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e8d42ae97845842600a2e91a2743782190042f46",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "akOX9n901M19sZspfWwfi0njVQhgKCdPXxQXMrrTCpM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lrerwe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Warning2146",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrerwe/pytorch_27x_no_longer_supports_pascal_architecture/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrerwe/pytorch_27x_no_longer_supports_pascal_architecture/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751622411,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Meaning, should alpha be double the rank or it doesn't matter much?",
          "author_fullname": "t2_10vfc5m6o1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a rule between alpha (α) and rank (r) for LoRA?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrgdzg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751628340,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Meaning, should alpha be double the rank or it doesn&amp;#39;t matter much?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lrgdzg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TechNerd10191",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrgdzg/is_there_a_rule_between_alpha_α_and_rank_r_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrgdzg/is_there_a_rule_between_alpha_α_and_rank_r_for/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751628340,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Original post got lost while I had reddit suspended while taking pictures smh. Anyways in short I have an additional 3090 and a 3rd 2.5 inch drive that I need to install. I know I will need risers and some sort of mount. Case is a coolermaster masterbox td500 mesh. The smaller pcie slots are occupied by 2 usb expansion cards and the other x16 one is open so I could support another 3090 the problem is just making everything fit. Was hoping that someone more experienced and/or creative than I could give me some ideas. I rather not have to get a different case and rebuild the whole thing because I really like this case actually but I am fearful this might be necessary. and I know my cable management is awful, don’t judge me too hard. I don’t really care if its not pretty as long as it works and is safe. Pictures attached as an imgur link:\n\nhttps://imgur.com/a/2iKC6OT\n\nAny help would be very greatly appreciated also would like to note I have no experience with using risers or really any pc building techniques that deviate from utilizing intentional design and just putting things where they go. Thank you all for your time and happy 4th. ",
          "author_fullname": "t2_1flwpwd3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help fitting second gpu + 3rd drive",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrskbk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751660868,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Original post got lost while I had reddit suspended while taking pictures smh. Anyways in short I have an additional 3090 and a 3rd 2.5 inch drive that I need to install. I know I will need risers and some sort of mount. Case is a coolermaster masterbox td500 mesh. The smaller pcie slots are occupied by 2 usb expansion cards and the other x16 one is open so I could support another 3090 the problem is just making everything fit. Was hoping that someone more experienced and/or creative than I could give me some ideas. I rather not have to get a different case and rebuild the whole thing because I really like this case actually but I am fearful this might be necessary. and I know my cable management is awful, don’t judge me too hard. I don’t really care if its not pretty as long as it works and is safe. Pictures attached as an imgur link:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/2iKC6OT\"&gt;https://imgur.com/a/2iKC6OT&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any help would be very greatly appreciated also would like to note I have no experience with using risers or really any pc building techniques that deviate from utilizing intentional design and just putting things where they go. Thank you all for your time and happy 4th. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/PKb3CbPsXEivFF-a1w3WIdjgeMpA0sWla1FBk_K7bCc.jpg?auto=webp&amp;s=ae3513de56dc25e07290f26113a9e27a98c1a21f",
                  "width": 3024,
                  "height": 4032
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/PKb3CbPsXEivFF-a1w3WIdjgeMpA0sWla1FBk_K7bCc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4cddf0a8cc12e8717419408be07265a2d8e4d0a",
                    "width": 108,
                    "height": 144
                  },
                  {
                    "url": "https://external-preview.redd.it/PKb3CbPsXEivFF-a1w3WIdjgeMpA0sWla1FBk_K7bCc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b5f6a6a6f8bff18f474303af4964ab8c16067fa7",
                    "width": 216,
                    "height": 288
                  },
                  {
                    "url": "https://external-preview.redd.it/PKb3CbPsXEivFF-a1w3WIdjgeMpA0sWla1FBk_K7bCc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8d2721ea11149452e5d684d7efad9e25fced01c8",
                    "width": 320,
                    "height": 426
                  },
                  {
                    "url": "https://external-preview.redd.it/PKb3CbPsXEivFF-a1w3WIdjgeMpA0sWla1FBk_K7bCc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e956ad39b06a59aebe637de3ad2a0ba8ed8f959d",
                    "width": 640,
                    "height": 853
                  },
                  {
                    "url": "https://external-preview.redd.it/PKb3CbPsXEivFF-a1w3WIdjgeMpA0sWla1FBk_K7bCc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=93af68acb0ffa89cafdcbe7c9c175c333dd96b46",
                    "width": 960,
                    "height": 1280
                  },
                  {
                    "url": "https://external-preview.redd.it/PKb3CbPsXEivFF-a1w3WIdjgeMpA0sWla1FBk_K7bCc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc51137301230ee8445798cfa666ab9f5a0d1afe",
                    "width": 1080,
                    "height": 1440
                  }
                ],
                "variants": {},
                "id": "dGm2JvtNGavqfVU4wROz6T97IZ2rM5wpj2TNRZRinMw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lrskbk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WyattTheSkid",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrskbk/need_help_fitting_second_gpu_3rd_drive/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrskbk/need_help_fitting_second_gpu_3rd_drive/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751660868,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Disclaimer: I am not a programmer at all, and vibecoding thanks to LLMs has already brought me immense joy to my embedded hobby. (it just runs and nothing is critical and I am happy).\n\nWith MCP having been around longer by now and with it not seemingly not going away any time soon, how come setting up a MCP server is still such a coding-heavy chore? \"oh you need a token here, set this client up there...\" Why can't we have an AppStore / HuggingFace experience where you have the ability to just search for and \"download\" all MCP servers directly on one platform with all tokens, logging in etc. being handled in the background by some known GUI (akin to LMStudio, Jan, etc..)?\n\nI realised yesterday that neither Qwen3 4B nor 8B is able to solve quadratic equations (because it doesn't want to do the actual calculation like squaring (times-ing by itself), it just talks itself into a loop of *wanting* to do so but then giving up when trying, but it \"*knows\"* what it has do to). \n\nSo I googled and there is a calculator MCP. There surely are weather MCPs, RAG MCPs, environments to test code etc, so **why is there no straightforward local MCP server platform?** What am I oblivious to?",
          "author_fullname": "t2_omawcpyf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "No Race for the leading MCP Server GUI?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrsi1e",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751660697,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Disclaimer: I am not a programmer at all, and vibecoding thanks to LLMs has already brought me immense joy to my embedded hobby. (it just runs and nothing is critical and I am happy).&lt;/p&gt;\n\n&lt;p&gt;With MCP having been around longer by now and with it not seemingly not going away any time soon, how come setting up a MCP server is still such a coding-heavy chore? &amp;quot;oh you need a token here, set this client up there...&amp;quot; Why can&amp;#39;t we have an AppStore / HuggingFace experience where you have the ability to just search for and &amp;quot;download&amp;quot; all MCP servers directly on one platform with all tokens, logging in etc. being handled in the background by some known GUI (akin to LMStudio, Jan, etc..)?&lt;/p&gt;\n\n&lt;p&gt;I realised yesterday that neither Qwen3 4B nor 8B is able to solve quadratic equations (because it doesn&amp;#39;t want to do the actual calculation like squaring (times-ing by itself), it just talks itself into a loop of &lt;em&gt;wanting&lt;/em&gt; to do so but then giving up when trying, but it &amp;quot;&lt;em&gt;knows&amp;quot;&lt;/em&gt; what it has do to). &lt;/p&gt;\n\n&lt;p&gt;So I googled and there is a calculator MCP. There surely are weather MCPs, RAG MCPs, environments to test code etc, so &lt;strong&gt;why is there no straightforward local MCP server platform?&lt;/strong&gt; What am I oblivious to?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lrsi1e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Karim_acing_it",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrsi1e/no_race_for_the_leading_mcp_server_gui/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrsi1e/no_race_for_the_leading_mcp_server_gui/",
          "subreddit_subscribers": 494897,
          "created_utc": 1751660697,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}