{
  "kind": "Listing",
  "data": {
    "after": "t3_1lvyfws",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_akbc8z42",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Friendly reminder that Grok 3 should be now open-sourced",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 66,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "raegcyabl8cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 79,
                  "x": 108,
                  "u": "https://preview.redd.it/raegcyabl8cf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a3f5977fb2ef290bc9faa26c48196830e24255f4"
                },
                {
                  "y": 159,
                  "x": 216,
                  "u": "https://preview.redd.it/raegcyabl8cf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a468b0185b4f7b434f08151d96e28310cad0c0cf"
                },
                {
                  "y": 236,
                  "x": 320,
                  "u": "https://preview.redd.it/raegcyabl8cf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ab7d4890eac1ce8c8b25640cd33b80f550ed8dd"
                },
                {
                  "y": 472,
                  "x": 640,
                  "u": "https://preview.redd.it/raegcyabl8cf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=856df6b3e241260ab4b3a557168a338411f558a5"
                }
              ],
              "s": {
                "y": 654,
                "x": 886,
                "u": "https://preview.redd.it/raegcyabl8cf1.jpg?width=886&amp;format=pjpg&amp;auto=webp&amp;s=019517ed88de38406f7e69f1360d664c43e79d7e"
              },
              "id": "raegcyabl8cf1"
            },
            "nue26crvk8cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 50,
                  "x": 108,
                  "u": "https://preview.redd.it/nue26crvk8cf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1a2298d44574e1d558db56d97be489fcda6639ad"
                },
                {
                  "y": 101,
                  "x": 216,
                  "u": "https://preview.redd.it/nue26crvk8cf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3657e0d1aa681245138d5f48fccbff413ec77ddd"
                },
                {
                  "y": 151,
                  "x": 320,
                  "u": "https://preview.redd.it/nue26crvk8cf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5233d19a9dcc01c7b6f4411e5f9410e2591f8d12"
                },
                {
                  "y": 302,
                  "x": 640,
                  "u": "https://preview.redd.it/nue26crvk8cf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=52d14f23bda0cdd68e34872168d852b83011ae34"
                }
              ],
              "s": {
                "y": 302,
                "x": 640,
                "u": "https://preview.redd.it/nue26crvk8cf1.jpg?width=640&amp;format=pjpg&amp;auto=webp&amp;s=0708b731fca6beabcc3c2e020854b8efb2c7b967"
              },
              "id": "nue26crvk8cf1"
            }
          },
          "name": "t3_1lx5awq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 649,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "nue26crvk8cf1",
                "id": 703502340
              },
              {
                "media_id": "raegcyabl8cf1",
                "id": 703502341
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 649,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/933OQwllbA1hY7_1-xQgZI7EOZf5Fdt9pi7_3gUoRkc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752236028,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lx5awq",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lx5awq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Wrong_User_Logged",
          "discussion_type": null,
          "num_comments": 110,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lx5awq",
          "subreddit_subscribers": 497502,
          "created_utc": 1752236028,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kw6e4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Nvidia being Nvidia: FP8 is 150 Tflops faster when kernel name contain \"cutlass\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx62hd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 288,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 288,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752238238,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/triton-lang/triton/pull/7298/commits/a5e23d8e7e64b8a11af3edc1705407d91084b01d",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lx62hd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bora_ach",
          "discussion_type": null,
          "num_comments": 39,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx62hd/nvidia_being_nvidia_fp8_is_150_tflops_faster_when/",
          "stickied": false,
          "url": "https://github.com/triton-lang/triton/pull/7298/commits/a5e23d8e7e64b8a11af3edc1705407d91084b01d",
          "subreddit_subscribers": 497502,
          "created_utc": 1752238238,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.\n\n# [](https://huggingface.co/moonshotai/Kimi-K2-Instruct#key-features)Key Features\n\n* Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.\n* MuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up.\n* Agentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving.\n\n# [](https://huggingface.co/moonshotai/Kimi-K2-Instruct#model-variants)\n\n# Model Variants\n\n* **Kimi-K2-Base**: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions.\n* **Kimi-K2-Instruct**: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking.",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "moonshotai/Kimi-K2-Instruct (and Kimi-K2-Base)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx8xdm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": "#bbbdbf",
          "ups": 148,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 148,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=029228d543016d143b4cf39cb9567707fc07d245",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752245561,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.&lt;/p&gt;\n\n&lt;h1&gt;&lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Instruct#key-features\"&gt;&lt;/a&gt;Key Features&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.&lt;/li&gt;\n&lt;li&gt;MuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up.&lt;/li&gt;\n&lt;li&gt;Agentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;&lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Instruct#model-variants\"&gt;&lt;/a&gt;&lt;/h1&gt;\n\n&lt;h1&gt;Model Variants&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Kimi-K2-Base&lt;/strong&gt;: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Kimi-K2-Instruct&lt;/strong&gt;: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/moonshotai/Kimi-K2-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?auto=webp&amp;s=78218534a59407b3e56ec5c79df38546f4efe70c",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=014f5215759a7ee46cc335661cfd741228ef1b1e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7a09f794f1ff77c0c16776942ad4b842977ccb84",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=01057b7f796b87e54735f133cbd2404a4a8425d2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=65e4d917b0768ba9727a840f3e7b4ddd3fdb7ea3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fc89ea5ea896d832e6642ea7b443df8584200eab",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d43168a98c7fb53d5480aa0b7e96d2c75f889729",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lx8xdm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 61,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct_and_kimik2base/",
          "stickied": false,
          "url": "https://huggingface.co/moonshotai/Kimi-K2-Instruct",
          "subreddit_subscribers": 497502,
          "created_utc": 1752245561,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/Kimi_Moonshot/status/1943687594560332025?t=imY6uyPkkt-nqaao67g04Q&amp;s=19",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Damn this is deepseek moment one of the 3bst coding model and it's open source and by far it's so good !!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx9pny",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 130,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 130,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/XmFr9p6wjG4zaZt3Q0ue15-JML6QPKU4zgPWDXTPyvo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752247404,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/Kimi_Moonshot/status/1943687594560332025?t=imY6uyPkkt-nqaao67g04Q&amp;amp;s=19\"&gt;https://x.com/Kimi_Moonshot/status/1943687594560332025?t=imY6uyPkkt-nqaao67g04Q&amp;amp;s=19&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/a1tzaif5j9cf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/a1tzaif5j9cf1.jpeg?auto=webp&amp;s=69860e7d09ceaf76c01a684ba7e550cf7f22eacb",
                  "width": 1080,
                  "height": 1844
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/a1tzaif5j9cf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9647bc9a930559ead2c29c4398ea6f566b948c9",
                    "width": 108,
                    "height": 184
                  },
                  {
                    "url": "https://preview.redd.it/a1tzaif5j9cf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d862796af18a45ef6479302025862b9b6facff4",
                    "width": 216,
                    "height": 368
                  },
                  {
                    "url": "https://preview.redd.it/a1tzaif5j9cf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=69637fac4fceb45a4b0e39004fa32bd19109abd7",
                    "width": 320,
                    "height": 546
                  },
                  {
                    "url": "https://preview.redd.it/a1tzaif5j9cf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3898e31bb7d9fbf8198401001a795859f33eafcb",
                    "width": 640,
                    "height": 1092
                  },
                  {
                    "url": "https://preview.redd.it/a1tzaif5j9cf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bffbd0e2d5485529e9fdc36381b54fcfd8477616",
                    "width": 960,
                    "height": 1639
                  },
                  {
                    "url": "https://preview.redd.it/a1tzaif5j9cf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5c721e8a4f60f953f4b1a22807d9773484f7ee9c",
                    "width": 1080,
                    "height": 1844
                  }
                ],
                "variants": {},
                "id": "UZVI56woInLaMKwWHApifja7RwNz2PKUDQXcK1QKXgk"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lx9pny",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx9pny/damn_this_is_deepseek_moment_one_of_the_3bst/",
          "stickied": false,
          "url": "https://i.redd.it/a1tzaif5j9cf1.jpeg",
          "subreddit_subscribers": 497502,
          "created_utc": 1752247404,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/moonshotai/Kimi-K2-Base](https://huggingface.co/moonshotai/Kimi-K2-Base)",
          "author_fullname": "t2_agjaq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Kimi K2 - 1T MoE, 32B active params",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "snukbrpue9cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 162,
                  "x": 108,
                  "u": "https://preview.redd.it/snukbrpue9cf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=796d3bdbe8248f04f6f4ed687349e14dd6cb1ffa"
                },
                {
                  "y": 324,
                  "x": 216,
                  "u": "https://preview.redd.it/snukbrpue9cf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d2b1a397c6ecae80548f8d9752ae729a007865a"
                },
                {
                  "y": 480,
                  "x": 320,
                  "u": "https://preview.redd.it/snukbrpue9cf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6493bb6123b8bb0c91c7c00173ca89d74c5ae294"
                },
                {
                  "y": 960,
                  "x": 640,
                  "u": "https://preview.redd.it/snukbrpue9cf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3f22017530fb297141e7c8a43fc84c3b95974f2"
                },
                {
                  "y": 1440,
                  "x": 960,
                  "u": "https://preview.redd.it/snukbrpue9cf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9a37d5eb193be223d1467fd2b152feb6d09ee34c"
                },
                {
                  "y": 1620,
                  "x": 1080,
                  "u": "https://preview.redd.it/snukbrpue9cf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c7c3685e31acd7ef16fbe241ae9bd6bf1a5ac667"
                }
              ],
              "s": {
                "y": 1662,
                "x": 1108,
                "u": "https://preview.redd.it/snukbrpue9cf1.jpg?width=1108&amp;format=pjpg&amp;auto=webp&amp;s=babe38be407551e4f5afa732091358a48a9164ef"
              },
              "id": "snukbrpue9cf1"
            },
            "kjw6onm2f9cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 52,
                  "x": 108,
                  "u": "https://preview.redd.it/kjw6onm2f9cf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=50ea2fb7fb30e8ca947536791e407498a2884e76"
                },
                {
                  "y": 105,
                  "x": 216,
                  "u": "https://preview.redd.it/kjw6onm2f9cf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9811b76d09d0722786571c55e3badd97c976495"
                },
                {
                  "y": 156,
                  "x": 320,
                  "u": "https://preview.redd.it/kjw6onm2f9cf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=82b57bf75b54fbba5196263b413450e49252985c"
                },
                {
                  "y": 313,
                  "x": 640,
                  "u": "https://preview.redd.it/kjw6onm2f9cf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d147c9f5b0c487c667b46a0664db3dcb7c663bbd"
                },
                {
                  "y": 470,
                  "x": 960,
                  "u": "https://preview.redd.it/kjw6onm2f9cf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7104b993300b87d1bdc96768d121a2e8d8c6fa7e"
                },
                {
                  "y": 529,
                  "x": 1080,
                  "u": "https://preview.redd.it/kjw6onm2f9cf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e4fd88eaadccc6d875640a0ccd2da48b2b7aaaf"
                }
              ],
              "s": {
                "y": 1336,
                "x": 2726,
                "u": "https://preview.redd.it/kjw6onm2f9cf1.jpg?width=2726&amp;format=pjpg&amp;auto=webp&amp;s=0ee5b01a83ab286f2c3a3780a44d30fddb5dba60"
              },
              "id": "kjw6onm2f9cf1"
            }
          },
          "name": "t3_1lx94ht",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 120,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "snukbrpue9cf1",
                "id": 703595261
              },
              {
                "media_id": "kjw6onm2f9cf1",
                "id": 703595262
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 120,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/vOQCL6pQbXue2TSAcO_fvTvDBTLRgjIBjMBbSQhYkWI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752246042,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Base\"&gt;https://huggingface.co/moonshotai/Kimi-K2-Base&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lx94ht",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lx94ht",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nunki08",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx94ht/kimi_k2_1t_moe_32b_active_params/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lx94ht",
          "subreddit_subscribers": 497502,
          "created_utc": 1752246042,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1sttd20rqq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "llama2.c running on the original 2007 iPhone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx6dcm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 172,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/3u6728ask8cf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1920,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/3u6728ask8cf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/3u6728ask8cf1/DASHPlaylist.mpd?a=1754849081%2CYTI2YmI1ZjVlYWZjMjEzNzJiY2Q2NDdlODNhZWIyZTA4NzI3ZTVlNTg0ZTk4NDhjYTQ5ODJiYjllN2IzNDc5MA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 44,
              "hls_url": "https://v.redd.it/3u6728ask8cf1/HLSPlaylist.m3u8?a=1754849081%2CMjg2Y2Q1YjQ4MmYwNTNlMWU3ZWEyODVkODkzYjU3MjdhMzQzM2QwN2Y3NmM0MDBmNTExMWQ3MWI2NjYxNjJjZQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 172,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=8a44b4b4dd94fc24ee6bf38f196b5967cb66b19b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752239050,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/3u6728ask8cf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?format=pjpg&amp;auto=webp&amp;s=da21c7832d018661bb3aeef8d752bf124d5ae7ad",
                  "width": 1080,
                  "height": 1920
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ea40f484a395e1158453d61b60702f7424ccede5",
                    "width": 108,
                    "height": 192
                  },
                  {
                    "url": "https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9e2e5d9472fdac4d398c7fbc0927b0c3fd26d853",
                    "width": 216,
                    "height": 384
                  },
                  {
                    "url": "https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e3811f347febe7fb9bced567d5f6f0e9fbc9dd63",
                    "width": 320,
                    "height": 568
                  },
                  {
                    "url": "https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=483b8d7383780e5e9f5b1f36a2c4e8c0c10d349a",
                    "width": 640,
                    "height": 1137
                  },
                  {
                    "url": "https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a969ef2e021c0f6ce4226b04d98e7331c2566ba2",
                    "width": 960,
                    "height": 1706
                  },
                  {
                    "url": "https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c5a55789764ec9ee3ae34ff5bda31a4f3aaa98d1",
                    "width": 1080,
                    "height": 1920
                  }
                ],
                "variants": {},
                "id": "a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lx6dcm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kyousukegum",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/",
          "stickied": false,
          "url": "https://v.redd.it/3u6728ask8cf1",
          "subreddit_subscribers": 497502,
          "created_utc": 1752239050,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/3u6728ask8cf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1920,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/3u6728ask8cf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/3u6728ask8cf1/DASHPlaylist.mpd?a=1754849081%2CYTI2YmI1ZjVlYWZjMjEzNzJiY2Q2NDdlODNhZWIyZTA4NzI3ZTVlNTg0ZTk4NDhjYTQ5ODJiYjllN2IzNDc5MA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 44,
              "hls_url": "https://v.redd.it/3u6728ask8cf1/HLSPlaylist.m3u8?a=1754849081%2CMjg2Y2Q1YjQ4MmYwNTNlMWU3ZWEyODVkODkzYjU3MjdhMzQzM2QwN2Y3NmM0MDBmNTExMWQ3MWI2NjYxNjJjZQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "MedGemma 27B Multimodal for complex multimodal &amp; longitudinal EHR interpretation: [https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4](https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4)\n\nMedSigLIP: a lightweight image/text encoder for medical image retrieval/classification: [https://huggingface.co/google/medsiglip-448](https://huggingface.co/google/medsiglip-448)\n\nT5Gemma: lightweight yet powerful encoder-decoder research models: [https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86](https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86)",
          "author_fullname": "t2_agjaq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "This week, Google released in Open Source: MedGemma 27B Multimodal, MedSigLIP, T5Gemma",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 126,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx7l3k",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 79,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 79,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1dUy9ra4BOXxGh9C9vTqGqPmClTQ3z5eudzCu9deMEk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752242256,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MedGemma 27B Multimodal for complex multimodal &amp;amp; longitudinal EHR interpretation: &lt;a href=\"https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4\"&gt;https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;MedSigLIP: a lightweight image/text encoder for medical image retrieval/classification: &lt;a href=\"https://huggingface.co/google/medsiglip-448\"&gt;https://huggingface.co/google/medsiglip-448&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;T5Gemma: lightweight yet powerful encoder-decoder research models: &lt;a href=\"https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86\"&gt;https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/r2bp20do39cf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/r2bp20do39cf1.jpeg?auto=webp&amp;s=505dfa90a4c5b4814a6f8f5d8f21e458723fabe9",
                  "width": 1250,
                  "height": 1132
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/r2bp20do39cf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c06f842789e7c04781657d6ba7a8362b24d6127f",
                    "width": 108,
                    "height": 97
                  },
                  {
                    "url": "https://preview.redd.it/r2bp20do39cf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc0e48e964af821a64b767ea1cbd0ea1a0335b02",
                    "width": 216,
                    "height": 195
                  },
                  {
                    "url": "https://preview.redd.it/r2bp20do39cf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2535f1397a9eac8a49ccbcd0d6581431f7b8353a",
                    "width": 320,
                    "height": 289
                  },
                  {
                    "url": "https://preview.redd.it/r2bp20do39cf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aa04ae911a28403b4ac7702442fb11eb655146a3",
                    "width": 640,
                    "height": 579
                  },
                  {
                    "url": "https://preview.redd.it/r2bp20do39cf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=af44035733ae0f541d1021ecd9aef93869863c7b",
                    "width": 960,
                    "height": 869
                  },
                  {
                    "url": "https://preview.redd.it/r2bp20do39cf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a960ed19ee1bd5acb1b47e2621264c111bba5b74",
                    "width": 1080,
                    "height": 978
                  }
                ],
                "variants": {},
                "id": "WkBW3ZxSnDC2gTb6x_KnIpQDXfKRLjZymR9zHnbq-Zk"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lx7l3k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nunki08",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx7l3k/this_week_google_released_in_open_source_medgemma/",
          "stickied": false,
          "url": "https://i.redd.it/r2bp20do39cf1.jpeg",
          "subreddit_subscribers": 497502,
          "created_utc": 1752242256,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is from their website.",
          "author_fullname": "t2_jqxb4pte",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Moonshot AI about to release their 1T parameters model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx4qhp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 85,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 85,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ChSWZcFAvXpnw6im6mzGrLFx9WPPRuZGN9Lkq4hNy6I.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752234302,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is from their website.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/kts1w8a7g8cf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/kts1w8a7g8cf1.jpeg?auto=webp&amp;s=1606e91cf63397735995e76505d710ac74c6349d",
                  "width": 1284,
                  "height": 1411
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/kts1w8a7g8cf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e9dd3d8cd71b434beaff9edc84a3406f39002d67",
                    "width": 108,
                    "height": 118
                  },
                  {
                    "url": "https://preview.redd.it/kts1w8a7g8cf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=600f5ce90ef0898b5dfb9081730e81cd4f0d5097",
                    "width": 216,
                    "height": 237
                  },
                  {
                    "url": "https://preview.redd.it/kts1w8a7g8cf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=55cacfe23bcd2d3f6cdfcfd1035568203c6b4fd0",
                    "width": 320,
                    "height": 351
                  },
                  {
                    "url": "https://preview.redd.it/kts1w8a7g8cf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4f19a5b1a2112e5c15ddbc66a6e92a07eecb3c7",
                    "width": 640,
                    "height": 703
                  },
                  {
                    "url": "https://preview.redd.it/kts1w8a7g8cf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e9d1d65fe9704f4fdd42ba6f535cd9280cf4be5b",
                    "width": 960,
                    "height": 1054
                  },
                  {
                    "url": "https://preview.redd.it/kts1w8a7g8cf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df548806e13a9a754494d0ca2ee5933271287bc6",
                    "width": 1080,
                    "height": 1186
                  }
                ],
                "variants": {},
                "id": "7JJbXGdkfholjapvB1HrPeqCDKJEt1mFdhwkHKl9F2s"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lx4qhp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Conversation9561",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx4qhp/moonshot_ai_about_to_release_their_1t_parameters/",
          "stickied": false,
          "url": "https://i.redd.it/kts1w8a7g8cf1.jpeg",
          "subreddit_subscribers": 497502,
          "created_utc": 1752234302,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The 1T Kimi K2 model is using DeepSeek V3 architecture",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lxb0eo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": "#bbbdbf",
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/GLWo99y2VXsciDgoAKyzE3GZociHN4n_7g_ihtVi9I0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752250411,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/l3gpvb5or9cf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/l3gpvb5or9cf1.png?auto=webp&amp;s=40d8a85b1d07e964b63a7e5184f9a5d5809b715f",
                  "width": 1080,
                  "height": 1907
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/l3gpvb5or9cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff23740ffcaa59c97aa66e63a434e727e8b2ad4a",
                    "width": 108,
                    "height": 190
                  },
                  {
                    "url": "https://preview.redd.it/l3gpvb5or9cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bce6fa94a7bb620076dc98255430dc1b47e1e0a7",
                    "width": 216,
                    "height": 381
                  },
                  {
                    "url": "https://preview.redd.it/l3gpvb5or9cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8aea3ce7d05f335e3aa2e57966b68ef819eeeea9",
                    "width": 320,
                    "height": 565
                  },
                  {
                    "url": "https://preview.redd.it/l3gpvb5or9cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed19f0e0b0c28fcc15556f566717d11201a68611",
                    "width": 640,
                    "height": 1130
                  },
                  {
                    "url": "https://preview.redd.it/l3gpvb5or9cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e13852845fea6f89d5766368474e1f7307fd4b0",
                    "width": 960,
                    "height": 1695
                  },
                  {
                    "url": "https://preview.redd.it/l3gpvb5or9cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18f660077733393b7b3433b426f75cb65bfef0e5",
                    "width": 1080,
                    "height": 1907
                  }
                ],
                "variants": {},
                "id": "Q200XGQK2QDqTgopEJURrQJKELVtC6jLu8BfpN-NSrQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lxb0eo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lxb0eo/the_1t_kimi_k2_model_is_using_deepseek_v3/",
          "stickied": false,
          "url": "https://i.redd.it/l3gpvb5or9cf1.png",
          "subreddit_subscribers": 497502,
          "created_utc": 1752250411,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_gv6j3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ETH Zurich and EPFL will release a fully open-source LLM developed on public infrastructure. Trained on the “Alps” supercomputer at the Swiss National Supercomputing Centre (CSCS). Trained on 60% english/40% non-english, it will be released in 8B and 70B sizes.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx8qrz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 45,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=140&amp;height=69&amp;crop=140:69,smart&amp;auto=webp&amp;s=eb6d386e78fa6261648d91dd3aa26540b2de45a8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752245109,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "ethz.ch",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?auto=webp&amp;s=c444e4312b925e4518f67c13cd1d0d45a148e086",
                  "width": 1565,
                  "height": 782
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=00f03455ad8e9fc0d7ab20142af7d9f6c62b3273",
                    "width": 108,
                    "height": 53
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6c969005aff0c942b7f56ff899031ce58c131be",
                    "width": 216,
                    "height": 107
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f469ddb87f611d5472215734a3217eb0ca1795e4",
                    "width": 320,
                    "height": 159
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9147a736ba8213099df826437aa4aa8dcbfe8fe3",
                    "width": 640,
                    "height": 319
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=869859917c64b3c7518aec8cee6c925a903fa611",
                    "width": 960,
                    "height": 479
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5bd6e7e7d67e1ccb5d54f14559b874a510f09c70",
                    "width": 1080,
                    "height": 539
                  }
                ],
                "variants": {},
                "id": "TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lx8qrz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nat2r",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx8qrz/eth_zurich_and_epfl_will_release_a_fully/",
          "stickied": false,
          "url": "https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html",
          "subreddit_subscribers": 497502,
          "created_utc": 1752245109,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Every day, a bunch of models appear, making it difficult to choose which ones to use for uncensored role-playing. Previously, the Ayumi LLM Role Play &amp; ERP Ranking data was somewhat of a guide, but now I can't find a list that is even close to being up to date. It's difficult to choose from among the many models with fantasy names.\n\nIs there a list that might help with which models are better for role-playing?",
          "author_fullname": "t2_qemhv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Uncensored LLM ranking for roleplay?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx2hn2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 92,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 92,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752226265,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every day, a bunch of models appear, making it difficult to choose which ones to use for uncensored role-playing. Previously, the Ayumi LLM Role Play &amp;amp; ERP Ranking data was somewhat of a guide, but now I can&amp;#39;t find a list that is even close to being up to date. It&amp;#39;s difficult to choose from among the many models with fantasy names.&lt;/p&gt;\n\n&lt;p&gt;Is there a list that might help with which models are better for role-playing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lx2hn2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mikemend",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752226265,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Mistral released Devstral-Small-2507 - which is AWESOME!  But, they released without vision capability.  I didn't like that.\n\n[**Devstral-Vision-Small-2507**](https://huggingface.co/cognitivecomputations/Devstral-Vision-Small-2507)\n\n[**Devstral-Vision-Small-2507-gguf**](https://huggingface.co/cognitivecomputations/Devstral-Vision-Small-2507-gguf)\n\nI did some model surgery.  I started with Mistral-Small-3.2-24B-Instruct-2506, and replaced its language tower with Devstral-Small-2507.\n\nThe conversion script is in the repo, if you'd like to take a look.\n\nTested, it works fine.  I'm sure that it could do with a bit of RL to gel the vision and coding with real world use cases, but I'm releasing as is - a useful multimodal coding model.\n\nEnjoy.\n\n\\-Eric\n\nhttps://preview.redd.it/91wcnq9c96cf1.png?width=512&amp;format=png&amp;auto=webp&amp;s=85b2fe4a94b6cced9120eee0eaa751516c0e00a5\n\nhttps://preview.redd.it/c5qhdivd96cf1.png?width=1680&amp;format=png&amp;auto=webp&amp;s=0077976152e5702bab0f1cd7c13c88e32e5caf93",
          "author_fullname": "t2_m02vk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Devstral-Vision-Small-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "91wcnq9c96cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/91wcnq9c96cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c63395cb1240683fec4bdb7dd2da6b40a736e5ca"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/91wcnq9c96cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2e155b85ff603d398fe16a103675bf7414b72c52"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/91wcnq9c96cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=69b46ea59f7ead5456421d26ee53efc1f538514b"
                }
              ],
              "s": {
                "y": 384,
                "x": 512,
                "u": "https://preview.redd.it/91wcnq9c96cf1.png?width=512&amp;format=png&amp;auto=webp&amp;s=85b2fe4a94b6cced9120eee0eaa751516c0e00a5"
              },
              "id": "91wcnq9c96cf1"
            },
            "c5qhdivd96cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 116,
                  "x": 108,
                  "u": "https://preview.redd.it/c5qhdivd96cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bce95fed4ee0620c8a2c03c4d5ea4359fbb86bfd"
                },
                {
                  "y": 232,
                  "x": 216,
                  "u": "https://preview.redd.it/c5qhdivd96cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c578b8d20f8a12ff544ef449f01c4c247ed9b4f9"
                },
                {
                  "y": 344,
                  "x": 320,
                  "u": "https://preview.redd.it/c5qhdivd96cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e30554dbd44baa597c002e7923fe0495fd5950ce"
                },
                {
                  "y": 689,
                  "x": 640,
                  "u": "https://preview.redd.it/c5qhdivd96cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ef32fe98813b8f118e195eb2f47bcdaba4f894a8"
                },
                {
                  "y": 1034,
                  "x": 960,
                  "u": "https://preview.redd.it/c5qhdivd96cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d7e5da17b45956c7e26f48d673ed18d8cff53a2"
                },
                {
                  "y": 1163,
                  "x": 1080,
                  "u": "https://preview.redd.it/c5qhdivd96cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b5bd00bc851da4e2bec183c4c3b09f2dad20a990"
                }
              ],
              "s": {
                "y": 1810,
                "x": 1680,
                "u": "https://preview.redd.it/c5qhdivd96cf1.png?width=1680&amp;format=png&amp;auto=webp&amp;s=0077976152e5702bab0f1cd7c13c88e32e5caf93"
              },
              "id": "c5qhdivd96cf1"
            }
          },
          "name": "t3_1lx85jo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/gvyfd3vEbQ6Mp2mv-0QDRMEHLRbvSfXNoTHqOWwrwc0.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=64748d05f1b13f6ff7d66957868efa4d10417e51",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752243652,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Mistral released Devstral-Small-2507 - which is AWESOME!  But, they released without vision capability.  I didn&amp;#39;t like that.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/cognitivecomputations/Devstral-Vision-Small-2507\"&gt;&lt;strong&gt;Devstral-Vision-Small-2507&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/cognitivecomputations/Devstral-Vision-Small-2507-gguf\"&gt;&lt;strong&gt;Devstral-Vision-Small-2507-gguf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I did some model surgery.  I started with Mistral-Small-3.2-24B-Instruct-2506, and replaced its language tower with Devstral-Small-2507.&lt;/p&gt;\n\n&lt;p&gt;The conversion script is in the repo, if you&amp;#39;d like to take a look.&lt;/p&gt;\n\n&lt;p&gt;Tested, it works fine.  I&amp;#39;m sure that it could do with a bit of RL to gel the vision and coding with real world use cases, but I&amp;#39;m releasing as is - a useful multimodal coding model.&lt;/p&gt;\n\n&lt;p&gt;Enjoy.&lt;/p&gt;\n\n&lt;p&gt;-Eric&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/91wcnq9c96cf1.png?width=512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85b2fe4a94b6cced9120eee0eaa751516c0e00a5\"&gt;https://preview.redd.it/91wcnq9c96cf1.png?width=512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85b2fe4a94b6cced9120eee0eaa751516c0e00a5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/c5qhdivd96cf1.png?width=1680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0077976152e5702bab0f1cd7c13c88e32e5caf93\"&gt;https://preview.redd.it/c5qhdivd96cf1.png?width=1680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0077976152e5702bab0f1cd7c13c88e32e5caf93&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/gvyfd3vEbQ6Mp2mv-0QDRMEHLRbvSfXNoTHqOWwrwc0.png?auto=webp&amp;s=2ad4ef855d08112c2b719455a65730344699dc13",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/gvyfd3vEbQ6Mp2mv-0QDRMEHLRbvSfXNoTHqOWwrwc0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f0419c9f025891f3303819e45297a253ea78e7c",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/gvyfd3vEbQ6Mp2mv-0QDRMEHLRbvSfXNoTHqOWwrwc0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=61c23cb7bd7339e5797195e9dbf112f2d71b6ae7",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/gvyfd3vEbQ6Mp2mv-0QDRMEHLRbvSfXNoTHqOWwrwc0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f06092d414709c5a4856b998bfe7d118cfde072",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/gvyfd3vEbQ6Mp2mv-0QDRMEHLRbvSfXNoTHqOWwrwc0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a0b820cc0547e8c6ffaecf9eb33b63916abc0d61",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/gvyfd3vEbQ6Mp2mv-0QDRMEHLRbvSfXNoTHqOWwrwc0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c0ff78d92bf44e358275e8188b60d790094840db",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/gvyfd3vEbQ6Mp2mv-0QDRMEHLRbvSfXNoTHqOWwrwc0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ebc0bcdc1af75b56992701cd875010c1cf19e67c",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "gvyfd3vEbQ6Mp2mv-0QDRMEHLRbvSfXNoTHqOWwrwc0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lx85jo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "faldore",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx85jo/devstralvisionsmall2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx85jo/devstralvisionsmall2507/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752243652,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone, good news for AMD GPU users! It seems AMD is getting serious about boosting support for their graphics cards in llama.cpp\n\nWord is, someone from AMD dropped a pull request to tweak the code, aimed at adapting the project for use with AMD graphics cards.   \nDiscussions with the project leaders are planned in the near future to explore opportunities for further enhancements.  \n[https://github.com/ggml-org/llama.cpp/pull/14624](https://github.com/ggml-org/llama.cpp/pull/14624)",
          "author_fullname": "t2_i7v1u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD's Pull Request for llama.cpp: Enhancing GPU Support",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwta86",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 337,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 337,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752194746,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, good news for AMD GPU users! It seems AMD is getting serious about boosting support for their graphics cards in llama.cpp&lt;/p&gt;\n\n&lt;p&gt;Word is, someone from AMD dropped a pull request to tweak the code, aimed at adapting the project for use with AMD graphics cards.&lt;br/&gt;\nDiscussions with the project leaders are planned in the near future to explore opportunities for further enhancements.&lt;br/&gt;\n&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14624\"&gt;https://github.com/ggml-org/llama.cpp/pull/14624&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?auto=webp&amp;s=7e2a03c13fb4ce4c8558a9462d8d8db18654140b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4049b21c0ac9b7c3089ec2e3df2e59d8659989da",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=64c95decda4b1ae5bbb895753cfeceaa35e24a90",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa643aece72bc03f667bdb9cc869c4aa0b4e21c6",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f1dc1a1002471d9dfa784ab140da65e1281030d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8339bbe8f014bcbb3649aaeea0714cc8ef76827c",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d6cedcf5e6a230d8742e6b27dc0eda7e78cf0f16",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwta86",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rrraptr",
          "discussion_type": null,
          "num_comments": 53,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752194746,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've come to the conclusion that Qwen's 235b at Q2K~, perhaps unsurprisingly, is not better than Qwen3 32b Q4KL but I still wonder about the Q3? Gemma2 27b Q3KS used to be awesome, for example. Perhaps Qwen's 235b at Q3 will be amazing? Amazing enough to warrant 10 t/s?\n\nI'm in the process of getting a mish mash of RAM I have in the cupboard together to go from 96GB to 128GB which should allow me to test Q3... if it'll POST.\n\nIs anyone already running the Q3? Is it better for code / design work than the current 32b GOAT?\n\n",
          "author_fullname": "t2_by77ogdhr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is a heavily quantised Q235b any better than Q32b?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx2dw4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 46,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 46,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752225846,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve come to the conclusion that Qwen&amp;#39;s 235b at Q2K~, perhaps unsurprisingly, is not better than Qwen3 32b Q4KL but I still wonder about the Q3? Gemma2 27b Q3KS used to be awesome, for example. Perhaps Qwen&amp;#39;s 235b at Q3 will be amazing? Amazing enough to warrant 10 t/s?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in the process of getting a mish mash of RAM I have in the cupboard together to go from 96GB to 128GB which should allow me to test Q3... if it&amp;#39;ll POST.&lt;/p&gt;\n\n&lt;p&gt;Is anyone already running the Q3? Is it better for code / design work than the current 32b GOAT?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lx2dw4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Secure_Reflection409",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752225846,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt; Granite-speech-3.3-8b is a compact and efficient speech-language model, specifically designed for automatic speech recognition (ASR) and automatic speech translation (AST). Granite-speech-3.3-8b uses a two-pass design, unlike integrated models that combine speech and language into a single pass. Initial calls to granite-speech-3.3-8b will transcribe audio files into text. To process the transcribed text using the underlying Granite language model, users must make a second call as each step must be explicitly initiated.",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Granite-speech-3.3-8b",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwztnp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 67,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 67,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=053d27c16b7a3703e6c44e7947dee7416b794087",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752215624,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Granite-speech-3.3-8b is a compact and efficient speech-language model, specifically designed for automatic speech recognition (ASR) and automatic speech translation (AST). Granite-speech-3.3-8b uses a two-pass design, unlike integrated models that combine speech and language into a single pass. Initial calls to granite-speech-3.3-8b will transcribe audio files into text. To process the transcribed text using the underlying Granite language model, users must make a second call as each step must be explicitly initiated.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/ibm-granite/granite-speech-3.3-8b",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?auto=webp&amp;s=6a3208c0a6f02901382bbd3492727a406bc355d7",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b597cda2512d75e467a9d18009ec6b56f088c226",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d6ca5c778f9d46e7644bcaf275e25c54ed791e4b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d1466b35cfdeaf83843dfff5de0fd2b2fd99cce5",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3a3cfa1633e9a330cab59c33f8413530288842b0",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8f1cb95f3fb7fc5d831f7164e052e20bad2b6c7b",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2830c5eb6c21433c267b36769a91e9f7e7de0cac",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lwztnp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwztnp/granitespeech338b/",
          "stickied": false,
          "url": "https://huggingface.co/ibm-granite/granite-speech-3.3-8b",
          "subreddit_subscribers": 497502,
          "created_utc": 1752215624,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "(Disclaimers: Nothing new here especially given the recent posts, but was supposed to report back at u/Evening_Ad6637 et al. Furthermore, i am a total noob and do local LLM via LM Studio on Windows 11, so no fancy ik\\_llama.cpp etc., as it is just so convenient.)\n\nI finally received 2x64 GB DDR5 5600 MHz Sticks (Kingston [Datasheet](https://www.kingston.com/datasheets/KF556C36BBE-8.pdf)) giving me 128 GB RAM on my ITX Build. I did load the EXPO0 timing profile giving CL36 etc.   \nThis is complemented by a Low Profile RTX 4060 with 8 GB, all controlled by a Ryzen 9 7950X (any CPU would do).\n\nThrough LM Studio, I downloaded and ran both unsloth's 128K Q3\\_K\\_XL quant (103.7 GB) as well as managed to run the **IQ4\\_XS** quant (125.5 GB) on a freshly restarted windows machine. (Haven't tried crashing or stress testing it yet, it currently works without issues).  \nI left all model settings untouched and increased the context to \\~17000. \n\n**Time to first token** on a prompt about a Berlin neighborhood took **around 10 sec, then 3.3-2.7 tps.**\n\nI can try to provide any further information or run prompts for you and return the response as well as times. Just wanted to update you that this works. Cheers!  \n\n",
          "author_fullname": "t2_omawcpyf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "FYI Qwen3 235B A22B IQ4_XS works with 128 GB DDR5 + 8GB VRAM in Windows",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx5n8c",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752237036,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Disclaimers: Nothing new here especially given the recent posts, but was supposed to report back at &lt;a href=\"/u/Evening_Ad6637\"&gt;u/Evening_Ad6637&lt;/a&gt; et al. Furthermore, i am a total noob and do local LLM via LM Studio on Windows 11, so no fancy ik_llama.cpp etc., as it is just so convenient.)&lt;/p&gt;\n\n&lt;p&gt;I finally received 2x64 GB DDR5 5600 MHz Sticks (Kingston &lt;a href=\"https://www.kingston.com/datasheets/KF556C36BBE-8.pdf\"&gt;Datasheet&lt;/a&gt;) giving me 128 GB RAM on my ITX Build. I did load the EXPO0 timing profile giving CL36 etc.&lt;br/&gt;\nThis is complemented by a Low Profile RTX 4060 with 8 GB, all controlled by a Ryzen 9 7950X (any CPU would do).&lt;/p&gt;\n\n&lt;p&gt;Through LM Studio, I downloaded and ran both unsloth&amp;#39;s 128K Q3_K_XL quant (103.7 GB) as well as managed to run the &lt;strong&gt;IQ4_XS&lt;/strong&gt; quant (125.5 GB) on a freshly restarted windows machine. (Haven&amp;#39;t tried crashing or stress testing it yet, it currently works without issues).&lt;br/&gt;\nI left all model settings untouched and increased the context to ~17000. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Time to first token&lt;/strong&gt; on a prompt about a Berlin neighborhood took &lt;strong&gt;around 10 sec, then 3.3-2.7 tps.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I can try to provide any further information or run prompts for you and return the response as well as times. Just wanted to update you that this works. Cheers!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1lx5n8c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Karim_acing_it",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx5n8c/fyi_qwen3_235b_a22b_iq4_xs_works_with_128_gb_ddr5/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx5n8c/fyi_qwen3_235b_a22b_iq4_xs_works_with_128_gb_ddr5/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752237036,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m trying to build an AI application that transcribes long audio recordings (around hundreds of thousands of tokens) and allows interaction with an LLM. However, every answer I get from searches and inquiries tells me that I need to chunk and vectorize the long text.\n\nBut with LLMs like Gemini that support 1M-token context, isn’t building a RAG system somewhat extra?\n\nThanks a lot!",
          "author_fullname": "t2_vw0gx7hg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "With a 1M context Gemini, does it still make sense to do embedding or use RAG for long texts?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx10ja",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 44,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 44,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752220264,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m trying to build an AI application that transcribes long audio recordings (around hundreds of thousands of tokens) and allows interaction with an LLM. However, every answer I get from searches and inquiries tells me that I need to chunk and vectorize the long text.&lt;/p&gt;\n\n&lt;p&gt;But with LLMs like Gemini that support 1M-token context, isn’t building a RAG system somewhat extra?&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lx10ja",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GyozaHoop",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752220264,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/02jo0wkc98cf1.png?width=1738&amp;format=png&amp;auto=webp&amp;s=7bbe4b4e754526d79553aea53066d6a49f492960\n\nHey everyone, we're following up on the SmolLM3 release with the full dataset we used for post-training the model. It includes high-quality open datasets and new ones we created to balance model performance in dual reasoning + address the scarcity of reasoning datasets in certain domains such as multi-turn conversations, multilinguality, and alignment.   \n[https://huggingface.co/datasets/HuggingFaceTB/smoltalk2](https://huggingface.co/datasets/HuggingFaceTB/smoltalk2) \n\nWe hope you will build great models on top of it 🚀",
          "author_fullname": "t2_nmd4ubik",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmolTalk2: The dataset behind SmolLM3's dual reasoning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "02jo0wkc98cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/02jo0wkc98cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=746a3dbd60bd7beeaf902a9b2651b00efd344200"
                },
                {
                  "y": 142,
                  "x": 216,
                  "u": "https://preview.redd.it/02jo0wkc98cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f6850319d9d81938d333040baea8746155bc2e0"
                },
                {
                  "y": 210,
                  "x": 320,
                  "u": "https://preview.redd.it/02jo0wkc98cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8617e2b5fb605588b8b5175c7b1e987850325d19"
                },
                {
                  "y": 421,
                  "x": 640,
                  "u": "https://preview.redd.it/02jo0wkc98cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=56ad99f36712079f99a5adb96eeea7d574a39342"
                },
                {
                  "y": 631,
                  "x": 960,
                  "u": "https://preview.redd.it/02jo0wkc98cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8bd230e380a2e99ec131fc6522dea8fdd34bea80"
                },
                {
                  "y": 710,
                  "x": 1080,
                  "u": "https://preview.redd.it/02jo0wkc98cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=87bf3938dfe1e56d42d04a217133e424766ab9d7"
                }
              ],
              "s": {
                "y": 1144,
                "x": 1738,
                "u": "https://preview.redd.it/02jo0wkc98cf1.png?width=1738&amp;format=png&amp;auto=webp&amp;s=7bbe4b4e754526d79553aea53066d6a49f492960"
              },
              "id": "02jo0wkc98cf1"
            }
          },
          "name": "t3_1lx4hxt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/guh_Ilc0qa1iubfc8zva4ecZtUzKUNZdEcOEXQmd36A.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=89def1713bdcd3b9701cf27d9b72c6e580b80c14",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752233539,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/02jo0wkc98cf1.png?width=1738&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bbe4b4e754526d79553aea53066d6a49f492960\"&gt;https://preview.redd.it/02jo0wkc98cf1.png?width=1738&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bbe4b4e754526d79553aea53066d6a49f492960&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hey everyone, we&amp;#39;re following up on the SmolLM3 release with the full dataset we used for post-training the model. It includes high-quality open datasets and new ones we created to balance model performance in dual reasoning + address the scarcity of reasoning datasets in certain domains such as multi-turn conversations, multilinguality, and alignment.&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/datasets/HuggingFaceTB/smoltalk2\"&gt;https://huggingface.co/datasets/HuggingFaceTB/smoltalk2&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;We hope you will build great models on top of it 🚀&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/guh_Ilc0qa1iubfc8zva4ecZtUzKUNZdEcOEXQmd36A.png?auto=webp&amp;s=23d3a2ba40c843fb0712c7a64e9b09daae5220a7",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/guh_Ilc0qa1iubfc8zva4ecZtUzKUNZdEcOEXQmd36A.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=04717d5c88beb59639a322015eed7f0bd8ba211d",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/guh_Ilc0qa1iubfc8zva4ecZtUzKUNZdEcOEXQmd36A.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3cbef5e8d58438eac81a202d8c03fbd44472c9f0",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/guh_Ilc0qa1iubfc8zva4ecZtUzKUNZdEcOEXQmd36A.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e1c2e144c4e11c995f31e3e2bbb151552f72555",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/guh_Ilc0qa1iubfc8zva4ecZtUzKUNZdEcOEXQmd36A.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5fb36eb92fed67dfe7cf18375f3d278d7d7f435b",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/guh_Ilc0qa1iubfc8zva4ecZtUzKUNZdEcOEXQmd36A.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b57c3eb5c2d5eeae5310ad5663469436cec8c95e",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/guh_Ilc0qa1iubfc8zva4ecZtUzKUNZdEcOEXQmd36A.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4690978e7438d31290711ce34d45444a7fca052f",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "guh_Ilc0qa1iubfc8zva4ecZtUzKUNZdEcOEXQmd36A"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lx4hxt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "loubnabnl",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx4hxt/smoltalk2_the_dataset_behind_smollm3s_dual/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx4hxt/smoltalk2_the_dataset_behind_smollm3s_dual/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752233539,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The Creators of Earnie just published a new quantization algorithm that compress Ernie-300B to 85GB and Deepseek-V3 to 184 GB, with minimal (&lt;2%) performance degradation in benchmarks. Paper here: [https://arxiv.org/pdf/2507.07145](https://arxiv.org/pdf/2507.07145)\n\n",
          "author_fullname": "t2_g177e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "2-bit Quant: CCQ, Convolutional Code for Extreme Low-bit Quantization in LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwx50s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "#bd9e9e",
          "subreddit_type": "public",
          "ups": 77,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 77,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Alpaca"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752206258,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Creators of Earnie just published a new quantization algorithm that compress Ernie-300B to 85GB and Deepseek-V3 to 184 GB, with minimal (&amp;lt;2%) performance degradation in benchmarks. Paper here: &lt;a href=\"https://arxiv.org/pdf/2507.07145\"&gt;https://arxiv.org/pdf/2507.07145&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Alpaca",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwx50s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ortegaalfredo",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752206258,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Whereas prior generations of Granite LLMs utilized a conventional transformer architecture, all models in the Granite 4.0 family utilize a new **hybrid Mamba-2/Transformer architecture,** marrying the speed and efficiency of Mamba with the precision of transformer-based self-attention. \n\nGranite 4.0 Tiny-Preview, specifically, is a **fine-grained hybrid** [**mixture of experts (MoE)**](https://www.ibm.com/think/topics/mixture-of-experts) **model,** with 7B total parameters and only 1B active parameters at inference time.\n\n[https://huggingface.co/ibm-granite/granite-4.0-tiny-preview](https://huggingface.co/ibm-granite/granite-4.0-tiny-preview)\n\n[https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview](https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview)\n\n[https://huggingface.co/ibm-ai-platform/Bamba-9B-v1](https://huggingface.co/ibm-ai-platform/Bamba-9B-v1)\n\n",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Support for the upcoming IBM Granite 4.0 has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwsrx7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#bbbdbf",
          "ups": 154,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 154,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=58c542b2095a081e5dc71e3f0c3caf3b8d97bf2d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752193250,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whereas prior generations of Granite LLMs utilized a conventional transformer architecture, all models in the Granite 4.0 family utilize a new &lt;strong&gt;hybrid Mamba-2/Transformer architecture,&lt;/strong&gt; marrying the speed and efficiency of Mamba with the precision of transformer-based self-attention. &lt;/p&gt;\n\n&lt;p&gt;Granite 4.0 Tiny-Preview, specifically, is a &lt;strong&gt;fine-grained hybrid&lt;/strong&gt; &lt;a href=\"https://www.ibm.com/think/topics/mixture-of-experts\"&gt;&lt;strong&gt;mixture of experts (MoE)&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;model,&lt;/strong&gt; with 7B total parameters and only 1B active parameters at inference time.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/ibm-granite/granite-4.0-tiny-preview\"&gt;https://huggingface.co/ibm-granite/granite-4.0-tiny-preview&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview\"&gt;https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/ibm-ai-platform/Bamba-9B-v1\"&gt;https://huggingface.co/ibm-ai-platform/Bamba-9B-v1&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/13550",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?auto=webp&amp;s=599f611a372fbcdb39cff08d5b5708383ec3485e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d9dfc6caf6473cddc930c8672b2473ef6a39f9d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e20af04b4cab9b55eb315b230cb214fefe9b821d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c441275b8459e7c9069b5c09999881a7a3cca8bd",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=be85c7d84d9bd88e720aef5b6d229ca49e85ef9a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=17ffa9221de2bb975a61c193b0b654fb03fcf6b5",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c85dbce218541b4a90e6c34de0e348286caf478d",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lwsrx7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/13550",
          "subreddit_subscribers": 497502,
          "created_utc": 1752193250,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, \n\nI'm currently facing a weird issue.   \nI was testing different embedding models, with the goal being to integrate the best local one in a django application. \n\nArchitecture is as follows : \n\n\\- One Mac Book air running LMStudio, acting as a local server for llm and embedding operations\n\n\\- My PC for the django application, running the codebase \n\nI use CosineDistance to test the models. The functionality is a semantic search. \n\nI noticed the following : \n\n\\- Using the text-embedding-3-large model, (OAI API) gives great results  \n\\- Using Nomic embedding model gives great results also  \n\\- Using Qwen embedding models give very bad results, as if the encoding wouldn't make any sense. \n\ni'm using a aembed() method to call the embedding models, and I declare them using : \n\n    OpenAIEmbeddings(\n                        model=model_name,\n                        check_embedding_ctx_length=False,\n                        base_url=base_url,\n                        api_key=api_key,\n                    )\n\nAs LM studio provides an OpenAI-like API. Here are the values of the different tests I ran.\n\n[OpenAI cosine distance test results](https://preview.redd.it/cagenh3bs8cf1.png?width=1398&amp;format=png&amp;auto=webp&amp;s=412891b809b68d181ce304b2c56a5a5e71078b2b)\n\n[LM Studio Nomic cosine distance test](https://preview.redd.it/y5a8rizcs8cf1.png?width=1402&amp;format=png&amp;auto=webp&amp;s=0d1ffc2028a3ec98ef6c4ba02dcb184cb0d0999e)\n\n[LM Studio Qwen 3 cosine distance test ](https://preview.redd.it/ddyrcixfs8cf1.png?width=1396&amp;format=png&amp;auto=webp&amp;s=d870df0374cc82865c3fdcc8ad5f3735e7ca8742)\n\n\n\nI just can't figure out what's going on. Qwen 3 is supposed to be among the best models.   \nCan someone give advice ?",
          "author_fullname": "t2_1pagh8551x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Issues with Qwen 3 Embedding models (4B and 0.6B)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 20,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "y5a8rizcs8cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 19,
                  "x": 108,
                  "u": "https://preview.redd.it/y5a8rizcs8cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=44cfe2f2e542ebe49c6baad2a3be43cc630a6366"
                },
                {
                  "y": 38,
                  "x": 216,
                  "u": "https://preview.redd.it/y5a8rizcs8cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=999c2133fe33ab1a6239795385b5aa42c9e98ca4"
                },
                {
                  "y": 57,
                  "x": 320,
                  "u": "https://preview.redd.it/y5a8rizcs8cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f33f85fa6337dcba27c86e13a80abdd12158174e"
                },
                {
                  "y": 115,
                  "x": 640,
                  "u": "https://preview.redd.it/y5a8rizcs8cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=61854d235c844410648bdb272a1361342178db54"
                },
                {
                  "y": 173,
                  "x": 960,
                  "u": "https://preview.redd.it/y5a8rizcs8cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=991192bd1d9b4375a96f844110fd6419831c5808"
                },
                {
                  "y": 194,
                  "x": 1080,
                  "u": "https://preview.redd.it/y5a8rizcs8cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f70edd6cf0e0d3e2aad11e748a1ab88758ae4e9c"
                }
              ],
              "s": {
                "y": 253,
                "x": 1402,
                "u": "https://preview.redd.it/y5a8rizcs8cf1.png?width=1402&amp;format=png&amp;auto=webp&amp;s=0d1ffc2028a3ec98ef6c4ba02dcb184cb0d0999e"
              },
              "id": "y5a8rizcs8cf1"
            },
            "cagenh3bs8cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 16,
                  "x": 108,
                  "u": "https://preview.redd.it/cagenh3bs8cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=016b9033e5b711fb8ec955dfbdaf35f9e4a02de1"
                },
                {
                  "y": 32,
                  "x": 216,
                  "u": "https://preview.redd.it/cagenh3bs8cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e76892d526bb46541a8664d15dc126ae1cae8134"
                },
                {
                  "y": 47,
                  "x": 320,
                  "u": "https://preview.redd.it/cagenh3bs8cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65a51cf2b6d2a786b3652b8b82d4c8b5065b9328"
                },
                {
                  "y": 95,
                  "x": 640,
                  "u": "https://preview.redd.it/cagenh3bs8cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ac95c684c39c3753abbebebb0f671244f29edff"
                },
                {
                  "y": 142,
                  "x": 960,
                  "u": "https://preview.redd.it/cagenh3bs8cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=23fd04ac4d7f7d739b24c307a537ece4a8b500d9"
                },
                {
                  "y": 160,
                  "x": 1080,
                  "u": "https://preview.redd.it/cagenh3bs8cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=50022bb4d86c6461de71fff4b60bd45c662acd18"
                }
              ],
              "s": {
                "y": 208,
                "x": 1398,
                "u": "https://preview.redd.it/cagenh3bs8cf1.png?width=1398&amp;format=png&amp;auto=webp&amp;s=412891b809b68d181ce304b2c56a5a5e71078b2b"
              },
              "id": "cagenh3bs8cf1"
            },
            "ddyrcixfs8cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 18,
                  "x": 108,
                  "u": "https://preview.redd.it/ddyrcixfs8cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8281c3de83dfd9dd7fc4ecc485d574cb97977c63"
                },
                {
                  "y": 36,
                  "x": 216,
                  "u": "https://preview.redd.it/ddyrcixfs8cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8735314d3421b279d123739bb2c90bbea57fd77d"
                },
                {
                  "y": 53,
                  "x": 320,
                  "u": "https://preview.redd.it/ddyrcixfs8cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d718e00df6175fee45c3cb7b15aa85bb2b699579"
                },
                {
                  "y": 107,
                  "x": 640,
                  "u": "https://preview.redd.it/ddyrcixfs8cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1e0960946a6784ad308bb826916cd041c18eb19d"
                },
                {
                  "y": 161,
                  "x": 960,
                  "u": "https://preview.redd.it/ddyrcixfs8cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1fd7f7c4bab348a0ffeff88296bafa55f2b2ccec"
                },
                {
                  "y": 181,
                  "x": 1080,
                  "u": "https://preview.redd.it/ddyrcixfs8cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dc07ac196afdb6c7702e5972386c9921b07cd5d6"
                }
              ],
              "s": {
                "y": 235,
                "x": 1396,
                "u": "https://preview.redd.it/ddyrcixfs8cf1.png?width=1396&amp;format=png&amp;auto=webp&amp;s=d870df0374cc82865c3fdcc8ad5f3735e7ca8742"
              },
              "id": "ddyrcixfs8cf1"
            }
          },
          "name": "t3_1lx66on",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/YgiW1tXulPbYxegiQD_ZWMXklDrQz0iXGKWh4ebIFjE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752238565,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently facing a weird issue.&lt;br/&gt;\nI was testing different embedding models, with the goal being to integrate the best local one in a django application. &lt;/p&gt;\n\n&lt;p&gt;Architecture is as follows : &lt;/p&gt;\n\n&lt;p&gt;- One Mac Book air running LMStudio, acting as a local server for llm and embedding operations&lt;/p&gt;\n\n&lt;p&gt;- My PC for the django application, running the codebase &lt;/p&gt;\n\n&lt;p&gt;I use CosineDistance to test the models. The functionality is a semantic search. &lt;/p&gt;\n\n&lt;p&gt;I noticed the following : &lt;/p&gt;\n\n&lt;p&gt;- Using the text-embedding-3-large model, (OAI API) gives great results&lt;br/&gt;\n- Using Nomic embedding model gives great results also&lt;br/&gt;\n- Using Qwen embedding models give very bad results, as if the encoding wouldn&amp;#39;t make any sense. &lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;m using a aembed() method to call the embedding models, and I declare them using : &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;OpenAIEmbeddings(\n                    model=model_name,\n                    check_embedding_ctx_length=False,\n                    base_url=base_url,\n                    api_key=api_key,\n                )\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;As LM studio provides an OpenAI-like API. Here are the values of the different tests I ran.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cagenh3bs8cf1.png?width=1398&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=412891b809b68d181ce304b2c56a5a5e71078b2b\"&gt;OpenAI cosine distance test results&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/y5a8rizcs8cf1.png?width=1402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d1ffc2028a3ec98ef6c4ba02dcb184cb0d0999e\"&gt;LM Studio Nomic cosine distance test&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ddyrcixfs8cf1.png?width=1396&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d870df0374cc82865c3fdcc8ad5f3735e7ca8742\"&gt;LM Studio Qwen 3 cosine distance test &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I just can&amp;#39;t figure out what&amp;#39;s going on. Qwen 3 is supposed to be among the best models.&lt;br/&gt;\nCan someone give advice ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lx66on",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IndependentApart5556",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752238565,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7eslkpz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Prime Intellect on X: Releasing SYNTHETIC-2: our open dataset of 4m verified reasoning traces",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx4a3t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/RsCLv2fVTDsOJ-90ai9jQ1Qz1Ku8IDYGDNdI9C4VfkU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752232842,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/PrimeIntellect/status/1943424561116045389",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OI2iID36iwvBkHarjg-7dH6Sebf2j49Fh1hND_ikvDI.jpg?auto=webp&amp;s=621dd092f06b73ae2dc44a08ff4fdc123e27264d",
                  "width": 200,
                  "height": 200
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OI2iID36iwvBkHarjg-7dH6Sebf2j49Fh1hND_ikvDI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bafb8719e3b1cc5345fd3b0ddd04b3379b419c55",
                    "width": 108,
                    "height": 108
                  }
                ],
                "variants": {},
                "id": "GJtgM6KSRHIuX6i_TeTKEbRK7BxUUh_vFYMVwQtcUqs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lx4a3t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Marha01",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx4a3t/prime_intellect_on_x_releasing_synthetic2_our/",
          "stickied": false,
          "url": "https://x.com/PrimeIntellect/status/1943424561116045389",
          "subreddit_subscribers": 497502,
          "created_utc": 1752232842,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Skywork-R1V 3.0: an open source model that beats close source models on multi-modal reasoning.",
          "author_fullname": "t2_h4h2az0s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Skywork/Skywork-R1V3-38B · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx6yer",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=8988e36113bb1542fe9cef5c1896ea6ec6e101bd",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752240610,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Skywork-R1V 3.0: an open source model that beats close source models on multi-modal reasoning.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Skywork/Skywork-R1V3-38B",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?auto=webp&amp;s=0904a5e53f20449eb9e39e1123f3ac2b429d6ea5",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fa8785f2c86e0a266b664cbff742d81402b4d47e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4b7053fab392ec60745c8e182823d6449627b599",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e9cafa9cc62b546dc6b22c6bcb47f10d550e0b1",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9781820f621f09b406ca5a209d2d1f7685f966ef",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8020d2286dd487e1088189e55014fcaa1276bbf8",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0a077cc9c57cb266ae95ef65d4a0159b5d381622",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lx6yer",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tabspaces",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx6yer/skyworkskyworkr1v338b_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Skywork/Skywork-R1V3-38B",
          "subreddit_subscribers": 497502,
          "created_utc": 1752240610,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A finetune of ServiceNow's Alice 15B Thinker, but this prioritizes steerability and character adherence. Thinking will work most of the time but may need to wrangle it a bit.",
          "author_fullname": "t2_w6l58p741",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Drummer's Snowpiercer 15B v2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lxbsw0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/HXBx4eoymx-1BYUGWzf89bQNDef-5prUGW1GTtR1dHU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=27d7cf0abb5511f869a112318147847f6de305f6",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752252242,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A finetune of ServiceNow&amp;#39;s Alice 15B Thinker, but this prioritizes steerability and character adherence. Thinking will work most of the time but may need to wrangle it a bit.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v2",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HXBx4eoymx-1BYUGWzf89bQNDef-5prUGW1GTtR1dHU.png?auto=webp&amp;s=b5020482290360820f25669f054f8141798f0fa8",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HXBx4eoymx-1BYUGWzf89bQNDef-5prUGW1GTtR1dHU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=742b120ed0b7500c807528f76abf431a8c976940",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/HXBx4eoymx-1BYUGWzf89bQNDef-5prUGW1GTtR1dHU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=038980c2020ec74d7b1712b429a3bb5b46110cf4",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/HXBx4eoymx-1BYUGWzf89bQNDef-5prUGW1GTtR1dHU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dfe7bbb704977a8fc1c3f24f1b80891db02bae54",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/HXBx4eoymx-1BYUGWzf89bQNDef-5prUGW1GTtR1dHU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d4bca3c5e4e8fe8b6ea311aacca9d6f0ddbd42ee",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/HXBx4eoymx-1BYUGWzf89bQNDef-5prUGW1GTtR1dHU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a2905641a333d3773701fe82deff83bcdc8e30e",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/HXBx4eoymx-1BYUGWzf89bQNDef-5prUGW1GTtR1dHU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b21e192ac8c4583bf035214f35713e0269630bb",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "HXBx4eoymx-1BYUGWzf89bQNDef-5prUGW1GTtR1dHU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lxbsw0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheLocalDrummer",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxbsw0/drummers_snowpiercer_15b_v2/",
          "stickied": false,
          "url": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v2",
          "subreddit_subscribers": 497502,
          "created_utc": 1752252242,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/h7ayiozbt7cf1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=f25b54d35a6eaf43451f5d78d2046edf55d974c4\n\nHey there,\n\nin the last few weeks, I've spent a lot of time learning about Local LLMs but always noticed one glaringly obvious, missing thing: *good* tools to run LLM benchmarks on (in terms of output quality, not talking about speed here!) in order to decide which LLM is best suited for a given task.\n\nThus, I've built a small Python tool to run the new and often-discussed HLE benchmark on local Ollama models. Grok 4 just passed the 40% milestone, but how far can our local models go?\n\nMy tool supports:\n\n* both **vision-based** and **text-only** prompting\n* **automatic judging** by a third-party model not involved in answering the exam questions\n* **question randomization** and only testing for a small subset of HLE\n* export of the results to **machine-readable JSON**\n* running **several evaluations** for different models all in one go\n* support for **external Ollama instances** with Bearer Authentication\n\nThe entire source code is on GitHub! [https://github.com/mags0ft/hle-eval-ollama](https://github.com/mags0ft/hle-eval-ollama)\n\n**To anyone new to HLE (Humanity's Last Exam)**, let me give you a quick rundown: The benchmark has been made by the Center for AI Safety and is known to be one of the hardest currently available. Many people believe that once models reach close to 100%, we're only a few steps away from AGI (sounds more like buzz than actual facts to me, but whatever...)\n\nMy project extends the usability of HLE to local Ollama models. It also improves code quality over the provided benchmarking scripts by the HLE authors (because those only provided support for OpenAI API endpoints) due to more documentation and extended formatting efforts.\n\nI'd love to get some feedback, so don't hesitate to comment! Have fun trying it out!",
          "author_fullname": "t2_1tbbr8pu4s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a tool to run Humanity's Last Exam on your favorite local models!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "h7ayiozbt7cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/h7ayiozbt7cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b654093abcaead62062dd06d844d299331fd7a"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/h7ayiozbt7cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4effb970945f88d3442b1c15fe8c167d8bd5781"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/h7ayiozbt7cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f6ee76a86e3f3284a9f63766f56f445c9fdfd82"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/h7ayiozbt7cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b45b13f85b604fa1bd5a558d66c904b36c5603bc"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/h7ayiozbt7cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=734b07f48eec345736023b7f2a14fcfd437c86ce"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/h7ayiozbt7cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=120eff57181b5e5f7de80333a223d795d381b98d"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/h7ayiozbt7cf1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=f25b54d35a6eaf43451f5d78d2046edf55d974c4"
              },
              "id": "h7ayiozbt7cf1"
            }
          },
          "name": "t3_1lx2j1l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=6f57464b5bac63e85c04bfc757ab2feccce5637d",
          "edited": 1752226612,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752226420,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/h7ayiozbt7cf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f25b54d35a6eaf43451f5d78d2046edf55d974c4\"&gt;https://preview.redd.it/h7ayiozbt7cf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f25b54d35a6eaf43451f5d78d2046edf55d974c4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hey there,&lt;/p&gt;\n\n&lt;p&gt;in the last few weeks, I&amp;#39;ve spent a lot of time learning about Local LLMs but always noticed one glaringly obvious, missing thing: &lt;em&gt;good&lt;/em&gt; tools to run LLM benchmarks on (in terms of output quality, not talking about speed here!) in order to decide which LLM is best suited for a given task.&lt;/p&gt;\n\n&lt;p&gt;Thus, I&amp;#39;ve built a small Python tool to run the new and often-discussed HLE benchmark on local Ollama models. Grok 4 just passed the 40% milestone, but how far can our local models go?&lt;/p&gt;\n\n&lt;p&gt;My tool supports:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;both &lt;strong&gt;vision-based&lt;/strong&gt; and &lt;strong&gt;text-only&lt;/strong&gt; prompting&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;automatic judging&lt;/strong&gt; by a third-party model not involved in answering the exam questions&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;question randomization&lt;/strong&gt; and only testing for a small subset of HLE&lt;/li&gt;\n&lt;li&gt;export of the results to &lt;strong&gt;machine-readable JSON&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;running &lt;strong&gt;several evaluations&lt;/strong&gt; for different models all in one go&lt;/li&gt;\n&lt;li&gt;support for &lt;strong&gt;external Ollama instances&lt;/strong&gt; with Bearer Authentication&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The entire source code is on GitHub! &lt;a href=\"https://github.com/mags0ft/hle-eval-ollama\"&gt;https://github.com/mags0ft/hle-eval-ollama&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;To anyone new to HLE (Humanity&amp;#39;s Last Exam)&lt;/strong&gt;, let me give you a quick rundown: The benchmark has been made by the Center for AI Safety and is known to be one of the hardest currently available. Many people believe that once models reach close to 100%, we&amp;#39;re only a few steps away from AGI (sounds more like buzz than actual facts to me, but whatever...)&lt;/p&gt;\n\n&lt;p&gt;My project extends the usability of HLE to local Ollama models. It also improves code quality over the provided benchmarking scripts by the HLE authors (because those only provided support for OpenAI API endpoints) due to more documentation and extended formatting efforts.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to get some feedback, so don&amp;#39;t hesitate to comment! Have fun trying it out!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?auto=webp&amp;s=24f6d099e936365d8417c46a459bc48b783a7ad3",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=84aa3c4f7f239451e4fb6ade8dbdfcfe99152d99",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=44ea6164b5081b0a55b6d05d21b49f67da5ff416",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0356d76952e89d5ebe9effdd8ddeaeed050745ef",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=81757699f7df25dfc08d7353f703eb730b727ec6",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e280d37320d6a55abf3d1aaf76ad8b45d6c626d3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e4890c1abfcb9966afefbae98769a3fe282627bd",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lx2j1l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mags0ft",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752226420,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_gi7a36v6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "H-Net: a hierarchical network that replaces tokenization with a dynamic chunking process directly inside the model, automatically discovering and operating over meaningful units of data",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lxd7nh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752255519,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/pdf/2507.07955",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lxd7nh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HOLUPREDICTIONS",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxd7nh/hnet_a_hierarchical_network_that_replaces/",
          "stickied": false,
          "url": "https://arxiv.org/pdf/2507.07955",
          "subreddit_subscribers": 497502,
          "created_utc": 1752255519,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "what do you think?",
          "author_fullname": "t2_pgl1qlgf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A language model built for the public good",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx3jtc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=140&amp;height=69&amp;crop=140:69,smart&amp;auto=webp&amp;s=eb6d386e78fa6261648d91dd3aa26540b2de45a8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752230316,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "ethz.ch",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;what do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?auto=webp&amp;s=c444e4312b925e4518f67c13cd1d0d45a148e086",
                  "width": 1565,
                  "height": 782
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=00f03455ad8e9fc0d7ab20142af7d9f6c62b3273",
                    "width": 108,
                    "height": 53
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6c969005aff0c942b7f56ff899031ce58c131be",
                    "width": 216,
                    "height": 107
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f469ddb87f611d5472215734a3217eb0ca1795e4",
                    "width": 320,
                    "height": 159
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9147a736ba8213099df826437aa4aa8dcbfe8fe3",
                    "width": 640,
                    "height": 319
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=869859917c64b3c7518aec8cee6c925a903fa611",
                    "width": 960,
                    "height": 479
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5bd6e7e7d67e1ccb5d54f14559b874a510f09c70",
                    "width": 1080,
                    "height": 539
                  }
                ],
                "variants": {},
                "id": "TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lx3jtc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Better-Armadillo1371",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx3jtc/a_language_model_built_for_the_public_good/",
          "stickied": false,
          "url": "https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html",
          "subreddit_subscribers": 497502,
          "created_utc": 1752230316,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just sharing my new madness, really, not much to say about it, as its very early. \n\nSo the idea is very simple lets have an LLM engine that can run relatively large size models on constrained hardware.\n\n**So what it is (or going to be if I don't disappear into the abyss):**\n\nStarted hacking on this today. \n\nA **pure C (or I try to keep it that way)** runtime for LLaMA models, built using llama.cpp and my own ideas for tiny devices, embedded systems, and portability-first scenarios.\n\nSo let me introduce `llamac-lab`, a work-in-progress open-source runtime for LLaMA-based models.\n\nThink llama.cpp, but:\n\n* Flattened into straight-up C (no C++ or STL baggage)\n* Optimized for **minimal memory use**\n* Dead simple to embed into **any stack** (Rust, Python, or LUA or anything else that can interface with C)\n* Born for **edge devices**, MCUs, and other weird places LLMs don't usually go\n\nI'll work on some fun stuff as well, like adapting and converting large LLMs (as long as licensing allows) to this specialised runtime.\n\nNote: It’s super early. No model loading yet. No inference. Just early scaffolding and dreams.  \nBut if you're into LLMs, embedded stuff, or like watching weird low-level projects grow - follow along or contribute!\n\nRepo: \\[llamac\\](https://github.com/llamac-lab/llamac)\n\nNote: about the flair, could not decide between generation or new model so I went with new model. More like new runtime.",
          "author_fullname": "t2_1q37ljt3oe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New OSS project: llamac-lab or a pure C runtime for LLaMA models, made for the edge",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx4ya7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752234990,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just sharing my new madness, really, not much to say about it, as its very early. &lt;/p&gt;\n\n&lt;p&gt;So the idea is very simple lets have an LLM engine that can run relatively large size models on constrained hardware.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So what it is (or going to be if I don&amp;#39;t disappear into the abyss):&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Started hacking on this today. &lt;/p&gt;\n\n&lt;p&gt;A &lt;strong&gt;pure C (or I try to keep it that way)&lt;/strong&gt; runtime for LLaMA models, built using llama.cpp and my own ideas for tiny devices, embedded systems, and portability-first scenarios.&lt;/p&gt;\n\n&lt;p&gt;So let me introduce &lt;code&gt;llamac-lab&lt;/code&gt;, a work-in-progress open-source runtime for LLaMA-based models.&lt;/p&gt;\n\n&lt;p&gt;Think llama.cpp, but:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Flattened into straight-up C (no C++ or STL baggage)&lt;/li&gt;\n&lt;li&gt;Optimized for &lt;strong&gt;minimal memory use&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Dead simple to embed into &lt;strong&gt;any stack&lt;/strong&gt; (Rust, Python, or LUA or anything else that can interface with C)&lt;/li&gt;\n&lt;li&gt;Born for &lt;strong&gt;edge devices&lt;/strong&gt;, MCUs, and other weird places LLMs don&amp;#39;t usually go&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ll work on some fun stuff as well, like adapting and converting large LLMs (as long as licensing allows) to this specialised runtime.&lt;/p&gt;\n\n&lt;p&gt;Note: It’s super early. No model loading yet. No inference. Just early scaffolding and dreams.&lt;br/&gt;\nBut if you&amp;#39;re into LLMs, embedded stuff, or like watching weird low-level projects grow - follow along or contribute!&lt;/p&gt;\n\n&lt;p&gt;Repo: [llamac](&lt;a href=\"https://github.com/llamac-lab/llamac\"&gt;https://github.com/llamac-lab/llamac&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;Note: about the flair, could not decide between generation or new model so I went with new model. More like new runtime.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/lWjJdcJe5yyBPoySwGyf4ZMSAVXJpSrM43u2nmieDUY.png?auto=webp&amp;s=a3a05a15a5a23d6cd68aef22d033ab8f79da092f",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/lWjJdcJe5yyBPoySwGyf4ZMSAVXJpSrM43u2nmieDUY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=843e60c1255f40c94aa5aef6086d2d53756bb490",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/lWjJdcJe5yyBPoySwGyf4ZMSAVXJpSrM43u2nmieDUY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c613caf7eaf7d782e6415ed96da5eabc213f1b98",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/lWjJdcJe5yyBPoySwGyf4ZMSAVXJpSrM43u2nmieDUY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=81405381b463f0028759d01f0f39523a952042da",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/lWjJdcJe5yyBPoySwGyf4ZMSAVXJpSrM43u2nmieDUY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc02ed772d7763b4c227291c7397637b3ad1206a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/lWjJdcJe5yyBPoySwGyf4ZMSAVXJpSrM43u2nmieDUY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=08bbc9018399efc936b038e156cf8bdfcfab3b48",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/lWjJdcJe5yyBPoySwGyf4ZMSAVXJpSrM43u2nmieDUY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f9a769d0a1801bd9c73f0459564a58881d8821b9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "lWjJdcJe5yyBPoySwGyf4ZMSAVXJpSrM43u2nmieDUY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lx4ya7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rvnllm",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx4ya7/new_oss_project_llamaclab_or_a_pure_c_runtime_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx4ya7/new_oss_project_llamaclab_or_a_pure_c_runtime_for/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752234990,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I find the R1T2 Chimera shorter thinking very suitable for day-to-day code questions. I will give you an example where for the same simple question about modifications to be made to my java spring security cors configuration that allowed all origins, to allow only a specified domain and its subdomains:\n\nhttps://preview.redd.it/6aq4kdaff9cf1.png?width=1177&amp;format=png&amp;auto=webp&amp;s=9060f40e214b352e1f20fce21c610abc8077c62b\n\nhttps://preview.redd.it/8pf5m0ckf9cf1.png?width=1292&amp;format=png&amp;auto=webp&amp;s=8cd1a405ecccb1b15cbacde6f91955e68c94f81a\n\nhttps://preview.redd.it/zbvuc2otk9cf1.png?width=1222&amp;format=png&amp;auto=webp&amp;s=fc2725f46403b0dacf155960877868dae7d17157\n\nThe response was good for all three of them, but V3 responded with 500tokens (but only the updated line not all the method, but good enough for me), R1T2 took 1300tokens (2 minutes thinking + response), and R10528 needed 8200tokens (17 minutes thinking + response).  \nR1-0528 went on and on in the thinking stage wondering about what the developers settings will need to be to be able to test also locally, and allowed also localhost, [127.0.0.1](http://127.0.0.1) and other variations of them, even if I didn't ask for it. R1-0528 is overthinking almost everything, and I will still try it when I will arrive to a more complicated algorithm that maybe the R1T2 can't handle, but i didn't find something like that till now, I think the extra thinking is for difficult math problems or similar complex algorithms.  \nMy takeaway is that V3, R1T2, R1 all have the same knowledge base. For almost all the questions V3 is enough, for complex architecture ones, that require COT, R1T2 seems good enough without overthinking. For record breaking math solving skills, there is R1-0528. :)\n\nThese were the quants I used:  \n[https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/tree/main/IQ3\\_KS](https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/tree/main/IQ3_KS)  \n[https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/tree/main/IQ3\\_KS](https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/tree/main/IQ3_KS)  \n[https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD/tree/main/UD-Q3\\_K\\_XL](https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD/tree/main/UD-Q3_K_XL)\n\nIs your finding similar?\n\nP.S. I hope in the future will have a think/no\\_think or even deep\\_think option so we can load a unique model that suits all kinds of requests.",
          "author_fullname": "t2_j8fit2p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DeepSeek-TNG-R1T2-Chimera vs DeepSeek R1-0528 quick test",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 28,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "zbvuc2otk9cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 61,
                  "x": 108,
                  "u": "https://preview.redd.it/zbvuc2otk9cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd9c2d9b07897afe5021f95d21995f29c7b12efc"
                },
                {
                  "y": 122,
                  "x": 216,
                  "u": "https://preview.redd.it/zbvuc2otk9cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ae8ec16a7742d46f269cf6de105c512e6e7592d1"
                },
                {
                  "y": 181,
                  "x": 320,
                  "u": "https://preview.redd.it/zbvuc2otk9cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=88bfd869b23d0959770adb22ccfa8b082a83f8ef"
                },
                {
                  "y": 363,
                  "x": 640,
                  "u": "https://preview.redd.it/zbvuc2otk9cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9a0441ebd4940513ffe7c7af96f1bcd1ebb3a3f0"
                },
                {
                  "y": 545,
                  "x": 960,
                  "u": "https://preview.redd.it/zbvuc2otk9cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e6ee4e1e658376ed9c0121f628f0c845bc815ba9"
                },
                {
                  "y": 614,
                  "x": 1080,
                  "u": "https://preview.redd.it/zbvuc2otk9cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e0cbef41ad8e3573747152a2fc4f0e22b516db3a"
                }
              ],
              "s": {
                "y": 695,
                "x": 1222,
                "u": "https://preview.redd.it/zbvuc2otk9cf1.png?width=1222&amp;format=png&amp;auto=webp&amp;s=fc2725f46403b0dacf155960877868dae7d17157"
              },
              "id": "zbvuc2otk9cf1"
            },
            "6aq4kdaff9cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 21,
                  "x": 108,
                  "u": "https://preview.redd.it/6aq4kdaff9cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0f0a8a23a27781fcbc965c6f1717451afac2943f"
                },
                {
                  "y": 43,
                  "x": 216,
                  "u": "https://preview.redd.it/6aq4kdaff9cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a770d2d4d3608954da5c71466dbaacb109388fbb"
                },
                {
                  "y": 64,
                  "x": 320,
                  "u": "https://preview.redd.it/6aq4kdaff9cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=68af46fcc46570195c916ff956ea66af158ae7f2"
                },
                {
                  "y": 128,
                  "x": 640,
                  "u": "https://preview.redd.it/6aq4kdaff9cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1e983b4856c7708b69ba1bdc900f17c52878ac84"
                },
                {
                  "y": 193,
                  "x": 960,
                  "u": "https://preview.redd.it/6aq4kdaff9cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ba80d4ea541510b1b31678c80fad816d57edb3de"
                },
                {
                  "y": 217,
                  "x": 1080,
                  "u": "https://preview.redd.it/6aq4kdaff9cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ebc14eecb6b2bbc7739ffd740d6d1ef7e01b11b7"
                }
              ],
              "s": {
                "y": 237,
                "x": 1177,
                "u": "https://preview.redd.it/6aq4kdaff9cf1.png?width=1177&amp;format=png&amp;auto=webp&amp;s=9060f40e214b352e1f20fce21c610abc8077c62b"
              },
              "id": "6aq4kdaff9cf1"
            },
            "8pf5m0ckf9cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 40,
                  "x": 108,
                  "u": "https://preview.redd.it/8pf5m0ckf9cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=483c8dd42834a8cd306ee253d944df846b0cd717"
                },
                {
                  "y": 81,
                  "x": 216,
                  "u": "https://preview.redd.it/8pf5m0ckf9cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c5e52e5cac5ae519225c5d3f7e87ecd8e202008"
                },
                {
                  "y": 120,
                  "x": 320,
                  "u": "https://preview.redd.it/8pf5m0ckf9cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=afaac5d3747dbb429bdd6f87526a662f6ab1fee2"
                },
                {
                  "y": 241,
                  "x": 640,
                  "u": "https://preview.redd.it/8pf5m0ckf9cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0fe64d72dc6365c7b0f97854fee196a68b96ea55"
                },
                {
                  "y": 361,
                  "x": 960,
                  "u": "https://preview.redd.it/8pf5m0ckf9cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1c0c86ead672f2595c519a311f370c05c8cb450b"
                },
                {
                  "y": 407,
                  "x": 1080,
                  "u": "https://preview.redd.it/8pf5m0ckf9cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=66c4e24287a2fc4fd31538ca57dd7c58ff9d1987"
                }
              ],
              "s": {
                "y": 487,
                "x": 1292,
                "u": "https://preview.redd.it/8pf5m0ckf9cf1.png?width=1292&amp;format=png&amp;auto=webp&amp;s=8cd1a405ecccb1b15cbacde6f91955e68c94f81a"
              },
              "id": "8pf5m0ckf9cf1"
            }
          },
          "name": "t3_1lxa4hy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/XvodyXdpc2JgU7KT-qGirrTHxA_APKfHvxxbBimftEw.jpg",
          "edited": 1752248667,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752248362,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I find the R1T2 Chimera shorter thinking very suitable for day-to-day code questions. I will give you an example where for the same simple question about modifications to be made to my java spring security cors configuration that allowed all origins, to allow only a specified domain and its subdomains:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6aq4kdaff9cf1.png?width=1177&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9060f40e214b352e1f20fce21c610abc8077c62b\"&gt;https://preview.redd.it/6aq4kdaff9cf1.png?width=1177&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9060f40e214b352e1f20fce21c610abc8077c62b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8pf5m0ckf9cf1.png?width=1292&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8cd1a405ecccb1b15cbacde6f91955e68c94f81a\"&gt;https://preview.redd.it/8pf5m0ckf9cf1.png?width=1292&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8cd1a405ecccb1b15cbacde6f91955e68c94f81a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zbvuc2otk9cf1.png?width=1222&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc2725f46403b0dacf155960877868dae7d17157\"&gt;https://preview.redd.it/zbvuc2otk9cf1.png?width=1222&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc2725f46403b0dacf155960877868dae7d17157&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The response was good for all three of them, but V3 responded with 500tokens (but only the updated line not all the method, but good enough for me), R1T2 took 1300tokens (2 minutes thinking + response), and R10528 needed 8200tokens (17 minutes thinking + response).&lt;br/&gt;\nR1-0528 went on and on in the thinking stage wondering about what the developers settings will need to be to be able to test also locally, and allowed also localhost, &lt;a href=\"http://127.0.0.1\"&gt;127.0.0.1&lt;/a&gt; and other variations of them, even if I didn&amp;#39;t ask for it. R1-0528 is overthinking almost everything, and I will still try it when I will arrive to a more complicated algorithm that maybe the R1T2 can&amp;#39;t handle, but i didn&amp;#39;t find something like that till now, I think the extra thinking is for difficult math problems or similar complex algorithms.&lt;br/&gt;\nMy takeaway is that V3, R1T2, R1 all have the same knowledge base. For almost all the questions V3 is enough, for complex architecture ones, that require COT, R1T2 seems good enough without overthinking. For record breaking math solving skills, there is R1-0528. :)&lt;/p&gt;\n\n&lt;p&gt;These were the quants I used:&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/tree/main/IQ3_KS\"&gt;https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/tree/main/IQ3_KS&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/tree/main/IQ3_KS\"&gt;https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/tree/main/IQ3_KS&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD/tree/main/UD-Q3_K_XL\"&gt;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD/tree/main/UD-Q3_K_XL&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Is your finding similar?&lt;/p&gt;\n\n&lt;p&gt;P.S. I hope in the future will have a think/no_think or even deep_think option so we can load a unique model that suits all kinds of requests.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lxa4hy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ciprianveg",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxa4hy/deepseektngr1t2chimera_vs_deepseek_r10528_quick/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxa4hy/deepseektngr1t2chimera_vs_deepseek_r10528_quick/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752248362,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B](https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B)",
          "author_fullname": "t2_21gfv9kq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The New Nvidia Model is Really Chatty",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwl9ai",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 221,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/8bnc2od6i3cf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/8bnc2od6i3cf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/8bnc2od6i3cf1/DASHPlaylist.mpd?a=1754849081%2COTgyOTMzYWNhYTRjZGE0YWUwNjg3ZDNiZGQ2YjNiY2U3Njc3MGYwZWQ3OTcyZjlhODM2MzI4N2ZiYTYxZDIzOQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 24,
              "hls_url": "https://v.redd.it/8bnc2od6i3cf1/HLSPlaylist.m3u8?a=1754849081%2CZDk2ZDk2MTg1NDk3NTNjY2NmNTliY2IzOTViMTM4OTBkNDBjMGNkNDgwNDQ2YjdlOGIyOGM5YjU4MzlhOGQxNA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 221,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=50e25d3cdefacfdb52a19c3f3a73de8bb21ce0fe",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752174469,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B\"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/8bnc2od6i3cf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?format=pjpg&amp;auto=webp&amp;s=f7fe1fbe2bbf6cb35381d40fd5c4d9ffa87ee25c",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=44e66a13145d153d9d61c0c47cb1211182cf6bb4",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=458974266980903b8e094c9c4ea463e90f2a1f60",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=48aca6fabb03f9890200e56d65734ebccc58eb7f",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c8d5f1f78f383053dd71459b11bfc8680ef3571b",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=293ca88332e9cedef3647f159cafa6d6e23d0d10",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c2c58decc87168c504dfb2951ff10a337d22ec56",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lwl9ai",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SpyderJack",
          "discussion_type": null,
          "num_comments": 48,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwl9ai/the_new_nvidia_model_is_really_chatty/",
          "stickied": false,
          "url": "https://v.redd.it/8bnc2od6i3cf1",
          "subreddit_subscribers": 497502,
          "created_utc": 1752174469,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/8bnc2od6i3cf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/8bnc2od6i3cf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/8bnc2od6i3cf1/DASHPlaylist.mpd?a=1754849081%2COTgyOTMzYWNhYTRjZGE0YWUwNjg3ZDNiZGQ2YjNiY2U3Njc3MGYwZWQ3OTcyZjlhODM2MzI4N2ZiYTYxZDIzOQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 24,
              "hls_url": "https://v.redd.it/8bnc2od6i3cf1/HLSPlaylist.m3u8?a=1754849081%2CZDk2ZDk2MTg1NDk3NTNjY2NmNTliY2IzOTViMTM4OTBkNDBjMGNkNDgwNDQ2YjdlOGIyOGM5YjU4MzlhOGQxNA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am trying to learn about MCPs using a lightweight local LLM (windows laptop with 16GB RAM).\n\nI want a simple project, to integrate the LLM with existing MCP servers, to see what it's all about, experiment, and have fun.\n\n\nCan someone suggest a starting point, or a video tutorial?\n\n(most I've seen so far either don't explain step by step, or use \"exotic\" apps like Obsidian etc...)\n",
          "author_fullname": "t2_1441omqx4c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Simple barebones MCP tutorial?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx7loe",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752242300,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to learn about MCPs using a lightweight local LLM (windows laptop with 16GB RAM).&lt;/p&gt;\n\n&lt;p&gt;I want a simple project, to integrate the LLM with existing MCP servers, to see what it&amp;#39;s all about, experiment, and have fun.&lt;/p&gt;\n\n&lt;p&gt;Can someone suggest a starting point, or a video tutorial?&lt;/p&gt;\n\n&lt;p&gt;(most I&amp;#39;ve seen so far either don&amp;#39;t explain step by step, or use &amp;quot;exotic&amp;quot; apps like Obsidian etc...)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lx7loe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cangaroo_hamam",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx7loe/simple_barebones_mcp_tutorial/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx7loe/simple_barebones_mcp_tutorial/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752242300,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I encountered this really cool project, EuroEval, which has LLM benchmarks of many open-weights models in different European languages (🇩🇰 Danish, 🇳🇱 Dutch, 🇬🇧 English, 🇫🇴 Faroese, 🇫🇮 Finnish, 🇫🇷 French, 🇩🇪 German, 🇮🇸 Icelandic, 🇮🇹 Italian, 🇳🇴 Norwegian, 🇪🇸 Spanish, 🇸🇪 Swedish).\n  \n&gt;EuroEval is a language model benchmarking framework that supports evaluating all types of language models out there: encoders, decoders, encoder-decoders, base models, and instruction tuned models. EuroEval has been battle-tested for more than three years and are the standard evaluation benchmark for many companies, universities and organisations around Europe.\n\n&gt;Check out the [leaderboards](https://euroeval.com/leaderboards) to see how different language models perform on a wide range of tasks in various European languages. The leaderboards are updated regularly with new models and new results. All benchmark results have been computed using the associated [EuroEval Python package](https://euroeval.com/python-package), which you can use to replicate all the results. It supports all models on the [Hugging Face Hub](https://huggingface.co/models), as well as models accessible through 100+ different APIs, including models you are hosting yourself via, e.g., [Ollama](https://ollama.com/) or [LM Studio](https://lmstudio.ai/).\n\n&gt;The idea of EuroEval grew out of the development of Danish language model RøBÆRTa in 2021, when we realised that there was no standard way to evaluate Danish language models. It started as a hobby project including Danish, Swedish and Norwegian, but has since grown to include 12+ European languages.\n\n&gt;EuroEval is maintained by [Dan Saattrup Smart](https://www.saattrupdan.com/) from the [Alexandra Institute](https://alexandra.dk/), and is funded by the EU project [TrustLLM](https://trustllm.eu/).\n\n* Leaderboard: [https://euroeval.com/leaderboards/](https://euroeval.com/leaderboards/)\n* Source code: [https://github.com/EuroEval/EuroEval](https://github.com/EuroEval/EuroEval)",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "EuroEval: The robust European language model benchmark.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx3u8s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752231366,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "euroeval.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I encountered this really cool project, EuroEval, which has LLM benchmarks of many open-weights models in different European languages (🇩🇰 Danish, 🇳🇱 Dutch, 🇬🇧 English, 🇫🇴 Faroese, 🇫🇮 Finnish, 🇫🇷 French, 🇩🇪 German, 🇮🇸 Icelandic, 🇮🇹 Italian, 🇳🇴 Norwegian, 🇪🇸 Spanish, 🇸🇪 Swedish).&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;EuroEval is a language model benchmarking framework that supports evaluating all types of language models out there: encoders, decoders, encoder-decoders, base models, and instruction tuned models. EuroEval has been battle-tested for more than three years and are the standard evaluation benchmark for many companies, universities and organisations around Europe.&lt;/p&gt;\n\n&lt;p&gt;Check out the &lt;a href=\"https://euroeval.com/leaderboards\"&gt;leaderboards&lt;/a&gt; to see how different language models perform on a wide range of tasks in various European languages. The leaderboards are updated regularly with new models and new results. All benchmark results have been computed using the associated &lt;a href=\"https://euroeval.com/python-package\"&gt;EuroEval Python package&lt;/a&gt;, which you can use to replicate all the results. It supports all models on the &lt;a href=\"https://huggingface.co/models\"&gt;Hugging Face Hub&lt;/a&gt;, as well as models accessible through 100+ different APIs, including models you are hosting yourself via, e.g., &lt;a href=\"https://ollama.com/\"&gt;Ollama&lt;/a&gt; or &lt;a href=\"https://lmstudio.ai/\"&gt;LM Studio&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;The idea of EuroEval grew out of the development of Danish language model RøBÆRTa in 2021, when we realised that there was no standard way to evaluate Danish language models. It started as a hobby project including Danish, Swedish and Norwegian, but has since grown to include 12+ European languages.&lt;/p&gt;\n\n&lt;p&gt;EuroEval is maintained by &lt;a href=\"https://www.saattrupdan.com/\"&gt;Dan Saattrup Smart&lt;/a&gt; from the &lt;a href=\"https://alexandra.dk/\"&gt;Alexandra Institute&lt;/a&gt;, and is funded by the EU project &lt;a href=\"https://trustllm.eu/\"&gt;TrustLLM&lt;/a&gt;.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Leaderboard: &lt;a href=\"https://euroeval.com/leaderboards/\"&gt;https://euroeval.com/leaderboards/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Source code: &lt;a href=\"https://github.com/EuroEval/EuroEval\"&gt;https://github.com/EuroEval/EuroEval&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://euroeval.com/leaderboards/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lx3u8s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx3u8s/euroeval_the_robust_european_language_model/",
          "stickied": false,
          "url": "https://euroeval.com/leaderboards/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752231366,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1162lx9rgr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "mistralai/Devstral-Small-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwe5y8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#ab96c2",
          "ups": 423,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d40ca12a-0e73-11ee-8563-f216e082168e",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 423,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=c63ef6cf39d328392932c6db5dd51a45f12fb26e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 2"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752157759,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/mistralai/Devstral-Small-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?auto=webp&amp;s=decfdf8365e20616b3021280253b6b910ea41fcf",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f04a531a9dfe8f1024433fe94c145846144b089b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=65c06acf21401f8c0323339e94227ba2948590c6",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=677035b0e8f6dea7569670a08409cd8abbadf838",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=299e4f7a5df68d789749c7d30f346b534a08b8ba",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4edd4a83f17a895ce6231f54e721e098610b6952",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f80413cc943fa2d192927b6a8c1f8031a81dce0f",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 2",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lwe5y8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yoracale",
          "discussion_type": null,
          "num_comments": 138,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lwe5y8/mistralaidevstralsmall2507/",
          "stickied": false,
          "url": "https://huggingface.co/mistralai/Devstral-Small-2507",
          "subreddit_subscribers": 497502,
          "created_utc": 1752157759,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In terms of minutes/hours or number of query/response?\n\nI'm averaging around 90 minutes on good days and 30 minutes on bad days.",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How much do you use your local model on average on a day?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lxbynb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752252611,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In terms of minutes/hours or number of query/response?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m averaging around 90 minutes on good days and 30 minutes on bad days.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lxbynb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752252611,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Choosing the right on-device LLM is a major challenge 🤔. How do you balance speed, size, and true intelligence? To find a definitive answer, we created the BastionRank Benchmark.We put 10 of the most promising models through a rigorous gauntlet of tests designed to simulate real-world developer and user needs 🥊. Our evaluation covered three critical areas:\n\n⚡️ Raw Performance: We measured Time-To-First-Token (responsiveness) and Tokens/Second (generation speed) to find the true speed kings.\n\n🧠 Qualitative Intelligence: Can a model understand the nuance of literary prose (Moby Dick) and the precision of a technical paper? We tested both.\n\n🤖 Structured Reasoning: The ultimate test for building local AI agents. We assessed each model's ability to extract clean, structured data from a business memo.The results were fascinating, revealing a clear hierarchy of performance and some surprising nuances in model behavior.\n\nFind out which models made the top of our tiered rankings 🏆 and see our full analysis in the complete blog post. Read the full report on our official blog or on Medium:\n\n👉 Medium: [https://medium.com/@freddyayala/the-bastionrank-showdown-crowning-the-best-on-device-ai-models-of-2025-95a3c058401e](https://medium.com/@freddyayala/the-bastionrank-showdown-crowning-the-best-on-device-ai-models-of-2025-95a3c058401e)",
          "author_fullname": "t2_2cmiiytn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The BastionRank Showdown: Crowning the Best On-Device AI Models of 2025",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lxaz08",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752250316,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Choosing the right on-device LLM is a major challenge 🤔. How do you balance speed, size, and true intelligence? To find a definitive answer, we created the BastionRank Benchmark.We put 10 of the most promising models through a rigorous gauntlet of tests designed to simulate real-world developer and user needs 🥊. Our evaluation covered three critical areas:&lt;/p&gt;\n\n&lt;p&gt;⚡️ Raw Performance: We measured Time-To-First-Token (responsiveness) and Tokens/Second (generation speed) to find the true speed kings.&lt;/p&gt;\n\n&lt;p&gt;🧠 Qualitative Intelligence: Can a model understand the nuance of literary prose (Moby Dick) and the precision of a technical paper? We tested both.&lt;/p&gt;\n\n&lt;p&gt;🤖 Structured Reasoning: The ultimate test for building local AI agents. We assessed each model&amp;#39;s ability to extract clean, structured data from a business memo.The results were fascinating, revealing a clear hierarchy of performance and some surprising nuances in model behavior.&lt;/p&gt;\n\n&lt;p&gt;Find out which models made the top of our tiered rankings 🏆 and see our full analysis in the complete blog post. Read the full report on our official blog or on Medium:&lt;/p&gt;\n\n&lt;p&gt;👉 Medium: &lt;a href=\"https://medium.com/@freddyayala/the-bastionrank-showdown-crowning-the-best-on-device-ai-models-of-2025-95a3c058401e\"&gt;https://medium.com/@freddyayala/the-bastionrank-showdown-crowning-the-best-on-device-ai-models-of-2025-95a3c058401e&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/fI5k02DR1sOEKQLbuJoh-Ag2mqh9Q28OHsSdk9zyjQE.png?auto=webp&amp;s=d40ea71c2aabd9bac682acf02c6bfa2699dbe704",
                  "width": 410,
                  "height": 410
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/fI5k02DR1sOEKQLbuJoh-Ag2mqh9Q28OHsSdk9zyjQE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd8c820178ab5cc2c4f33f2be0e9c5ff6994dd37",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/fI5k02DR1sOEKQLbuJoh-Ag2mqh9Q28OHsSdk9zyjQE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d5ed78eac1b9e41d0b0219518d102b626658a3f2",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/fI5k02DR1sOEKQLbuJoh-Ag2mqh9Q28OHsSdk9zyjQE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c30d8f95d348fc8617d81b15858638bf86f8bde2",
                    "width": 320,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "fI5k02DR1sOEKQLbuJoh-Ag2mqh9Q28OHsSdk9zyjQE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lxaz08",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "frayala87",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxaz08/the_bastionrank_showdown_crowning_the_best/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxaz08/the_bastionrank_showdown_crowning_the_best/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752250316,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Context here: WSLv2, Win11, Blackwell Pro 6000 workstation.\n\nI've beaten my head against the wall with W8A8 FP8 support and kind of loosely eyed NVFP4 from a distance, fully expecting it to be a nightmare. Like may of you I've seen on here, I went through the gauntlet and very specific hell of trying to build vllm + flash-attention + flashinfer from HEAD on nightly pytorch to get W8A8 support only to have things blow up in my face. Partial CUTLASS support, lack of Gemma-3 vision support, flash-attention version failures when combined with certain models, flashinfer failures, etc.\n\nSo my question to the community: has anyone gotten FP8 support working in Blackwell and lived to tell the tale? What about TensorRT-LLM w/NVFP4 support? If so - got any pointers for how to do it?\n\nFully acknowledging that vllm Blackwell enablement isn't done: [link](https://github.com/vllm-project/vllm/issues/18153), but should be done enough to work at this point?\n\nIdeally we could get a set of gists together on github to automate the setup of both environments that we all collaborate on to unstick this, assuming I'm not just completely failing at something obvious.\n\nPart of the problem as well seems to be in model choice; I've been specifically trying to get a Gemma-3-27b + Devstral-Small stack together and going for various Roo pipeline steps, and it seems like running those newer models in the TensorRT-LLM ecosystem is extra painful.\n\nedit: Lest I be the asshole just generally complaining and asking for things without giving back, here's a current(ish?) version of a script to build vllm and deps from HEAD that I've been using locally below in comments. Could be augmented to calculate the correct MAX_JOBS for `flash-attention` and `vllm` builds based on available system memory; right now I have it calibrated for my ~96GB system ram I'm allocating in WSLv2.",
          "author_fullname": "t2_b78412ov",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Blackwell FP8 W8A8 NVFP4 support discussion",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx4zpr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752243703,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752235114,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context here: WSLv2, Win11, Blackwell Pro 6000 workstation.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve beaten my head against the wall with W8A8 FP8 support and kind of loosely eyed NVFP4 from a distance, fully expecting it to be a nightmare. Like may of you I&amp;#39;ve seen on here, I went through the gauntlet and very specific hell of trying to build vllm + flash-attention + flashinfer from HEAD on nightly pytorch to get W8A8 support only to have things blow up in my face. Partial CUTLASS support, lack of Gemma-3 vision support, flash-attention version failures when combined with certain models, flashinfer failures, etc.&lt;/p&gt;\n\n&lt;p&gt;So my question to the community: has anyone gotten FP8 support working in Blackwell and lived to tell the tale? What about TensorRT-LLM w/NVFP4 support? If so - got any pointers for how to do it?&lt;/p&gt;\n\n&lt;p&gt;Fully acknowledging that vllm Blackwell enablement isn&amp;#39;t done: &lt;a href=\"https://github.com/vllm-project/vllm/issues/18153\"&gt;link&lt;/a&gt;, but should be done enough to work at this point?&lt;/p&gt;\n\n&lt;p&gt;Ideally we could get a set of gists together on github to automate the setup of both environments that we all collaborate on to unstick this, assuming I&amp;#39;m not just completely failing at something obvious.&lt;/p&gt;\n\n&lt;p&gt;Part of the problem as well seems to be in model choice; I&amp;#39;ve been specifically trying to get a Gemma-3-27b + Devstral-Small stack together and going for various Roo pipeline steps, and it seems like running those newer models in the TensorRT-LLM ecosystem is extra painful.&lt;/p&gt;\n\n&lt;p&gt;edit: Lest I be the asshole just generally complaining and asking for things without giving back, here&amp;#39;s a current(ish?) version of a script to build vllm and deps from HEAD that I&amp;#39;ve been using locally below in comments. Could be augmented to calculate the correct MAX_JOBS for &lt;code&gt;flash-attention&lt;/code&gt; and &lt;code&gt;vllm&lt;/code&gt; builds based on available system memory; right now I have it calibrated for my ~96GB system ram I&amp;#39;m allocating in WSLv2.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/w9EtvIrUzqo5Lu-tyiPoB9wYov3Zce-6YSW9Kr_hD50.png?auto=webp&amp;s=a8f7bf991ab8e8c135c5eb5a289e66e89c97a09b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/w9EtvIrUzqo5Lu-tyiPoB9wYov3Zce-6YSW9Kr_hD50.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4ce923d5f84359f99cae7bf1ae54488a8122402",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/w9EtvIrUzqo5Lu-tyiPoB9wYov3Zce-6YSW9Kr_hD50.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=23347a018f59eee8b2601cd17d3f163a314f6d41",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/w9EtvIrUzqo5Lu-tyiPoB9wYov3Zce-6YSW9Kr_hD50.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=22b0db7c43cd7eb9c413d0a9650aeaa30d754958",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/w9EtvIrUzqo5Lu-tyiPoB9wYov3Zce-6YSW9Kr_hD50.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=824d367ada5c76edcccc96f6433c6b2d58001226",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/w9EtvIrUzqo5Lu-tyiPoB9wYov3Zce-6YSW9Kr_hD50.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=889dd00ece8bf23b9817fadb20775fa36640bfbb",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/w9EtvIrUzqo5Lu-tyiPoB9wYov3Zce-6YSW9Kr_hD50.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2bec6d3d2649ff2d8c7a94a1f56359e03275dc20",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "w9EtvIrUzqo5Lu-tyiPoB9wYov3Zce-6YSW9Kr_hD50"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lx4zpr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kitchen-Year-8434",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752235114,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We just hit [15K users](https://www.designarena.ai/)! For context of course, see [this post](https://www.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/). Since then, we have added Grok 4, several Devstral Small, Devstral Medium, Gemini 2.5 Flash, and Qwen-235B-A22B. \n\nWe now thankfully have more access to various kind of models (particularly OS and open weight) thanks to [Fireworks AI](https://app.fireworks.ai/account/home) and we'll be periodically adding more models throughout the weekend. \n\nWhich models would you like to see added to the leaderboard? We're looking to add as many as possible. ",
          "author_fullname": "t2_c3b3edv5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "What other models would you like to see on Design Arena?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 54,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "u37y8ocla6cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 42,
                  "x": 108,
                  "u": "https://preview.redd.it/u37y8ocla6cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=603d7c8a5b45893503082927fd92b8fc691da07d"
                },
                {
                  "y": 84,
                  "x": 216,
                  "u": "https://preview.redd.it/u37y8ocla6cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d45720f8bd30c8f377cefa5c0b72436a8caf7e33"
                },
                {
                  "y": 124,
                  "x": 320,
                  "u": "https://preview.redd.it/u37y8ocla6cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17f4a35166d50e91b498f5a82c907ed970138884"
                },
                {
                  "y": 249,
                  "x": 640,
                  "u": "https://preview.redd.it/u37y8ocla6cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=24a387c6b022593520eefe83359fc4c1a87ea68b"
                },
                {
                  "y": 374,
                  "x": 960,
                  "u": "https://preview.redd.it/u37y8ocla6cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=551687536b6a08f931b5b6de8cb223432a0e9142"
                },
                {
                  "y": 421,
                  "x": 1080,
                  "u": "https://preview.redd.it/u37y8ocla6cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9cdb462bc79223b557cb59c6866cdf353a016c57"
                }
              ],
              "s": {
                "y": 1144,
                "x": 2932,
                "u": "https://preview.redd.it/u37y8ocla6cf1.png?width=2932&amp;format=png&amp;auto=webp&amp;s=321e6c9d757e587d730e3b63a07c0de4b34c1b6b"
              },
              "id": "u37y8ocla6cf1"
            },
            "qhfls1vma6cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/qhfls1vma6cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=586646b880b662d777d4b83169d07f463ccdf102"
                },
                {
                  "y": 129,
                  "x": 216,
                  "u": "https://preview.redd.it/qhfls1vma6cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=41ef70b20168be55a59584cfa321cd22d09809f1"
                },
                {
                  "y": 191,
                  "x": 320,
                  "u": "https://preview.redd.it/qhfls1vma6cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=57d164f455a45692905803c6386c3cf1e7c15b6a"
                },
                {
                  "y": 383,
                  "x": 640,
                  "u": "https://preview.redd.it/qhfls1vma6cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc1a5c84ab16c3a8fb5c060c0879122464d74c54"
                },
                {
                  "y": 574,
                  "x": 960,
                  "u": "https://preview.redd.it/qhfls1vma6cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3c112b68b731ef17710c79393edc403f99ef2dd1"
                },
                {
                  "y": 646,
                  "x": 1080,
                  "u": "https://preview.redd.it/qhfls1vma6cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5d5c5686a8a6146270a80327e1352070b3971c73"
                }
              ],
              "s": {
                "y": 1300,
                "x": 2172,
                "u": "https://preview.redd.it/qhfls1vma6cf1.png?width=2172&amp;format=png&amp;auto=webp&amp;s=3678317de4b842e0038c3d0fb948bd5300984be4"
              },
              "id": "qhfls1vma6cf1"
            }
          },
          "name": "t3_1lwxr2l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 23,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "u37y8ocla6cf1",
                "id": 703328100
              },
              {
                "media_id": "qhfls1vma6cf1",
                "id": 703328101
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/vyaiGpslyuDvhVMrcw8-eJ5unh7Eixe4i35Y204tnqU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752208226,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We just hit &lt;a href=\"https://www.designarena.ai/\"&gt;15K users&lt;/a&gt;! For context of course, see &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/\"&gt;this post&lt;/a&gt;. Since then, we have added Grok 4, several Devstral Small, Devstral Medium, Gemini 2.5 Flash, and Qwen-235B-A22B. &lt;/p&gt;\n\n&lt;p&gt;We now thankfully have more access to various kind of models (particularly OS and open weight) thanks to &lt;a href=\"https://app.fireworks.ai/account/home\"&gt;Fireworks AI&lt;/a&gt; and we&amp;#39;ll be periodically adding more models throughout the weekend. &lt;/p&gt;\n\n&lt;p&gt;Which models would you like to see added to the leaderboard? We&amp;#39;re looking to add as many as possible. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lwxr2l",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwxr2l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adviceguru25",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwxr2l/what_other_models_would_you_like_to_see_on_design/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lwxr2l",
          "subreddit_subscribers": 497502,
          "created_utc": 1752208226,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone, Reka just open-sourced a new quantisation method which looks promising for local inference and VRAM-limited setups.\n\nAccording to their benchmarks, the new method significantly outperforms llama.cpp's standard Q3_K_S, narrowing the performance gap with Q4_K_M or higher quants. This could be great news for the local inference community.\n\nWhat are your thoughts on this new method?\n\n- Blog Post: [Reka Quantization Technology](https://reka.ai/news/reka-quantization-technology)  \n- Source Code: [GitHub](https://github.com/reka-ai/rekaquant) \n- Quantised Model: [reka-flash-3.1-rekaquant-q3_k_s](https://huggingface.co/RekaAI/reka-flash-3.1-rekaquant-q3_k_s)",
          "author_fullname": "t2_15dxzs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Reka Flash 3.1 benchmarks show strong progress in LLM quantisation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "6fmj9xb0f3cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 149,
                  "x": 108,
                  "u": "https://preview.redd.it/6fmj9xb0f3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d37cab47fe2b3167bc53103b90c338816e15de0d"
                },
                {
                  "y": 298,
                  "x": 216,
                  "u": "https://preview.redd.it/6fmj9xb0f3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2cf1a5cc9422bc6023f07a5ee540837948d71f6a"
                },
                {
                  "y": 442,
                  "x": 320,
                  "u": "https://preview.redd.it/6fmj9xb0f3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d839370f16205a62e18ab63a2cf94d3f6147cb81"
                },
                {
                  "y": 885,
                  "x": 640,
                  "u": "https://preview.redd.it/6fmj9xb0f3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4dfeb34ba2915d43ccf36d67f8c49ae0b7f55d8"
                },
                {
                  "y": 1328,
                  "x": 960,
                  "u": "https://preview.redd.it/6fmj9xb0f3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=462a693ebabbc4781fed4f885c72dbf8609dc4ba"
                }
              ],
              "s": {
                "y": 1384,
                "x": 1000,
                "u": "https://preview.redd.it/6fmj9xb0f3cf1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=2d006b859958e34c2bb2c62f83c43a20d225b16d"
              },
              "id": "6fmj9xb0f3cf1"
            }
          },
          "name": "t3_1lwkrg4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 118,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 118,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=6ee9c0ff9008ca4bef6848d1f294880cec91b504",
          "edited": 1752173696,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752173307,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, Reka just open-sourced a new quantisation method which looks promising for local inference and VRAM-limited setups.&lt;/p&gt;\n\n&lt;p&gt;According to their benchmarks, the new method significantly outperforms llama.cpp&amp;#39;s standard Q3_K_S, narrowing the performance gap with Q4_K_M or higher quants. This could be great news for the local inference community.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on this new method?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Blog Post: &lt;a href=\"https://reka.ai/news/reka-quantization-technology\"&gt;Reka Quantization Technology&lt;/a&gt;&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Source Code: &lt;a href=\"https://github.com/reka-ai/rekaquant\"&gt;GitHub&lt;/a&gt; &lt;/li&gt;\n&lt;li&gt;Quantised Model: &lt;a href=\"https://huggingface.co/RekaAI/reka-flash-3.1-rekaquant-q3_k_s\"&gt;reka-flash-3.1-rekaquant-q3_k_s&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?auto=webp&amp;s=e91d4b8daf73181934b280324eae98b2b6a23205",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=789320b03d84c0e8a0e0035cd6e312b24ffd1166",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bce1ce16c6e71b4e054c184c1a68566e5fe9ad9d",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d045817fb3786098058c612f14c54d5c59bb9d9",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1933f58a3a9b626d8fe0fc66ca8cbd276198d9d8",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=75350929166b4a401f3f2c927023d7707dde1a5a",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6734799977ec9496e4557c0d1c927b5337f4cff4",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwkrg4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "benja0x40",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwkrg4/reka_flash_31_benchmarks_show_strong_progress_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwkrg4/reka_flash_31_benchmarks_show_strong_progress_in/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752173307,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7pfgfkis",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Grok 4 on Fiction.liveBench Long Context Comprehension",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwn3ut",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "ups": 80,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 80,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/O1GNGtr4DIDHp9RkCuk3_-GgG-OAjpoDKNEauoT6i-A.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752178868,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/rzwo8emcv3cf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/rzwo8emcv3cf1.png?auto=webp&amp;s=c51c914943cfb77ccb73689f4d460162925d4a03",
                  "width": 1704,
                  "height": 2486
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/rzwo8emcv3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=00960dbf00b92ec8a09e74a1754bc8c7439cff4c",
                    "width": 108,
                    "height": 157
                  },
                  {
                    "url": "https://preview.redd.it/rzwo8emcv3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b89c1e8c1e3b63f98479b37d9cdd9cad4824355",
                    "width": 216,
                    "height": 315
                  },
                  {
                    "url": "https://preview.redd.it/rzwo8emcv3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=714ae0eced0e2b444937f0c1272ce3dfa8b2df0c",
                    "width": 320,
                    "height": 466
                  },
                  {
                    "url": "https://preview.redd.it/rzwo8emcv3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=87eea0d411941e119cea7f8020ad7157923da79a",
                    "width": 640,
                    "height": 933
                  },
                  {
                    "url": "https://preview.redd.it/rzwo8emcv3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=58cf39362c4b8b76fd9d44ce4da6642118fd0051",
                    "width": 960,
                    "height": 1400
                  },
                  {
                    "url": "https://preview.redd.it/rzwo8emcv3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a0881f3b0ae158c64ec03b0235b354a266f5f0f",
                    "width": 1080,
                    "height": 1575
                  }
                ],
                "variants": {},
                "id": "5NEN-_gSilO1hhg4KWRyfkPXBZDPAN0Y9f7M4g7ixgw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lwn3ut",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fictionlive",
          "discussion_type": null,
          "num_comments": 42,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwn3ut/grok_4_on_fictionlivebench_long_context/",
          "stickied": false,
          "url": "https://i.redd.it/rzwo8emcv3cf1.png",
          "subreddit_subscribers": 497502,
          "created_utc": 1752178868,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have only been utilizing LLMs for a little more than a year and only started down the local LLM path a few months ago, so bare with me if there are already papers about my following question:\n\nAre there any models/agents that can cache previous context windows, encode the cached context into weight scalers, and then apply the new weights to the models, so they essentially hardcode all conversations into their own training data?\n\nI understand why you wouldn't want to do this on a public LLM as you are relying on the integrity of the entire userbase not to intentionally break the model with adversarial prompts, but is this potentially possible within the limitations of the current llama.cpp toolset?",
          "author_fullname": "t2_7ltgyzy0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Potentially Noob Question Regarding Live Weight Adjustments",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx7kfh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752242208,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have only been utilizing LLMs for a little more than a year and only started down the local LLM path a few months ago, so bare with me if there are already papers about my following question:&lt;/p&gt;\n\n&lt;p&gt;Are there any models/agents that can cache previous context windows, encode the cached context into weight scalers, and then apply the new weights to the models, so they essentially hardcode all conversations into their own training data?&lt;/p&gt;\n\n&lt;p&gt;I understand why you wouldn&amp;#39;t want to do this on a public LLM as you are relying on the integrity of the entire userbase not to intentionally break the model with adversarial prompts, but is this potentially possible within the limitations of the current llama.cpp toolset?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lx7kfh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pyromancer777",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx7kfh/potentially_noob_question_regarding_live_weight/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx7kfh/potentially_noob_question_regarding_live_weight/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752242208,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi! I'm compiling a list of document parsers available on the market and testing their feature coverage. \n\nSo far, I've tested 14 OCRs/parsers for tables, equations, handwriting, two-column layouts, and multiple-column layouts. You can view the outputs from each parser in the \\`results\\` folder. The ones I've tested are mostly open source or with generous free quota.\n\n🚩 Coming soon: benchmarks for each OCR - score from 0 (doesn't work) to 5 (perfect)\n\nFeedback &amp; contribution are welcome!",
          "author_fullname": "t2_bb96oc9v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I'm curating a list of every OCR out there and running tests on their features. Contribution welcome!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwgohu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 161,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 161,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=50783bab933a2e12113a00ee8a3801748c9411b9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752163778,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I&amp;#39;m compiling a list of document parsers available on the market and testing their feature coverage. &lt;/p&gt;\n\n&lt;p&gt;So far, I&amp;#39;ve tested 14 OCRs/parsers for tables, equations, handwriting, two-column layouts, and multiple-column layouts. You can view the outputs from each parser in the `results` folder. The ones I&amp;#39;ve tested are mostly open source or with generous free quota.&lt;/p&gt;\n\n&lt;p&gt;🚩 Coming soon: benchmarks for each OCR - score from 0 (doesn&amp;#39;t work) to 5 (perfect)&lt;/p&gt;\n\n&lt;p&gt;Feedback &amp;amp; contribution are welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/GiftMungmeeprued/document-parsers-list",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?auto=webp&amp;s=55f6f74a365dbd25b015f7179727d7c7f96094a6",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=441cb0387ac300bda66840c0aabf6acfd239e0c4",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d5e3f11dbbf36dc0fcaee2c4608c5a7b3323a4a6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ddb31672cb824bc055562606a31ed21c689e7b5a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=845ce9e8cd2f0eec39ba036028db301cc1226bab",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e11f548f6fd8ccb9943603db747b29acdad91bb",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=84284766869437005589220b274c461043b79652",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwgohu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Help9178",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwgohu/im_curating_a_list_of_every_ocr_out_there_and/",
          "stickied": false,
          "url": "https://github.com/GiftMungmeeprued/document-parsers-list",
          "subreddit_subscribers": 497502,
          "created_utc": 1752163778,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It understands it in the beginning, but as conversation increases, it starts becoming a paragraph spewing machine. \n\nOnly way I can think of is to re-run responses on a 2nd AI conversation and ask it to re-write it shortly, then channel it back to the conversation. ",
          "author_fullname": "t2_1oeu2j1o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I force the LLM to respond shortly?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx3p4i",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752230862,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It understands it in the beginning, but as conversation increases, it starts becoming a paragraph spewing machine. &lt;/p&gt;\n\n&lt;p&gt;Only way I can think of is to re-run responses on a 2nd AI conversation and ask it to re-write it shortly, then channel it back to the conversation. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lx3p4i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "freecodeio",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx3p4i/how_do_i_force_the_llm_to_respond_shortly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx3p4i/how_do_i_force_the_llm_to_respond_shortly/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752230862,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "During my first months at Hugging Face, I worked on Hybrid Quantization, also known as Sensitivity-Aware Mixed Precision Quantization. Each layer is quantized based on its sensitivity score: robust layers receive more aggressive quantization, and sensitive layers are preserved at higher precision.\n\nThe key question is how to effectively measure these sensitivity scores. While known methods such as Hessian-based approaches exist, I found them too slow and not scalable. Instead, I used what I call a divergence-based method, which relies on computing the Jensen-Shannon Divergence (JSD) between the layer logits of the full-precision model and those of the model with one layer quantized at a time.\n\nThe detailed work can be found here: [https://huggingface.co/blog/badaoui/sensitivity-aware-mixed-precision-quantizer-v1](https://huggingface.co/blog/badaoui/sensitivity-aware-mixed-precision-quantizer-v1) \n\nWould love to hear your thoughts on it!",
          "author_fullname": "t2_m3yt1z76p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Sensitivity Aware Mixed Precision Quantization",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx3ngj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752230691,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;During my first months at Hugging Face, I worked on Hybrid Quantization, also known as Sensitivity-Aware Mixed Precision Quantization. Each layer is quantized based on its sensitivity score: robust layers receive more aggressive quantization, and sensitive layers are preserved at higher precision.&lt;/p&gt;\n\n&lt;p&gt;The key question is how to effectively measure these sensitivity scores. While known methods such as Hessian-based approaches exist, I found them too slow and not scalable. Instead, I used what I call a divergence-based method, which relies on computing the Jensen-Shannon Divergence (JSD) between the layer logits of the full-precision model and those of the model with one layer quantized at a time.&lt;/p&gt;\n\n&lt;p&gt;The detailed work can be found here: &lt;a href=\"https://huggingface.co/blog/badaoui/sensitivity-aware-mixed-precision-quantizer-v1\"&gt;https://huggingface.co/blog/badaoui/sensitivity-aware-mixed-precision-quantizer-v1&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Would love to hear your thoughts on it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZSg1l9Yd03KVonWynPruA_LkKrURCTR2Tg9rjTv5dJc.png?auto=webp&amp;s=a20bea077eff102be8452cd17395bee82eec5e55",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZSg1l9Yd03KVonWynPruA_LkKrURCTR2Tg9rjTv5dJc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6558e013b15e77a0b562e69edc7c0dcca57b878c",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/ZSg1l9Yd03KVonWynPruA_LkKrURCTR2Tg9rjTv5dJc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=623719a65845c5962acf075e1fc032beb5f6d3fe",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/ZSg1l9Yd03KVonWynPruA_LkKrURCTR2Tg9rjTv5dJc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c65992174cba2853d0c8a0371fd92de9f3eeb6a0",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/ZSg1l9Yd03KVonWynPruA_LkKrURCTR2Tg9rjTv5dJc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=59b1cf1fa355bd1835103399a97b2e05ad83fc18",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/ZSg1l9Yd03KVonWynPruA_LkKrURCTR2Tg9rjTv5dJc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4252eb6a337052fdd9c7508d231abc78ad3cc42d",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/ZSg1l9Yd03KVonWynPruA_LkKrURCTR2Tg9rjTv5dJc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9256d61a46503f22b0c306993f9c577bce01f13b",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "ZSg1l9Yd03KVonWynPruA_LkKrURCTR2Tg9rjTv5dJc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lx3ngj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Swimming-Heart-8667",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx3ngj/sensitivity_aware_mixed_precision_quantization/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx3ngj/sensitivity_aware_mixed_precision_quantization/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752230691,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi there guys, hope you're having a good day!\n\nAfter latest improvements on ik llamacpp, [https://github.com/ikawrakow/ik\\_llama.cpp/commits/main/](https://github.com/ikawrakow/ik_llama.cpp/commits/main/), I have found that DeepSeek MoE models runs noticeably faster than llamacpp, at the point that I get about half PP t/s and 0.85-0.9X TG t/s vs ikllamacpp. This is the case only for MoE models I'm testing.\n\nMy setup is:\n\n* AMD Ryzen 7 7800X3D\n* 192GB RAM, DDR5 6000Mhz, max bandwidth at about 60-62 GB/s\n* 3 1600W PSUs (Corsair 1600i)\n* AM5 MSI Carbon X670E\n* 5090/5090 at PCIe X8/X8 5.0\n* 4090/4090 at PCIe X4/X4 4.0\n* 3090/3090 at PCIe X4/X4 4.0\n* A6000 at PCIe X4 4.0.\n* Fedora Linux 41 (instead of 42 just because I'm lazy doing some roundabouts to compile with GCC15, waiting until NVIDIA adds support to it)\n* SATA and USB-&gt;M2 Storage\n\nThe benchmarks are based on mostly, R1-0528, BUT it has the same size and it's quants on V3-0324 and TNG-R1T2-Chimera.\n\nI have tested the next models:\n\n* [unsloth](https://huggingface.co/unsloth) DeepSeek Q2\\_K\\_XL:\n   * llm\\_load\\_print\\_meta: model size       = 233.852 GiB (2.994 BPW)\n* [unsloth](https://huggingface.co/unsloth) DeepSeek IQ3\\_XXS:\n   * llm\\_load\\_print\\_meta: model size       = 254.168 GiB (3.254 BPW)\n* [unsloth](https://huggingface.co/unsloth) DeepSeek Q3\\_K\\_XL:\n   * llm\\_load\\_print\\_meta: model size       = 275.576 GiB (3.528 BPW)\n* [ubergarm](https://huggingface.co/ubergarm) DeepSeek IQ3\\_KS:\n   * llm\\_load\\_print\\_meta: model size       = 281.463 GiB (3.598 BPW)\n* [unsloth](https://huggingface.co/unsloth) DeepSeek IQ4\\_XS:\n   * llm\\_load\\_print\\_meta: model size       = 333.130 GiB (4.264 BPW)\n\nEach model may have been tested on different formats. Q2\\_K\\_XL and IQ3\\_XXS has less info, but the rest have a lot more. So here we go!\n\n# unsloth DeepSeek Q2_K_XL\n\nRunning the model with:\n\n    ./llama-server -m '/models_llm/DeepSeek-R1-0528-UD-Q2_K_XL-merged.gguf' \\\n    -c 32768 --no-mmap -ngl 999 \\\n    -ot \"blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0\" \\\n    -ot \"blk.(8|9|10|11).ffn.=CUDA1\" \\\n    -ot \"blk.(12|13|14|15).ffn.=CUDA2\" \\\n    -ot \"blk.(16|17|18|19|20).ffn.=CUDA3\" \\\n    -ot \"blk.(21|22|23|24).ffn.=CUDA4\" \\\n    -ot \"blk.(25|26|27|28).ffn.=CUDA5\" \\\n    -ot \"blk.(29|30|31|32|33|34|35|36|37|38).ffn.=CUDA6\" \\\n    -ot exps=CPU \\\n    -fa -mg 0 -ub 5120 -b 5120 -mla 3 -amb 256 -fmoe\n\nI get:\n\nmain: n\\_kv\\_max = 32768, n\\_batch = 5120, n\\_ubatch = 5120, flash\\_attn = 1, n\\_gpu\\_layers = 999, n\\_threads = 8, n\\_threads\\_batch = 8\n\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  5120 |   1280 |      0 |   12.481 |   410.21 |  104.088 |    12.30 |\n    |  5120 |   1280 |   5120 |   14.630 |   349.98 |  109.724 |    11.67 |\n    |  5120 |   1280 |  10240 |   17.167 |   298.25 |  112.938 |    11.33 |\n    |  5120 |   1280 |  15360 |   20.008 |   255.90 |  119.037 |    10.75 |\n    |  5120 |   1280 |  20480 |   22.444 |   228.12 |  122.706 |    10.43 |\n\n[Perf comparison \\(ignore 4096 as I forgor to save the perf\\)](https://preview.redd.it/r9tt4pktt3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=9e55870952c3069d16bb1a1b211b83b870e87da7)\n\nQ2\\_K\\_XL performs really good for a system like this! And it's performance as LLM is really good as well. I still prefer this above any other local model, for example, even if it's at 3bpw.\n\n# unsloth DeepSeek IQ3_XXS\n\nRunning the model with:\n\n    ./llama-server -m '/models_llm/DeepSeek-R1-0528-UD-IQ3_XXS-merged.gguf' \\\n    -c 32768 --no-mmap -ngl 999 \\\n    -ot \"blk.(0|1|2|3|4|5|6).ffn.=CUDA0\" \\\n    -ot \"blk.(7|8|9|10).ffn.=CUDA1\" \\\n    -ot \"blk.(11|12|13|14).ffn.=CUDA2\" \\\n    -ot \"blk.(15|16|17|18|19).ffn.=CUDA3\" \\\n    -ot \"blk.(20|21|22|23).ffn.=CUDA4\" \\\n    -ot \"blk.(24|25|26|27).ffn.=CUDA5\" \\\n    -ot \"blk.(28|29|30|31|32|33|34|35).ffn.=CUDA6\" \\\n    -ot exps=CPU \\\n    -fa -mg 0 -ub 4096 -b 4096 -mla 3 -amb 256 -fmoe\n\nI get\n\n    Small test for this one!\n    \n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  4096 |   1024 |      0 |   10.671 |   383.83 |  117.496 |     8.72 |\n    |  4096 |   1024 |   4096 |   11.322 |   361.77 |  120.192 |     8.52 |\n\nhttps://preview.redd.it/dtrfsnabu3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=34fa15b35573c0d7ce936d9da953d4d483320902\n\nSorry on this one to have few data! IQ3\\_XXS quality is really good for it's size.\n\n# unsloth DeepSeek Q3_K_XL\n\nNow we enter a bigger territory. Note that you will notice Q3\\_K\\_XL being faster than IQ3\\_XXS, despite being bigger.\n\nRunning the faster PP one with:\n\n    ./llama-server -m '/DeepSeek-R1-0528-UD-Q3_K_XL-merged.gguf' \\\n    -c 32768 --no-mmap -ngl 999 \\\n    -ot \"blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0\" \\\n    -ot \"blk.(8|9|10|11).ffn.=CUDA1\" \\\n    -ot \"blk.(12|13|14|15).ffn.=CUDA2\" \\\n    -ot \"blk.(16|17|18|19|20).ffn.=CUDA3\" \\\n    -ot \"blk.(21|22|23).ffn.=CUDA4\" \\\n    -ot \"blk.(24|25|26).ffn.=CUDA5\" \\\n    -ot \"blk.(27|28|29|30|31|32|33|34).ffn.=CUDA6\" \\\n    -ot exps=CPU \\\n    -fa -mg 0 -ub 2560 -b 2560 -mla 1 -fmoe -amb 256\n\nResults look like this:\n\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  2560 |    640 |      0 |    9.781 |   261.72 |   65.367 |     9.79 |\n    |  2560 |    640 |   2560 |   10.048 |   254.78 |   65.824 |     9.72 |\n    |  2560 |    640 |   5120 |   10.625 |   240.93 |   66.134 |     9.68 |\n    |  2560 |    640 |   7680 |   11.167 |   229.24 |   67.225 |     9.52 |\n    |  2560 |    640 |  10240 |   12.268 |   208.68 |   67.475 |     9.49 |\n    |  2560 |    640 |  12800 |   13.433 |   190.58 |   68.743 |     9.31 |\n    |  2560 |    640 |  15360 |   14.564 |   175.78 |   69.585 |     9.20 |\n    |  2560 |    640 |  17920 |   15.734 |   162.70 |   70.589 |     9.07 |\n    |  2560 |    640 |  20480 |   16.889 |   151.58 |   72.524 |     8.82 |\n    |  2560 |    640 |  23040 |   18.100 |   141.43 |   74.534 |     8.59 |\n\nWith more layers on GPU, but smaller batch size, I get\n\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  2048 |    512 |      0 |    9.017 |   227.12 |   50.612 |    10.12 |\n    |  2048 |    512 |   2048 |    9.113 |   224.73 |   51.027 |    10.03 |\n    |  2048 |    512 |   4096 |    9.436 |   217.05 |   51.864 |     9.87 |\n    |  2048 |    512 |   6144 |    9.680 |   211.56 |   52.818 |     9.69 |\n    |  2048 |    512 |   8192 |    9.984 |   205.12 |   53.354 |     9.60 |\n    |  2048 |    512 |  10240 |   10.349 |   197.90 |   53.896 |     9.50 |\n    |  2048 |    512 |  12288 |   10.936 |   187.27 |   54.600 |     9.38 |\n    |  2048 |    512 |  14336 |   11.688 |   175.22 |   55.150 |     9.28 |\n    |  2048 |    512 |  16384 |   12.419 |   164.91 |   55.852 |     9.17 |\n    |  2048 |    512 |  18432 |   13.113 |   156.18 |   56.436 |     9.07 |\n    |  2048 |    512 |  20480 |   13.871 |   147.65 |   56.823 |     9.01 |\n    |  2048 |    512 |  22528 |   14.594 |   140.33 |   57.590 |     8.89 |\n    |  2048 |    512 |  24576 |   15.335 |   133.55 |   58.278 |     8.79 |\n    |  2048 |    512 |  26624 |   16.073 |   127.42 |   58.723 |     8.72 |\n    |  2048 |    512 |  28672 |   16.794 |   121.95 |   59.553 |     8.60 |\n    |  2048 |    512 |  30720 |   17.522 |   116.88 |   59.921 |     8.54 |\n\nAnd with less GPU layers on GPU, but higher batch size, I get\n\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  4096 |   1024 |      0 |   12.005 |   341.19 |  111.632 |     9.17 |\n    |  4096 |   1024 |   4096 |   12.515 |   327.28 |  138.930 |     7.37 |\n    |  4096 |   1024 |   8192 |   13.389 |   305.91 |  118.220 |     8.66 |\n    |  4096 |   1024 |  12288 |   15.018 |   272.74 |  119.289 |     8.58 |\n\nSo then, performance for different batch sizes and layers, looks like this:\n\n[Higher ub\\/b is because I ended the test earlier!](https://preview.redd.it/l9jswixxu3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=fed2cf27373207f9bb3e0e702f391214334adcd1)\n\nSo you can choose between having more TG t/s with having possibly smaller batch sizes (so then slower PP), or try to max PP by offloading more layers to the CPU.\n\n# ubergarm DeepSeek IQ3_KS ([TNG-R1T2-Chimera](https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF))\n\nThis one is really good! And it has some more optimizations that may apply more on iklcpp.\n\nRunning this one with:\n\n    ./llama-server -m '/GGUFs/DeepSeek-TNG-R1T2-Chimera-IQ3_KS-merged.gguf' \\\n    -c 32768 --no-mmap -ngl 999 \\\n    -ot \"blk.(0|1|2|3|4|5|6).ffn.=CUDA0\" \\\n    -ot \"blk.(7|8|9).ffn.=CUDA1\" \\\n    -ot \"blk.(10|11|12).ffn.=CUDA2\" \\\n    -ot \"blk.(13|14|15|16).ffn.=CUDA3\" \\\n    -ot \"blk.(17|18|19).ffn.=CUDA4\" \\\n    -ot \"blk.(20|21|22).ffn.=CUDA5\" \\\n    -ot \"blk.(23|24|25|26|27|28|29|30).ffn.=CUDA6\" \\\n    -ot exps=CPU \\\n    -fa -mg 0 -ub 6144 -b 6144 -mla 3 -fmoe -amb 256\n\nI  get\n\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  6144 |   1536 |      0 |   15.406 |   398.81 |  174.929 |     8.78 |\n    |  6144 |   1536 |   6144 |   18.289 |   335.94 |  180.393 |     8.51 |\n    |  6144 |   1536 |  12288 |   22.229 |   276.39 |  186.113 |     8.25 |\n    |  6144 |   1536 |  18432 |   24.533 |   250.44 |  191.037 |     8.04 |\n    |  6144 |   1536 |  24576 |   28.122 |   218.48 |  196.268 |     7.83 |\n\nOr 8192 batch size/ubatch size, I get\n\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  8192 |   2048 |      0 |   20.147 |   406.61 |  232.476 |     8.81 |\n    |  8192 |   2048 |   8192 |   26.009 |   314.97 |  242.648 |     8.44 |\n    |  8192 |   2048 |  16384 |   32.628 |   251.07 |  253.309 |     8.09 |\n    |  8192 |   2048 |  24576 |   39.010 |   210.00 |  264.415 |     7.75 |\n\nSo the graph looks like this\n\nhttps://preview.redd.it/rj0kip6gw3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=ac996b223c86d5d30668be4995436a4aa1bc8dbf\n\nAgain, this model is really good, and really fast! Totally recommended.\n\n# unsloth DeepSeek IQ4_XS\n\nAt this point is where I have to do compromises to run it on my PC, by either having less PP, less TG or use more RAM at the absolute limit.\n\nRunning this model with the best balance with:\n\n    ./llama-sweep-bench -m '/models_llm/DeepSeek-R1-0528-IQ4_XS-merged.gguf' -c 32768 --no-mmap -ngl 999 \\\n    -ot \"blk.(0|1|2|3|4|5|6).ffn.=CUDA0\" \\\n    -ot \"blk.(7|8|9).ffn.=CUDA1\" \\\n    -ot \"blk.(10|11|12).ffn.=CUDA2\" \\\n    -ot \"blk.(13|14|15|16).ffn.=CUDA3\" \\\n    -ot \"blk.(17|18|19).ffn.=CUDA4\" \\\n    -ot \"blk.(20|21|22).ffn.=CUDA5\" \\\n    -ot \"blk.(23|24|25|26|27|28|29).ffn.=CUDA6\" \\\n    -ot \"blk.30.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA1\" \\\n    -ot \"blk.30.ffn_gate_exps.weight=CUDA1\" \\\n    -ot \"blk.30.ffn_down_exps.weight=CUDA2\" \\\n    -ot \"blk.30.ffn_up_exps.weight=CUDA4\" \\\n    -ot \"blk.31.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA5\" \\\n    -ot \"blk.31.ffn_gate_exps.weight=CUDA5\" \\\n    -ot \"blk.31.ffn_down_exps.weight=CUDA0\" \\\n    -ot \"blk.31.ffn_up_exps.weight=CUDA3\" \\\n    -ot \"blk.32.ffn_gate_exps.weight=CUDA1\" \\\n    -ot \"blk.32.ffn_down_exps.weight=CUDA2\" \\\n    -ot exps=CPU \\\n    -fa -mg 0 -ub 1024 -mla 1 -amb 256\n\nUsing 161GB of RAM and the GPUs totally maxed, I get\n\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  1024 |    256 |      0 |    9.336 |   109.69 |   31.102 |     8.23 |\n    |  1024 |    256 |   1024 |    9.345 |   109.57 |   31.224 |     8.20 |\n    |  1024 |    256 |   2048 |    9.392 |   109.03 |   31.193 |     8.21 |\n    |  1024 |    256 |   3072 |    9.452 |   108.34 |   31.472 |     8.13 |\n    |  1024 |    256 |   4096 |    9.540 |   107.34 |   31.623 |     8.10 |\n    |  1024 |    256 |   5120 |    9.750 |   105.03 |   32.674 |     7.83 |\n\nRunning a variant with less layers on GPU, but more on CPU, using 177GB RAM and higher ubatch size, at 1792:\n\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  1792 |    448 |      0 |   10.701 |   167.46 |   56.284 |     7.96 |\n    |  1792 |    448 |   1792 |   10.729 |   167.02 |   56.638 |     7.91 |\n    |  1792 |    448 |   3584 |   10.947 |   163.71 |   57.194 |     7.83 |\n    |  1792 |    448 |   5376 |   11.099 |   161.46 |   58.003 |     7.72 |\n    |  1792 |    448 |   7168 |   11.267 |   159.06 |   58.127 |     7.71 |\n    |  1792 |    448 |   8960 |   11.450 |   156.51 |   58.697 |     7.63 |\n    |  1792 |    448 |  10752 |   11.627 |   154.12 |   59.421 |     7.54 |\n    |  1792 |    448 |  12544 |   11.809 |   151.75 |   59.686 |     7.51 |\n    |  1792 |    448 |  14336 |   12.007 |   149.24 |   60.075 |     7.46 |\n    |  1792 |    448 |  16128 |   12.251 |   146.27 |   60.624 |     7.39 |\n    |  1792 |    448 |  17920 |   12.639 |   141.79 |   60.977 |     7.35 |\n    |  1792 |    448 |  19712 |   13.113 |   136.66 |   61.481 |     7.29 |\n    |  1792 |    448 |  21504 |   13.639 |   131.39 |   62.117 |     7.21 |\n    |  1792 |    448 |  23296 |   14.184 |   126.34 |   62.393 |     7.18 |\n\nAnd there is a less efficient result with ub 1536, but this will be shown on the graph, which looks like this:\n\nhttps://preview.redd.it/r8xka0tcx3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=9d374193172785b18b03858204abb37a0ac8b9fa\n\nAs you can see, the most conservative one with RAM has really slow PP, but a bit faster TG. While with less layers on GPU and more RAM usage, since we left some layers, we can increase PP and increment is noticeable.\n\n# Final comparison\n\nAn image comparing 1 of each in one image, looks like this\n\nhttps://preview.redd.it/owlrn4cqx3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=f05a460e56395c8890bc2dc18d6e78aa15cf91a6\n\nI don't have PPL values in hand sadly, besides the PPL on TNG-R1T2-Chimera that ubergarm did, in where DeepSeek R1 0528 is just 3% better than this quant at 3.8bpw (`3.2119 +/- 0.01697` vs 3.3167 +/- 0.01789), but take in mind that original TNG-R1T2-Chimera is already, at Q8, a bit worse on PPL vs R1 0528, **so these quants are quite good quality.**\n\nFor the models on the post and based for max batch size (less layers on GPU, so more RAM usage because offloading more to CPU), or based on max TG speed (more layers on GPU, less on RAM):\n\n* 90-95GB RAM on Q2\\_K\\_XL, rest on VRAM.\n* 100-110GB RAM on IQ3\\_XXS, rest on VRAM.\n* 115-140GB RAM on Q3\\_K\\_XL, rest on VRAM.\n* 115-135GB RAM on IQ3\\_KS, rest on VRAM.\n* 161-177GB RAM on IQ4\\_XS, rest on VRAM.\n\nSomeone may be wondering that with these values, it is still not total 400GB (192GB RAM + 208GB VRAM), and it's because I have not contemplated the compute buffer sizes, which can range between 512MB up to 5GB per GPU.\n\nFor DeepSeek models with MLA, in general it is 1GB per 8K ctx at fp16. So 1GB per 16K with q8\\_0 ctx (I didn't use it here, but it lets me use 64K at q8 with the same config as 32K at f16).\n\nHope this post can help someone interested in these results, any question is welcome!",
          "author_fullname": "t2_j1kqr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Performance benchmarks on DeepSeek V3-0324/R1-0528/TNG-R1T2-Chimera on consumer CPU (7800X3D, 192GB RAM at 6000Mhz) and 208GB VRAM (5090x2/4090x2/3090x2/A6000) on ikllamacpp! From 3bpw (Q2_K_XL) to 4.2 bpw (IQ4_XS)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "rj0kip6gw3cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/rj0kip6gw3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1f101fc6491542a1a0d5232c62e944c92274120"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/rj0kip6gw3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9511e2308f22e080b1ceecf696c988a7e9d44dde"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/rj0kip6gw3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc4b301b035b11bb121d463ba4d2cde5bd111d2e"
                },
                {
                  "y": 333,
                  "x": 640,
                  "u": "https://preview.redd.it/rj0kip6gw3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=437e96f03df2f2d0622934a1195ec3771ca24181"
                },
                {
                  "y": 499,
                  "x": 960,
                  "u": "https://preview.redd.it/rj0kip6gw3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f6b3f9cb824b56c49dee448ac1916c538919b228"
                },
                {
                  "y": 561,
                  "x": 1080,
                  "u": "https://preview.redd.it/rj0kip6gw3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18249a8e504568e0fdb251e3f559bc4576973e14"
                }
              ],
              "s": {
                "y": 1998,
                "x": 3840,
                "u": "https://preview.redd.it/rj0kip6gw3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=ac996b223c86d5d30668be4995436a4aa1bc8dbf"
              },
              "id": "rj0kip6gw3cf1"
            },
            "r9tt4pktt3cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/r9tt4pktt3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d06714ed9be3cf158f64b49625f04d4d3c1f811"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/r9tt4pktt3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef051b2e1c39b6fd156b5c24ba9927d364fa613f"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/r9tt4pktt3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2bb45907231ce9906909fc31595392a8ea256940"
                },
                {
                  "y": 333,
                  "x": 640,
                  "u": "https://preview.redd.it/r9tt4pktt3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4f664b75a1378a0d7f3c9ca85df7c3bd682e3863"
                },
                {
                  "y": 499,
                  "x": 960,
                  "u": "https://preview.redd.it/r9tt4pktt3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e4006e3d3ab018f96cc70afdde829cacd8e17a7"
                },
                {
                  "y": 561,
                  "x": 1080,
                  "u": "https://preview.redd.it/r9tt4pktt3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=66b3ccced4271b1a7bad7f5174460be8221e4edc"
                }
              ],
              "s": {
                "y": 1998,
                "x": 3840,
                "u": "https://preview.redd.it/r9tt4pktt3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=9e55870952c3069d16bb1a1b211b83b870e87da7"
              },
              "id": "r9tt4pktt3cf1"
            },
            "dtrfsnabu3cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/dtrfsnabu3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0fc0140a716d4f3f1b0ceeaacd5a84bc2be68565"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/dtrfsnabu3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ba825367494d413ad942db2b001da7b0ead3a0c"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/dtrfsnabu3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fdad97203dc8fdcfaabe6027fe6692e4548aac60"
                },
                {
                  "y": 333,
                  "x": 640,
                  "u": "https://preview.redd.it/dtrfsnabu3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=46bee9fde8e9875eb11bed658996aa56ee543827"
                },
                {
                  "y": 499,
                  "x": 960,
                  "u": "https://preview.redd.it/dtrfsnabu3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b648d4bb124f102339fb52c54397c79f215c38a4"
                },
                {
                  "y": 561,
                  "x": 1080,
                  "u": "https://preview.redd.it/dtrfsnabu3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c1898a494375997a6904dfbbda045e69fec24a72"
                }
              ],
              "s": {
                "y": 1998,
                "x": 3840,
                "u": "https://preview.redd.it/dtrfsnabu3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=34fa15b35573c0d7ce936d9da953d4d483320902"
              },
              "id": "dtrfsnabu3cf1"
            },
            "owlrn4cqx3cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/owlrn4cqx3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bca3112656c90be4044439b52e00946321961046"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/owlrn4cqx3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e268313470f242e9afa03279fc22c8ac2b078da2"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/owlrn4cqx3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1451526e6065a4be1fa87a59be3f3b3d96e6496"
                },
                {
                  "y": 333,
                  "x": 640,
                  "u": "https://preview.redd.it/owlrn4cqx3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2bbd26aa16d1131f087e6e6dc7a3dcfccd748228"
                },
                {
                  "y": 499,
                  "x": 960,
                  "u": "https://preview.redd.it/owlrn4cqx3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b5091dff9d48e6a04095f1c8d1d4e5045aca3aa"
                },
                {
                  "y": 561,
                  "x": 1080,
                  "u": "https://preview.redd.it/owlrn4cqx3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=30d20fa0f3120330480aeb16bbc72c15e2fbfd40"
                }
              ],
              "s": {
                "y": 1998,
                "x": 3840,
                "u": "https://preview.redd.it/owlrn4cqx3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=f05a460e56395c8890bc2dc18d6e78aa15cf91a6"
              },
              "id": "owlrn4cqx3cf1"
            },
            "l9jswixxu3cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/l9jswixxu3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=98693343116b5932d4e4febfade79a9540bf5251"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/l9jswixxu3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c19aafa0adf244051402fb495f7ea3e83ce4d332"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/l9jswixxu3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=88a4c3ce4e0a67659285ff944ea2caf076534397"
                },
                {
                  "y": 333,
                  "x": 640,
                  "u": "https://preview.redd.it/l9jswixxu3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e6a67f633cd95a5690c3eec22b4e533133bd30e9"
                },
                {
                  "y": 499,
                  "x": 960,
                  "u": "https://preview.redd.it/l9jswixxu3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=47fd0b46a4f4552c164259485d6d18c3810490ec"
                },
                {
                  "y": 561,
                  "x": 1080,
                  "u": "https://preview.redd.it/l9jswixxu3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f47e3de82b9f829542cf88b78ddc69d029a420f8"
                }
              ],
              "s": {
                "y": 1998,
                "x": 3840,
                "u": "https://preview.redd.it/l9jswixxu3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=fed2cf27373207f9bb3e0e702f391214334adcd1"
              },
              "id": "l9jswixxu3cf1"
            },
            "r8xka0tcx3cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/r8xka0tcx3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=60c2c8b2ff94822236365cd794d77a9025518b37"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/r8xka0tcx3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3923f4e719843f30db5fb36d50cd44f6c92cde1c"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/r8xka0tcx3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=15850541050f33ee722c2a69833096aaee3fab2b"
                },
                {
                  "y": 333,
                  "x": 640,
                  "u": "https://preview.redd.it/r8xka0tcx3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=07def092ebeb76d951427fa69df4343132d82320"
                },
                {
                  "y": 499,
                  "x": 960,
                  "u": "https://preview.redd.it/r8xka0tcx3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3c5bff32a3671665b53452ff08c02de89fa87d27"
                },
                {
                  "y": 561,
                  "x": 1080,
                  "u": "https://preview.redd.it/r8xka0tcx3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0554a466e15e1086df7bee957146b2ec82ca05b7"
                }
              ],
              "s": {
                "y": 1998,
                "x": 3840,
                "u": "https://preview.redd.it/r8xka0tcx3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=9d374193172785b18b03858204abb37a0ac8b9fa"
              },
              "id": "r8xka0tcx3cf1"
            }
          },
          "name": "t3_1lwnj5x",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 63,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 63,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/2-o5OWNt03DgmcUIElRxpmapK3b8mnf9fOYvDpwJaPg.jpg",
          "edited": 1752181687,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 405B"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752179851,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there guys, hope you&amp;#39;re having a good day!&lt;/p&gt;\n\n&lt;p&gt;After latest improvements on ik llamacpp, &lt;a href=\"https://github.com/ikawrakow/ik_llama.cpp/commits/main/\"&gt;https://github.com/ikawrakow/ik_llama.cpp/commits/main/&lt;/a&gt;, I have found that DeepSeek MoE models runs noticeably faster than llamacpp, at the point that I get about half PP t/s and 0.85-0.9X TG t/s vs ikllamacpp. This is the case only for MoE models I&amp;#39;m testing.&lt;/p&gt;\n\n&lt;p&gt;My setup is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt;\n&lt;li&gt;192GB RAM, DDR5 6000Mhz, max bandwidth at about 60-62 GB/s&lt;/li&gt;\n&lt;li&gt;3 1600W PSUs (Corsair 1600i)&lt;/li&gt;\n&lt;li&gt;AM5 MSI Carbon X670E&lt;/li&gt;\n&lt;li&gt;5090/5090 at PCIe X8/X8 5.0&lt;/li&gt;\n&lt;li&gt;4090/4090 at PCIe X4/X4 4.0&lt;/li&gt;\n&lt;li&gt;3090/3090 at PCIe X4/X4 4.0&lt;/li&gt;\n&lt;li&gt;A6000 at PCIe X4 4.0.&lt;/li&gt;\n&lt;li&gt;Fedora Linux 41 (instead of 42 just because I&amp;#39;m lazy doing some roundabouts to compile with GCC15, waiting until NVIDIA adds support to it)&lt;/li&gt;\n&lt;li&gt;SATA and USB-&amp;gt;M2 Storage&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The benchmarks are based on mostly, R1-0528, BUT it has the same size and it&amp;#39;s quants on V3-0324 and TNG-R1T2-Chimera.&lt;/p&gt;\n\n&lt;p&gt;I have tested the next models:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/unsloth\"&gt;unsloth&lt;/a&gt; DeepSeek Q2_K_XL:\n\n&lt;ul&gt;\n&lt;li&gt;llm_load_print_meta: model size       = 233.852 GiB (2.994 BPW)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/unsloth\"&gt;unsloth&lt;/a&gt; DeepSeek IQ3_XXS:\n\n&lt;ul&gt;\n&lt;li&gt;llm_load_print_meta: model size       = 254.168 GiB (3.254 BPW)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/unsloth\"&gt;unsloth&lt;/a&gt; DeepSeek Q3_K_XL:\n\n&lt;ul&gt;\n&lt;li&gt;llm_load_print_meta: model size       = 275.576 GiB (3.528 BPW)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/ubergarm\"&gt;ubergarm&lt;/a&gt; DeepSeek IQ3_KS:\n\n&lt;ul&gt;\n&lt;li&gt;llm_load_print_meta: model size       = 281.463 GiB (3.598 BPW)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/unsloth\"&gt;unsloth&lt;/a&gt; DeepSeek IQ4_XS:\n\n&lt;ul&gt;\n&lt;li&gt;llm_load_print_meta: model size       = 333.130 GiB (4.264 BPW)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Each model may have been tested on different formats. Q2_K_XL and IQ3_XXS has less info, but the rest have a lot more. So here we go!&lt;/p&gt;\n\n&lt;h1&gt;unsloth DeepSeek Q2_K_XL&lt;/h1&gt;\n\n&lt;p&gt;Running the model with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-server -m &amp;#39;/models_llm/DeepSeek-R1-0528-UD-Q2_K_XL-merged.gguf&amp;#39; \\\n-c 32768 --no-mmap -ngl 999 \\\n-ot &amp;quot;blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0&amp;quot; \\\n-ot &amp;quot;blk.(8|9|10|11).ffn.=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.(12|13|14|15).ffn.=CUDA2&amp;quot; \\\n-ot &amp;quot;blk.(16|17|18|19|20).ffn.=CUDA3&amp;quot; \\\n-ot &amp;quot;blk.(21|22|23|24).ffn.=CUDA4&amp;quot; \\\n-ot &amp;quot;blk.(25|26|27|28).ffn.=CUDA5&amp;quot; \\\n-ot &amp;quot;blk.(29|30|31|32|33|34|35|36|37|38).ffn.=CUDA6&amp;quot; \\\n-ot exps=CPU \\\n-fa -mg 0 -ub 5120 -b 5120 -mla 3 -amb 256 -fmoe\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I get:&lt;/p&gt;\n\n&lt;p&gt;main: n_kv_max = 32768, n_batch = 5120, n_ubatch = 5120, flash_attn = 1, n_gpu_layers = 999, n_threads = 8, n_threads_batch = 8&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  5120 |   1280 |      0 |   12.481 |   410.21 |  104.088 |    12.30 |\n|  5120 |   1280 |   5120 |   14.630 |   349.98 |  109.724 |    11.67 |\n|  5120 |   1280 |  10240 |   17.167 |   298.25 |  112.938 |    11.33 |\n|  5120 |   1280 |  15360 |   20.008 |   255.90 |  119.037 |    10.75 |\n|  5120 |   1280 |  20480 |   22.444 |   228.12 |  122.706 |    10.43 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/r9tt4pktt3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e55870952c3069d16bb1a1b211b83b870e87da7\"&gt;Perf comparison (ignore 4096 as I forgor to save the perf)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Q2_K_XL performs really good for a system like this! And it&amp;#39;s performance as LLM is really good as well. I still prefer this above any other local model, for example, even if it&amp;#39;s at 3bpw.&lt;/p&gt;\n\n&lt;h1&gt;unsloth DeepSeek IQ3_XXS&lt;/h1&gt;\n\n&lt;p&gt;Running the model with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-server -m &amp;#39;/models_llm/DeepSeek-R1-0528-UD-IQ3_XXS-merged.gguf&amp;#39; \\\n-c 32768 --no-mmap -ngl 999 \\\n-ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \\\n-ot &amp;quot;blk.(7|8|9|10).ffn.=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.(11|12|13|14).ffn.=CUDA2&amp;quot; \\\n-ot &amp;quot;blk.(15|16|17|18|19).ffn.=CUDA3&amp;quot; \\\n-ot &amp;quot;blk.(20|21|22|23).ffn.=CUDA4&amp;quot; \\\n-ot &amp;quot;blk.(24|25|26|27).ffn.=CUDA5&amp;quot; \\\n-ot &amp;quot;blk.(28|29|30|31|32|33|34|35).ffn.=CUDA6&amp;quot; \\\n-ot exps=CPU \\\n-fa -mg 0 -ub 4096 -b 4096 -mla 3 -amb 256 -fmoe\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I get&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Small test for this one!\n\n|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  4096 |   1024 |      0 |   10.671 |   383.83 |  117.496 |     8.72 |\n|  4096 |   1024 |   4096 |   11.322 |   361.77 |  120.192 |     8.52 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dtrfsnabu3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34fa15b35573c0d7ce936d9da953d4d483320902\"&gt;https://preview.redd.it/dtrfsnabu3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34fa15b35573c0d7ce936d9da953d4d483320902&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sorry on this one to have few data! IQ3_XXS quality is really good for it&amp;#39;s size.&lt;/p&gt;\n\n&lt;h1&gt;unsloth DeepSeek Q3_K_XL&lt;/h1&gt;\n\n&lt;p&gt;Now we enter a bigger territory. Note that you will notice Q3_K_XL being faster than IQ3_XXS, despite being bigger.&lt;/p&gt;\n\n&lt;p&gt;Running the faster PP one with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-server -m &amp;#39;/DeepSeek-R1-0528-UD-Q3_K_XL-merged.gguf&amp;#39; \\\n-c 32768 --no-mmap -ngl 999 \\\n-ot &amp;quot;blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0&amp;quot; \\\n-ot &amp;quot;blk.(8|9|10|11).ffn.=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.(12|13|14|15).ffn.=CUDA2&amp;quot; \\\n-ot &amp;quot;blk.(16|17|18|19|20).ffn.=CUDA3&amp;quot; \\\n-ot &amp;quot;blk.(21|22|23).ffn.=CUDA4&amp;quot; \\\n-ot &amp;quot;blk.(24|25|26).ffn.=CUDA5&amp;quot; \\\n-ot &amp;quot;blk.(27|28|29|30|31|32|33|34).ffn.=CUDA6&amp;quot; \\\n-ot exps=CPU \\\n-fa -mg 0 -ub 2560 -b 2560 -mla 1 -fmoe -amb 256\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Results look like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  2560 |    640 |      0 |    9.781 |   261.72 |   65.367 |     9.79 |\n|  2560 |    640 |   2560 |   10.048 |   254.78 |   65.824 |     9.72 |\n|  2560 |    640 |   5120 |   10.625 |   240.93 |   66.134 |     9.68 |\n|  2560 |    640 |   7680 |   11.167 |   229.24 |   67.225 |     9.52 |\n|  2560 |    640 |  10240 |   12.268 |   208.68 |   67.475 |     9.49 |\n|  2560 |    640 |  12800 |   13.433 |   190.58 |   68.743 |     9.31 |\n|  2560 |    640 |  15360 |   14.564 |   175.78 |   69.585 |     9.20 |\n|  2560 |    640 |  17920 |   15.734 |   162.70 |   70.589 |     9.07 |\n|  2560 |    640 |  20480 |   16.889 |   151.58 |   72.524 |     8.82 |\n|  2560 |    640 |  23040 |   18.100 |   141.43 |   74.534 |     8.59 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;With more layers on GPU, but smaller batch size, I get&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  2048 |    512 |      0 |    9.017 |   227.12 |   50.612 |    10.12 |\n|  2048 |    512 |   2048 |    9.113 |   224.73 |   51.027 |    10.03 |\n|  2048 |    512 |   4096 |    9.436 |   217.05 |   51.864 |     9.87 |\n|  2048 |    512 |   6144 |    9.680 |   211.56 |   52.818 |     9.69 |\n|  2048 |    512 |   8192 |    9.984 |   205.12 |   53.354 |     9.60 |\n|  2048 |    512 |  10240 |   10.349 |   197.90 |   53.896 |     9.50 |\n|  2048 |    512 |  12288 |   10.936 |   187.27 |   54.600 |     9.38 |\n|  2048 |    512 |  14336 |   11.688 |   175.22 |   55.150 |     9.28 |\n|  2048 |    512 |  16384 |   12.419 |   164.91 |   55.852 |     9.17 |\n|  2048 |    512 |  18432 |   13.113 |   156.18 |   56.436 |     9.07 |\n|  2048 |    512 |  20480 |   13.871 |   147.65 |   56.823 |     9.01 |\n|  2048 |    512 |  22528 |   14.594 |   140.33 |   57.590 |     8.89 |\n|  2048 |    512 |  24576 |   15.335 |   133.55 |   58.278 |     8.79 |\n|  2048 |    512 |  26624 |   16.073 |   127.42 |   58.723 |     8.72 |\n|  2048 |    512 |  28672 |   16.794 |   121.95 |   59.553 |     8.60 |\n|  2048 |    512 |  30720 |   17.522 |   116.88 |   59.921 |     8.54 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And with less GPU layers on GPU, but higher batch size, I get&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  4096 |   1024 |      0 |   12.005 |   341.19 |  111.632 |     9.17 |\n|  4096 |   1024 |   4096 |   12.515 |   327.28 |  138.930 |     7.37 |\n|  4096 |   1024 |   8192 |   13.389 |   305.91 |  118.220 |     8.66 |\n|  4096 |   1024 |  12288 |   15.018 |   272.74 |  119.289 |     8.58 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So then, performance for different batch sizes and layers, looks like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/l9jswixxu3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fed2cf27373207f9bb3e0e702f391214334adcd1\"&gt;Higher ub/b is because I ended the test earlier!&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So you can choose between having more TG t/s with having possibly smaller batch sizes (so then slower PP), or try to max PP by offloading more layers to the CPU.&lt;/p&gt;\n\n&lt;h1&gt;ubergarm DeepSeek IQ3_KS (&lt;a href=\"https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF\"&gt;TNG-R1T2-Chimera&lt;/a&gt;)&lt;/h1&gt;\n\n&lt;p&gt;This one is really good! And it has some more optimizations that may apply more on iklcpp.&lt;/p&gt;\n\n&lt;p&gt;Running this one with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-server -m &amp;#39;/GGUFs/DeepSeek-TNG-R1T2-Chimera-IQ3_KS-merged.gguf&amp;#39; \\\n-c 32768 --no-mmap -ngl 999 \\\n-ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \\\n-ot &amp;quot;blk.(7|8|9).ffn.=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.(10|11|12).ffn.=CUDA2&amp;quot; \\\n-ot &amp;quot;blk.(13|14|15|16).ffn.=CUDA3&amp;quot; \\\n-ot &amp;quot;blk.(17|18|19).ffn.=CUDA4&amp;quot; \\\n-ot &amp;quot;blk.(20|21|22).ffn.=CUDA5&amp;quot; \\\n-ot &amp;quot;blk.(23|24|25|26|27|28|29|30).ffn.=CUDA6&amp;quot; \\\n-ot exps=CPU \\\n-fa -mg 0 -ub 6144 -b 6144 -mla 3 -fmoe -amb 256\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I  get&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  6144 |   1536 |      0 |   15.406 |   398.81 |  174.929 |     8.78 |\n|  6144 |   1536 |   6144 |   18.289 |   335.94 |  180.393 |     8.51 |\n|  6144 |   1536 |  12288 |   22.229 |   276.39 |  186.113 |     8.25 |\n|  6144 |   1536 |  18432 |   24.533 |   250.44 |  191.037 |     8.04 |\n|  6144 |   1536 |  24576 |   28.122 |   218.48 |  196.268 |     7.83 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Or 8192 batch size/ubatch size, I get&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  8192 |   2048 |      0 |   20.147 |   406.61 |  232.476 |     8.81 |\n|  8192 |   2048 |   8192 |   26.009 |   314.97 |  242.648 |     8.44 |\n|  8192 |   2048 |  16384 |   32.628 |   251.07 |  253.309 |     8.09 |\n|  8192 |   2048 |  24576 |   39.010 |   210.00 |  264.415 |     7.75 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So the graph looks like this&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rj0kip6gw3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac996b223c86d5d30668be4995436a4aa1bc8dbf\"&gt;https://preview.redd.it/rj0kip6gw3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac996b223c86d5d30668be4995436a4aa1bc8dbf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Again, this model is really good, and really fast! Totally recommended.&lt;/p&gt;\n\n&lt;h1&gt;unsloth DeepSeek IQ4_XS&lt;/h1&gt;\n\n&lt;p&gt;At this point is where I have to do compromises to run it on my PC, by either having less PP, less TG or use more RAM at the absolute limit.&lt;/p&gt;\n\n&lt;p&gt;Running this model with the best balance with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-sweep-bench -m &amp;#39;/models_llm/DeepSeek-R1-0528-IQ4_XS-merged.gguf&amp;#39; -c 32768 --no-mmap -ngl 999 \\\n-ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \\\n-ot &amp;quot;blk.(7|8|9).ffn.=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.(10|11|12).ffn.=CUDA2&amp;quot; \\\n-ot &amp;quot;blk.(13|14|15|16).ffn.=CUDA3&amp;quot; \\\n-ot &amp;quot;blk.(17|18|19).ffn.=CUDA4&amp;quot; \\\n-ot &amp;quot;blk.(20|21|22).ffn.=CUDA5&amp;quot; \\\n-ot &amp;quot;blk.(23|24|25|26|27|28|29).ffn.=CUDA6&amp;quot; \\\n-ot &amp;quot;blk.30.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.30.ffn_gate_exps.weight=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.30.ffn_down_exps.weight=CUDA2&amp;quot; \\\n-ot &amp;quot;blk.30.ffn_up_exps.weight=CUDA4&amp;quot; \\\n-ot &amp;quot;blk.31.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA5&amp;quot; \\\n-ot &amp;quot;blk.31.ffn_gate_exps.weight=CUDA5&amp;quot; \\\n-ot &amp;quot;blk.31.ffn_down_exps.weight=CUDA0&amp;quot; \\\n-ot &amp;quot;blk.31.ffn_up_exps.weight=CUDA3&amp;quot; \\\n-ot &amp;quot;blk.32.ffn_gate_exps.weight=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.32.ffn_down_exps.weight=CUDA2&amp;quot; \\\n-ot exps=CPU \\\n-fa -mg 0 -ub 1024 -mla 1 -amb 256\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Using 161GB of RAM and the GPUs totally maxed, I get&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  1024 |    256 |      0 |    9.336 |   109.69 |   31.102 |     8.23 |\n|  1024 |    256 |   1024 |    9.345 |   109.57 |   31.224 |     8.20 |\n|  1024 |    256 |   2048 |    9.392 |   109.03 |   31.193 |     8.21 |\n|  1024 |    256 |   3072 |    9.452 |   108.34 |   31.472 |     8.13 |\n|  1024 |    256 |   4096 |    9.540 |   107.34 |   31.623 |     8.10 |\n|  1024 |    256 |   5120 |    9.750 |   105.03 |   32.674 |     7.83 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Running a variant with less layers on GPU, but more on CPU, using 177GB RAM and higher ubatch size, at 1792:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  1792 |    448 |      0 |   10.701 |   167.46 |   56.284 |     7.96 |\n|  1792 |    448 |   1792 |   10.729 |   167.02 |   56.638 |     7.91 |\n|  1792 |    448 |   3584 |   10.947 |   163.71 |   57.194 |     7.83 |\n|  1792 |    448 |   5376 |   11.099 |   161.46 |   58.003 |     7.72 |\n|  1792 |    448 |   7168 |   11.267 |   159.06 |   58.127 |     7.71 |\n|  1792 |    448 |   8960 |   11.450 |   156.51 |   58.697 |     7.63 |\n|  1792 |    448 |  10752 |   11.627 |   154.12 |   59.421 |     7.54 |\n|  1792 |    448 |  12544 |   11.809 |   151.75 |   59.686 |     7.51 |\n|  1792 |    448 |  14336 |   12.007 |   149.24 |   60.075 |     7.46 |\n|  1792 |    448 |  16128 |   12.251 |   146.27 |   60.624 |     7.39 |\n|  1792 |    448 |  17920 |   12.639 |   141.79 |   60.977 |     7.35 |\n|  1792 |    448 |  19712 |   13.113 |   136.66 |   61.481 |     7.29 |\n|  1792 |    448 |  21504 |   13.639 |   131.39 |   62.117 |     7.21 |\n|  1792 |    448 |  23296 |   14.184 |   126.34 |   62.393 |     7.18 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And there is a less efficient result with ub 1536, but this will be shown on the graph, which looks like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/r8xka0tcx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d374193172785b18b03858204abb37a0ac8b9fa\"&gt;https://preview.redd.it/r8xka0tcx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d374193172785b18b03858204abb37a0ac8b9fa&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As you can see, the most conservative one with RAM has really slow PP, but a bit faster TG. While with less layers on GPU and more RAM usage, since we left some layers, we can increase PP and increment is noticeable.&lt;/p&gt;\n\n&lt;h1&gt;Final comparison&lt;/h1&gt;\n\n&lt;p&gt;An image comparing 1 of each in one image, looks like this&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/owlrn4cqx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f05a460e56395c8890bc2dc18d6e78aa15cf91a6\"&gt;https://preview.redd.it/owlrn4cqx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f05a460e56395c8890bc2dc18d6e78aa15cf91a6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have PPL values in hand sadly, besides the PPL on TNG-R1T2-Chimera that ubergarm did, in where DeepSeek R1 0528 is just 3% better than this quant at 3.8bpw (&lt;code&gt;3.2119 +/- 0.01697&lt;/code&gt; vs 3.3167 +/- 0.01789), but take in mind that original TNG-R1T2-Chimera is already, at Q8, a bit worse on PPL vs R1 0528, &lt;strong&gt;so these quants are quite good quality.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For the models on the post and based for max batch size (less layers on GPU, so more RAM usage because offloading more to CPU), or based on max TG speed (more layers on GPU, less on RAM):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;90-95GB RAM on Q2_K_XL, rest on VRAM.&lt;/li&gt;\n&lt;li&gt;100-110GB RAM on IQ3_XXS, rest on VRAM.&lt;/li&gt;\n&lt;li&gt;115-140GB RAM on Q3_K_XL, rest on VRAM.&lt;/li&gt;\n&lt;li&gt;115-135GB RAM on IQ3_KS, rest on VRAM.&lt;/li&gt;\n&lt;li&gt;161-177GB RAM on IQ4_XS, rest on VRAM.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Someone may be wondering that with these values, it is still not total 400GB (192GB RAM + 208GB VRAM), and it&amp;#39;s because I have not contemplated the compute buffer sizes, which can range between 512MB up to 5GB per GPU.&lt;/p&gt;\n\n&lt;p&gt;For DeepSeek models with MLA, in general it is 1GB per 8K ctx at fp16. So 1GB per 16K with q8_0 ctx (I didn&amp;#39;t use it here, but it lets me use 64K at q8 with the same config as 32K at f16).&lt;/p&gt;\n\n&lt;p&gt;Hope this post can help someone interested in these results, any question is welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 405B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwnj5x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "panchovix",
          "discussion_type": null,
          "num_comments": 59,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752179851,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[Mistral.rs](http://Mistral.rs) has support for Mistral AI's newest model (no affiliation)!\n\nGrab optimized UQFF files here: [https://huggingface.co/EricB/Devstral-Small-2507-UQFF](https://huggingface.co/EricB/Devstral-Small-2507-UQFF)\n\nMore information: [https://github.com/EricLBuehler/mistral.rs](https://github.com/EricLBuehler/mistral.rs)\n\nIn my testing, this model is really great at tool calling, and works very well with some of our newest features:\n\n* **Agentic/automatic tool calling**: you can specify custom tool callbacks in Python or Rust and dedicate the entire toolcalling workflow to mistral.rs!\n* **OpenAI web search support**: [mistral.rs](http://mistral.rs) allows models to have access to automatic web search, 100% compatible with the OpenAI API.\n* **MCP client**: there is a builtin MCP client! Just like ChatGPT or Claude, all you need to do is specify the MCP server and it just works!\n\nThese features make [mistral.rs](http://mistral.rs) a really powerful tool for leveraging the strong capabilities of Devstral!\n\nWhat do you think? Excited to see what you build with this 🚀!",
          "author_fullname": "t2_87jryn0i3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Devstral 2707 with mistral.rs - MCP client, automatic tool calling!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwmpqf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 59,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 59,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752188378,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752177957,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"http://Mistral.rs\"&gt;Mistral.rs&lt;/a&gt; has support for Mistral AI&amp;#39;s newest model (no affiliation)!&lt;/p&gt;\n\n&lt;p&gt;Grab optimized UQFF files here: &lt;a href=\"https://huggingface.co/EricB/Devstral-Small-2507-UQFF\"&gt;https://huggingface.co/EricB/Devstral-Small-2507-UQFF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;More information: &lt;a href=\"https://github.com/EricLBuehler/mistral.rs\"&gt;https://github.com/EricLBuehler/mistral.rs&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In my testing, this model is really great at tool calling, and works very well with some of our newest features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Agentic/automatic tool calling&lt;/strong&gt;: you can specify custom tool callbacks in Python or Rust and dedicate the entire toolcalling workflow to mistral.rs!&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;OpenAI web search support&lt;/strong&gt;: &lt;a href=\"http://mistral.rs\"&gt;mistral.rs&lt;/a&gt; allows models to have access to automatic web search, 100% compatible with the OpenAI API.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;MCP client&lt;/strong&gt;: there is a builtin MCP client! Just like ChatGPT or Claude, all you need to do is specify the MCP server and it just works!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;These features make &lt;a href=\"http://mistral.rs\"&gt;mistral.rs&lt;/a&gt; a really powerful tool for leveraging the strong capabilities of Devstral!&lt;/p&gt;\n\n&lt;p&gt;What do you think? Excited to see what you build with this 🚀!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwmpqf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EricBuehler",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwmpqf/new_devstral_2707_with_mistralrs_mcp_client/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwmpqf/new_devstral_2707_with_mistralrs_mcp_client/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752177957,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA!As a web dev tinkering with local AI, I created Local AI Monster: A React app using MLC's WebLLM and WebGPU to run quantized Instruct models (e.g., Llama-3-8B, Phi-3-mini-4k, Gemma-2-9B) entirely client-side. No installs, no servers—just open in Chrome/Edge and chat.Key Features:\n\n* Auto-Detect VRAM &amp; Models: Estimates your GPU memory, picks the best fit from Hugging Face MLC models (fallbacks for low VRAM).\n* Chat Perks: Multi-chats, local storage, temperature/max tokens controls, streaming responses with markdown and code highlighting (Shiki).\n* Privacy: Fully local, no data outbound.\n* Performance: Loads in \\~30-60s on mid-range GPUs, generates 15-30 tokens/sec depending on hardware.\n\nIdeal for quick tests or coding help without heavy tools.Get StartedOpen-source on GitHub: [https://github.com/ShadovvBeast/local-ai-monster](https://github.com/ShadovvBeast/local-ai-monster) (MIT—fork/PRs welcome!).\n\nYou're welcome to try it at [https://localai.monster/](https://localai.monster/)\n\nFeedback?\n\n* Runs on your setup? (Share VRAM/speed!)\n* Model/feature ideas?\n* Comparisons to your workflows?\n\nLet's make browser AI better!",
          "author_fullname": "t2_erroa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Introducing Local AI Monster: Run Powerful LLMs Right in Your Browser 🚀",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lxd7ki",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752255513,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;!As a web dev tinkering with local AI, I created Local AI Monster: A React app using MLC&amp;#39;s WebLLM and WebGPU to run quantized Instruct models (e.g., Llama-3-8B, Phi-3-mini-4k, Gemma-2-9B) entirely client-side. No installs, no servers—just open in Chrome/Edge and chat.Key Features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Auto-Detect VRAM &amp;amp; Models: Estimates your GPU memory, picks the best fit from Hugging Face MLC models (fallbacks for low VRAM).&lt;/li&gt;\n&lt;li&gt;Chat Perks: Multi-chats, local storage, temperature/max tokens controls, streaming responses with markdown and code highlighting (Shiki).&lt;/li&gt;\n&lt;li&gt;Privacy: Fully local, no data outbound.&lt;/li&gt;\n&lt;li&gt;Performance: Loads in ~30-60s on mid-range GPUs, generates 15-30 tokens/sec depending on hardware.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Ideal for quick tests or coding help without heavy tools.Get StartedOpen-source on GitHub: &lt;a href=\"https://github.com/ShadovvBeast/local-ai-monster\"&gt;https://github.com/ShadovvBeast/local-ai-monster&lt;/a&gt; (MIT—fork/PRs welcome!).&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;re welcome to try it at &lt;a href=\"https://localai.monster/\"&gt;https://localai.monster/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Feedback?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Runs on your setup? (Share VRAM/speed!)&lt;/li&gt;\n&lt;li&gt;Model/feature ideas?&lt;/li&gt;\n&lt;li&gt;Comparisons to your workflows?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Let&amp;#39;s make browser AI better!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lxd7ki",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ShadovvBeast",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxd7ki/introducing_local_ai_monster_run_powerful_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxd7ki/introducing_local_ai_monster_run_powerful_llms/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752255513,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recently added Shortcuts support to my iOS app [Locally AI](https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692) and worked to integrate it with Siri.\n\nIt's using Apple MLX to run the models.\n\nHere's a demo of me asking Qwen 3 a question via Siri (sorry for my accent). It will call the app shortcut, get the answer and forward it to the Siri interface. It works with the Siri interface but also with AirPods or HomePod where Siri reads it. \n\nEverything running on-device.\n\nDid my best to have a seamless integration. It doesn’t require any setup other than downloading a model first.",
          "author_fullname": "t2_1jgkfm9u25",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Using Siri to talk to a local LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwif50",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 85,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/wjksocsoy2cf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1920,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/wjksocsoy2cf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/wjksocsoy2cf1/DASHPlaylist.mpd?a=1754849081%2CMmM3YzdjZWY4MDk3YmZjY2M3NDNlYTQ2ODk2MzM2N2UyNTE4NTJjOTYzN2FjODJkNTc1MjYyYjQ4NjNlMmQyMQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 20,
              "hls_url": "https://v.redd.it/wjksocsoy2cf1/HLSPlaylist.m3u8?a=1754849081%2CZjI3NjkyOTNkZTIwMmRkMWY2OTNlZjlkZjBjMjA0YjI4NWQyOWQ3YWIxOTgxNTZhMWQ2ZDJlNTk2NmQzNjg2MA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 85,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=c9da73cbdedd1c5bc87febbd0a7035d09051ff58",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752167877,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently added Shortcuts support to my iOS app &lt;a href=\"https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692\"&gt;Locally AI&lt;/a&gt; and worked to integrate it with Siri.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s using Apple MLX to run the models.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a demo of me asking Qwen 3 a question via Siri (sorry for my accent). It will call the app shortcut, get the answer and forward it to the Siri interface. It works with the Siri interface but also with AirPods or HomePod where Siri reads it. &lt;/p&gt;\n\n&lt;p&gt;Everything running on-device.&lt;/p&gt;\n\n&lt;p&gt;Did my best to have a seamless integration. It doesn’t require any setup other than downloading a model first.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/wjksocsoy2cf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?format=pjpg&amp;auto=webp&amp;s=eca2e9d3117bf49c61a925dbcb5abfe4658cc2de",
                  "width": 1080,
                  "height": 1920
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d0a0ec3c011de00aecdae2dc6e55601786643f9d",
                    "width": 108,
                    "height": 192
                  },
                  {
                    "url": "https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0b7f03d45f7cfcaba6e5d80b06cdb7100762d90d",
                    "width": 216,
                    "height": 384
                  },
                  {
                    "url": "https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=12e65ecf15c38208675551217d60186cc0218d5a",
                    "width": 320,
                    "height": 568
                  },
                  {
                    "url": "https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=44b72fe431e174c03868006089658ccbb163202e",
                    "width": 640,
                    "height": 1137
                  },
                  {
                    "url": "https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d30a61cb8c29fe7692658fd361c337579b4e2d90",
                    "width": 960,
                    "height": 1706
                  },
                  {
                    "url": "https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=794b886de2931d9dcd5096f3d4ecbbb7be4a5f22",
                    "width": 1080,
                    "height": 1920
                  }
                ],
                "variants": {},
                "id": "aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lwif50",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adrgrondin",
          "discussion_type": null,
          "num_comments": 50,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwif50/using_siri_to_talk_to_a_local_llm/",
          "stickied": false,
          "url": "https://v.redd.it/wjksocsoy2cf1",
          "subreddit_subscribers": 497502,
          "created_utc": 1752167877,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/wjksocsoy2cf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1920,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/wjksocsoy2cf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/wjksocsoy2cf1/DASHPlaylist.mpd?a=1754849081%2CMmM3YzdjZWY4MDk3YmZjY2M3NDNlYTQ2ODk2MzM2N2UyNTE4NTJjOTYzN2FjODJkNTc1MjYyYjQ4NjNlMmQyMQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 20,
              "hls_url": "https://v.redd.it/wjksocsoy2cf1/HLSPlaylist.m3u8?a=1754849081%2CZjI3NjkyOTNkZTIwMmRkMWY2OTNlZjlkZjBjMjA0YjI4NWQyOWQ3YWIxOTgxNTZhMWQ2ZDJlNTk2NmQzNjg2MA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RekaAI/reka-flash-3.1 · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwgy9m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 97,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 97,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=18f6bb91bbe16150609da54a7de899f8385bcc2c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752164422,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/RekaAI/reka-flash-3.1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?auto=webp&amp;s=527e78e54ce9f81f747fa5d8da5d0b3e5c061035",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=599e68463353c67ca6e526699691302c49400ed9",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=729790fec8affd24ce49456ab22f9352e1a2139e",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0995f9e90aa7bdd0d2280a12556a84fda81cb45e",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c35dd881d1547a11e35792b26300d6dd95afdbaa",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=11d79042c75767b4724474ec466818aaf7b34865",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=58ea894910fae00bb9e7236d3aaa95cfdde4255d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lwgy9m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwgy9m/rekaairekaflash31_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/RekaAI/reka-flash-3.1",
          "subreddit_subscribers": 497502,
          "created_utc": 1752164422,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I all, I've been using Claude Coder at work for a while now and LOVE it. Is there any high quality alternatives where you can use local models / openAI endpoint(s)? ",
          "author_fullname": "t2_cnkw6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Source Claude Coder alternative?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lww2w9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752202934,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I all, I&amp;#39;ve been using Claude Coder at work for a while now and LOVE it. Is there any high quality alternatives where you can use local models / openAI endpoint(s)? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lww2w9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "maxwell321",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lww2w9/open_source_claude_coder_alternative/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lww2w9/open_source_claude_coder_alternative/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752202934,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been experimenting building my own UI and having it load and run some Llama models. I have an RTX 4080 (16GB VRAM) and I run the Llama 3.1 13B at 50 tokens/s. I was unable to get Llama 4 17B to run any faster than 0.2 Tokens/s.\n\nLlama 3.1 13B is not up to my tasks other than being a standard chatbot. Llama 4 17B gave me some actual good reasoning and completed my tests, but the speed is too slow.\n\nI see people on reddit say something along the line \"You don't need to load the entire model into VRAM, there are many ways to do it as long as you are okay with tokens/s at your read speed\" and went on suggesting a 32B model on a 4080 to the guy. How?\n\nAm I able to load a 32B on my system and have it generate text at read speed (Read speed is relative) but certainly faster than 0.2 tokens/s.\n\nMy system:\n\n64GB RAM  \nRyzen 5900X  \nRTX 4080 (16GB)  \n\n\n  \nMy goal is to have 2-3 models to switch between. One for generic chatbot stuff, one for high reasoning and one for coding. Al tough, chatbot stuff and reasoning could be one model.",
          "author_fullname": "t2_urx6gulz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx4mad",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752233931,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been experimenting building my own UI and having it load and run some Llama models. I have an RTX 4080 (16GB VRAM) and I run the Llama 3.1 13B at 50 tokens/s. I was unable to get Llama 4 17B to run any faster than 0.2 Tokens/s.&lt;/p&gt;\n\n&lt;p&gt;Llama 3.1 13B is not up to my tasks other than being a standard chatbot. Llama 4 17B gave me some actual good reasoning and completed my tests, but the speed is too slow.&lt;/p&gt;\n\n&lt;p&gt;I see people on reddit say something along the line &amp;quot;You don&amp;#39;t need to load the entire model into VRAM, there are many ways to do it as long as you are okay with tokens/s at your read speed&amp;quot; and went on suggesting a 32B model on a 4080 to the guy. How?&lt;/p&gt;\n\n&lt;p&gt;Am I able to load a 32B on my system and have it generate text at read speed (Read speed is relative) but certainly faster than 0.2 tokens/s.&lt;/p&gt;\n\n&lt;p&gt;My system:&lt;/p&gt;\n\n&lt;p&gt;64GB RAM&lt;br/&gt;\nRyzen 5900X&lt;br/&gt;\nRTX 4080 (16GB)  &lt;/p&gt;\n\n&lt;p&gt;My goal is to have 2-3 models to switch between. One for generic chatbot stuff, one for high reasoning and one for coding. Al tough, chatbot stuff and reasoning could be one model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lx4mad",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Aelexi93",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx4mad/need_help/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx4mad/need_help/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752233931,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying to understand why does something like say llama 3.1 8b need further instruction by something like alpaca? If you just load the base model and ask something of it it just responds with gibberish. If you train it with say even just 1000 samples of alpaca data it starts responding coherently. But why does that happen when the original is already trained on next token generation? The q/a instruction training is also next token generation why does a little nudge in the weights from alpaca or other small data sets suddenly get it to respond with coherent responses. When I've looked around in sites etc it just says the further instruction gets the model to align to respond but doesn't say why. How come a few samples (say just 1000 alpaca samples) of 'fine tuning' next token generation suddenly go from gibberish to coherent responses when that is also just doing next token generation as well. I get its training directed towards producing responses to questions so it would shift the weights towards that but the original next token training would have had similar q/a data sets in it already so why doesn't it already do it?\n\n\n\nJust for context i'm using [https://huggingface.co/meta-llama/Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) with lora to train on the alpaca data.",
          "author_fullname": "t2_1h4o7f23eh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why do base models give gibberish and need further 'fine tuning'",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwk84b",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752172244,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752172046,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to understand why does something like say llama 3.1 8b need further instruction by something like alpaca? If you just load the base model and ask something of it it just responds with gibberish. If you train it with say even just 1000 samples of alpaca data it starts responding coherently. But why does that happen when the original is already trained on next token generation? The q/a instruction training is also next token generation why does a little nudge in the weights from alpaca or other small data sets suddenly get it to respond with coherent responses. When I&amp;#39;ve looked around in sites etc it just says the further instruction gets the model to align to respond but doesn&amp;#39;t say why. How come a few samples (say just 1000 alpaca samples) of &amp;#39;fine tuning&amp;#39; next token generation suddenly go from gibberish to coherent responses when that is also just doing next token generation as well. I get its training directed towards producing responses to questions so it would shift the weights towards that but the original next token training would have had similar q/a data sets in it already so why doesn&amp;#39;t it already do it?&lt;/p&gt;\n\n&lt;p&gt;Just for context i&amp;#39;m using &lt;a href=\"https://huggingface.co/meta-llama/Llama-3.1-8B\"&gt;https://huggingface.co/meta-llama/Llama-3.1-8B&lt;/a&gt; with lora to train on the alpaca data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?auto=webp&amp;s=bb56ed1972a4758624102c003d823e903d87bcb2",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a573e4b70357cc304f738189e72c00b44622fc1",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e66f71d268dc1e20de7ad544103444fce5f85488",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b4002d63751a26c36e1b700aa599d15f0e652d30",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7bfdad6461fb7b0282d5a1a935024a36731858fd",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=724d0bec544adf6ea5bb67e4a291ff7a10fe78eb",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c8c9bcd52d928c8a6142e9d4bb6f78ddfe70d726",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwk84b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "QFGTrialByFire",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752172046,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recently built a voice-based AI interviewer that runs in real time, asks job-specific follow-up questions, and can even look things up mid-conversation. It uses LiveKit for audio, Gemini for speech and reasoning, and Maxim to log and evaluate everything the agent does.\n\nWhat sets this apart from other voice agents is observability. Every prompt, search, response, and transition is logged. You can trace exactly how the agent interpreted your answer, what it did next, and why. That transparency made it easier to fix hallucinations, tighten the flow, and debug weird edge cases.\n\nIt’s designed to mimic real interviews, so it adapts to your job description and goes beyond generic “tell me about yourself” questions. You can customize the system prompt or plug in other use cases like sales calls or support agents.\n\nBuilt it mostly to experiment with audio + evals, but i want to know how others are approaching observability in voice agents.",
          "author_fullname": "t2_6fxogmyi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Made a Mock Interview Agent That Talks, Listens, Searches - and Logs Everything",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx2uwr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752227724,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently built a voice-based AI interviewer that runs in real time, asks job-specific follow-up questions, and can even look things up mid-conversation. It uses LiveKit for audio, Gemini for speech and reasoning, and Maxim to log and evaluate everything the agent does.&lt;/p&gt;\n\n&lt;p&gt;What sets this apart from other voice agents is observability. Every prompt, search, response, and transition is logged. You can trace exactly how the agent interpreted your answer, what it did next, and why. That transparency made it easier to fix hallucinations, tighten the flow, and debug weird edge cases.&lt;/p&gt;\n\n&lt;p&gt;It’s designed to mimic real interviews, so it adapts to your job description and goes beyond generic “tell me about yourself” questions. You can customize the system prompt or plug in other use cases like sales calls or support agents.&lt;/p&gt;\n\n&lt;p&gt;Built it mostly to experiment with audio + evals, but i want to know how others are approaching observability in voice agents.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lx2uwr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dinkinflika0",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx2uwr/made_a_mock_interview_agent_that_talks_listens/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx2uwr/made_a_mock_interview_agent_that_talks_listens/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752227724,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\nWe’re a hospital building an on-prem system for health and medical data analytics using LLMs. Our setup includes an RTX 6000 Pro and a 5090, and we’re working with a $10~$19k budget.\n\nI have already tried Gemma3 on 5090 but can’t unleash the 96gb vram capabilities.\n\nWe’re looking to:\n\t•\tRun a large open-source LLM locally (currently putting eyes in llama4)\n\t•\tDo fine-tuning (LoRA or full) on structured clinical data and unstructured medical notes\n\t•\tUse the model for summarization, Q&amp;A, and EHR-related tasks\n\nWe’d love recommendations on:\n\t1.\tThe best large open-source LLM to use in this context\n\t2.\tHow much CPU matters for performance (inference + fine-tuning) alongside these GPUs\n\nWould really appreciate any suggestions based on real-world setups—especially if you’ve done similar work in the health/biomed space.\n\nThanks in advance!\n",
          "author_fullname": "t2_e4ojre534",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best large open-source LLM for health/medical data analytics (RTX 6000 Pro, $10k budget)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwrd38",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752191297,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752189389,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,\nWe’re a hospital building an on-prem system for health and medical data analytics using LLMs. Our setup includes an RTX 6000 Pro and a 5090, and we’re working with a $10~$19k budget.&lt;/p&gt;\n\n&lt;p&gt;I have already tried Gemma3 on 5090 but can’t unleash the 96gb vram capabilities.&lt;/p&gt;\n\n&lt;p&gt;We’re looking to:\n    • Run a large open-source LLM locally (currently putting eyes in llama4)\n    • Do fine-tuning (LoRA or full) on structured clinical data and unstructured medical notes\n    • Use the model for summarization, Q&amp;amp;A, and EHR-related tasks&lt;/p&gt;\n\n&lt;p&gt;We’d love recommendations on:\n    1.  The best large open-source LLM to use in this context\n    2.  How much CPU matters for performance (inference + fine-tuning) alongside these GPUs&lt;/p&gt;\n\n&lt;p&gt;Would really appreciate any suggestions based on real-world setups—especially if you’ve done similar work in the health/biomed space.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwrd38",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LeastExperience1579",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752189389,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "* **Chat**\n   * Explore and contribute to the open sourced GitHub Copilot Chat extension ([Read our blog post](https://code.visualstudio.com/blogs/2025/06/30/openSourceAIEditorFirstMilestone)).\n   * Generate custom instructions that reflect your project's conventions ([Show more](https://code.visualstudio.com/updates/v1_102#_generate-custom-instructions)).\n   * Use custom modes to tailor chat for tasks like planning or research ([Show more](https://code.visualstudio.com/updates/v1_102#_chat-mode-improvements)).\n   * Automatically approve selected terminal commands ([Show more](https://code.visualstudio.com/updates/v1_102#_terminal-auto-approval-experimental)).\n   * Edit and resubmit previous chat requests ([Show more](https://code.visualstudio.com/updates/v1_102#_edit-previous-requests-experimental)).\n* **MCP**\n   * MCP support is now generally available in VS Code ([Show more](https://code.visualstudio.com/updates/v1_102#_mcp-support-in-vs-code-is-generally-available)).\n   * Easily install and manage MCP servers with the MCP view and gallery ([Show more](https://code.visualstudio.com/updates/v1_102#_mcp-server-discovery-and-installation)).\n   * MCP servers as first-class resources in profiles and Settings Sync ([Show more](https://code.visualstudio.com/updates/v1_102#_mcp-servers-as-first-class-resources)).\n* **Editor experience**\n   * Delegate tasks to Copilot coding agent and let it handle them in the background ([Show more](https://code.visualstudio.com/updates/v1_102#_start-a-coding-agent-session-preview)).\n   * Scroll the editor on middle click ([Show more](https://code.visualstudio.com/updates/v1_102#_scroll-on-middle-click)).\n\n&gt;VS Code pm here in case there are any questions I am happy to answer.",
          "author_fullname": "t2_1k174c7o8k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "VS Code June 2025 (version 1.102)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwlw1j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE.jpeg?width=140&amp;height=69&amp;crop=140:69,smart&amp;auto=webp&amp;s=0c82605b9ec19de72cace6d5533da2aa5d6e0222",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752175989,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "code.visualstudio.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Explore and contribute to the open sourced GitHub Copilot Chat extension (&lt;a href=\"https://code.visualstudio.com/blogs/2025/06/30/openSourceAIEditorFirstMilestone\"&gt;Read our blog post&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;Generate custom instructions that reflect your project&amp;#39;s conventions (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_generate-custom-instructions\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;Use custom modes to tailor chat for tasks like planning or research (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_chat-mode-improvements\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;Automatically approve selected terminal commands (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_terminal-auto-approval-experimental\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;Edit and resubmit previous chat requests (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_edit-previous-requests-experimental\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;MCP&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;MCP support is now generally available in VS Code (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_mcp-support-in-vs-code-is-generally-available\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;Easily install and manage MCP servers with the MCP view and gallery (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_mcp-server-discovery-and-installation\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;MCP servers as first-class resources in profiles and Settings Sync (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_mcp-servers-as-first-class-resources\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Editor experience&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Delegate tasks to Copilot coding agent and let it handle them in the background (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_start-a-coding-agent-session-preview\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;Scroll the editor on middle click (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_scroll-on-middle-click\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;VS Code pm here in case there are any questions I am happy to answer.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://code.visualstudio.com/updates/v1_102",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE.jpeg?auto=webp&amp;s=e93357d092d26269a5ce43251b2869e329d68ae1",
                  "width": 1069,
                  "height": 534
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b41d571481b4e96109b9abbdd45f66cd0298931",
                    "width": 108,
                    "height": 53
                  },
                  {
                    "url": "https://external-preview.redd.it/U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d66d5052f2ad700bd7fb1a2ef87d2c080ecde87",
                    "width": 216,
                    "height": 107
                  },
                  {
                    "url": "https://external-preview.redd.it/U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f8dacd08bf9e94649d34590f77620cfedf76f859",
                    "width": 320,
                    "height": 159
                  },
                  {
                    "url": "https://external-preview.redd.it/U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fea22c6187bb94fe796b6c79160a51ef50be7f7d",
                    "width": 640,
                    "height": 319
                  },
                  {
                    "url": "https://external-preview.redd.it/U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=87831ccde168e2a6fd2b06da16fdd86be4b391e6",
                    "width": 960,
                    "height": 479
                  }
                ],
                "variants": {},
                "id": "U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwlw1j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "isidor_n",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwlw1j/vs_code_june_2025_version_1102/",
          "stickied": false,
          "url": "https://code.visualstudio.com/updates/v1_102",
          "subreddit_subscribers": 497502,
          "created_utc": 1752175989,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/8vkkyh08q8cf1.png?width=2195&amp;format=png&amp;auto=webp&amp;s=091a7abe78f1e07e430aa2a6b516837983d7ec3a\n\nI know you can use langchain and whatnot to do this, vis a vi editing a python document, but is there any simplified, smoothed out front end, that makes the process tactile, clicky, wired, physical, and simple?\n\nPerhaps one that accepts a local API -- preferably not a wrapper for LlamaCPP; I already have quite a few of those, lol. I like the LMStudio pipeline and would like to stick with that as the core.\n\nSomething like that has to exist by now, right? If it doesn't, anyone wanna help me make an LMStudio plug in that gives us that capability?",
          "author_fullname": "t2_d887zj2w0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the best way to work with granulized AI tasks or \"agents.\" Any front-end UI/program?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 63,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8vkkyh08q8cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 48,
                  "x": 108,
                  "u": "https://preview.redd.it/8vkkyh08q8cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b336905f578380404b3c1355f4244e270c64ccf"
                },
                {
                  "y": 97,
                  "x": 216,
                  "u": "https://preview.redd.it/8vkkyh08q8cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=446b27630728d73a104da58729ec8996eaa3ab69"
                },
                {
                  "y": 145,
                  "x": 320,
                  "u": "https://preview.redd.it/8vkkyh08q8cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=45a48d8f78284e89b013fb09970c29c79db70e2c"
                },
                {
                  "y": 290,
                  "x": 640,
                  "u": "https://preview.redd.it/8vkkyh08q8cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=80fb9830671ec734b757584d06d5ae2f7a391c5c"
                },
                {
                  "y": 435,
                  "x": 960,
                  "u": "https://preview.redd.it/8vkkyh08q8cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=db6b3295737101716b82c4e940fc5dc45b545676"
                },
                {
                  "y": 489,
                  "x": 1080,
                  "u": "https://preview.redd.it/8vkkyh08q8cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4bd9d2dbffbb7bd35cea0157972c254314ba2798"
                }
              ],
              "s": {
                "y": 995,
                "x": 2195,
                "u": "https://preview.redd.it/8vkkyh08q8cf1.png?width=2195&amp;format=png&amp;auto=webp&amp;s=091a7abe78f1e07e430aa2a6b516837983d7ec3a"
              },
              "id": "8vkkyh08q8cf1"
            }
          },
          "name": "t3_1lx5wvp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/PdigHC71qxpDq6fOStIJNtCuT4ldKcg6xtzSGMG5y84.jpg",
          "edited": 1752238027,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752237797,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/8vkkyh08q8cf1.png?width=2195&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=091a7abe78f1e07e430aa2a6b516837983d7ec3a\"&gt;https://preview.redd.it/8vkkyh08q8cf1.png?width=2195&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=091a7abe78f1e07e430aa2a6b516837983d7ec3a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I know you can use langchain and whatnot to do this, vis a vi editing a python document, but is there any simplified, smoothed out front end, that makes the process tactile, clicky, wired, physical, and simple?&lt;/p&gt;\n\n&lt;p&gt;Perhaps one that accepts a local API -- preferably not a wrapper for LlamaCPP; I already have quite a few of those, lol. I like the LMStudio pipeline and would like to stick with that as the core.&lt;/p&gt;\n\n&lt;p&gt;Something like that has to exist by now, right? If it doesn&amp;#39;t, anyone wanna help me make an LMStudio plug in that gives us that capability?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lx5wvp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Jattoe",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx5wvp/whats_the_best_way_to_work_with_granulized_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx5wvp/whats_the_best_way_to_work_with_granulized_ai/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752237797,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/kadavilrahul/coding\\_task\\_manager](https://github.com/kadavilrahul/coding_task_manager)",
          "author_fullname": "t2_adnzl8f8x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I have made a github repository for streamlining AI coding flow. Please suggest improvements as additions and substraction to the codebase.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx5p9b",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752237201,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/kadavilrahul/coding_task_manager\"&gt;https://github.com/kadavilrahul/coding_task_manager&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lx5p9b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Maleficent_Mess6445",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx5p9b/i_have_made_a_github_repository_for_streamlining/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx5p9b/i_have_made_a_github_repository_for_streamlining/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752237201,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking for both a good app and an availability of a good and capable LLM. \nThanks! ",
          "author_fullname": "t2_v7g4kt4q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help me find the best Android app for running LLMs locally",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwwuwq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752205352,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for both a good app and an availability of a good and capable LLM. \nThanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwwuwq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DanielD2724",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwwuwq/help_me_find_the_best_android_app_for_running/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwwuwq/help_me_find_the_best_android_app_for_running/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752205352,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey Reddit!\n\nBuilding a RAG app focused on **Q&amp;A**, and I need a good **open-source model that runs well locally**.\n\nWhat's your go-to for performance vs. hardware (GPU/RAM) on a local setup for answering questions?\n\nThanks for the help!\n\n\\#RAG #LocalLLM #OpenSource #AI #QandA",
          "author_fullname": "t2_m9tb5bko",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Quick Question: Best Open-Source Model for Local Q&amp;A RAG App? 🤔",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx5jc1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752236718,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Reddit!&lt;/p&gt;\n\n&lt;p&gt;Building a RAG app focused on &lt;strong&gt;Q&amp;amp;A&lt;/strong&gt;, and I need a good &lt;strong&gt;open-source model that runs well locally&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s your go-to for performance vs. hardware (GPU/RAM) on a local setup for answering questions?&lt;/p&gt;\n\n&lt;p&gt;Thanks for the help!&lt;/p&gt;\n\n&lt;p&gt;#RAG #LocalLLM #OpenSource #AI #QandA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lx5jc1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Due-Wind6781",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx5jc1/quick_question_best_opensource_model_for_local_qa/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx5jc1/quick_question_best_opensource_model_for_local_qa/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752236718,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "There is a new pull request to support GLM-4 MoE on VLLM.\n\nHopefully we will have a new powerful model!\n\n[https://github.com/vllm-project/vllm/pull/20736](https://github.com/vllm-project/vllm/pull/20736)",
          "author_fullname": "t2_hoxc8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4 MoE incoming",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw71av",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 157,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 157,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752134298,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There is a new pull request to support GLM-4 MoE on VLLM.&lt;/p&gt;\n\n&lt;p&gt;Hopefully we will have a new powerful model!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/vllm-project/vllm/pull/20736\"&gt;https://github.com/vllm-project/vllm/pull/20736&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/uLQUYJzfZFruZAn57VwKQmTVHdq10TT6JiYeU7Uj7yY.png?auto=webp&amp;s=b456605a4d6c184722438a164c5d25e6b2b287a6",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/uLQUYJzfZFruZAn57VwKQmTVHdq10TT6JiYeU7Uj7yY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=366b6dc79bf3c070bc858477ad20f59843aea2d0",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/uLQUYJzfZFruZAn57VwKQmTVHdq10TT6JiYeU7Uj7yY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=222aed5669cc36c67fede38661ffba9d8551d46d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/uLQUYJzfZFruZAn57VwKQmTVHdq10TT6JiYeU7Uj7yY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a3b025279090a032a0033b3289198754b85a79e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/uLQUYJzfZFruZAn57VwKQmTVHdq10TT6JiYeU7Uj7yY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=def8751711047e8e98ec850bc828c6bc06e00bcc",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/uLQUYJzfZFruZAn57VwKQmTVHdq10TT6JiYeU7Uj7yY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b546242d637804a00d3ec67f71657e3275d4bdb8",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/uLQUYJzfZFruZAn57VwKQmTVHdq10TT6JiYeU7Uj7yY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a439eb185d750fdba01a91c26dd402622509b7a4",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "uLQUYJzfZFruZAn57VwKQmTVHdq10TT6JiYeU7Uj7yY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lw71av",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "matteogeniaccio",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw71av/glm4_moe_incoming/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw71av/glm4_moe_incoming/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752134298,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Some people think AI agents are hype and glorified workflows.\n\nBut agents that actually work don’t try to be JARVIS, not yet. The ones that succeed stick to structured workflows. And that’s not a bad thing. When I was in school, we studied Little Computer 3 to understand how computer architecture starts with state machines. I attached that diagram, and that's just the simplest computer architecture just for education purpose.\n\nA workflow is just a finite state machine (FSM) with memory and tool use. LLMs are surprisingly good at that. These agents complete real tasks that used to take human time and effort.\n\nRetell AI is a great example. It handles real phone calls for things like loans and pharmacy refills. It knows what step it’s on, when to speak, when to listen, and when to escalate. That kind of structure makes it reliable. Simplify is doing the same for job applications. It finds postings, autofills forms, tracks everything, and updates the user. These are clear, scoped workflows with success criteria, and that’s where LLMs perform really well.\n\nPlugging LLM in workflows isn’t enough. The teams behind these tools constantly monitor what’s happening. They trace every call, evaluate outputs, catch failure patterns, and improve prompts. I believe they have a very complicated workflow, and tools like Keywords AI make that kind of observability easy. Without it, even a well-built agent will drift.\n\nNot every agent is magic. But the ones that work? They’re already saving time, money, and headcount. That's what we need in the current state.",
          "author_fullname": "t2_1pnlpczpqa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Workflows aren’t a weakness in AI agents, they’re why they work",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwniq0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752179821,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some people think AI agents are hype and glorified workflows.&lt;/p&gt;\n\n&lt;p&gt;But agents that actually work don’t try to be JARVIS, not yet. The ones that succeed stick to structured workflows. And that’s not a bad thing. When I was in school, we studied Little Computer 3 to understand how computer architecture starts with state machines. I attached that diagram, and that&amp;#39;s just the simplest computer architecture just for education purpose.&lt;/p&gt;\n\n&lt;p&gt;A workflow is just a finite state machine (FSM) with memory and tool use. LLMs are surprisingly good at that. These agents complete real tasks that used to take human time and effort.&lt;/p&gt;\n\n&lt;p&gt;Retell AI is a great example. It handles real phone calls for things like loans and pharmacy refills. It knows what step it’s on, when to speak, when to listen, and when to escalate. That kind of structure makes it reliable. Simplify is doing the same for job applications. It finds postings, autofills forms, tracks everything, and updates the user. These are clear, scoped workflows with success criteria, and that’s where LLMs perform really well.&lt;/p&gt;\n\n&lt;p&gt;Plugging LLM in workflows isn’t enough. The teams behind these tools constantly monitor what’s happening. They trace every call, evaluate outputs, catch failure patterns, and improve prompts. I believe they have a very complicated workflow, and tools like Keywords AI make that kind of observability easy. Without it, even a well-built agent will drift.&lt;/p&gt;\n\n&lt;p&gt;Not every agent is magic. But the ones that work? They’re already saving time, money, and headcount. That&amp;#39;s what we need in the current state.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwniq0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Main-Fisherman-2075",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwniq0/workflows_arent_a_weakness_in_ai_agents_theyre/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwniq0/workflows_arent_a_weakness_in_ai_agents_theyre/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752179821,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just add `/no_think` in the system prompt and the model will mostly stop reasoning \n\nYou can also add your own conditions like `when i write /nt it means /no_think` or `always /no_think except if i write /think` if the model is smart enough it will mostly follow your orders\n\nTested on qwen3",
          "author_fullname": "t2_rxgre5u8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tired of writing /no_think every time you prompt?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwwh8s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.61,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752204167,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just add &lt;code&gt;/no_think&lt;/code&gt; in the system prompt and the model will mostly stop reasoning &lt;/p&gt;\n\n&lt;p&gt;You can also add your own conditions like &lt;code&gt;when i write /nt it means /no_think&lt;/code&gt; or &lt;code&gt;always /no_think except if i write /think&lt;/code&gt; if the model is smart enough it will mostly follow your orders&lt;/p&gt;\n\n&lt;p&gt;Tested on qwen3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lwwh8s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Iq1pl",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwwh8s/tired_of_writing_no_think_every_time_you_prompt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwwh8s/tired_of_writing_no_think_every_time_you_prompt/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752204167,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We've just released what might be the most comprehensive documentation of AI data quality evaluation metrics available. This covers everything from pre-training data assessment to multimodal evaluation.\n\nWhat's included:\n\n* 50+ evaluation metrics across text, image, and multimodal data\n* Academic citations for every metric (RedPajama, CLIP, NIMA, etc.)\n* Rule-based and LLM-based evaluation approaches\n* Practical usage examples and API documentation\n\nKey categories:\n\n* Text Quality: Completeness, Fluency, Relevance, Effectiveness\n* Image Quality: Clarity, Similarity, Validity\n* Security: Political sensitivity, prohibited content, harmful information\n* Classification: Topic categorization, content classification\n\nThis is particularly useful for:\n\n* Data scientists working on model training\n* Researchers needing standardized evaluation frameworks\n* Anyone dealing with large-scale data quality assessment\n\nThe documentation includes detailed academic references and practical implementation examples. All open source and ready to use.\n\nLink: [https://github.com/MigoXLab/dingo/blob/dev/docs/metrics.md](https://github.com/MigoXLab/dingo/blob/dev/docs/metrics.md)\n\nThoughts? What metrics do you find most valuable in your work?",
          "author_fullname": "t2_nrnbu0gz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[OC] Comprehensive AI Data Quality Metrics Documentation - 50+ Evaluation Metrics with Academic Sources",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwuzjo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752199736,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve just released what might be the most comprehensive documentation of AI data quality evaluation metrics available. This covers everything from pre-training data assessment to multimodal evaluation.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s included:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;50+ evaluation metrics across text, image, and multimodal data&lt;/li&gt;\n&lt;li&gt;Academic citations for every metric (RedPajama, CLIP, NIMA, etc.)&lt;/li&gt;\n&lt;li&gt;Rule-based and LLM-based evaluation approaches&lt;/li&gt;\n&lt;li&gt;Practical usage examples and API documentation&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Key categories:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Text Quality: Completeness, Fluency, Relevance, Effectiveness&lt;/li&gt;\n&lt;li&gt;Image Quality: Clarity, Similarity, Validity&lt;/li&gt;\n&lt;li&gt;Security: Political sensitivity, prohibited content, harmful information&lt;/li&gt;\n&lt;li&gt;Classification: Topic categorization, content classification&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This is particularly useful for:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data scientists working on model training&lt;/li&gt;\n&lt;li&gt;Researchers needing standardized evaluation frameworks&lt;/li&gt;\n&lt;li&gt;Anyone dealing with large-scale data quality assessment&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The documentation includes detailed academic references and practical implementation examples. All open source and ready to use.&lt;/p&gt;\n\n&lt;p&gt;Link: &lt;a href=\"https://github.com/MigoXLab/dingo/blob/dev/docs/metrics.md\"&gt;https://github.com/MigoXLab/dingo/blob/dev/docs/metrics.md&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thoughts? What metrics do you find most valuable in your work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/c340hQeOye9TxtgDQ0X1CY8WX4WAKvr0pumN26Zmq9o.png?auto=webp&amp;s=56ab51e042e97f89e887aa652cd019dae9c3daa0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/c340hQeOye9TxtgDQ0X1CY8WX4WAKvr0pumN26Zmq9o.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=761a7fb258827c359161c668421aeb498ed39406",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/c340hQeOye9TxtgDQ0X1CY8WX4WAKvr0pumN26Zmq9o.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5c0c73ddeb762b9a74a2657a3000eba0d4cc994",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/c340hQeOye9TxtgDQ0X1CY8WX4WAKvr0pumN26Zmq9o.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=23e31bf29b7085bf3c31b2fb70388afdf309c64b",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/c340hQeOye9TxtgDQ0X1CY8WX4WAKvr0pumN26Zmq9o.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e55312dd2a2d390a3e35bd9c65919e2618d36021",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/c340hQeOye9TxtgDQ0X1CY8WX4WAKvr0pumN26Zmq9o.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f5e8dbc4d83ea236579446db22e044925ce723d3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/c340hQeOye9TxtgDQ0X1CY8WX4WAKvr0pumN26Zmq9o.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=53c82d075ef31239b56bc72982cc480d4843520a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "c340hQeOye9TxtgDQ0X1CY8WX4WAKvr0pumN26Zmq9o"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwuzjo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "chupei0",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwuzjo/oc_comprehensive_ai_data_quality_metrics/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwuzjo/oc_comprehensive_ai_data_quality_metrics/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752199736,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks, I am quite new to the local model space and having a hard time to decide which models to invest further in (by giving more cores/gpu focus toward - and add docs for RAG). \n\nMain goals:\n\n\\- Completely offline models for privacy / security\n\n\\- High token count and focused on best English writing / summarizations of large text or documents. \n\n\\- Crafting emails given a source and context",
          "author_fullname": "t2_i5gbl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LM Studio model recommendation for writing, emails, and general summarizations",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwxnf0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752207904,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I am quite new to the local model space and having a hard time to decide which models to invest further in (by giving more cores/gpu focus toward - and add docs for RAG). &lt;/p&gt;\n\n&lt;p&gt;Main goals:&lt;/p&gt;\n\n&lt;p&gt;- Completely offline models for privacy / security&lt;/p&gt;\n\n&lt;p&gt;- High token count and focused on best English writing / summarizations of large text or documents. &lt;/p&gt;\n\n&lt;p&gt;- Crafting emails given a source and context&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwxnf0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vdog313",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwxnf0/lm_studio_model_recommendation_for_writing_emails/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwxnf0/lm_studio_model_recommendation_for_writing_emails/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752207904,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[UGI-Leaderboard](https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard)\n\nIt has a lower willingness (W/10) than Grok-3, so it'll refuse more, but it makes up for that because of its massive intelligence (NatInt) increase.\n\nLooking through its political stats, it is less progressive with social issues than Grok-3, but it is overall more left leaning because of things like it being less religious, less bioconservative, and less nationalistic.\n\nWhen comparing other proprietary models, Grok 1, 2, and 4 stick out the most for being the least socially progressive.",
          "author_fullname": "t2_e79ya7rd7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Added Grok-4 to the UGI-Leaderboard",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 48,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw9ch2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 79,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 79,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/6gDMLMNjgN3fCDeA-9V_AcrHvobFBdL-txWjdpF4agU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752143531,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard\"&gt;UGI-Leaderboard&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It has a lower willingness (W/10) than Grok-3, so it&amp;#39;ll refuse more, but it makes up for that because of its massive intelligence (NatInt) increase.&lt;/p&gt;\n\n&lt;p&gt;Looking through its political stats, it is less progressive with social issues than Grok-3, but it is overall more left leaning because of things like it being less religious, less bioconservative, and less nationalistic.&lt;/p&gt;\n\n&lt;p&gt;When comparing other proprietary models, Grok 1, 2, and 4 stick out the most for being the least socially progressive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6g4lpxpay0cf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6g4lpxpay0cf1.png?auto=webp&amp;s=329a51335cf9e548b3393e7c3a8812515113036b",
                  "width": 1862,
                  "height": 651
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6g4lpxpay0cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=08b4c4d0535521b882b9b633a0c70bc6c0e3794f",
                    "width": 108,
                    "height": 37
                  },
                  {
                    "url": "https://preview.redd.it/6g4lpxpay0cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=33b3e33da215e6ff6c46f95c22ad7f8b8ef2d6b4",
                    "width": 216,
                    "height": 75
                  },
                  {
                    "url": "https://preview.redd.it/6g4lpxpay0cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2ade97a3b108109d6a571fc32964e5f2f459e5bd",
                    "width": 320,
                    "height": 111
                  },
                  {
                    "url": "https://preview.redd.it/6g4lpxpay0cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=696d0258f779029c9cde582e77066bdfaf475731",
                    "width": 640,
                    "height": 223
                  },
                  {
                    "url": "https://preview.redd.it/6g4lpxpay0cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=795b8752660d7157827c8348b7f4e78ba409a35b",
                    "width": 960,
                    "height": 335
                  },
                  {
                    "url": "https://preview.redd.it/6g4lpxpay0cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f15f6b5a9d85155e4d4dd4c0cbf1094eb604acb6",
                    "width": 1080,
                    "height": 377
                  }
                ],
                "variants": {},
                "id": "w74Yuu1shaH3gNqOgsCpN1drersHrujFVDnSXw05WpM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lw9ch2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DontPlanToEnd",
          "discussion_type": null,
          "num_comments": 63,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw9ch2/added_grok4_to_the_ugileaderboard/",
          "stickied": false,
          "url": "https://i.redd.it/6g4lpxpay0cf1.png",
          "subreddit_subscribers": 497502,
          "created_utc": 1752143531,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I think people are not able conceive AI agents of future. Many are just trying to connect some LLM to applications of past era and make some small tasks work, but I don't think it is an agent in any sense. The LLM and applications are mostly separate still.\nI think the real agent will look something like claude code AI terminal editor which can control absolutely everything that it touches.",
          "author_fullname": "t2_adnzl8f8x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What do you think future AI agents will look like?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx32mx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752228523,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think people are not able conceive AI agents of future. Many are just trying to connect some LLM to applications of past era and make some small tasks work, but I don&amp;#39;t think it is an agent in any sense. The LLM and applications are mostly separate still.\nI think the real agent will look something like claude code AI terminal editor which can control absolutely everything that it touches.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lx32mx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Maleficent_Mess6445",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx32mx/what_do_you_think_future_ai_agents_will_look_like/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx32mx/what_do_you_think_future_ai_agents_will_look_like/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752228523,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "xAI has just announced its smartest AI models to date: Grok 4 and Grok 4 Heavy. Both are subscription-based, with Grok 4 Heavy priced at approximately $300 per month. Excited to see what these new models can do! \n",
          "author_fullname": "t2_1gjiyteyd7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Grok 4 Benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 88,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "vkta3pkjczbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/vkta3pkjczbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=985e0b38c83e43fccece7a240817ebe6a1a8eba8"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/vkta3pkjczbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=528fc189ced27a3f9fc86b8588234aaea81ef748"
                },
                {
                  "y": 167,
                  "x": 320,
                  "u": "https://preview.redd.it/vkta3pkjczbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1bca4291fd82942ef284753cb2df4fd685c7ce08"
                },
                {
                  "y": 334,
                  "x": 640,
                  "u": "https://preview.redd.it/vkta3pkjczbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8ea9f5a5e718ac354b76b52c74fad37b69f24253"
                },
                {
                  "y": 501,
                  "x": 960,
                  "u": "https://preview.redd.it/vkta3pkjczbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c84964d8d05185d68d720b3ac5c6713e873bd6b8"
                },
                {
                  "y": 563,
                  "x": 1080,
                  "u": "https://preview.redd.it/vkta3pkjczbf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=388b13184d6beb98d4c046c812f8d937839df00f"
                }
              ],
              "s": {
                "y": 1069,
                "x": 2048,
                "u": "https://preview.redd.it/vkta3pkjczbf1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;s=4be479618f0d9fbf867557ee57600e698060eba9"
              },
              "id": "vkta3pkjczbf1"
            },
            "x5h8ytejczbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 57,
                  "x": 108,
                  "u": "https://preview.redd.it/x5h8ytejczbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8767e1932373c32a8fb779261021d7cd279c698e"
                },
                {
                  "y": 114,
                  "x": 216,
                  "u": "https://preview.redd.it/x5h8ytejczbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4ca74f22b86dd65a5e91be0d64dff56d64552603"
                },
                {
                  "y": 170,
                  "x": 320,
                  "u": "https://preview.redd.it/x5h8ytejczbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cbaa8de03512446cb0c97a21432b344d8db060a2"
                },
                {
                  "y": 340,
                  "x": 640,
                  "u": "https://preview.redd.it/x5h8ytejczbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a84fc79d39eff248a0ad46509f6c96fe714f12d3"
                },
                {
                  "y": 510,
                  "x": 960,
                  "u": "https://preview.redd.it/x5h8ytejczbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b1dc6dc558944077b5f9fde8266051936fbb2134"
                },
                {
                  "y": 574,
                  "x": 1080,
                  "u": "https://preview.redd.it/x5h8ytejczbf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1672b7097ff282ee5d25785e5c0ec859d7f2db17"
                }
              ],
              "s": {
                "y": 1089,
                "x": 2048,
                "u": "https://preview.redd.it/x5h8ytejczbf1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;s=8d33e2023cc464790627b78f2e327535fecea7f7"
              },
              "id": "x5h8ytejczbf1"
            },
            "ymt1ov4jczbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/ymt1ov4jczbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9f66af2f6f502f5deebd5807a443121ea47b84a4"
                },
                {
                  "y": 136,
                  "x": 216,
                  "u": "https://preview.redd.it/ymt1ov4jczbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6071072e3fab55789582cd426957efb12f641de6"
                },
                {
                  "y": 202,
                  "x": 320,
                  "u": "https://preview.redd.it/ymt1ov4jczbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ea8768c25e63c564169611e307c9128259dac03f"
                },
                {
                  "y": 404,
                  "x": 640,
                  "u": "https://preview.redd.it/ymt1ov4jczbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b5ceb7bf5556f216baf44091df64eb73c96b6e"
                },
                {
                  "y": 607,
                  "x": 960,
                  "u": "https://preview.redd.it/ymt1ov4jczbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c7671eb13ada8539b1019df31cba90aa7b332a8d"
                },
                {
                  "y": 683,
                  "x": 1080,
                  "u": "https://preview.redd.it/ymt1ov4jczbf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5db18304c76f8400621584f31f5436391f72258a"
                }
              ],
              "s": {
                "y": 1814,
                "x": 2868,
                "u": "https://preview.redd.it/ymt1ov4jczbf1.jpg?width=2868&amp;format=pjpg&amp;auto=webp&amp;s=c26c33ad3c76200371c281aa06a385a63c1fcde0"
              },
              "id": "ymt1ov4jczbf1"
            }
          },
          "name": "t3_1lw4eej",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "ups": 205,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "ymt1ov4jczbf1",
                "id": 702623499
              },
              {
                "caption": "",
                "media_id": "x5h8ytejczbf1",
                "id": 702623500
              },
              {
                "caption": "",
                "media_id": "vkta3pkjczbf1",
                "id": 702623501
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 205,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/MLSe5RpW1tkFFRdIT2JZeSSIlKUblh8PvP8Gk8nPH1E.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752124109,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;xAI has just announced its smartest AI models to date: Grok 4 and Grok 4 Heavy. Both are subscription-based, with Grok 4 Heavy priced at approximately $300 per month. Excited to see what these new models can do! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lw4eej",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lw4eej",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DigitusDesigner",
          "discussion_type": null,
          "num_comments": 171,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw4eej/grok_4_benchmarks/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lw4eej",
          "subreddit_subscribers": 497502,
          "created_utc": 1752124109,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone else feel like things have gone quieter in the open-source Llama scene lately?  \nEarlier this year, there were constant updates, fine-tunes, and people sharing their custom Llama workflows. But these past weeks, I’ve seen less buzz—even though projects like DeepSeek and Gemma keep getting mentioned in broader AI circles.\n\n* **Is development still going strong behind the scenes?**\n* Are people switching to closed models, or just not posting as much here?\n* What are the most exciting recent breakthroughs or fine-tunes in the local Llama space that might have flown under the radar?\n\nI found [this article](https://maktoobai.com/open-source-ai/) that discusses the sudden “silence” around open-source AI and how it could impact the future of local models like Llama.  \nWould love to hear from anyone who’s still actively using or training Llama—what’s working, what’s stalling, and any tips for keeping the momentum going!\n\nLet’s swap updates and see what’s brewing locally! 👇",
          "author_fullname": "t2_tcapu8ix",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has Local Llama Development Slowed Down, or Am I Missing Something? 🤔",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx6g3p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752239254,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone else feel like things have gone quieter in the open-source Llama scene lately?&lt;br/&gt;\nEarlier this year, there were constant updates, fine-tunes, and people sharing their custom Llama workflows. But these past weeks, I’ve seen less buzz—even though projects like DeepSeek and Gemma keep getting mentioned in broader AI circles.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Is development still going strong behind the scenes?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Are people switching to closed models, or just not posting as much here?&lt;/li&gt;\n&lt;li&gt;What are the most exciting recent breakthroughs or fine-tunes in the local Llama space that might have flown under the radar?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I found &lt;a href=\"https://maktoobai.com/open-source-ai/\"&gt;this article&lt;/a&gt; that discusses the sudden “silence” around open-source AI and how it could impact the future of local models like Llama.&lt;br/&gt;\nWould love to hear from anyone who’s still actively using or training Llama—what’s working, what’s stalling, and any tips for keeping the momentum going!&lt;/p&gt;\n\n&lt;p&gt;Let’s swap updates and see what’s brewing locally! 👇&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lx6g3p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shaker-ameen",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx6g3p/has_local_llama_development_slowed_down_or_am_i/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx6g3p/has_local_llama_development_slowed_down_or_am_i/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752239254,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_21qaqh1p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OpenAI's open source LLM is a reasoning model, coming Next Thursday!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvr3ym",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 1018,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 1018,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/sUJ6lB6ZDEcKStX1WdRzKRM8EK_d-_hgFEOIZhdoHt0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752087510,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/q01afp6lbwbf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/q01afp6lbwbf1.png?auto=webp&amp;s=0cbd17fbc9a41c400eb85d31978f75ff70df9ddc",
                  "width": 863,
                  "height": 867
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/q01afp6lbwbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6502fad51b4a09ae4274df4e9ed45962541004cf",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/q01afp6lbwbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2253b99563fe69f4f533f1f3ce4fa62b8d4b4ac5",
                    "width": 216,
                    "height": 217
                  },
                  {
                    "url": "https://preview.redd.it/q01afp6lbwbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=96e87778edaf7cf837968133bfc385a30b74c9bf",
                    "width": 320,
                    "height": 321
                  },
                  {
                    "url": "https://preview.redd.it/q01afp6lbwbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3e9bd873a7a7d4e956171cdc1ac61d5f5cae52e7",
                    "width": 640,
                    "height": 642
                  }
                ],
                "variants": {},
                "id": "YzEa9NJG57828WaQr473XCycHU_4jPbLlMIMENhyJMQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lvr3ym",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dulldata",
          "discussion_type": null,
          "num_comments": 268,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/",
          "stickied": false,
          "url": "https://i.redd.it/q01afp6lbwbf1.png",
          "subreddit_subscribers": 497502,
          "created_utc": 1752087510,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_qjpsv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Phi-4-mini-flash-reasoning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw3729",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "#93b1ba",
          "ups": 174,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 174,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=80e6b8d263960de48dbffc84a7d614f7d4381b87",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752120032,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?auto=webp&amp;s=f60794d5e67107c2691276e7de5249c893966a7d",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=121dfa243da145563b7fad4abe0571ae415c0f2e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=33741334973d3270c726098fa1178a0754f7488b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2419000ac55c1ee0561885437f8b594342cf8ed9",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a162d9d243ab8293c0214f3e9bf055ec2af6d514",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1aab3419db55a9c46f6044faa18576d3c0a1fd01",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0d4ebfa6e74a4ed84b2c017c38248f9b55998dc4",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lw3729",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ninjasaid13",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lw3729/phi4miniflashreasoning/",
          "stickied": false,
          "url": "https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning",
          "subreddit_subscribers": 497502,
          "created_utc": 1752120032,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm running Ollama &amp; OpenWebUI on a headless Linux server, as Docker (with Compose) containers, with an NVIDIA GPU. This setup works great, but I want to add MCP servers to my environment, to improve the results from Ollama invocations.\n\nThe [documentation for OpenWebUI](https://docs.openwebui.com/openapi-servers/mcp/) suggests running a single container per MCP server. However, that will get unwieldy quickly.\n\nHow are other people exposing multiple MCP servers as a *singular* Docker service, as part of their Docker Compose stack?",
          "author_fullname": "t2_lophktmpt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Manage multiple MCP servers for Ollama + OpenWebUI as Docker service",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx0b5w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752217473,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running Ollama &amp;amp; OpenWebUI on a headless Linux server, as Docker (with Compose) containers, with an NVIDIA GPU. This setup works great, but I want to add MCP servers to my environment, to improve the results from Ollama invocations.&lt;/p&gt;\n\n&lt;p&gt;The &lt;a href=\"https://docs.openwebui.com/openapi-servers/mcp/\"&gt;documentation for OpenWebUI&lt;/a&gt; suggests running a single container per MCP server. However, that will get unwieldy quickly.&lt;/p&gt;\n\n&lt;p&gt;How are other people exposing multiple MCP servers as a &lt;em&gt;singular&lt;/em&gt; Docker service, as part of their Docker Compose stack?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lx0b5w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "trevorstr",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx0b5w/manage_multiple_mcp_servers_for_ollama_openwebui/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx0b5w/manage_multiple_mcp_servers_for_ollama_openwebui/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752217473,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1a6diqhz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MAXSUN preparing all-Intel Mini Station: up to Core Ultra 9 285HX and two Arc Pro B60 GPU - VideoCardz.com",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwm3w0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?width=140&amp;height=72&amp;crop=140:72,smart&amp;auto=webp&amp;s=82d9d63f02ebef6c18b91cb920b6680936a61a2e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752176508,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "videocardz.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://videocardz.com/newz/maxsun-preparing-all-intel-mini-station-up-to-core-ultra-9-285hx-and-two-arc-pro-b60-gpu",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?auto=webp&amp;s=b96f24d56654376cfb89dab8e48912779471be27",
                  "width": 2000,
                  "height": 1040
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=469ff89e7c00f97d5ba6ee9c12917b5f86debf38",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f7c469ec7711eb7038015668001068e972213af",
                    "width": 216,
                    "height": 112
                  },
                  {
                    "url": "https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a181e8b20e1c0903f3490dbff3dd98d3ee80b613",
                    "width": 320,
                    "height": 166
                  },
                  {
                    "url": "https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eabcea64775657596068577ad159149b4f0dd034",
                    "width": 640,
                    "height": 332
                  },
                  {
                    "url": "https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cea11dfdb6d7b094bbff52a04dfac13d1ca5e944",
                    "width": 960,
                    "height": 499
                  },
                  {
                    "url": "https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=77500a8858594b33f2a21f3033b5fb41d03548b2",
                    "width": 1080,
                    "height": 561
                  }
                ],
                "variants": {},
                "id": "6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwm3w0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EasternBeyond",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwm3w0/maxsun_preparing_allintel_mini_station_up_to_core/",
          "stickied": false,
          "url": "https://videocardz.com/newz/maxsun-preparing-all-intel-mini-station-up-to-core-ultra-9-285hx-and-two-arc-pro-b60-gpu",
          "subreddit_subscribers": 497502,
          "created_utc": 1752176508,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_dyvrh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Possible size of new the open model from openai",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 96,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvwya4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 354,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 354,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/SWE3wKhFq64W19U8vrSnM4JhB3rrFvjK8ka3DKWgysk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752101694,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/622w5dyvhxbf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/622w5dyvhxbf1.png?auto=webp&amp;s=a2c619e25718a02777bbfccf1e457faeec66291e",
                  "width": 1080,
                  "height": 746
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/622w5dyvhxbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=69c3323c05b9e9e24c72ce6d4331170952c6539b",
                    "width": 108,
                    "height": 74
                  },
                  {
                    "url": "https://preview.redd.it/622w5dyvhxbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e2058d759bbe6568a23d5ba18b34f5d8e8f676b3",
                    "width": 216,
                    "height": 149
                  },
                  {
                    "url": "https://preview.redd.it/622w5dyvhxbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1585e0e58618c37cdfe1fdba6c0c1fbcd64e53c3",
                    "width": 320,
                    "height": 221
                  },
                  {
                    "url": "https://preview.redd.it/622w5dyvhxbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f278161d7e564140ede28f9eff15dc776e5ab6df",
                    "width": 640,
                    "height": 442
                  },
                  {
                    "url": "https://preview.redd.it/622w5dyvhxbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=15aac87a5266c6f4637f48e9a986a0d830cea8e5",
                    "width": 960,
                    "height": 663
                  },
                  {
                    "url": "https://preview.redd.it/622w5dyvhxbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=77bb6536d895705fee43ebe991bd07b835b00a2a",
                    "width": 1080,
                    "height": 746
                  }
                ],
                "variants": {},
                "id": "8YmohOSgc8VabZC-CUEtYfc1-XLSW8tYibkJW3Qz4Ac"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lvwya4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "celsowm",
          "discussion_type": null,
          "num_comments": 103,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvwya4/possible_size_of_new_the_open_model_from_openai/",
          "stickied": false,
          "url": "https://i.redd.it/622w5dyvhxbf1.png",
          "subreddit_subscribers": 497502,
          "created_utc": 1752101694,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🧠📝 Research [Blog post](https://huggingface.co/blog/AI-MO/kimina-prover)\n\n🚀 Demo: [https://demo.projectnumina.ai/](https://demo.projectnumina.ai/)\n\n[🤗](https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5) Models (72B, 8B or 1.7B) - [🤗](https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5) [HuggingFace](https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5)\n\n\n72B with Test-time RL pipeline gets 92.2% on miniF2F.\n\nPass@32 for each size:\n\n* 72B → 84.0% (86.4% with error-fixing)\n* 8B → 78.3%\n* 1.7B → 73.4%\n\n8B/1.7B are Qwen 3 with 72B distilled into them.",
          "author_fullname": "t2_4avfsbdbf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimina Prover - Test-time RL to reach 92.2% on miniF2F",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwcixn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752153536,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🧠📝 Research &lt;a href=\"https://huggingface.co/blog/AI-MO/kimina-prover\"&gt;Blog post&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;🚀 Demo: &lt;a href=\"https://demo.projectnumina.ai/\"&gt;https://demo.projectnumina.ai/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5\"&gt;🤗&lt;/a&gt; Models (72B, 8B or 1.7B) - &lt;a href=\"https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5\"&gt;🤗&lt;/a&gt; &lt;a href=\"https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5\"&gt;HuggingFace&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;72B with Test-time RL pipeline gets 92.2% on miniF2F.&lt;/p&gt;\n\n&lt;p&gt;Pass@32 for each size:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;72B → 84.0% (86.4% with error-fixing)&lt;/li&gt;\n&lt;li&gt;8B → 78.3%&lt;/li&gt;\n&lt;li&gt;1.7B → 73.4%&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;8B/1.7B are Qwen 3 with 72B distilled into them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/iF34aV5P23-aFKb2pKuUTTK8P3zuyZ-3Sfm4GeLU6Fs.png?auto=webp&amp;s=43ca537868c03110c79ad9406d2a1c7d09b9226d",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/iF34aV5P23-aFKb2pKuUTTK8P3zuyZ-3Sfm4GeLU6Fs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4fcf60d1352e405fba05a4101718f49e5efbe37c",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/iF34aV5P23-aFKb2pKuUTTK8P3zuyZ-3Sfm4GeLU6Fs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=270d774025925d11d35101182605fb1d0c4c4afa",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/iF34aV5P23-aFKb2pKuUTTK8P3zuyZ-3Sfm4GeLU6Fs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f478c1ae527103176857bfed980a9d83621f5195",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/iF34aV5P23-aFKb2pKuUTTK8P3zuyZ-3Sfm4GeLU6Fs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=407ca479fabf50d67111673880368a5a4073cb48",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/iF34aV5P23-aFKb2pKuUTTK8P3zuyZ-3Sfm4GeLU6Fs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4726149801837a40c84050900b055b80fc9d186f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/iF34aV5P23-aFKb2pKuUTTK8P3zuyZ-3Sfm4GeLU6Fs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=597e3473da5b4e9fc349358114dd7a90bbc86343",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "iF34aV5P23-aFKb2pKuUTTK8P3zuyZ-3Sfm4GeLU6Fs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lwcixn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "frunkp",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwcixn/kimina_prover_testtime_rl_to_reach_922_on_minif2f/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwcixn/kimina_prover_testtime_rl_to_reach_922_on_minif2f/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752153536,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We have built an extensible open source platform that enables developers to build, run and integrate AI agents into their applications and deliver AI native experiences all running locally on phones.\n\nThe SDK is lightweight built upon Executorch/ONNX and provides a higher level abstraction for developers to integrate in Kotlin or Swift. The AI workflow is orchestrated via Python which is natively supported as part of the on-device SDK. We currently support Llama 3.2 1B, Qwen 3 0.6B (tool-calling), Gemini Nano and soon Gemma 3n.\n\nWe have also created an Agent marketplace which provides plug and play agents and would love to get contributions from this community. \n\n[Here](https://github.com/NimbleEdge/deliteAI/tree/main/nimblenet_py/simulation_assets) are some example Python scripts for both traditional ML and AI workloads - note that the Kotlin/Swift layer can invoke these python functions and vice-versa which enables tool calling for both dynamic context and actions in the app.\n\nYou can also check out our open-source on-device [AI assistant](https://github.com/NimbleEdge/assistant) built upon the “DeliteAI” platform. \n\nWe love to hear from you on our APIs and if you would like to contribute please join our Discord community (link in the comment below). ",
          "author_fullname": "t2_74zl16jw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DeliteAI: Open platform for building and running agents on Mobile",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwfn7n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=18dfb27d0448541028b35c87abdd2ec5cdcbb872",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752161306,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have built an extensible open source platform that enables developers to build, run and integrate AI agents into their applications and deliver AI native experiences all running locally on phones.&lt;/p&gt;\n\n&lt;p&gt;The SDK is lightweight built upon Executorch/ONNX and provides a higher level abstraction for developers to integrate in Kotlin or Swift. The AI workflow is orchestrated via Python which is natively supported as part of the on-device SDK. We currently support Llama 3.2 1B, Qwen 3 0.6B (tool-calling), Gemini Nano and soon Gemma 3n.&lt;/p&gt;\n\n&lt;p&gt;We have also created an Agent marketplace which provides plug and play agents and would love to get contributions from this community. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/NimbleEdge/deliteAI/tree/main/nimblenet_py/simulation_assets\"&gt;Here&lt;/a&gt; are some example Python scripts for both traditional ML and AI workloads - note that the Kotlin/Swift layer can invoke these python functions and vice-versa which enables tool calling for both dynamic context and actions in the app.&lt;/p&gt;\n\n&lt;p&gt;You can also check out our open-source on-device &lt;a href=\"https://github.com/NimbleEdge/assistant\"&gt;AI assistant&lt;/a&gt; built upon the “DeliteAI” platform. &lt;/p&gt;\n\n&lt;p&gt;We love to hear from you on our APIs and if you would like to contribute please join our Discord community (link in the comment below). &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/NimbleEdge/deliteAI",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?auto=webp&amp;s=a9dffc08e8f89577f0ef8cbadf6345ed58ebd6fe",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a09579064e0135c0a7c9c8b1f91ffc6f7c6d151",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c53e782a497e280f238c0ed65a3b9e899425ef4d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0c8a2e4992aade019738f29bafcc6adcb8548c08",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4a7922aa047ec08e04553915bf0c4265bcd75e35",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=786fec691eebcbe24932bf8ab178dc6075f03e85",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=980b1144f88939647513f1e4180755927a3ff31f",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwfn7n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Economy-Mud-6626",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwfn7n/deliteai_open_platform_for_building_and_running/",
          "stickied": false,
          "url": "https://github.com/NimbleEdge/deliteAI",
          "subreddit_subscribers": 497502,
          "created_utc": 1752161306,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone, my goal is to finetune an embedding model for the domain of railway engineering. I was wondering if you had recommendation.\n\nI tried a few things and I am now considering the option of filtering fine web using keywords.",
          "author_fullname": "t2_38ilynpn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a domain specific dataset",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwphbh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752184618,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, my goal is to finetune an embedding model for the domain of railway engineering. I was wondering if you had recommendation.&lt;/p&gt;\n\n&lt;p&gt;I tried a few things and I am now considering the option of filtering fine web using keywords.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwphbh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "T2WIN",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwphbh/building_a_domain_specific_dataset/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwphbh/building_a_domain_specific_dataset/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752184618,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The flattening of nuanced distinctions is part of the joke (pre-emptive disclaimer for the pedantic) \n\n* **Pheromone trails ↔ value functions / reward shaping** Both steer future exploration toward paths that historically looked good.\n* **Stochastic exploration** in ants (random walks with pheromone bias) ↔ **ε-greedy / entropy-regularised exploration** in RL.\n* **Updating pheromones over time** ↔ **policy/value updates** in RL or **gradient steps** in supervised fine-tuning.\n* **Demonstration pheromones** (ants following an experienced scout’s trail) ↔ **Learning from Demonstration**.",
          "author_fullname": "t2_k7mcz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvzonf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "ups": 147,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 147,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/zG0DO4HRBwmlgpN0eGz-KACaRlDjquI0nzb0s4yCKtQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752109320,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The flattening of nuanced distinctions is part of the joke (pre-emptive disclaimer for the pedantic) &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Pheromone trails ↔ value functions / reward shaping&lt;/strong&gt; Both steer future exploration toward paths that historically looked good.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Stochastic exploration&lt;/strong&gt; in ants (random walks with pheromone bias) ↔ &lt;strong&gt;ε-greedy / entropy-regularised exploration&lt;/strong&gt; in RL.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Updating pheromones over time&lt;/strong&gt; ↔ &lt;strong&gt;policy/value updates&lt;/strong&gt; in RL or &lt;strong&gt;gradient steps&lt;/strong&gt; in supervised fine-tuning.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Demonstration pheromones&lt;/strong&gt; (ants following an experienced scout’s trail) ↔ &lt;strong&gt;Learning from Demonstration&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/vq8hwq904ybf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/vq8hwq904ybf1.png?auto=webp&amp;s=d6cebc0aaa2411e35102122fe941ad8023b97179",
                  "width": 1536,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/vq8hwq904ybf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c92efb7f1f6b6ad387f774a711db4568891357e6",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/vq8hwq904ybf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=963e102fa350a9b226179f2d94f144e9f2b738f4",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/vq8hwq904ybf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=25bf5c52dc2e61bfd44a78fa7c5951ea6558c556",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://preview.redd.it/vq8hwq904ybf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d05add52383cb1c64996c7d198a25c8644d9f33f",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://preview.redd.it/vq8hwq904ybf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=816ed6cea0bb5e65645540fcced19afd8edc6b37",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/vq8hwq904ybf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e11e5400a7ed7b874178fdbea8f23e4075d30b36",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "j4x-d47033S34x5kvZfx721caGXGakmvOXqs2eSy9qw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lvzonf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "chitown160",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvzonf/httpsenwikipediaorgwikiant_colony_optimization/",
          "stickied": false,
          "url": "https://i.redd.it/vq8hwq904ybf1.png",
          "subreddit_subscribers": 497502,
          "created_utc": 1752109320,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been working on a collaborative database that is an MCP server.  You can use it to remember any type of data you define: diet and fitness history, work-related data, to-do lists, bookmarked links, journal entries, bugs in software projects, favorite books/movies. [See it in action.](https://youtu.be/zAxepwoONq0)\n\nIt’s called Dry (“don’t repeat yourself”).  Dry lets you:\n\n* Add long-term memories in Claude and other MCP clients that persist across chats.\n* Specify your own custom data type without any coding.\n* Automatically generate a full graphical user interface (tables, charts, maps, lists, etc.).  \n* Share with a team or keep it private. \n\nWe think that in the [long term](https://dry.ai/vision), memories like this will give AI assistants the scaffolding they need to replace most SaaS tools and apps.\n\nHere’s our alpha you can try: [ ](https://dry.ai/joinDryBuilder)[https://dry.ai/getClaudeMemory](https://dry.ai/getClaudeMemory)\n\nWould love feedback from anyone here. Are there features you'd want? What would you use this for? Happy to answer any questions! \n\nThanks.",
          "author_fullname": "t2_8divqlzb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MCP server that is a memory for MCP clients (AI assistants) with your custom data types + full UI + team sharing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwhy37",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752166788,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been working on a collaborative database that is an MCP server.  You can use it to remember any type of data you define: diet and fitness history, work-related data, to-do lists, bookmarked links, journal entries, bugs in software projects, favorite books/movies. &lt;a href=\"https://youtu.be/zAxepwoONq0\"&gt;See it in action.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It’s called Dry (“don’t repeat yourself”).  Dry lets you:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Add long-term memories in Claude and other MCP clients that persist across chats.&lt;/li&gt;\n&lt;li&gt;Specify your own custom data type without any coding.&lt;/li&gt;\n&lt;li&gt;Automatically generate a full graphical user interface (tables, charts, maps, lists, etc.).  &lt;/li&gt;\n&lt;li&gt;Share with a team or keep it private. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We think that in the &lt;a href=\"https://dry.ai/vision\"&gt;long term&lt;/a&gt;, memories like this will give AI assistants the scaffolding they need to replace most SaaS tools and apps.&lt;/p&gt;\n\n&lt;p&gt;Here’s our alpha you can try: &lt;a href=\"https://dry.ai/joinDryBuilder\"&gt; &lt;/a&gt;&lt;a href=\"https://dry.ai/getClaudeMemory\"&gt;https://dry.ai/getClaudeMemory&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love feedback from anyone here. Are there features you&amp;#39;d want? What would you use this for? Happy to answer any questions! &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Pe-2QF4HM1vrYYdhnBTkzpQ6BumurPivfxkVGOFtlGQ.jpeg?auto=webp&amp;s=fecaf7c855cdafe6fa5f25f36d32d776046a7575",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Pe-2QF4HM1vrYYdhnBTkzpQ6BumurPivfxkVGOFtlGQ.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e0a319bf64f07bdababd422731e8551d03324ac4",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/Pe-2QF4HM1vrYYdhnBTkzpQ6BumurPivfxkVGOFtlGQ.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6693b2aafb27f01d9d0b0e268956bbcc1227d445",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/Pe-2QF4HM1vrYYdhnBTkzpQ6BumurPivfxkVGOFtlGQ.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=681f6c08bfe85886d49c2712737283b6773b198f",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "Pe-2QF4HM1vrYYdhnBTkzpQ6BumurPivfxkVGOFtlGQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwhy37",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Jazzlike_Water4911",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwhy37/mcp_server_that_is_a_memory_for_mcp_clients_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwhy37/mcp_server_that_is_a_memory_for_mcp_clients_ai/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752166788,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have heard about it, but don't really see folks talking about using it nor does it have the same excitement as the other agentic toolkits.   As a matter of fact, anytime I hear about it, it's from someone working for Block who I'm following on social media.\n\nFor anyone using it, if you can compare it to other tools, how does it rate?  I'm particular curious in comparison to Aider, claude coder and gemini-cli",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone using Block's goose?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lww2ld",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752202911,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have heard about it, but don&amp;#39;t really see folks talking about using it nor does it have the same excitement as the other agentic toolkits.   As a matter of fact, anytime I hear about it, it&amp;#39;s from someone working for Block who I&amp;#39;m following on social media.&lt;/p&gt;\n\n&lt;p&gt;For anyone using it, if you can compare it to other tools, how does it rate?  I&amp;#39;m particular curious in comparison to Aider, claude coder and gemini-cli&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lww2ld",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lww2ld/anyone_using_blocks_goose/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lww2ld/anyone_using_blocks_goose/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752202911,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is anyone facing this issue? Do you know how to fit it? I am using unsloth q5 UD gguf\n\nThanks!",
          "author_fullname": "t2_az9jb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hunyuan responding with &lt;answer&gt; &lt;/answer&gt; tag on LMstudio",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwvuuv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752202276,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is anyone facing this issue? Do you know how to fit it? I am using unsloth q5 UD gguf&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwvuuv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kuane",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwvuuv/hunyuan_responding_with_answer_answer_tag_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwvuuv/hunyuan_responding_with_answer_answer_tag_on/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752202276,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey Everyone, I’ve been working on a project called Fissure. It is a personal AI server that runs on your own machine. The goal is to make it pretty simple to run a local LLM, access it from your phone or laptop, and keep everything private.   \n  \nRight now I’m focused on the desktop app, which runs your chosen model through Ollama and sets up remote access using Tailscale(have been looking into alternatives). Once it's running, you get a private Tailscale url you can plug into a mobile app or anything that can hit an api, and this works across devices as long as they’re on your Tailscale network. Its still pretty early but I’m trying to make this easier than putting together a bunch of tools, more of a one click or couple clicks to set up and get going.  \n  \nI'm curious if anyone else would find this useful, and what features or pain points you’d want solved. Id really appreciate any feedback or thoughts.",
          "author_fullname": "t2_1oqtfoffx3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local AI server with Ollama and Tailscale integration looking for feedback",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwvrev",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752201992,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Everyone, I’ve been working on a project called Fissure. It is a personal AI server that runs on your own machine. The goal is to make it pretty simple to run a local LLM, access it from your phone or laptop, and keep everything private.   &lt;/p&gt;\n\n&lt;p&gt;Right now I’m focused on the desktop app, which runs your chosen model through Ollama and sets up remote access using Tailscale(have been looking into alternatives). Once it&amp;#39;s running, you get a private Tailscale url you can plug into a mobile app or anything that can hit an api, and this works across devices as long as they’re on your Tailscale network. Its still pretty early but I’m trying to make this easier than putting together a bunch of tools, more of a one click or couple clicks to set up and get going.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious if anyone else would find this useful, and what features or pain points you’d want solved. Id really appreciate any feedback or thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwvrev",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Stay-2193",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwvrev/local_ai_server_with_ollama_and_tailscale/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwvrev/local_ai_server_with_ollama_and_tailscale/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752201992,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is IAC for a binary classifier that could help folks get started with AI engineering. It’s a variation of the classic AWS example binary classifier but with an API endpoint to use for inferring and a scheduler for turning the endpoint off when not in use because that’s expensive. \n\nhttps://github.com/jenastar/comprehend_phishing_public jenastar/comprehend_phishing_public: Build a model with test data, retrain the model, spin down expensive endpoints when not in use.",
          "author_fullname": "t2_62qnn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Terraformed Binary Classifier",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwva7f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752200602,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is IAC for a binary classifier that could help folks get started with AI engineering. It’s a variation of the classic AWS example binary classifier but with an API endpoint to use for inferring and a scheduler for turning the endpoint off when not in use because that’s expensive. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/jenastar/comprehend_phishing_public\"&gt;https://github.com/jenastar/comprehend_phishing_public&lt;/a&gt; jenastar/comprehend_phishing_public: Build a model with test data, retrain the model, spin down expensive endpoints when not in use.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oOX91S0GtJ5C3DDt-OLrOB9Trr9AiY8Gs-ers4ZsuWs.png?auto=webp&amp;s=68ceff4773b4a74ddb114de1a9a25f90b804cf75",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oOX91S0GtJ5C3DDt-OLrOB9Trr9AiY8Gs-ers4ZsuWs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=77f88e42466d472f95a151834ef200b29d7df819",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/oOX91S0GtJ5C3DDt-OLrOB9Trr9AiY8Gs-ers4ZsuWs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=afca94223e1be7ab832240891124b5032f4cbd41",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/oOX91S0GtJ5C3DDt-OLrOB9Trr9AiY8Gs-ers4ZsuWs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2661a4ff6f3e5ec813d67fbe3e4797ba6fc1a2d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/oOX91S0GtJ5C3DDt-OLrOB9Trr9AiY8Gs-ers4ZsuWs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd4642b764d9d42a609b634a53f3276594670586",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/oOX91S0GtJ5C3DDt-OLrOB9Trr9AiY8Gs-ers4ZsuWs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5d7bc39f914f36add1f52c739de1ab36bc64480d",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/oOX91S0GtJ5C3DDt-OLrOB9Trr9AiY8Gs-ers4ZsuWs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=737cdba682db59996442b1d5cf332f5a47e495ce",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "oOX91S0GtJ5C3DDt-OLrOB9Trr9AiY8Gs-ers4ZsuWs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwva7f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jenastar",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwva7f/terraformed_binary_classifier/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwva7f/terraformed_binary_classifier/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752200602,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hunyuan-A13B is now available for LM Studio with Unsloth GGUF. I am on the Beta track for both LM Studio and llama.cpp backend. Here are my initial impression:\n\nIt is fast! I am getting 40 tokens per second initially dropping to maybe 30 tokens per second when the context has build up some. This is on M4 Max Macbook Pro and q4.\n\nThe context is HUGE. 256k. I don't expect I will be using that much, but it is nice that I am unlikely to hit the ceiling in practical use.\n\nIt made a chess game for me and it did ok. No errors but the game was not complete. It did complete it after a few prompts and it also fixed one error that happened in the javascript console.\n\nIt did spend some time thinking, but not as much as I have seen other models do. I would say it is doing the middle ground here, but I am still to test this extensively. The model card claims you can somehow influence how much thinking it will do. But I am not sure how yet.\n\nIt appears to wrap the final answer in &lt;answer&gt;the answer here&lt;/answer&gt; just like it does for &lt;think&gt;&lt;/think&gt;. This may or may not be a problem for tools? Maybe we need to update our software to strip this out.\n\nThe total memory usage for the Unsloth 4 bit UD quant is 61 GB. I will test 6 bit and 8 bit also, but I am quite in love with the speed of the 4 bit and it appears to have good quality regardless. So maybe I will just stick with 4 bit?\n\nThis is a 80b model that is very fast. Feels like the future.\n\nEdit: The 61 GB size is with 8 bit KV cache quantization. However I just noticed that they claim this is bad in the model card, so I disabled KV cache quantization. This increased memory usage to 76 GB. That is with the full 256k context size enabled. I expect you can just lower that if you don't have enough memory. Or stay with KV cache quantization because it did appear to work just fine. I would say this could work on a 64 GB machine if you just use KV cache quantization and maybe lower the context size to 128k. ",
          "author_fullname": "t2_bvqb8ng0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hunyuan-A13B is here for real!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvvkh2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 176,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 176,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752099902,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752098151,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hunyuan-A13B is now available for LM Studio with Unsloth GGUF. I am on the Beta track for both LM Studio and llama.cpp backend. Here are my initial impression:&lt;/p&gt;\n\n&lt;p&gt;It is fast! I am getting 40 tokens per second initially dropping to maybe 30 tokens per second when the context has build up some. This is on M4 Max Macbook Pro and q4.&lt;/p&gt;\n\n&lt;p&gt;The context is HUGE. 256k. I don&amp;#39;t expect I will be using that much, but it is nice that I am unlikely to hit the ceiling in practical use.&lt;/p&gt;\n\n&lt;p&gt;It made a chess game for me and it did ok. No errors but the game was not complete. It did complete it after a few prompts and it also fixed one error that happened in the javascript console.&lt;/p&gt;\n\n&lt;p&gt;It did spend some time thinking, but not as much as I have seen other models do. I would say it is doing the middle ground here, but I am still to test this extensively. The model card claims you can somehow influence how much thinking it will do. But I am not sure how yet.&lt;/p&gt;\n\n&lt;p&gt;It appears to wrap the final answer in &amp;lt;answer&amp;gt;the answer here&amp;lt;/answer&amp;gt; just like it does for &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;. This may or may not be a problem for tools? Maybe we need to update our software to strip this out.&lt;/p&gt;\n\n&lt;p&gt;The total memory usage for the Unsloth 4 bit UD quant is 61 GB. I will test 6 bit and 8 bit also, but I am quite in love with the speed of the 4 bit and it appears to have good quality regardless. So maybe I will just stick with 4 bit?&lt;/p&gt;\n\n&lt;p&gt;This is a 80b model that is very fast. Feels like the future.&lt;/p&gt;\n\n&lt;p&gt;Edit: The 61 GB size is with 8 bit KV cache quantization. However I just noticed that they claim this is bad in the model card, so I disabled KV cache quantization. This increased memory usage to 76 GB. That is with the full 256k context size enabled. I expect you can just lower that if you don&amp;#39;t have enough memory. Or stay with KV cache quantization because it did appear to work just fine. I would say this could work on a 64 GB machine if you just use KV cache quantization and maybe lower the context size to 128k. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lvvkh2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Baldur-Norddahl",
          "discussion_type": null,
          "num_comments": 115,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvvkh2/hunyuana13b_is_here_for_real/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvvkh2/hunyuana13b_is_here_for_real/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752098151,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Dear LocalLLaMA,\n\nI have been a member since the very beginning (ish) and have learned so much from many of you. I’m hoping to get some more specific advice on the best vision-language models for extracting cursive handwritten text from a large set of documents (about 400 000 images). I have access to A40, A100, and V100 GPUs and can run multiple GPUs of the same type in parallel if necessary.\n\nHere’s my current workflow:\n\n1. I feed a high-quality scan into a YOLO model to extract the region containing the name information.\n2. I construct a prompt asking the model to provide the full name (surname + given name). I already have each individual’s surname (from previous manual work) and include it in the prompt.\n3. I feed the cropped image and prompt simultaneously to two models:\n   * [Qwen/Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)\n   * [OpenGVLab/InternVL3-8B](https://huggingface.co/OpenGVLab/InternVL3-8B)\n4. Some light post-processing later, I only accept results where both models agree 100%, to avoid errors and hallucinations (since I assume the their errors are uncorrelated).\n\nWith this setup, only about 25–35 % of outputs agree. Qwen2.5-VL outperforms InternVL3 by about 18–20 %, but as you see my performance is limited by the weaker model. I’ve also experimented with Moondream (which performed poorly for my task, but still ok considering it's size) and THUDM/GLM-4.1V-9B-Thinking (which didn’t impress during my short experiment and is slow). I even fine-tuned Qwen2.5-VL-7B locally on 2 000 gold-standard sample using Unsloth, but saw no significant improvement over the base model. The smart play would be to finetune InternVL3 since its the weak link but I'm not good at finetuning and it seems more complex because it needs to be fed split images that are stitched together by the model (and more importantly unsure if Unsloth ready)\n\nI have a ton of question but these should cover most:\n\n* Are there alternative VL models I should consider instead of InternVL3?\n* Are there quantized versions of these (or other) models that offer similar accuracy with lower VRAM requirements, so I can process larger batches cheapely at least and can rerun the images until I get agreement?\n* Any other suggestions or insights on improving my workflow would be immensely appreciated.\n\nThank you again guys!",
          "author_fullname": "t2_10vzjm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need advice on how to improve Handwritten Text Recognition of names using Vision models (for academic research purposes)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwpi5p",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752184678,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear LocalLLaMA,&lt;/p&gt;\n\n&lt;p&gt;I have been a member since the very beginning (ish) and have learned so much from many of you. I’m hoping to get some more specific advice on the best vision-language models for extracting cursive handwritten text from a large set of documents (about 400 000 images). I have access to A40, A100, and V100 GPUs and can run multiple GPUs of the same type in parallel if necessary.&lt;/p&gt;\n\n&lt;p&gt;Here’s my current workflow:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I feed a high-quality scan into a YOLO model to extract the region containing the name information.&lt;/li&gt;\n&lt;li&gt;I construct a prompt asking the model to provide the full name (surname + given name). I already have each individual’s surname (from previous manual work) and include it in the prompt.&lt;/li&gt;\n&lt;li&gt;I feed the cropped image and prompt simultaneously to two models:\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct\"&gt;Qwen/Qwen2.5-VL-7B-Instruct&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/OpenGVLab/InternVL3-8B\"&gt;OpenGVLab/InternVL3-8B&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Some light post-processing later, I only accept results where both models agree 100%, to avoid errors and hallucinations (since I assume the their errors are uncorrelated).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;With this setup, only about 25–35 % of outputs agree. Qwen2.5-VL outperforms InternVL3 by about 18–20 %, but as you see my performance is limited by the weaker model. I’ve also experimented with Moondream (which performed poorly for my task, but still ok considering it&amp;#39;s size) and THUDM/GLM-4.1V-9B-Thinking (which didn’t impress during my short experiment and is slow). I even fine-tuned Qwen2.5-VL-7B locally on 2 000 gold-standard sample using Unsloth, but saw no significant improvement over the base model. The smart play would be to finetune InternVL3 since its the weak link but I&amp;#39;m not good at finetuning and it seems more complex because it needs to be fed split images that are stitched together by the model (and more importantly unsure if Unsloth ready)&lt;/p&gt;\n\n&lt;p&gt;I have a ton of question but these should cover most:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Are there alternative VL models I should consider instead of InternVL3?&lt;/li&gt;\n&lt;li&gt;Are there quantized versions of these (or other) models that offer similar accuracy with lower VRAM requirements, so I can process larger batches cheapely at least and can rerun the images until I get agreement?&lt;/li&gt;\n&lt;li&gt;Any other suggestions or insights on improving my workflow would be immensely appreciated.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you again guys!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?auto=webp&amp;s=17bf72c4d47d131612ab2f5b554d85da02a85539",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d0bf812fba94f9f50669a2e76037d0e7886bde2",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=99a214b39375ee0ec6cdcffc1958d0a4b34e4690",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8e945b1c768d68948eaa7f830a3b219c2df4c13c",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b846b869885ffbeadf6199126a3c0fab1ed22be2",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c94fbe6213102c60c53f38bc3207e1f1bde9733a",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e2dee8bb7abc532aeb2cfeec783420f84dba72c6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwpi5p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "joosefm9",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwpi5p/need_advice_on_how_to_improve_handwritten_text/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwpi5p/need_advice_on_how_to_improve_handwritten_text/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752184678,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Read my [recent post for context](https://www.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/). We've been working hard the past few days for a more formal launch next week and to address valuable user feedback. We'll hopefully be launching our preference dataset, more detailed methodology, and more models for you all next week. \n\nThat said, in light of xAI's launch today, we've added Grok 4 as well as some models such as Qwen, more Mistral models, and a few image models (with more to come). How do you think [Grok 4 will do in the arena](https://www.designarena.ai/leaderboard)? ",
          "author_fullname": "t2_c3b3edv5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "UI/UX Benchmark Update: We've added Grok 4 and more models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 95,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw5nxi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/LW6cwnAYxfCPs8RS-dbfMPYwSG9PbwmrGvqsclUJ55Y.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752128828,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Read my &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/\"&gt;recent post for context&lt;/a&gt;. We&amp;#39;ve been working hard the past few days for a more formal launch next week and to address valuable user feedback. We&amp;#39;ll hopefully be launching our preference dataset, more detailed methodology, and more models for you all next week. &lt;/p&gt;\n\n&lt;p&gt;That said, in light of xAI&amp;#39;s launch today, we&amp;#39;ve added Grok 4 as well as some models such as Qwen, more Mistral models, and a few image models (with more to come). How do you think &lt;a href=\"https://www.designarena.ai/leaderboard\"&gt;Grok 4 will do in the arena&lt;/a&gt;? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6536neojqzbf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6536neojqzbf1.png?auto=webp&amp;s=696c5e09ab70f20932faef95b4380a36a2dbc35f",
                  "width": 2120,
                  "height": 1442
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6536neojqzbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7b79b2b4d69ab3658652da16ed51d2b805e2f542",
                    "width": 108,
                    "height": 73
                  },
                  {
                    "url": "https://preview.redd.it/6536neojqzbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=83d1cd4edadaca85f3bb616e14d453097ee08fd4",
                    "width": 216,
                    "height": 146
                  },
                  {
                    "url": "https://preview.redd.it/6536neojqzbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dbaad0a0e613f4e128ab41ff38dbeed69be36511",
                    "width": 320,
                    "height": 217
                  },
                  {
                    "url": "https://preview.redd.it/6536neojqzbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0b40568dd674e55ab0597d798331e486bdf0023c",
                    "width": 640,
                    "height": 435
                  },
                  {
                    "url": "https://preview.redd.it/6536neojqzbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=13fb3a2f048e7bb8893ff2687823c872f77c984c",
                    "width": 960,
                    "height": 652
                  },
                  {
                    "url": "https://preview.redd.it/6536neojqzbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8da65fe3f7d4f09dec10900e77946827c46aa1c0",
                    "width": 1080,
                    "height": 734
                  }
                ],
                "variants": {},
                "id": "Iyv10jomYApdRuBGRbzVXR48QuV_MB2jAoHl-74abX0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lw5nxi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adviceguru25",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw5nxi/uiux_benchmark_update_weve_added_grok_4_and_more/",
          "stickied": false,
          "url": "https://i.redd.it/6536neojqzbf1.png",
          "subreddit_subscribers": 497502,
          "created_utc": 1752128828,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "so the problem is i generated one image okay but i said hey edit that image with character consistency this is where we are lagging anybody can help me pls with this ",
          "author_fullname": "t2_1kpb32ao3j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "hey guys im working in comapany they gave me a task to download open souce ai image generation model and run in the local system but the problem im facing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx20h2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.22,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752224356,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;so the problem is i generated one image okay but i said hey edit that image with character consistency this is where we are lagging anybody can help me pls with this &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lx20h2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Select_Dream634",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx20h2/hey_guys_im_working_in_comapany_they_gave_me_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx20h2/hey_guys_im_working_in_comapany_they_gave_me_a/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752224356,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "32GB VRAM suppose to fit 24-27B models at 8b quant right?\n\nHere is what i am trying via `vllm serve`\n\nThis works fine\n\n```\n--model unsloth/Devstral-Small-2505-unsloth-bnb-4bit  --port 80  --quantization=\"bitsandbytes\" --load-format bitsandbytes --pipeline-parallel-size 2  --max-num-seqs 1 --max-model-len 40960\n```\n\nEven qwen3-32B AWQ works fine:\n\n```\n--model Qwen/Qwen3-32B-AWQ --port 80  --tensor-parallel-size 2  --chat-template /qwen3_nonthinking.jinja\n```\n\n\nBut this errors out with OOM\n\n\n\n```bash\n--model unsloth/gemma-3-27b-it-qat-unsloth-bnb-4bit  --port 80  --quantization=\"bitsandbytes\" --load-format bitsandbytes --pipeline-parallel-size 2 --max-num-seqs 1        \n```\n\nerror :\n\n```\ninference-1    | (VllmWorker rank=1 pid=165) ERROR 07-10 12:57:03 [multiproc_executor.py:487] ValueError: Free memory on device (15.34/15.57 GiB) on startup is less than desired GPU memory utilization (1.0, 15.57 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n```\n\n",
          "author_fullname": "t2_86dk0gye",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Whats wrong with my vLLM Config? I have 2x4070TiSupers and I couldn't run many models at bnb-4bit Quants.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwmxbx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752230407,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752178448,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;32GB VRAM suppose to fit 24-27B models at 8b quant right?&lt;/p&gt;\n\n&lt;p&gt;Here is what i am trying via &lt;code&gt;vllm serve&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;This works fine&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n--model unsloth/Devstral-Small-2505-unsloth-bnb-4bit  --port 80  --quantization=&amp;quot;bitsandbytes&amp;quot; --load-format bitsandbytes --pipeline-parallel-size 2  --max-num-seqs 1 --max-model-len 40960\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Even qwen3-32B AWQ works fine:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n--model Qwen/Qwen3-32B-AWQ --port 80  --tensor-parallel-size 2  --chat-template /qwen3_nonthinking.jinja\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;But this errors out with OOM&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;bash\n--model unsloth/gemma-3-27b-it-qat-unsloth-bnb-4bit  --port 80  --quantization=&amp;quot;bitsandbytes&amp;quot; --load-format bitsandbytes --pipeline-parallel-size 2 --max-num-seqs 1        \n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;error :&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\ninference-1    | (VllmWorker rank=1 pid=165) ERROR 07-10 12:57:03 [multiproc_executor.py:487] ValueError: Free memory on device (15.34/15.57 GiB) on startup is less than desired GPU memory utilization (1.0, 15.57 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwmxbx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Voxandr",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752178448,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This new open language model will be available on Azure, Hugging Face, and other large cloud providers. Sources describe the model as “similar to o3 mini,” complete with the reasoning capabilities that have made OpenAI’s latest models so powerful.",
          "author_fullname": "t2_dwsi5kcn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OpenAI's open-weight model will debut as soon as next week",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvn1sd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 316,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 316,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=140&amp;height=72&amp;crop=140:72,smart&amp;auto=webp&amp;s=cbe74efe41c0422ff6c89df48e8067db7b4bcb18",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752078046,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "theverge.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This new open language model will be available on Azure, Hugging Face, and other large cloud providers. Sources describe the model as “similar to o3 mini,” complete with the reasoning capabilities that have made OpenAI’s latest models so powerful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.theverge.com/notepad-microsoft-newsletter/702848/openai-open-language-model-o3-mini-notepad",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?auto=webp&amp;s=e27e8e7e9f6e1280c504b8de58aa2ee38cf2f52c",
                  "width": 1200,
                  "height": 624
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=363661360a23752155759177a733a98290ccfb73",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f08c6c29c7e54c046340fbc6ff70bd45aa8517f7",
                    "width": 216,
                    "height": 112
                  },
                  {
                    "url": "https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7f79d32c9e262cc402f82f084f978f3a6f996b05",
                    "width": 320,
                    "height": 166
                  },
                  {
                    "url": "https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5aaee471edf64881fedf697cc7cda1494ca5f3cd",
                    "width": 640,
                    "height": 332
                  },
                  {
                    "url": "https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6709fae1ea9a4bc304b8f19d2da77783dc24e38a",
                    "width": 960,
                    "height": 499
                  },
                  {
                    "url": "https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f334b2dd8dfc2d654e2afee401e6ba3864c2e6b8",
                    "width": 1080,
                    "height": 561
                  }
                ],
                "variants": {},
                "id": "42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lvn1sd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "phantasm_ai",
          "discussion_type": null,
          "num_comments": 112,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvn1sd/openais_openweight_model_will_debut_as_soon_as/",
          "stickied": false,
          "url": "https://www.theverge.com/notepad-microsoft-newsletter/702848/openai-open-language-model-o3-mini-notepad",
          "subreddit_subscribers": 497502,
          "created_utc": 1752078046,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hey guys!\n\nbecause of privacy conerns and censorship i;ve decided to give local LLM a try.\n\ndownloaded studio LM and installed mistarl 7B and so far things are  fine. might give ollama a chance as well in the future.\n\ncouple of questions:\n\ncan the model collect data? I asked it and he said he does communicate with the internet to get some more accurate information. isn't it fully oflline?\n\ndo you have any other models that you recommended?\n\nis there a way to \"stream\" the model to my network so I will be able to acsses and ask things from othe computers?\n\nis there something else i need to know about local LLMs?\n\nThank you!\n\n  \nEDIT: after playing around with all kind of models, i've found that meta- llama3-8b-instruct was the best both in preformence speed and answeer quality, so for now i'll stick with it, thank you all!",
          "author_fullname": "t2_e6v0plyv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "running local LLM for the first time",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwafqm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752225536,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752147328,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey guys!&lt;/p&gt;\n\n&lt;p&gt;because of privacy conerns and censorship i;ve decided to give local LLM a try.&lt;/p&gt;\n\n&lt;p&gt;downloaded studio LM and installed mistarl 7B and so far things are  fine. might give ollama a chance as well in the future.&lt;/p&gt;\n\n&lt;p&gt;couple of questions:&lt;/p&gt;\n\n&lt;p&gt;can the model collect data? I asked it and he said he does communicate with the internet to get some more accurate information. isn&amp;#39;t it fully oflline?&lt;/p&gt;\n\n&lt;p&gt;do you have any other models that you recommended?&lt;/p&gt;\n\n&lt;p&gt;is there a way to &amp;quot;stream&amp;quot; the model to my network so I will be able to acsses and ask things from othe computers?&lt;/p&gt;\n\n&lt;p&gt;is there something else i need to know about local LLMs?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;EDIT: after playing around with all kind of models, i&amp;#39;ve found that meta- llama3-8b-instruct was the best both in preformence speed and answeer quality, so for now i&amp;#39;ll stick with it, thank you all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwafqm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Routine_Author961",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752147328,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Does it involve first building a model that creates generates a dataset with &lt;think&gt; tokens.\n\nThen generate a reward model.\n\nFinally fine tune model with RL and reward model?",
          "author_fullname": "t2_1t3515o2d2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How are reasonable models built? Are they fine tuned from base non reasoning models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwrad1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752189190,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does it involve first building a model that creates generates a dataset with &amp;lt;think&amp;gt; tokens.&lt;/p&gt;\n\n&lt;p&gt;Then generate a reward model.&lt;/p&gt;\n\n&lt;p&gt;Finally fine tune model with RL and reward model?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwrad1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rockybaby2025",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwrad1/how_are_reasonable_models_built_are_they_fine/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwrad1/how_are_reasonable_models_built_are_they_fine/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752189190,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone, i am implementing tool calling with an LLM in langgraph and i want to stream only the final llm response (not the intermediate tool call messages). I am curious if anyone got any idea with the lowest latency possible?",
          "author_fullname": "t2_rymgqdpii",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to stream only final LLM response while tool calling",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwr8eh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752189049,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, i am implementing tool calling with an LLM in langgraph and i want to stream only the final llm response (not the intermediate tool call messages). I am curious if anyone got any idea with the lowest latency possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwr8eh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dry_Yam_322",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwr8eh/how_to_stream_only_final_llm_response_while_tool/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwr8eh/how_to_stream_only_final_llm_response_while_tool/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752189049,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm using a local AI model qwen2.5:14b-instruct running via Ollama, but it doesn't automatically call tools follow by my prompt instruction like OpenAI models do.\n\nalso i have limited gpu: NVIDIA A10 24gb ram, so I just only use lightweight model\n\nHow should I design my prompt to ensure the model understands when and how to trigger tool use properly.\n\nAlso im currently using N8N and workflow look like this\n\nhttps://preview.redd.it/jzvxl7pn96cf1.png?width=938&amp;format=png&amp;auto=webp&amp;s=1c794dba7d676078e22c02dc0a267ce5eee23838\n\nHere is my current prompts:\n\nYou are an assistant specialized in SQL reporting and visualization.\n\nYou have access to the following tools:\n\n\\- \\`Execute\\_sql\\_query\\`\n\n\\- \\`list\\_chart\\_available\\`\n\n\\- \\`create\\_chart\\`\n\n\\## Instructions:\n\n1. \\*\\*If the user asks about anything related to the database, tables, or data\\*\\*, you must:- Analyze the request.- Formulate an appropriate SQL query.- Then \\*\\*always use the \\`Execute\\_sql\\_query\\` tool\\*\\* to get the data result.\n2. \\*\\*If the user asks to create a chart\\*\\*, you must:- First, use the \\`list\\_chart\\_available\\` tool to get the available chart types.- Then, based on the result and the user’s intent, choose the most appropriate chart type.- Finally, use the \\`create\\_chart\\` tool.-  do not create any link from \\`create\\_chart\\` tool, just show the image\n\nNever use \\`create\\_chart\\` directly without first calling \\`list\\_chart\\_available\\`.\n\n3. In any SQL query you generate, \\*\\*always include \\`LIMIT 20\\`\\*\\*, unless explicitly told otherwise.\n\n4. Only call tools when necessary. If the question can be answered without executing a query or generating a chart, respond naturally using your knowledge.\n\n4. You are working with the following table(s):\n\n\\*\\*Tables information\\*\\*:  \n...",
          "author_fullname": "t2_efnhbt7w0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama calling tools",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "jzvxl7pn96cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 38,
                  "x": 108,
                  "u": "https://preview.redd.it/jzvxl7pn96cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e6519dcd51005aeba5952281ff3886363a6f9d6c"
                },
                {
                  "y": 76,
                  "x": 216,
                  "u": "https://preview.redd.it/jzvxl7pn96cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0abf01dcd08ed4252808072f4b0419db2a5a603f"
                },
                {
                  "y": 113,
                  "x": 320,
                  "u": "https://preview.redd.it/jzvxl7pn96cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=279fc9fa51385702e0de2c9330884b4a221b9c8d"
                },
                {
                  "y": 227,
                  "x": 640,
                  "u": "https://preview.redd.it/jzvxl7pn96cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=892895841f0f07df2c694f439e145130159a14ab"
                }
              ],
              "s": {
                "y": 334,
                "x": 938,
                "u": "https://preview.redd.it/jzvxl7pn96cf1.png?width=938&amp;format=png&amp;auto=webp&amp;s=1c794dba7d676078e22c02dc0a267ce5eee23838"
              },
              "id": "jzvxl7pn96cf1"
            }
          },
          "name": "t3_1lwxrai",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752208774,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752208245,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using a local AI model qwen2.5:14b-instruct running via Ollama, but it doesn&amp;#39;t automatically call tools follow by my prompt instruction like OpenAI models do.&lt;/p&gt;\n\n&lt;p&gt;also i have limited gpu: NVIDIA A10 24gb ram, so I just only use lightweight model&lt;/p&gt;\n\n&lt;p&gt;How should I design my prompt to ensure the model understands when and how to trigger tool use properly.&lt;/p&gt;\n\n&lt;p&gt;Also im currently using N8N and workflow look like this&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/jzvxl7pn96cf1.png?width=938&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c794dba7d676078e22c02dc0a267ce5eee23838\"&gt;https://preview.redd.it/jzvxl7pn96cf1.png?width=938&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c794dba7d676078e22c02dc0a267ce5eee23838&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here is my current prompts:&lt;/p&gt;\n\n&lt;p&gt;You are an assistant specialized in SQL reporting and visualization.&lt;/p&gt;\n\n&lt;p&gt;You have access to the following tools:&lt;/p&gt;\n\n&lt;p&gt;- `Execute_sql_query`&lt;/p&gt;\n\n&lt;p&gt;- `list_chart_available`&lt;/p&gt;\n\n&lt;p&gt;- `create_chart`&lt;/p&gt;\n\n&lt;p&gt;## Instructions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;**If the user asks about anything related to the database, tables, or data**, you must:- Analyze the request.- Formulate an appropriate SQL query.- Then **always use the `Execute_sql_query` tool** to get the data result.&lt;/li&gt;\n&lt;li&gt;**If the user asks to create a chart**, you must:- First, use the `list_chart_available` tool to get the available chart types.- Then, based on the result and the user’s intent, choose the most appropriate chart type.- Finally, use the `create_chart` tool.-  do not create any link from `create_chart` tool, just show the image&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Never use `create_chart` directly without first calling `list_chart_available`.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;In any SQL query you generate, **always include `LIMIT 20`**, unless explicitly told otherwise.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Only call tools when necessary. If the question can be answered without executing a query or generating a chart, respond naturally using your knowledge.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;You are working with the following table(s):&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;**Tables information**:&lt;br/&gt;\n...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwxrai",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Practical-Corgi-9906",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwxrai/ollama_calling_tools/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwxrai/ollama_calling_tools/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752208245,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nThere are two JS libraries, Transformers.js and WebLLM, for embedding language models in a web application. They seems to target different applications, with a significant(?) overlap.\n\nWhat is your experience with any of these, in terms of efficency, coverage, and precision, for a non-interactive (i.e. not chat with user) application? Does any of them offer better support for more cutting-edge models?\n\nConsider text-summarisation as an example application. Which one is better in providing that?",
          "author_fullname": "t2_127kho",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Transformers.js vs WebLLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw6jz5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752136093,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752132310,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;There are two JS libraries, Transformers.js and WebLLM, for embedding language models in a web application. They seems to target different applications, with a significant(?) overlap.&lt;/p&gt;\n\n&lt;p&gt;What is your experience with any of these, in terms of efficency, coverage, and precision, for a non-interactive (i.e. not chat with user) application? Does any of them offer better support for more cutting-edge models?&lt;/p&gt;\n\n&lt;p&gt;Consider text-summarisation as an example application. Which one is better in providing that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lw6jz5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ihatebeinganonymous",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw6jz5/transformersjs_vs_webllm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw6jz5/transformersjs_vs_webllm/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752132310,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Tried to checkout the website for Ai-first Comet browser from Perplexity.\nWas shown this page.\n\nI understand it’s only rolled out to their $200 paying Pro customers.\nBut a better 403 page would be nice. \nJust a heads up to the Perplexity team.\n\nAlso waiting for preview access to this browser. 😉",
          "author_fullname": "t2_t0syffr8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Comet (AI first) browser from Perplexity needs better 403 page",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 121,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx0e8i",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/lV_lnjDMCfemLDWXIPYIENGhT2m5uWZQ4VsieizPiCo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752217797,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tried to checkout the website for Ai-first Comet browser from Perplexity.\nWas shown this page.&lt;/p&gt;\n\n&lt;p&gt;I understand it’s only rolled out to their $200 paying Pro customers.\nBut a better 403 page would be nice. \nJust a heads up to the Perplexity team.&lt;/p&gt;\n\n&lt;p&gt;Also waiting for preview access to this browser. 😉&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/76b20ni437cf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/76b20ni437cf1.jpeg?auto=webp&amp;s=c8f1dbb12e944b1edaba830e700e6b5f44945c4c",
                  "width": 1179,
                  "height": 1023
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/76b20ni437cf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7b0f54d5e4850a778c1d877746eba43bc6a714a7",
                    "width": 108,
                    "height": 93
                  },
                  {
                    "url": "https://preview.redd.it/76b20ni437cf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b320e6e7f2c00feebf009bfaa3f614b184d301c7",
                    "width": 216,
                    "height": 187
                  },
                  {
                    "url": "https://preview.redd.it/76b20ni437cf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc79697dba1311e4ab6811ede05978a9ffb0b277",
                    "width": 320,
                    "height": 277
                  },
                  {
                    "url": "https://preview.redd.it/76b20ni437cf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9cf1504fb9eadd9adb55bcb655eaf7a7bf533006",
                    "width": 640,
                    "height": 555
                  },
                  {
                    "url": "https://preview.redd.it/76b20ni437cf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=15c89f80570d5dac52e09a40308cff6d5b832dad",
                    "width": 960,
                    "height": 832
                  },
                  {
                    "url": "https://preview.redd.it/76b20ni437cf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e76d81f6433b56115741ab9795d236db715007e5",
                    "width": 1080,
                    "height": 937
                  }
                ],
                "variants": {},
                "id": "p-AUWyAojHyWJqEHf2ZDNMt2MlN7Au9G2_ParjIhGEE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lx0e8i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NoobMLDude",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx0e8i/comet_ai_first_browser_from_perplexity_needs/",
          "stickied": false,
          "url": "https://i.redd.it/76b20ni437cf1.jpeg",
          "subreddit_subscribers": 497502,
          "created_utc": 1752217797,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi!  \nI want to fine-tune a small pre-trained LLM to help users write code in a specific language. This language is very specific to a particular machinery and does not have widespread usage. We have a manual in PDF format and a few examples for the code. We want to build a chat agent where users can write code, and the agent writes the code. I am very new to training LLM and willing to learn whatever is necessary. I have a basic understanding of working with LLMs using Ollama and LangChain. Could someone please guide me on where to start? I have a good machine with an NVIDIA RTX 4090, 24 GB GPU. I want to build the entire system on this machine. \n\nThanks in advance for all the help. ",
          "author_fullname": "t2_1qt1co6pyc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fine Tune a smaller LLM for Code generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw1qp5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 36,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 36,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752115392,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;br/&gt;\nI want to fine-tune a small pre-trained LLM to help users write code in a specific language. This language is very specific to a particular machinery and does not have widespread usage. We have a manual in PDF format and a few examples for the code. We want to build a chat agent where users can write code, and the agent writes the code. I am very new to training LLM and willing to learn whatever is necessary. I have a basic understanding of working with LLMs using Ollama and LangChain. Could someone please guide me on where to start? I have a good machine with an NVIDIA RTX 4090, 24 GB GPU. I want to build the entire system on this machine. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for all the help. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lw1qp5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GlobeAndGeek",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752115392,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Or some website that will let me know for a rtx 4090, 32gb ram, what the performance of deepseek-r1 will be? \n\nThanks, i don't know where to start. \n\nI have an rtx 4080s (16gb graphics ram) with 64gb ram on a 13700k...",
          "author_fullname": "t2_el036",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there some localllm benchmarking tool to see how well your system will handle a model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwp7tv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752183949,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Or some website that will let me know for a rtx 4090, 32gb ram, what the performance of deepseek-r1 will be? &lt;/p&gt;\n\n&lt;p&gt;Thanks, i don&amp;#39;t know where to start. &lt;/p&gt;\n\n&lt;p&gt;I have an rtx 4080s (16gb graphics ram) with 64gb ram on a 13700k...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwp7tv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blackashi",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752183949,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just wondering if anyone has used a model to help map 3d meshes for expected countours/shapes/colors.  Ie.  Map 90% of a vase but you are unable to scan the bottom/back.  AI could assist in finding the real world match and correcting.  \n\nIn theory you could use video feed of a 3d space to map with actual modelled objects.  Think repeating bleacher chairs in an arena.\n\nEditing for clarity.\n\nThink of a deck.  You take a quick video of the top and underside of the deck.\n\nLLM identifies the boards as 2x6 softwood and builds the 3d environment with each board as a categorized object.  It would know the black railings are like aluminum spindles and do both a count and correctly render each spindle perfectly.  Think Autocad.\n\nLink to an mcp that specializes in looking up existing 3d models and identifies what it sees in the video to source and place the models.\n\nDirect phone/video to wireframe cad with objects.",
          "author_fullname": "t2_ze8yz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can AI assist with 3d mapping?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwnxhz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752187795,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752180802,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just wondering if anyone has used a model to help map 3d meshes for expected countours/shapes/colors.  Ie.  Map 90% of a vase but you are unable to scan the bottom/back.  AI could assist in finding the real world match and correcting.  &lt;/p&gt;\n\n&lt;p&gt;In theory you could use video feed of a 3d space to map with actual modelled objects.  Think repeating bleacher chairs in an arena.&lt;/p&gt;\n\n&lt;p&gt;Editing for clarity.&lt;/p&gt;\n\n&lt;p&gt;Think of a deck.  You take a quick video of the top and underside of the deck.&lt;/p&gt;\n\n&lt;p&gt;LLM identifies the boards as 2x6 softwood and builds the 3d environment with each board as a categorized object.  It would know the black railings are like aluminum spindles and do both a count and correctly render each spindle perfectly.  Think Autocad.&lt;/p&gt;\n\n&lt;p&gt;Link to an mcp that specializes in looking up existing 3d models and identifies what it sees in the video to source and place the models.&lt;/p&gt;\n\n&lt;p&gt;Direct phone/video to wireframe cad with objects.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwnxhz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Bohdanowicz",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwnxhz/can_ai_assist_with_3d_mapping/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwnxhz/can_ai_assist_with_3d_mapping/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752180802,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am using qwen3:14b it works well for my day to day life and reducing my online llm dependencies. Like you can see in both screenshot I got almost equilant result",
          "author_fullname": "t2_1b8utegv8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Local llms works great!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 134,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "rguqv7yfqzbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 87,
                  "x": 108,
                  "u": "https://preview.redd.it/rguqv7yfqzbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=626302458bfda67ad957ee9cdcc8f040499c0d46"
                },
                {
                  "y": 175,
                  "x": 216,
                  "u": "https://preview.redd.it/rguqv7yfqzbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d6ba092f6a65ba21be36891a5a456d8e11af146"
                },
                {
                  "y": 259,
                  "x": 320,
                  "u": "https://preview.redd.it/rguqv7yfqzbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c9534569f15401c2a32a0348c9aebcd8bae701d"
                },
                {
                  "y": 519,
                  "x": 640,
                  "u": "https://preview.redd.it/rguqv7yfqzbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3c30a937306299b861ee9e84520efb23dd17a4ea"
                }
              ],
              "s": {
                "y": 673,
                "x": 829,
                "u": "https://preview.redd.it/rguqv7yfqzbf1.png?width=829&amp;format=png&amp;auto=webp&amp;s=bcf4c7e41c811fc4574da56560718ca70cf71f81"
              },
              "id": "rguqv7yfqzbf1"
            },
            "z17qfoqaqzbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 103,
                  "x": 108,
                  "u": "https://preview.redd.it/z17qfoqaqzbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f6d65a7c3f5e753a2fe5be75a81095e8491ccd37"
                },
                {
                  "y": 206,
                  "x": 216,
                  "u": "https://preview.redd.it/z17qfoqaqzbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fc07cd122eb1bd64d272332d97bf962dcaea38fe"
                },
                {
                  "y": 306,
                  "x": 320,
                  "u": "https://preview.redd.it/z17qfoqaqzbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d03a961bf02aa3298f87650e7be4058ce123e93d"
                },
                {
                  "y": 613,
                  "x": 640,
                  "u": "https://preview.redd.it/z17qfoqaqzbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2c618fe19453648dfcef7f713754d3e9e4960408"
                }
              ],
              "s": {
                "y": 780,
                "x": 814,
                "u": "https://preview.redd.it/z17qfoqaqzbf1.png?width=814&amp;format=png&amp;auto=webp&amp;s=ee64572f1eda92358aa472e1e6400d9c6c970820"
              },
              "id": "z17qfoqaqzbf1"
            }
          },
          "name": "t3_1lw5oco",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "ups": 16,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "z17qfoqaqzbf1",
                "id": 702654422
              },
              {
                "media_id": "rguqv7yfqzbf1",
                "id": 702654423
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/gv1XJzy7IzUkjUhfUJWUZXarplALZeUC0i1uy9Wz9jQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752128871,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using qwen3:14b it works well for my day to day life and reducing my online llm dependencies. Like you can see in both screenshot I got almost equilant result&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lw5oco",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lw5oco",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "InsideResolve4517",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw5oco/local_llms_works_great/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lw5oco",
          "subreddit_subscribers": 497502,
          "created_utc": 1752128871,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "How important is the speed and latency of the system ram when you run out of VRAM when running a local LLM?\n\nI know that vram is multitudes faster than ram, and I have experienced the difference myself when I exceeded the vram buffer of my PC.\n\nBut I wanted to ask what happens if the plan is to exceed the vram and use system ram?\n\nIf I had the same system, but one had a gpu and one didn’t, supposing that the gpu didn’t have enough vram, is there still an appreciable difference in llm performance with the two systems?\n\nRight now I have a 7900 xt and 32gb of ddr5 6000 cl36 ram. Would getting a kit of faster 96gb kit of ddr5 6400 do more than getting a used gpu like the rx 6800 for 16 more gen of vram?\n\nIn the scenarios I am assuming that the model spills out into the ram either way.\n\nIf the llm spills out into the ram, is it cpu inference now?",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ram Speed importance when exceeding VRAM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw8lvt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752140750,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How important is the speed and latency of the system ram when you run out of VRAM when running a local LLM?&lt;/p&gt;\n\n&lt;p&gt;I know that vram is multitudes faster than ram, and I have experienced the difference myself when I exceeded the vram buffer of my PC.&lt;/p&gt;\n\n&lt;p&gt;But I wanted to ask what happens if the plan is to exceed the vram and use system ram?&lt;/p&gt;\n\n&lt;p&gt;If I had the same system, but one had a gpu and one didn’t, supposing that the gpu didn’t have enough vram, is there still an appreciable difference in llm performance with the two systems?&lt;/p&gt;\n\n&lt;p&gt;Right now I have a 7900 xt and 32gb of ddr5 6000 cl36 ram. Would getting a kit of faster 96gb kit of ddr5 6400 do more than getting a used gpu like the rx 6800 for 16 more gen of vram?&lt;/p&gt;\n\n&lt;p&gt;In the scenarios I am assuming that the model spills out into the ram either way.&lt;/p&gt;\n\n&lt;p&gt;If the llm spills out into the ram, is it cpu inference now?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lw8lvt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752140750,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In this thread I want to explore something I don’t see being covered much: running LLMs on extremely low-power edge devices. \n\nI want to build something that I could run during an energy crisis or extended power black-out. This is mostly an academic exercise, but I think it would be prudent to have a plan. \n\nThe goal would be to run and maintain a knowledge base of survival information (first aid, medical diagnosis &amp; treatments, how to service common machinery etc) that could be collated during power-abundant times then queried via RAG by a lightweight edge device with a chat interface. TOPS doesn’t need to be very high here, but responses would still need to be somewhat realtime.  \n\nWhat would you spec out? I’m leaning towards android mobile devices for their ubiquity and power efficiency. Solid state storage makes more sense for power reasons but cold storage might be wise for resilience. ",
          "author_fullname": "t2_4efmo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Survivalist Edge AI?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw7igq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752136260,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In this thread I want to explore something I don’t see being covered much: running LLMs on extremely low-power edge devices. &lt;/p&gt;\n\n&lt;p&gt;I want to build something that I could run during an energy crisis or extended power black-out. This is mostly an academic exercise, but I think it would be prudent to have a plan. &lt;/p&gt;\n\n&lt;p&gt;The goal would be to run and maintain a knowledge base of survival information (first aid, medical diagnosis &amp;amp; treatments, how to service common machinery etc) that could be collated during power-abundant times then queried via RAG by a lightweight edge device with a chat interface. TOPS doesn’t need to be very high here, but responses would still need to be somewhat realtime.  &lt;/p&gt;\n\n&lt;p&gt;What would you spec out? I’m leaning towards android mobile devices for their ubiquity and power efficiency. Solid state storage makes more sense for power reasons but cold storage might be wise for resilience. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lw7igq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xibbie",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/",
          "subreddit_subscribers": 497502,
          "created_utc": 1752136260,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_i5os0v0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLamaCPP just merged Mamba/Jamba support!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvyfws",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=bd15897bc569fad578c22335085f1c50bc5fdc47",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752105732,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/7531",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?auto=webp&amp;s=1e28f2615bcb62135d8d732a8ea85dd226f1b014",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=340e18fba3f412557fd7374f9b789abc78d4f2eb",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd61a7c984debedf58dfafd58331a67a8d8fd322",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=da7395dd510f76d8224e3e4fee5cb162c818d6c0",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e1458a795a3b7a775e1a94d7767c299e1d8f3ef",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=48a7f32bb6df4e15a5e213c8c8dc317d5fa14ce0",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fb2f9e222f5c82bc7c1c7600a290f7addc8c2012",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lvyfws",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "thebadslime",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvyfws/llamacpp_just_merged_mambajamba_support/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/7531",
          "subreddit_subscribers": 497502,
          "created_utc": 1752105732,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}