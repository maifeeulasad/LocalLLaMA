{
  "kind": "Listing",
  "data": {
    "after": "t3_1meze5n",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1nisx8ggay",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "all I need....",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfgj0g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 706,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 706,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/CGttwXkoUDYh4g0k8yxukZYUVavOKH_4hrW2zTWYkis.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754105691,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ggc3dzhr0jgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?auto=webp&amp;s=56a0a807d6543ba43266157bcaa0b48793bcd3df",
                  "width": 1024,
                  "height": 1536
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0faf4ee7c2bcdc1e4161739543ede55c4684b2b8",
                    "width": 108,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b2e17cf5370e4cca658ed8a629406a0522d68d7e",
                    "width": 216,
                    "height": 324
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ffb58e48e0282066e26e22c2d00d814c00c5ded2",
                    "width": 320,
                    "height": 480
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=939ec0dfb5d8aad25a06b51c38644b3ee7d0d9cd",
                    "width": 640,
                    "height": 960
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0d2db9397b5cffd5e90a0cc1250191ed983494cb",
                    "width": 960,
                    "height": 1440
                  }
                ],
                "variants": {},
                "id": "FTO20S33694FU83Br474HCo8srCRihH9Gw7Oe1llTgs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mfgj0g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ILoveMy2Balls",
          "discussion_type": null,
          "num_comments": 70,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfgj0g/all_i_need/",
          "stickied": false,
          "url": "https://i.redd.it/ggc3dzhr0jgf1.png",
          "subreddit_subscribers": 508769,
          "created_utc": 1754105691,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "new models from Skywork:\n\nWe introduce **MindLink**, a new family of large language models developed by **Kunlun Inc**. Built on **Qwen**, these models incorporate our latest advances in post-training techniques. MindLink demonstrates strong performance across various common benchmarks and is widely applicable in diverse AI scenarios. We welcome feedback to help us continuously optimize and improve our models.\n\n* **Plan-based Reasoning**: Without the \"think\" tag, MindLink achieves competitive performance with leading proprietary models across a wide range of reasoning and general tasks. It significantly reduces inference cost, and improves multi-turn capabilities.\n* **Mathematical Framework**: It analyzes the effectiveness of both **Chain-of-Thought (CoT)** and **Plan-based Reasoning**.\n* **Adaptive Reasoning**: it automatically adapts its reasoning strategy based on task complexity: complex tasks produce detailed reasoning traces, while simpler tasks yield concise outputs.\n\n[https://huggingface.co/Skywork/MindLink-32B-0801](https://huggingface.co/Skywork/MindLink-32B-0801)\n\n[https://huggingface.co/Skywork/MindLink-72B-0801](https://huggingface.co/Skywork/MindLink-72B-0801)\n\n[https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF](https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF)\n\n\n\n",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Skywork MindLink 32B/72B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfitwb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": "#bbbdbf",
          "ups": 132,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 132,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/RNvygcC1t5B12yynYguEPL4rixiqNeO2Wo07y1iwph0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754113315,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;new models from Skywork:&lt;/p&gt;\n\n&lt;p&gt;We introduce &lt;strong&gt;MindLink&lt;/strong&gt;, a new family of large language models developed by &lt;strong&gt;Kunlun Inc&lt;/strong&gt;. Built on &lt;strong&gt;Qwen&lt;/strong&gt;, these models incorporate our latest advances in post-training techniques. MindLink demonstrates strong performance across various common benchmarks and is widely applicable in diverse AI scenarios. We welcome feedback to help us continuously optimize and improve our models.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Plan-based Reasoning&lt;/strong&gt;: Without the &amp;quot;think&amp;quot; tag, MindLink achieves competitive performance with leading proprietary models across a wide range of reasoning and general tasks. It significantly reduces inference cost, and improves multi-turn capabilities.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Mathematical Framework&lt;/strong&gt;: It analyzes the effectiveness of both &lt;strong&gt;Chain-of-Thought (CoT)&lt;/strong&gt; and &lt;strong&gt;Plan-based Reasoning&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Adaptive Reasoning&lt;/strong&gt;: it automatically adapts its reasoning strategy based on task complexity: complex tasks produce detailed reasoning traces, while simpler tasks yield concise outputs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/Skywork/MindLink-32B-0801\"&gt;https://huggingface.co/Skywork/MindLink-32B-0801&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/Skywork/MindLink-72B-0801\"&gt;https://huggingface.co/Skywork/MindLink-72B-0801&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF\"&gt;https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/im7w319dnjgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/im7w319dnjgf1.png?auto=webp&amp;s=47f7f89ad190a3c63e1ac553dd6a3484a9ad8ca0",
                  "width": 2042,
                  "height": 1536
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=01cd51cd78031eda5866ff7d7ad34590c408908f",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=39cd3616dd75544828cbb104491bd20bffd83ee2",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0e60ba4473193249923ba5fc4a97b6f3c8979b6",
                    "width": 320,
                    "height": 240
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7493c60ab05796cf114bd0fa0c600e5aa06497f7",
                    "width": 640,
                    "height": 481
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d17b7719956d66090e1c5852b8ee404b2d44a161",
                    "width": 960,
                    "height": 722
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9af2b894d2bfc41eb10b3bdbb12a1975e5e87cac",
                    "width": 1080,
                    "height": 812
                  }
                ],
                "variants": {},
                "id": "dMPU8D8NEUnZXYRxA5Ryy9ueDVT8WMowE2c2asQCAgs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mfitwb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 58,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfitwb/skywork_mindlink_32b72b/",
          "stickied": false,
          "url": "https://i.redd.it/im7w319dnjgf1.png",
          "subreddit_subscribers": 508769,
          "created_utc": 1754113315,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "|Model Name|Organization|HuggingFace Link|Size|Modality|\n|:-|:-|:-|:-|:-|\n|dots.ocr|REDnote Hilab|[https://huggingface.co/rednote-hilab/dots.ocr](https://huggingface.co/rednote-hilab/dots.ocr)|3B|Image-Text-to-Text|\n||||||\n|GLM 4.5|[Z.ai](http://Z.ai)|[https://huggingface.co/zai-org/GLM-4.5](https://huggingface.co/zai-org/GLM-4.5)|355B-A32B|Text-to-Text|\n|GLM 4.5 Base|[Z.ai](http://Z.ai)|[https://huggingface.co/zai-org/GLM-4.5-Base](https://huggingface.co/zai-org/GLM-4.5-Base)|355B-A32B|Text-to-Text|\n|GLM 4.5-Air|[Z.ai](http://Z.ai)|[https://huggingface.co/zai-org/GLM-4.5-Air](https://huggingface.co/zai-org/GLM-4.5-Air)|106B-A12B|Text-to-Text|\n|GLM 4.5 Air Base|[Z.ai](http://Z.ai)|[https://huggingface.co/zai-org/GLM-4.5-Air-Base](https://huggingface.co/zai-org/GLM-4.5-Air-Base)|106B-A12B|Text-to-Text|\n||||||\n|Qwen3 235B-A22B Instruct 2507|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507)|235B-A22B|Text-to-Text|\n|Qwen3 235B-A22B Thinking 2507|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507)|235B-A22B|Text-to-Text|\n|Qwen3 30B-A3B Instruct 2507|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507)|30B-A3B|Text-to-Text|\n|Qwen3 30B-A3B Thinking 2507|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507)|30B-A3B|Text-to-Text|\n|Qwen3 Coder 480B-A35B Instruct|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct](https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct)|480B-A35B|Text-to-Text|\n|Qwen3 Coder 30B-A3B Instruct|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct](https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct)|30B-A3B|Text-to-Text|\n||||||\n|Kimi K2 Instruct|Moonshot AI|[https://huggingface.co/moonshotai/Kimi-K2-Instruct](https://huggingface.co/moonshotai/Kimi-K2-Instruct)|1T-32B|Text-to-Text|\n|Kimi K2 Base|Moonshot AI|[https://huggingface.co/moonshotai/Kimi-K2-Base](https://huggingface.co/moonshotai/Kimi-K2-Base)|1T-32B|Text-to-Text|\n||||||\n|Intern S1|Shanghai AI Laboratory - Intern|[https://huggingface.co/internlm/Intern-S1](https://huggingface.co/internlm/Intern-S1)|241B-A22B|Image-Text-to-Text|\n||||||\n|Llama-3.3 Nemotron Super 49B v1.5|Nvidia|[https://huggingface.co/nvidia/Llama-3\\_3-Nemotron-Super-49B-v1\\_5](https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5)|49B|Text-to-Text|\n|OpenReasoning Nemotron 1.5B|Nvidia|[https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B)|1.5B|Text-to-Text|\n|OpenReasoning Nemotron 7B|Nvidia|[https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B)|7B|Text-to-Text|\n|OpenReasoning Nemotron 14B|Nvidia|[https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B)|14B|Text-to-Text|\n|OpenReasoning Nemotron 32B|Nvidia|[https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B)|32B|Text-to-Text|\n||||||\n|step3|StepFun|[https://huggingface.co/stepfun-ai/step3](https://huggingface.co/stepfun-ai/step3)|321B-A38B|Text-to-Text|\n||||||\n|SmallThinker 21B-A3B Instruct|IPADS - PowerInfer|[https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct](https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct)|21B-A3B|Text-to-Text|\n|SmallThinker 4B-A0.6B Instruct|IPADS - PowerInfer|[https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct](https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct)|4B-A0.6B|Text-to-Text|\n||||||\n|Seed X Instruct-7B|ByteDance Seed|[https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B](https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B)|7B|Machine Translation|\n|Seed X PPO-7B|ByteDance Seed|[https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B](https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B)|7B|Machine Translation|\n||||||\n|Magistral Small 2507|Mistral|[https://huggingface.co/mistralai/Magistral-Small-2507](https://huggingface.co/mistralai/Magistral-Small-2507)|24B|Text-to-Text|\n|Devstral Small 2507|Mistral|[https://huggingface.co/mistralai/Devstral-Small-2507](https://huggingface.co/mistralai/Devstral-Small-2507)|24B|Text-to-Text|\n|Voxtral Small 24B 2507|Mistral|[https://huggingface.co/mistralai/Voxtral-Small-24B-2507](https://huggingface.co/mistralai/Voxtral-Small-24B-2507)|24B|Audio-Text-to-Text|\n|Voxtral Mini 3B 2507|Mistral|[https://huggingface.co/mistralai/Voxtral-Mini-3B-2507](https://huggingface.co/mistralai/Voxtral-Mini-3B-2507)|3B|Audio-Text-to-Text|\n||||||\n|AFM 4.5B|Arcee AI|[https://huggingface.co/arcee-ai/AFM-4.5B](https://huggingface.co/arcee-ai/AFM-4.5B)|4.5B|Text-to-Text|\n|AFM 4.5B Base|Arcee AI|[https://huggingface.co/arcee-ai/AFM-4.5B-Base](https://huggingface.co/arcee-ai/AFM-4.5B-Base)|4B|Text-to-Text|\n||||||\n|Ling lite-1.5 2506|Ant Group - Inclusion AI|[https://huggingface.co/inclusionAI/Ling-lite-1.5-2506](https://huggingface.co/inclusionAI/Ling-lite-1.5-2506)|16B|Text-to-Text|\n|Ming Lite Omni-1.5|Ant Group - Inclusion AI|[https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5](https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5)|20.3B|Text-Audio-Video-Image-To-Text|\n||||||\n|UIGEN X 32B 0727|Tesslate|[https://huggingface.co/Tesslate/UIGEN-X-32B-0727](https://huggingface.co/Tesslate/UIGEN-X-32B-0727)|32B|Text-to-Text|\n|UIGEN X 4B 0729|Tesslate|[https://huggingface.co/Tesslate/UIGEN-X-4B-0729](https://huggingface.co/Tesslate/UIGEN-X-4B-0729)|4B|Text-to-Text|\n|UIGEN X 8B|Tesslate|[https://huggingface.co/Tesslate/UIGEN-X-8B](https://huggingface.co/Tesslate/UIGEN-X-8B)|8B|Text-to-Text|\n||||||\n|command a vision 07-2025|Cohere|[https://huggingface.co/CohereLabs/command-a-vision-07-2025](https://huggingface.co/CohereLabs/command-a-vision-07-2025)|112B|Image-Text-to-Text|\n||||||\n|KAT V1 40B|Kwaipilot|[https://huggingface.co/Kwaipilot/KAT-V1-40B](https://huggingface.co/Kwaipilot/KAT-V1-40B)|40B|Text-to-Text|\n||||||\n|EXAONE 4.0.1 32B|LG AI|[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B)|32B|Text-to-Text|\n|EXAONE 4.0.1 2B|LG AI|[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B)|2B|Text-to-Text|\n|EXAONE 4.0 32B|LG AI|[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B)|32B|Text-to-Text|\n||||||\n|cogito v2 preview deepseek-671B-MoE|Deep Cogito|[https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE](https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE)|671B-A37B|Text-to-Text|\n|cogito v2 preview llama-405B|Deep Cogito|[https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B](https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B)|405B|Text-to-Text|\n|cogito v2 preview llama-109B-MoE|Deep Cogito|[https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE](https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE)|109B-A17B|Image-Text-to-Text|\n|cogito v2 preview llama-70B|Deep Cogito|[https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B](https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B)|70B|Text-to-Text|\n||||||\n|A.X 4.0 VL Light|SK Telecom|[https://huggingface.co/skt/A.X-4.0-VL-Light](https://huggingface.co/skt/A.X-4.0-VL-Light)|8B|Image-Text-to-Text|\n|A.X 3.1|SK Telecom|[https://huggingface.co/skt/A.X-3.1](https://huggingface.co/skt/A.X-3.1)|35B|Text-to-Text|\n|olmOCR 7B 0725|AllenAI|[https://huggingface.co/allenai/olmOCR-7B-0725](https://huggingface.co/allenai/olmOCR-7B-0725)|7B|Image-Text-to-Text|\n||||||\n|kanana 1.5 15.7B-A3B instruct|Kakao|[https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct](https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct)|7B-A3B|Text-to-Text|\n|kanana 1.5v 3B instruct|Kakao|[https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct](https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct)|3B|Image-Text-to-Text|\n||||||\n|Tri 7B|Trillion Labs|[https://huggingface.co/trillionlabs/Tri-7B](https://huggingface.co/trillionlabs/Tri-7B)|7B|Text-to-Text|\n|Tri 21B|Trillion Labs|[https://huggingface.co/trillionlabs/Tri-21B](https://huggingface.co/trillionlabs/Tri-21B)|21B|Text-to-Text|\n|Tri 70B preview SFT|Trillion Labs|[https://huggingface.co/trillionlabs/Tri-70B-preview-SFT](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT)|70B|Text-to-Text|\n\nI tried to compile the latest models released over the past 2–3 weeks, and its kinda like there is a ground breaking model every 2 days. I’m really glad to be living in this era of rapid progress.\n\nThis list doesn’t even include other modalities like 3D, image, and audio, where there's also a ton of new models (Like Wan2.2 , Flux-Krea , ...)\n\nHope this can serve as a breakdown of the latest models.\n\n*Feel free to tag me if I missed any you think should be added!*\n\n\\[EDIT\\] \n\n**I see a lot of people saying that a leaderboard would be great to showcase the latest and greatest or just to keep up.**\n\n**Would it be a good idea to create a sort of LocalLLaMA community-driven leaderboard based only on vibe checks and upvotes (so no numbers)?**\n\n**Anyone could publish a new model—with some community approval to reduce junk and pure finetunes**?",
          "author_fullname": "t2_7zubl1l8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We're truly in the fastest-paced era of AI these days. (50 LLM Released these 2-3 Weeks)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfaigh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 432,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 432,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754117628,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754088000,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Organization&lt;/th&gt;\n&lt;th align=\"left\"&gt;HuggingFace Link&lt;/th&gt;\n&lt;th align=\"left\"&gt;Size&lt;/th&gt;\n&lt;th align=\"left\"&gt;Modality&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;dots.ocr&lt;/td&gt;\n&lt;td align=\"left\"&gt;REDnote Hilab&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/rednote-hilab/dots.ocr\"&gt;https://huggingface.co/rednote-hilab/dots.ocr&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GLM 4.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5\"&gt;https://huggingface.co/zai-org/GLM-4.5&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;355B-A32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GLM 4.5 Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5-Base\"&gt;https://huggingface.co/zai-org/GLM-4.5-Base&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;355B-A32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GLM 4.5-Air&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5-Air\"&gt;https://huggingface.co/zai-org/GLM-4.5-Air&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;106B-A12B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GLM 4.5 Air Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5-Air-Base\"&gt;https://huggingface.co/zai-org/GLM-4.5-Air-Base&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;106B-A12B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 235B-A22B Instruct 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507\"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;235B-A22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 235B-A22B Thinking 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507\"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;235B-A22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 30B-A3B Instruct 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507\"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 30B-A3B Thinking 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507\"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 Coder 480B-A35B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\"&gt;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;480B-A35B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 Coder 30B-A3B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct\"&gt;https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Kimi K2 Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Moonshot AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Instruct\"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;1T-32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Kimi K2 Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;Moonshot AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Base\"&gt;https://huggingface.co/moonshotai/Kimi-K2-Base&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;1T-32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Intern S1&lt;/td&gt;\n&lt;td align=\"left\"&gt;Shanghai AI Laboratory - Intern&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/internlm/Intern-S1\"&gt;https://huggingface.co/internlm/Intern-S1&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;241B-A22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama-3.3 Nemotron Super 49B v1.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\"&gt;https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;49B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenReasoning Nemotron 1.5B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.5B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenReasoning Nemotron 7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenReasoning Nemotron 14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenReasoning Nemotron 32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;step3&lt;/td&gt;\n&lt;td align=\"left\"&gt;StepFun&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/stepfun-ai/step3\"&gt;https://huggingface.co/stepfun-ai/step3&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;321B-A38B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;SmallThinker 21B-A3B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;IPADS - PowerInfer&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct\"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;21B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;SmallThinker 4B-A0.6B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;IPADS - PowerInfer&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct\"&gt;https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4B-A0.6B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Seed X Instruct-7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;ByteDance Seed&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B\"&gt;https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Machine Translation&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Seed X PPO-7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;ByteDance Seed&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B\"&gt;https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Machine Translation&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Magistral Small 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/mistralai/Magistral-Small-2507\"&gt;https://huggingface.co/mistralai/Magistral-Small-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;24B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Devstral Small 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/mistralai/Devstral-Small-2507\"&gt;https://huggingface.co/mistralai/Devstral-Small-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;24B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Voxtral Small 24B 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/mistralai/Voxtral-Small-24B-2507\"&gt;https://huggingface.co/mistralai/Voxtral-Small-24B-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;24B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Audio-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Voxtral Mini 3B 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/mistralai/Voxtral-Mini-3B-2507\"&gt;https://huggingface.co/mistralai/Voxtral-Mini-3B-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Audio-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;AFM 4.5B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Arcee AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/arcee-ai/AFM-4.5B\"&gt;https://huggingface.co/arcee-ai/AFM-4.5B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.5B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;AFM 4.5B Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;Arcee AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/arcee-ai/AFM-4.5B-Base\"&gt;https://huggingface.co/arcee-ai/AFM-4.5B-Base&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Ling lite-1.5 2506&lt;/td&gt;\n&lt;td align=\"left\"&gt;Ant Group - Inclusion AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/inclusionAI/Ling-lite-1.5-2506\"&gt;https://huggingface.co/inclusionAI/Ling-lite-1.5-2506&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;16B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Ming Lite Omni-1.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;Ant Group - Inclusion AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5\"&gt;https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-Audio-Video-Image-To-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;UIGEN X 32B 0727&lt;/td&gt;\n&lt;td align=\"left\"&gt;Tesslate&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-32B-0727\"&gt;https://huggingface.co/Tesslate/UIGEN-X-32B-0727&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;UIGEN X 4B 0729&lt;/td&gt;\n&lt;td align=\"left\"&gt;Tesslate&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-4B-0729\"&gt;https://huggingface.co/Tesslate/UIGEN-X-4B-0729&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;UIGEN X 8B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Tesslate&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-8B\"&gt;https://huggingface.co/Tesslate/UIGEN-X-8B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;8B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;command a vision 07-2025&lt;/td&gt;\n&lt;td align=\"left\"&gt;Cohere&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/CohereLabs/command-a-vision-07-2025\"&gt;https://huggingface.co/CohereLabs/command-a-vision-07-2025&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;112B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;KAT V1 40B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Kwaipilot&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Kwaipilot/KAT-V1-40B\"&gt;https://huggingface.co/Kwaipilot/KAT-V1-40B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;40B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;EXAONE 4.0.1 32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;LG AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;EXAONE 4.0.1 2B&lt;/td&gt;\n&lt;td align=\"left\"&gt;LG AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;2B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;EXAONE 4.0 32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;LG AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;cogito v2 preview deepseek-671B-MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;Deep Cogito&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;671B-A37B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;cogito v2 preview llama-405B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Deep Cogito&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;405B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;cogito v2 preview llama-109B-MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;Deep Cogito&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;109B-A17B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;cogito v2 preview llama-70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Deep Cogito&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A.X 4.0 VL Light&lt;/td&gt;\n&lt;td align=\"left\"&gt;SK Telecom&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/skt/A.X-4.0-VL-Light\"&gt;https://huggingface.co/skt/A.X-4.0-VL-Light&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;8B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A.X 3.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;SK Telecom&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/skt/A.X-3.1\"&gt;https://huggingface.co/skt/A.X-3.1&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;35B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;olmOCR 7B 0725&lt;/td&gt;\n&lt;td align=\"left\"&gt;AllenAI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/allenai/olmOCR-7B-0725\"&gt;https://huggingface.co/allenai/olmOCR-7B-0725&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;kanana 1.5 15.7B-A3B instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Kakao&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct\"&gt;https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;kanana 1.5v 3B instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Kakao&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct\"&gt;https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Tri 7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Trillion Labs&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/trillionlabs/Tri-7B\"&gt;https://huggingface.co/trillionlabs/Tri-7B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Tri 21B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Trillion Labs&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/trillionlabs/Tri-21B\"&gt;https://huggingface.co/trillionlabs/Tri-21B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;21B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Tri 70B preview SFT&lt;/td&gt;\n&lt;td align=\"left\"&gt;Trillion Labs&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/trillionlabs/Tri-70B-preview-SFT\"&gt;https://huggingface.co/trillionlabs/Tri-70B-preview-SFT&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I tried to compile the latest models released over the past 2–3 weeks, and its kinda like there is a ground breaking model every 2 days. I’m really glad to be living in this era of rapid progress.&lt;/p&gt;\n\n&lt;p&gt;This list doesn’t even include other modalities like 3D, image, and audio, where there&amp;#39;s also a ton of new models (Like Wan2.2 , Flux-Krea , ...)&lt;/p&gt;\n\n&lt;p&gt;Hope this can serve as a breakdown of the latest models.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Feel free to tag me if I missed any you think should be added!&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;[EDIT] &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I see a lot of people saying that a leaderboard would be great to showcase the latest and greatest or just to keep up.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Would it be a good idea to create a sort of LocalLLaMA community-driven leaderboard based only on vibe checks and upvotes (so no numbers)?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Anyone could publish a new model—with some community approval to reduce junk and pure finetunes&lt;/strong&gt;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?auto=webp&amp;s=83e0fd1aa924b9918306c02a99cedb9bbb2eb1cb",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=014ce09ab614e86be0bda115d3ee826dd4c7e72b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9fb10f0400ab7291afbb905ab3dfdfb49e477ed8",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=88b160d056e65a5fdd1da13d608db9a9c123e2d7",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=54fe70e1d1e50ac63262c7c7180e0173f9cc1673",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fa0d50402090bd3ad6e9c270f0f950421a2c1523",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dff2cd4e982c9356a88ce61af693c4ca57815b99",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfaigh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "citaman",
          "discussion_type": null,
          "num_comments": 76,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754088000,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_144o7g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI models are picking up hidden habits from each other | IBM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfll39",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": "#bbbdbf",
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=9f10577c2fcc3a35df02ac054b4815c973b08f82",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754123731,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "ibm.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.ibm.com/think/news/ai-models-subliminal-learning",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?auto=webp&amp;s=ffc26910e336a76ab6db1af971b2d262dbce6146",
                  "width": 1280,
                  "height": 720
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2723bb45305d3a150f76e1937c51a2690147d015",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=30aa75a6f3b7183ee11ff8f6294344790b27c98c",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d4ee8618c7e067b6087a6fbd41b990eb083b72f",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=46e4049e81a052bc62430cfe7e667f62662c693b",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ec61744cbe7d95092017e4c3bdcff62c20454303",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a031cd428aa62cc550e88505f888b329ed3ed9cf",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfll39",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ab2377",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfll39/ai_models_are_picking_up_hidden_habits_from_each/",
          "stickied": false,
          "url": "https://www.ibm.com/think/news/ai-models-subliminal-learning",
          "subreddit_subscribers": 508769,
          "created_utc": 1754123731,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The \"Leaked\" 120B OpenAI Model Is Trained In FP4\n",
          "author_fullname": "t2_9zkdy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The “Leaked” 120 B OpenAI Model is not Trained in FP4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 98,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf3tm9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 354,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 354,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/D9ovYsMF-MnoR2CRSeSS8Yh_VU6bc2G4S1R5gIz7WiE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754071895,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The &amp;quot;Leaked&amp;quot; 120B OpenAI Model Is Trained In FP4&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/g1yk8r6b8ggf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?auto=webp&amp;s=af4bbedc766a4ee5a39037f2ab17d7b5501cd231",
                  "width": 1290,
                  "height": 906
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=926137a58fce6f1ef8bee443ff019ae18b337863",
                    "width": 108,
                    "height": 75
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e501be0974ffc2db9bd5cda1678b62434d903b5",
                    "width": 216,
                    "height": 151
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f48501cc5bd5fb8e0b44c8d1575f9a4f16b061b",
                    "width": 320,
                    "height": 224
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd4ab4d6c8195a6e7189dc0435de525dd356fb06",
                    "width": 640,
                    "height": 449
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=002c3d3feadb04f86a25d1efd1608fcdeb907ef0",
                    "width": 960,
                    "height": 674
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f1b63bb57ddbe4e7a32c5af91d1f065caf81082f",
                    "width": 1080,
                    "height": 758
                  }
                ],
                "variants": {},
                "id": "xtQ4De0a5tVgoxoyHlb2WUymp023nocly_no7XupZ6k"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mf3tm9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "badbutt21",
          "discussion_type": null,
          "num_comments": 84,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf3tm9/the_leaked_120_b_openai_model_is_not_trained_in/",
          "stickied": false,
          "url": "https://i.redd.it/g1yk8r6b8ggf1.jpeg",
          "subreddit_subscribers": 508769,
          "created_utc": 1754071895,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "MAESTRO is a self-hosted AI application designed to streamline the research and writing process. It integrates a powerful document management system with two distinct operational modes: Research Mode (like deep research) and Writing Mode (AI assisted writing).\n\n# Autonomous Research Mode\n\nIn this mode, the application automates research tasks for you.\n\n* **Process**: You start by giving it a research question or a topic.\n* **Action**: The AI then searches for information in your uploaded documents or on the web.\n* **Output**: Based on what it finds, the AI generates organized notes and then writes a full research report.\n\nThis mode is useful when you need to quickly gather information on a topic or create a first draft of a document.\n\n# AI-Assisted Writing Mode\n\nThis mode provides help from an AI while you are writing.\n\n* **Interface**: It consists of a markdown text editor next to an AI chat window.\n* **Workflow**: You can write in the editor and ask the AI questions at the same time. The AI can access your document collections and the web to find answers.\n* **Function**: The AI provides the information you request in the chat window, which you can then use in the document you are writing.\n\nThis mode allows you to get research help without needing to leave your writing environment.\n\n# Document Management\n\nThe application is built around a document management system.\n\n* **Functionality**: You can upload your documents (currently only PDFs) and group them into \"folders.\"\n* **Purpose**: These collections serve as a specific knowledge base for your projects. You can instruct the AI in either mode to use only the documents within a particular collection, ensuring its work is based on the source materials you provide.",
          "author_fullname": "t2_281myw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "MAESTRO, a deep research assistant/RAG pipeline that runs on your local LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 106,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "wfye5hh28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=263452376481ede5f5ee30b3d1fa8d06b92acb2f"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f6985e77e3d428df5e2fae627ebdaccf43636351"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=36eabba987901a939cb1cba3fa220c59244f5980"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=54358ea98fe46953da35a682104b2d28c041974a"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0fce8a0da83bbecc8646448132fc88235fc0cc6f"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=41259159b0776715b20abd50ed3f735ab55ec141"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=defebde57be1e79d81fad254f827317795a27064"
              },
              "id": "wfye5hh28hgf1"
            },
            "vdsr7ch28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bdacf777510be4c1c1e2cb42159066c1880e6dab"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c356b2b93449bd2d60ae1d426ad98a061cd4282"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae0474fcb61f8fc562cb20c1d53c97e12decaa3c"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=32dbcb0b4526b601435315f9f9ad55eac8e9e9f1"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=54e5b3229d01b08c97d666d27a878563ef0471ae"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e566592ef69e058c675213f1b88982c9f112e66b"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=6a56c58428afe63b1a658254915645c82408e1ce"
              },
              "id": "vdsr7ch28hgf1"
            },
            "4tvg8ah28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=388d6a0156beacf091be4cabfc6a12ef9b2df0e0"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab7ae93af73319fe2da856256040e6bb81700659"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=36457622c836252465883bf076811b25d1ad38a2"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=60ee85a9f73c317a448f5f7a4a39ee239c5eed29"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a6a9af626a4376d663d8cf2d5ce76f802eb0d173"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=726f4a6fca4e608a57b66118d9f5186788c5d1b8"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=ace0180e9725fd088236b603c03e703e138e1af1"
              },
              "id": "4tvg8ah28hgf1"
            },
            "tjfnu7h28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=65253585bbce7fb9495344864b4dbf089fe4a866"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab36743c53f66ceaa0b69bd5950db8c4c34979cd"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1946f554204091e4ee226eecbc6b9b75297b047d"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb8016f74c1a4927f5217225504f99e78266bd6d"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6fff118076daf8093a4c9a0e956b59d667d8a17a"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5073f2ce72eef58984bea03e338686a824f604b1"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=16d847bb68c7a49b063970c2a53144217a358b64"
              },
              "id": "tjfnu7h28hgf1"
            },
            "ko10neh28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33c335bff8fd3207fbe0ef4cb7e045a625ad3790"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4245d285c8e6e167c8aa5f8a8196f6f0f79234b0"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ec4b568dd2d51940205e41d3a1168db82a30974"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bcb8c81cb1083135981d3576d1e94c4d7061ef1"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e3f2a7685b2f5d071bd47a85f2da0b70360fa499"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=379b494f484f127cc25b662a1a5f9a3753470d94"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/ko10neh28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=de810a3b9c7f9df343cc5c34d57dd4752a8b191d"
              },
              "id": "ko10neh28hgf1"
            },
            "xpft85h28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8b16877882ed90f113b1d8753896a42c5c1cb57"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d6125a00674b9020cb681a99893ad47b47e7913"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7cb747f3b0be9eca90601bb0dd0e4f364c7ebc2b"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd1b23bc33d392f682dd70713ef0e6770cafd430"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=43a97bc0e6dad1aeeb47843446cd709c83f052db"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3927f27a929df1bb9fa672c58a3616cffaa32c5d"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/xpft85h28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=a8d61da6b389396eadf3f74a5e84046b8a859996"
              },
              "id": "xpft85h28hgf1"
            },
            "n8f3d7h28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0099db17c53d752e1f52434bb4a16edfe724a7d1"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7b068877d833bcda5b55ab2245068d9b2e0148e"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e3c098055fc230578e3b0d4cf171776999a25ae"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=124fc23c5296c23dc74d6a6c55ce26db75d2d137"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=16d4bec4c048c58090daaad2dfcaf34e92f941cb"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4be81c901606198bf5e6e6c8097ebe64e6a6d39c"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=e915bfa4f37d301877e87b2951c78762e6a8ea17"
              },
              "id": "n8f3d7h28hgf1"
            },
            "9gzp5ch28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=74b4696764dfc847eef669961f79b0866d42f5d9"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=168ad0f4c7c2dbc7f867cf7085cff2fc27e24bc3"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d89d57702d0ba973c5772c39da3b51e2e6b5da5"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f80ab136ac496f933eb22700bd7e0c0cfabe288"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=978a98d62c65f1d83a8464507a025600df74f898"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5124a214a0cad85638935b440b2b58a3587824c6"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=f958ce1ae3061d6b0c94c35b4636b3aa5e8260d7"
              },
              "id": "9gzp5ch28hgf1"
            },
            "otdkteh28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=055054f2a3a59c53fa172087439b7f25945578f3"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63baa369d4fd7095a2c702fb261aebb398248ee5"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=993e59ad6c2ab02adfae15d2fbd66ec726a4bd13"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e5d52a002ea01c6b7d0bfa10e537c7d7969f822"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=49dcb5b218991d6cfc81b7dea2cdd7b72a730add"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=64546f9c94b5f46090ccd646df96e7d3c4477044"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/otdkteh28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=2f6b24cf0547caddaa5f756f6cd6778238257d37"
              },
              "id": "otdkteh28hgf1"
            }
          },
          "name": "t3_1mf92r1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#c7b594",
          "ups": 173,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "Deep Research Draft",
                "media_id": "wfye5hh28hgf1",
                "id": 719101200
              },
              {
                "caption": "Writing Draft with chat pulling data from your documents as well as the internet",
                "media_id": "ko10neh28hgf1",
                "id": 719101201
              },
              {
                "caption": "Write in markdown",
                "media_id": "vdsr7ch28hgf1",
                "id": 719101202
              },
              {
                "caption": "Make document folders to use with your research/writing projects",
                "media_id": "9gzp5ch28hgf1",
                "id": 719101203
              },
              {
                "caption": "Manage documents",
                "media_id": "4tvg8ah28hgf1",
                "id": 719101204
              },
              {
                "caption": "Deep dive into the Deep Researcher outputs like notes prepped from your sources",
                "media_id": "n8f3d7h28hgf1",
                "id": 719101205
              },
              {
                "caption": "Comprehensive research flow with iterative action/reflection loops",
                "media_id": "otdkteh28hgf1",
                "id": 719101206
              },
              {
                "caption": "Complete transparency in the model of your choice's reasoning and performance",
                "media_id": "tjfnu7h28hgf1",
                "id": 719101207
              },
              {
                "caption": "Complete transparency in the model of your choice's reasoning and performance",
                "media_id": "xpft85h28hgf1",
                "id": 719101208
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 173,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Tt0ml3YBBqO4cJ7-sHxE5os9lg6KgXNM6oovDynmETQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754084338,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MAESTRO is a self-hosted AI application designed to streamline the research and writing process. It integrates a powerful document management system with two distinct operational modes: Research Mode (like deep research) and Writing Mode (AI assisted writing).&lt;/p&gt;\n\n&lt;h1&gt;Autonomous Research Mode&lt;/h1&gt;\n\n&lt;p&gt;In this mode, the application automates research tasks for you.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: You start by giving it a research question or a topic.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: The AI then searches for information in your uploaded documents or on the web.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Based on what it finds, the AI generates organized notes and then writes a full research report.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This mode is useful when you need to quickly gather information on a topic or create a first draft of a document.&lt;/p&gt;\n\n&lt;h1&gt;AI-Assisted Writing Mode&lt;/h1&gt;\n\n&lt;p&gt;This mode provides help from an AI while you are writing.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Interface&lt;/strong&gt;: It consists of a markdown text editor next to an AI chat window.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Workflow&lt;/strong&gt;: You can write in the editor and ask the AI questions at the same time. The AI can access your document collections and the web to find answers.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Function&lt;/strong&gt;: The AI provides the information you request in the chat window, which you can then use in the document you are writing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This mode allows you to get research help without needing to leave your writing environment.&lt;/p&gt;\n\n&lt;h1&gt;Document Management&lt;/h1&gt;\n\n&lt;p&gt;The application is built around a document management system.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Functionality&lt;/strong&gt;: You can upload your documents (currently only PDFs) and group them into &amp;quot;folders.&amp;quot;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: These collections serve as a specific knowledge base for your projects. You can instruct the AI in either mode to use only the documents within a particular collection, ensuring its work is based on the source materials you provide.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mf92r1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf92r1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hedonihilistic",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mf92r1/maestro_a_deep_research_assistantrag_pipeline/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mf92r1",
          "subreddit_subscribers": 508769,
          "created_utc": 1754084338,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hg：https://huggingface.co/ScienceOne-AI/S1-Base-671B",
          "author_fullname": "t2_44shmmed9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China report the finetune deepseek scientific model 40.44% on HLE",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf8pdo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 176,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 176,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/UTfHZTk3AkcFKdrIagp_47SxVLFGVmG24S66FaWQdXU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754083417,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hg：&lt;a href=\"https://huggingface.co/ScienceOne-AI/S1-Base-671B\"&gt;https://huggingface.co/ScienceOne-AI/S1-Base-671B&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/rnyzqia76hgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?auto=webp&amp;s=06740559a06a04129cbe49eca027a0862fd98c3b",
                  "width": 1927,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=71982e38120d9120577e53d8fabd9588d8007e4b",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1b276889bf7a37bf56f4c27a131188e43815865c",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc9bc3ff25403be175e671f34f1139f244fb6d61",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=19933808cef3cec6dce268be3e9d5d269f435579",
                    "width": 640,
                    "height": 358
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fcd174de9a930fac1ee5ef6231c6a4a6d70fd332",
                    "width": 960,
                    "height": 538
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4f6e3196a3a315fef89102429c2a9e280f5ad273",
                    "width": 1080,
                    "height": 605
                  }
                ],
                "variants": {},
                "id": "l_Dz9A7On8qHXdUNNBTW3zJ0Gj8IkayixkIAvRolxWc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mf8pdo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Afraid_Hall_2971",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf8pdo/china_report_the_finetune_deepseek_scientific/",
          "stickied": false,
          "url": "https://i.redd.it/rnyzqia76hgf1.jpeg",
          "subreddit_subscribers": 508769,
          "created_utc": 1754083417,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\nHeads up to anyone considering Cerebras. This is my conclusion of today's top post that is now deleted... I bought it to try it out and wanted to report back on what I saw.\n\nThe marketing is misleading. While they advertise a 1,000-request limit, the actual daily constraint is a 7.5 million-token limit. This isn't mentioned anywhere before you purchase, and it feels like a bait and switch. I hit this token limit in only 300 requests, not the 1,000 they suggest is the daily cap. They also say in there FAQs at the very bottom of the page, updated 3 hours ago. That a request is based off of 8k tokens which is incredibly small for a coding centric API.",
          "author_fullname": "t2_czmwiot",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cerebras Pro Coder Deceptive Limits",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfeazc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 78,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 78,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754098861,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Heads up to anyone considering Cerebras. This is my conclusion of today&amp;#39;s top post that is now deleted... I bought it to try it out and wanted to report back on what I saw.&lt;/p&gt;\n\n&lt;p&gt;The marketing is misleading. While they advertise a 1,000-request limit, the actual daily constraint is a 7.5 million-token limit. This isn&amp;#39;t mentioned anywhere before you purchase, and it feels like a bait and switch. I hit this token limit in only 300 requests, not the 1,000 they suggest is the daily cap. They also say in there FAQs at the very bottom of the page, updated 3 hours ago. That a request is based off of 8k tokens which is incredibly small for a coding centric API.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfeazc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "snipsthekittycat",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfeazc/cerebras_pro_coder_deceptive_limits/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfeazc/cerebras_pro_coder_deceptive_limits/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754098861,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just because you are hosting locally, doesn't mean your LLM agent is necessarily private. I wrote a blog about how LLMs can be fine-tuned to execute malicious tool calls with popular MCP servers. I included links to the code and dataset in the article. Enjoy!",
          "author_fullname": "t2_kfjfm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DoubleAgents: Fine-tuning LLMs for Covert Malicious Tool Calls",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 76,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfbw8a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 86,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 86,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=140&amp;height=76&amp;crop=140:76,smart&amp;auto=webp&amp;s=178741032ad68bb72212a1f0482ccf59165855d7",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754091780,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "medium.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just because you are hosting locally, doesn&amp;#39;t mean your LLM agent is necessarily private. I wrote a blog about how LLMs can be fine-tuned to execute malicious tool calls with popular MCP servers. I included links to the code and dataset in the article. Enjoy!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://medium.com/@justin_45141/doubleagents-fine-tuning-llms-for-covert-malicious-tool-calls-b8ff00bf513e",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?auto=webp&amp;s=db6f154eef502137479106eb0ee5e3497a2c8a5d",
                  "width": 1024,
                  "height": 559
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6fc30b818f499ebfea16a1a44bc05f5b89c31100",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d8f36d4f360255f41bf1da0fda787ab734a46fbd",
                    "width": 216,
                    "height": 117
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ad1e44aa12420eb39ba559c3fdbad4ad0407e0b",
                    "width": 320,
                    "height": 174
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=24b8b3213823bb044c73076e1852d1957545a17f",
                    "width": 640,
                    "height": 349
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4cb31249007f2a139fccd496d749e3a58bdc7c0d",
                    "width": 960,
                    "height": 524
                  }
                ],
                "variants": {},
                "id": "1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfbw8a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JAlbrethsen",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfbw8a/doubleagents_finetuning_llms_for_covert_malicious/",
          "stickied": false,
          "url": "https://medium.com/@justin_45141/doubleagents-fine-tuning-llms-for-covert-malicious-tool-calls-b8ff00bf513e",
          "subreddit_subscribers": 508769,
          "created_utc": 1754091780,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Dhanishtha-2.0-preview can now tool call. \n\nUpdated Model link:- [https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825](https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825)  \nAPI and Chat page :- [https://helpingai.co](https://helpingai.co)\n\n",
          "author_fullname": "t2_hewf57lw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tool calling is now supported on World's first Intermediate Reasoning model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfn7pv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754130292,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dhanishtha-2.0-preview can now tool call. &lt;/p&gt;\n\n&lt;p&gt;Updated Model link:- &lt;a href=\"https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825\"&gt;https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825&lt;/a&gt;&lt;br/&gt;\nAPI and Chat page :- &lt;a href=\"https://helpingai.co\"&gt;https://helpingai.co&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?auto=webp&amp;s=26ef07178b4b9394a1c2c1b0dfc81de665a07de8",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6528fc720427c7c1ed30f20ddd332d01526c4f8f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bffe53726d3c274e1c28ce6856f9194b9aca7fd2",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=53fe26fae2e2b849fb6ac025e7f99b8738728cd0",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=14d7ff05a119f24e15918fc09489c78087b00a3e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e635b759cb0f70c7db7a222e92c287229ba6fb41",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8a3feb1fc2bfd7f88481601ee7cdf04a9ff9b5ed",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfn7pv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Quiet-Moment-338",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfn7pv/tool_calling_is_now_supported_on_worlds_first/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfn7pv/tool_calling_is_now_supported_on_worlds_first/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754130292,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So firstly, I should mention that my setup is a Lenovo Legion 4090 Laptop, which should be pretty quick to render text &amp; speech - about equivalent to a 4080 Desktop. At least similar in VRAM, Tensors, etc.\n\nI also prefer to use CLI only, because I want everything to eventually be for a robot I'm working on (because of this I don't really want a UI interface). For some I haven't fully tested only the CLI, and for some I've tested both. I will update this post when I do more testing. Also, feel free to recommend any others I should test.\n\nI will say the UI counterpart can be quite a bit quicker than using CLI linked with an ollama model. With that being said, here's my personal \"rankings\".\n\n* **Bark/Coqui TTS -**\n   * **The Good:** The emotions are next level... kinda. At least they have it, is the main thing. What I've done is create a custom Llama model, that knows when to send a \\[laughs\\], \\[sighs\\], etc. that's appropriate, given the conversation. The custom ollama model is pretty good at this (if you're curious how to do this as well you can create a basefile and a modelfile). And it sounds somewhat human. But at least it can somewhat mimic human emotions a little, which many cannot.\n   * **The Bad:** It's pretty slow. Sometimes takes up to 30 seconds to a minute which is pretty undoable, given I want my robot to have fluid conversation. I will note that none of them are able to do it seconds or less, sadly, via CLI, but one was for UI. It also \"trails off\", if that makes sense. Meaning - the ollama may produce a text, and the Bark/Coqui TTS does not always follow it accurately. I'm using a custom voice model as well, and the cloning, although sometimes okay, can and does switch between male and female characters, and doesn't sometimes even follow the cloned voice. However, when it does, it's somewhat decent. But given how it often does not, it's not really too usable.\n* **F5 TTS -**\n   * **The Good:** Extremely consistent voice cloning, from the UI and CLI. I will say that the UI is a bit faster than using CLI, however, it still takes about 8seconds or so to get a response even with the UI, which is faster than Bark/Coqui, but still not fast enough, for my uses at least. Honestly, the voice cloning alone is very impressive. I'd say it's better than Bark/Coqui, except that Bark/Coqui has the ability to laugh, sigh, etc. But if you value consistent voicing, that's close to and can rival ElevenLabs without paying, this is a great option. Even with the CLI it doesn't trail off. It will finish speaking until the text from my custom ollama model is done being spoken.\n   * **The Bad:** As mentioned, it can take about 8-10 seconds for the UI, but longer for the CLI. I'd say it's about 15 seconds (on average) for the CLI and up to 30 seconds (for about 1.75 minutes of speech) for the CLI, or so depending on how long the text is. The problem is can't do emotions (like laughing, etc) at all. And when I try to use an exclamation mark, it changes the voice quite a bit, where it almost doesn't sound like the same person. If you prompt your ollama model to not use exclamations, it does fine though. It's pretty good, but not perfect.\n* **Orpheus TTS**\n   * **The Good:** This one can also do laughing, yawning, etc. and it's decent at it. But not as good as Coqui/Bark. Although it's still better than what most offer, since it has the ability at all. There's a decent amount of tone in the voice, enough to keep it from sounding too robotic. The voices, although not cloneable, are a lot more consistent than Bark/Coqui, however. They never really deviate like Bark/Coqui did. It also reads all of the text as well and doesn't trail off.\n   * **The Bad:** This one is a pain to set up, at least if you try to go the normal route, via CLI. I've only been able to set it up via Docker, actually, unfortunately. Even in the UI, it takes quite a bit of time to generate text. I'd say about 1 second per 1 second of speech. There also times where certain tags (like yawning) doesn't get picked up, and it just says \"yawn\", instead. Coqui didn't really seem to do that, unless it was a tag that was unrecognizable (sometimes my custom ollama model would generate non-available tags on accident).\n* **Kokoro TTS**\n   * **The Good:** Man, the UI is blazing FAST. If I had to guess about \\~ 1 second or so. And that's using 2-3 sentences. For a about 4 minutes of speech, it takes about 4 seconds to generate text, which although isn't perfect, it's probably as good as it gets and really quick. So about 1 second per 1 minute of speech. Pretty impressive! It also doesn't trail off and reads all the speech too, which is nice.\n   * **The Bad:** It sounds a little bland. Some of the models, even if they don't have explicit emotion tags, still have tone, and this model is lacking there imo. It sounds too robotic to me, and doesn't distinct between exclamation, or questions, much. It's not terrible, but sounds like an average Speech to Text, that you'd find on an average book reader, for example. Also doesn't offer native voice cloning, that I'm aware of at least, but I could be wrong.",
          "author_fullname": "t2_4guqxmy4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "TTS Model Comparisons: My Personal Rankings (So far) of TTS Models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfjn88",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754116331,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So firstly, I should mention that my setup is a Lenovo Legion 4090 Laptop, which should be pretty quick to render text &amp;amp; speech - about equivalent to a 4080 Desktop. At least similar in VRAM, Tensors, etc.&lt;/p&gt;\n\n&lt;p&gt;I also prefer to use CLI only, because I want everything to eventually be for a robot I&amp;#39;m working on (because of this I don&amp;#39;t really want a UI interface). For some I haven&amp;#39;t fully tested only the CLI, and for some I&amp;#39;ve tested both. I will update this post when I do more testing. Also, feel free to recommend any others I should test.&lt;/p&gt;\n\n&lt;p&gt;I will say the UI counterpart can be quite a bit quicker than using CLI linked with an ollama model. With that being said, here&amp;#39;s my personal &amp;quot;rankings&amp;quot;.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Bark/Coqui TTS -&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; The emotions are next level... kinda. At least they have it, is the main thing. What I&amp;#39;ve done is create a custom Llama model, that knows when to send a [laughs], [sighs], etc. that&amp;#39;s appropriate, given the conversation. The custom ollama model is pretty good at this (if you&amp;#39;re curious how to do this as well you can create a basefile and a modelfile). And it sounds somewhat human. But at least it can somewhat mimic human emotions a little, which many cannot.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; It&amp;#39;s pretty slow. Sometimes takes up to 30 seconds to a minute which is pretty undoable, given I want my robot to have fluid conversation. I will note that none of them are able to do it seconds or less, sadly, via CLI, but one was for UI. It also &amp;quot;trails off&amp;quot;, if that makes sense. Meaning - the ollama may produce a text, and the Bark/Coqui TTS does not always follow it accurately. I&amp;#39;m using a custom voice model as well, and the cloning, although sometimes okay, can and does switch between male and female characters, and doesn&amp;#39;t sometimes even follow the cloned voice. However, when it does, it&amp;#39;s somewhat decent. But given how it often does not, it&amp;#39;s not really too usable.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;F5 TTS -&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; Extremely consistent voice cloning, from the UI and CLI. I will say that the UI is a bit faster than using CLI, however, it still takes about 8seconds or so to get a response even with the UI, which is faster than Bark/Coqui, but still not fast enough, for my uses at least. Honestly, the voice cloning alone is very impressive. I&amp;#39;d say it&amp;#39;s better than Bark/Coqui, except that Bark/Coqui has the ability to laugh, sigh, etc. But if you value consistent voicing, that&amp;#39;s close to and can rival ElevenLabs without paying, this is a great option. Even with the CLI it doesn&amp;#39;t trail off. It will finish speaking until the text from my custom ollama model is done being spoken.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; As mentioned, it can take about 8-10 seconds for the UI, but longer for the CLI. I&amp;#39;d say it&amp;#39;s about 15 seconds (on average) for the CLI and up to 30 seconds (for about 1.75 minutes of speech) for the CLI, or so depending on how long the text is. The problem is can&amp;#39;t do emotions (like laughing, etc) at all. And when I try to use an exclamation mark, it changes the voice quite a bit, where it almost doesn&amp;#39;t sound like the same person. If you prompt your ollama model to not use exclamations, it does fine though. It&amp;#39;s pretty good, but not perfect.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Orpheus TTS&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; This one can also do laughing, yawning, etc. and it&amp;#39;s decent at it. But not as good as Coqui/Bark. Although it&amp;#39;s still better than what most offer, since it has the ability at all. There&amp;#39;s a decent amount of tone in the voice, enough to keep it from sounding too robotic. The voices, although not cloneable, are a lot more consistent than Bark/Coqui, however. They never really deviate like Bark/Coqui did. It also reads all of the text as well and doesn&amp;#39;t trail off.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; This one is a pain to set up, at least if you try to go the normal route, via CLI. I&amp;#39;ve only been able to set it up via Docker, actually, unfortunately. Even in the UI, it takes quite a bit of time to generate text. I&amp;#39;d say about 1 second per 1 second of speech. There also times where certain tags (like yawning) doesn&amp;#39;t get picked up, and it just says &amp;quot;yawn&amp;quot;, instead. Coqui didn&amp;#39;t really seem to do that, unless it was a tag that was unrecognizable (sometimes my custom ollama model would generate non-available tags on accident).&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Kokoro TTS&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; Man, the UI is blazing FAST. If I had to guess about ~ 1 second or so. And that&amp;#39;s using 2-3 sentences. For a about 4 minutes of speech, it takes about 4 seconds to generate text, which although isn&amp;#39;t perfect, it&amp;#39;s probably as good as it gets and really quick. So about 1 second per 1 minute of speech. Pretty impressive! It also doesn&amp;#39;t trail off and reads all the speech too, which is nice.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; It sounds a little bland. Some of the models, even if they don&amp;#39;t have explicit emotion tags, still have tone, and this model is lacking there imo. It sounds too robotic to me, and doesn&amp;#39;t distinct between exclamation, or questions, much. It&amp;#39;s not terrible, but sounds like an average Speech to Text, that you&amp;#39;d find on an average book reader, for example. Also doesn&amp;#39;t offer native voice cloning, that I&amp;#39;m aware of at least, but I could be wrong.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfjn88",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "iKontact",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfjn88/tts_model_comparisons_my_personal_rankings_so_far/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfjn88/tts_model_comparisons_my_personal_rankings_so_far/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754116331,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’d like to start a small art project and I’m looking for a model that speaks German well. I’m currently using Gemma 3n:e4b and I’m quite satisfied with it. However, I’d like to know if there are any other models of a similar size that have even better German language capabilities. The whole thing should be run with Ollama on a PC with a maximum of 8GB of VRAM – ideally no more than 6GB.",
          "author_fullname": "t2_1tcpn4d5tw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Small LLM in german",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfldxj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754122940,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’d like to start a small art project and I’m looking for a model that speaks German well. I’m currently using Gemma 3n:e4b and I’m quite satisfied with it. However, I’d like to know if there are any other models of a similar size that have even better German language capabilities. The whole thing should be run with Ollama on a PC with a maximum of 8GB of VRAM – ideally no more than 6GB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfldxj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ghulaschsuppe",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754122940,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Beta seems really solid from early testing, not a magnitude better than what SOTA's offer but still impressive",
          "author_fullname": "t2_7tlxcyy6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Alpha vs Horizon Beta",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfgwyu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 25,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/dg8cy7ia4jgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1786,
              "scrubber_media_url": "https://v.redd.it/dg8cy7ia4jgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/dg8cy7ia4jgf1/DASHPlaylist.mpd?a=1756729624%2CYjQ5ZDhmN2I2MGZhZTU4MTcyNjViZTQ3NmM3MGNmZjNlYWExOGY3ODFmMzU1ZTlmYzZlMmE0YTI1YzI4OTcwZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 69,
              "hls_url": "https://v.redd.it/dg8cy7ia4jgf1/HLSPlaylist.m3u8?a=1756729624%2COTAzNGE5NzYxMjBiOGYyZDVmOWQ4NGI0MDVlNmQwZTFkYzBmMTdjMzFlN2ZiZTdjODRkZjU1YmQzZDQ5MmVkNg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=140&amp;height=84&amp;crop=140:84,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=dd582f91752e7fd7a72f3be30d66c628b1cabf42",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754106912,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Beta seems really solid from early testing, not a magnitude better than what SOTA&amp;#39;s offer but still impressive&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/dg8cy7ia4jgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?format=pjpg&amp;auto=webp&amp;s=30b632f2cdd42a3234c72ef686841eed57c63aec",
                  "width": 3600,
                  "height": 2178
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d42e84556d577f46b8f2cc66b308d63c6ba3aa39",
                    "width": 108,
                    "height": 65
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0a342e02b97eb15e7ae8d55711f5dc21ed403177",
                    "width": 216,
                    "height": 130
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1c923808071843a8b872aa37e16f734fbae495b3",
                    "width": 320,
                    "height": 193
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bbb1ef3633d6485f7f54280916bb8b15600f3044",
                    "width": 640,
                    "height": 387
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d1f330f77b0817cdba51f914a124707d5bffe8a1",
                    "width": 960,
                    "height": 580
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e9a786dc189b4a8425437a29d560c6d7825e64a0",
                    "width": 1080,
                    "height": 653
                  }
                ],
                "variants": {},
                "id": "d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfgwyu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sirjoaco",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfgwyu/horizon_alpha_vs_horizon_beta/",
          "stickied": false,
          "url": "https://v.redd.it/dg8cy7ia4jgf1",
          "subreddit_subscribers": 508769,
          "created_utc": 1754106912,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/dg8cy7ia4jgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1786,
              "scrubber_media_url": "https://v.redd.it/dg8cy7ia4jgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/dg8cy7ia4jgf1/DASHPlaylist.mpd?a=1756729624%2CYjQ5ZDhmN2I2MGZhZTU4MTcyNjViZTQ3NmM3MGNmZjNlYWExOGY3ODFmMzU1ZTlmYzZlMmE0YTI1YzI4OTcwZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 69,
              "hls_url": "https://v.redd.it/dg8cy7ia4jgf1/HLSPlaylist.m3u8?a=1756729624%2COTAzNGE5NzYxMjBiOGYyZDVmOWQ4NGI0MDVlNmQwZTFkYzBmMTdjMzFlN2ZiZTdjODRkZjU1YmQzZDQ5MmVkNg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been having a lot of fun playing around with the new Qwen coder as a 100% local agentic coding. A lot of going on with in the demo above: \n\n- Roo Code with [Unsloth Qwen3 Coder 30B Q8](https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF)\n- [llama-swap](https://github.com/mostlygeek/llama-swap) with new Activity page with real time updates. \n- [VibeCities MCP server](https://github.com/mostlygeek/vibecities) for hosting the pages\n- Dual 3090s with Q8 gives about 50 tok/sec to 55 tok/sec. The UD Q4_K_XL quant was not able to one shot the spinning pentagon. \n\nHere's my llama-swap config: \n\n```\nmacros:\n  \"qwen3-coder-server\": |\n    /path/to/llama-server/llama-server-latest\n    --host 127.0.0.1 --port ${PORT}\n    --flash-attn -ngl 999 -ngld 999\n    --no-mmap\n    --cache-type-k q8_0 --cache-type-v q8_0\n    --temp 0.7 --top-k 20 --top-p 0.8 --repeat_penalty 1.05\n    --jinja\n    --swa-full\n\nmodels:\n  \"Q3-30B-CODER-3090\":\n    env:\n      - \"CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10\"\n    name: \"Qwen3 30B Coder Dual 3090 (Q3-30B-CODER-3090)\"\n    description: \"Q8_K_XL, 180K context, 2x3090\"\n    filters:\n      # enforce recommended params for model\n      strip_params: \"temperature, top_k, top_p, repeat_penalty\"\n    cmd: |\n      ${qwen3-coder-server}\n      --model /path/to/models/Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf\n      --ctx-size 184320\n      # rebalance layers/context a bit better across dual GPUs\n      --tensor-split 46,54\n```\n\nRoo code MCP settings: \n\n```\n{\n  \"mcpServers\": {\n    \"vibecities\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"http://10.0.1.173:8888/mcp\",\n      \"headers\": {\n        \"X-API-Key\": \"your-secure-api-key\"\n      },\n      \"alwaysAllow\": [\n        \"page_list\",\n        \"page_set\",\n        \"page_get\"\n      ],\n      \"disabled\": false\n    }\n  }\n}\n```\n\n\n\n",
          "author_fullname": "t2_11gh93nhos",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "All local Roo Code and qwen3 coder 30B Q8",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfariy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#bbbdbf",
          "ups": 57,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g5aj1csfjhgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g5aj1csfjhgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g5aj1csfjhgf1/DASHPlaylist.mpd?a=1756729624%2COWUwY2NmMjQ0MmZjMDk2ZDcxYmU0NzRlODRiNjViZDUzNzNlMTRhNGRmOGI3MmFjMmIyOTdjMDcwNDJlOThiMw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 158,
              "hls_url": "https://v.redd.it/g5aj1csfjhgf1/HLSPlaylist.m3u8?a=1756729624%2CM2U2N2M2YmMzNzExMWM5YTZhMmUzNzY4ZmMyNGVhY2M0NGIzMGRhODg5Y2E4YTBhMDkyODIyZmIxNGZmYzcyMQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 57,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=140&amp;height=87&amp;crop=140:87,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=96e2057bb1f1ef1e9f3beb1d9a28a9ccd4dcaa6b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754088672,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been having a lot of fun playing around with the new Qwen coder as a 100% local agentic coding. A lot of going on with in the demo above: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Roo Code with &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF\"&gt;Unsloth Qwen3 Coder 30B Q8&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/mostlygeek/llama-swap\"&gt;llama-swap&lt;/a&gt; with new Activity page with real time updates. &lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/mostlygeek/vibecities\"&gt;VibeCities MCP server&lt;/a&gt; for hosting the pages&lt;/li&gt;\n&lt;li&gt;Dual 3090s with Q8 gives about 50 tok/sec to 55 tok/sec. The UD Q4_K_XL quant was not able to one shot the spinning pentagon. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here&amp;#39;s my llama-swap config: &lt;/p&gt;\n\n&lt;p&gt;```\nmacros:\n  &amp;quot;qwen3-coder-server&amp;quot;: |\n    /path/to/llama-server/llama-server-latest\n    --host 127.0.0.1 --port ${PORT}\n    --flash-attn -ngl 999 -ngld 999\n    --no-mmap\n    --cache-type-k q8_0 --cache-type-v q8_0\n    --temp 0.7 --top-k 20 --top-p 0.8 --repeat_penalty 1.05\n    --jinja\n    --swa-full&lt;/p&gt;\n\n&lt;p&gt;models:\n  &amp;quot;Q3-30B-CODER-3090&amp;quot;:\n    env:\n      - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10&amp;quot;\n    name: &amp;quot;Qwen3 30B Coder Dual 3090 (Q3-30B-CODER-3090)&amp;quot;\n    description: &amp;quot;Q8_K_XL, 180K context, 2x3090&amp;quot;\n    filters:\n      # enforce recommended params for model\n      strip_params: &amp;quot;temperature, top_k, top_p, repeat_penalty&amp;quot;\n    cmd: |\n      ${qwen3-coder-server}\n      --model /path/to/models/Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf\n      --ctx-size 184320\n      # rebalance layers/context a bit better across dual GPUs\n      --tensor-split 46,54\n```&lt;/p&gt;\n\n&lt;p&gt;Roo code MCP settings: &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n{\n  &amp;quot;mcpServers&amp;quot;: {\n    &amp;quot;vibecities&amp;quot;: {\n      &amp;quot;type&amp;quot;: &amp;quot;streamable-http&amp;quot;,\n      &amp;quot;url&amp;quot;: &amp;quot;http://10.0.1.173:8888/mcp&amp;quot;,\n      &amp;quot;headers&amp;quot;: {\n        &amp;quot;X-API-Key&amp;quot;: &amp;quot;your-secure-api-key&amp;quot;\n      },\n      &amp;quot;alwaysAllow&amp;quot;: [\n        &amp;quot;page_list&amp;quot;,\n        &amp;quot;page_set&amp;quot;,\n        &amp;quot;page_get&amp;quot;\n      ],\n      &amp;quot;disabled&amp;quot;: false\n    }\n  }\n}\n&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/g5aj1csfjhgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?format=pjpg&amp;auto=webp&amp;s=a799694f3cd2a8d09be3eac7cc9981be88d234a1",
                  "width": 1920,
                  "height": 1197
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7b56a58c7f6f027ee7357cad95a460ff999afeea",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5adfd5aba83e12c59bf4648d145f4ab40fd5648e",
                    "width": 216,
                    "height": 134
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4be2e0e3ac56fa7c11be8c2c58c9a02a90039429",
                    "width": 320,
                    "height": 199
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c11415fad487d232adecf2767cc5b4b8ac2ab42f",
                    "width": 640,
                    "height": 399
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fd1f305092aa1774fdb5bb8d64c33f4d4acc5781",
                    "width": 960,
                    "height": 598
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a0c4665784dbdfe56aa44706a6b06cd7fb427df9",
                    "width": 1080,
                    "height": 673
                  }
                ],
                "variants": {},
                "id": "OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfariy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Statement-0001",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfariy/all_local_roo_code_and_qwen3_coder_30b_q8/",
          "stickied": false,
          "url": "https://v.redd.it/g5aj1csfjhgf1",
          "subreddit_subscribers": 508769,
          "created_utc": 1754088672,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g5aj1csfjhgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g5aj1csfjhgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g5aj1csfjhgf1/DASHPlaylist.mpd?a=1756729624%2COWUwY2NmMjQ0MmZjMDk2ZDcxYmU0NzRlODRiNjViZDUzNzNlMTRhNGRmOGI3MmFjMmIyOTdjMDcwNDJlOThiMw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 158,
              "hls_url": "https://v.redd.it/g5aj1csfjhgf1/HLSPlaylist.m3u8?a=1756729624%2CM2U2N2M2YmMzNzExMWM5YTZhMmUzNzY4ZmMyNGVhY2M0NGIzMGRhODg5Y2E4YTBhMDkyODIyZmIxNGZmYzcyMQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Couldn't find a direct comparison between the M1 Macbook pro and the new RTX 5060 Ti for local LLM inference. So, I decided to run a 16 small benchmark myself, and I think the results will be useful for others in the same boat.\n\nI ran a quick benchmark on the RTX 5060 Ti 16GB, and I'm quite impressed with the results, especially coming from my M1 Macbook pro with 16GB ram. \nI used the Qwen3 8B model with Ollama to test the performance, and I've also included the RTX 4090 results for a broader comparison.\nI'm also planning to run some fine-tuning benchmarks later.",
          "author_fullname": "t2_a85vzco5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Benchmarking Qwen3 8B Inference: M1 vs RTX 5060 Ti 16 vs RTX 4090",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 57,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mfnq2r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/YkVLBy_Uabot6hZJtvt-2lgE4cyPGdL7zFQtJSHp-Xk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754132261,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Couldn&amp;#39;t find a direct comparison between the M1 Macbook pro and the new RTX 5060 Ti for local LLM inference. So, I decided to run a 16 small benchmark myself, and I think the results will be useful for others in the same boat.&lt;/p&gt;\n\n&lt;p&gt;I ran a quick benchmark on the RTX 5060 Ti 16GB, and I&amp;#39;m quite impressed with the results, especially coming from my M1 Macbook pro with 16GB ram. \nI used the Qwen3 8B model with Ollama to test the performance, and I&amp;#39;ve also included the RTX 4090 results for a broader comparison.\nI&amp;#39;m also planning to run some fine-tuning benchmarks later.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/erib4a6t7lgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?auto=webp&amp;s=5c7238d3074b0c94ed28a56fd05d130ca36e4062",
                  "width": 1280,
                  "height": 525
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8aa6f94f1fd0b07761158c32a4f411fea4ff01e",
                    "width": 108,
                    "height": 44
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4fd7073605b2305ef0bdfa02c28ed14504e5cadf",
                    "width": 216,
                    "height": 88
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f51b098c5db731c8b7427cd4c5e5918907eaf170",
                    "width": 320,
                    "height": 131
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=78f7c7660b84535a6ababa69d8821a1d6acfd96f",
                    "width": 640,
                    "height": 262
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f3fce6514c1c2d99e8304633aabf8e744412a87",
                    "width": 960,
                    "height": 393
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4087fbfd9cd44fbf60685b1fe8cf9814b669d42c",
                    "width": 1080,
                    "height": 442
                  }
                ],
                "variants": {},
                "id": "tc9bMPtFnkPdsr6iuVPGSgsfnHwUyG7fExgbegYm3H4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfnq2r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kargafe",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/",
          "stickied": false,
          "url": "https://i.redd.it/erib4a6t7lgf1.jpeg",
          "subreddit_subscribers": 508769,
          "created_utc": 1754132261,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_dphk4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-2507 is the top open weights model on lmarena",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf0qlf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 177,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 177,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754064880,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/lmarena_ai/status/1951308670375174457",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mf0qlf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tarruda",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf0qlf/qwen3235ba22b2507_is_the_top_open_weights_model/",
          "stickied": false,
          "url": "https://x.com/lmarena_ai/status/1951308670375174457",
          "subreddit_subscribers": 508769,
          "created_utc": 1754064880,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_xq83l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta - new openai open source model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfda7s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=e6bc83d22983565c851331294c37a95f480af7fa",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754095791,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "openrouter.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://openrouter.ai/openrouter/horizon-beta",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?auto=webp&amp;s=76311bc0d854d91946fad4dd34c15d2aabd68203",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f17bb8ad3532cb9e5aee2735555aab1785143fb",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6fde130f24de4941de6382c0b47920888676cb02",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e2b3b64a4ebeaaef6a2c94effc7c44b3d4bf9e5",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d1e79479fdaa990ea889c0b392a6ab4a884ffc4",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5298d732c100973951f754378b36e657d827055b",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7f7a4a4538b90aad59b31dde3f164c87c0a08175",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mfda7s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "popsumbong",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfda7s/horizon_beta_new_openai_open_source_model/",
          "stickied": false,
          "url": "https://openrouter.ai/openrouter/horizon-beta",
          "subreddit_subscribers": 508769,
          "created_utc": 1754095791,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone tried [https://huggingface.co/MetaStoneTec/XBai-o4](https://huggingface.co/MetaStoneTec/XBai-o4) ? Big if true -\n\n\\&gt; We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3-mini's performance \n\nHave not tried it myself, downloading atm from [https://huggingface.co/mradermacher/XBai-o4-GGUF](https://huggingface.co/mradermacher/XBai-o4-GGUF)",
          "author_fullname": "t2_4rlver1q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MetaStoneTec/XBai-o4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfk3y2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754118021,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone tried &lt;a href=\"https://huggingface.co/MetaStoneTec/XBai-o4\"&gt;https://huggingface.co/MetaStoneTec/XBai-o4&lt;/a&gt; ? Big if true -&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3-mini&amp;#39;s performance &lt;/p&gt;\n\n&lt;p&gt;Have not tried it myself, downloading atm from &lt;a href=\"https://huggingface.co/mradermacher/XBai-o4-GGUF\"&gt;https://huggingface.co/mradermacher/XBai-o4-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?auto=webp&amp;s=21ea26fb14e3912f80713c95ddb95b37cd316f66",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f69ac45b09b4ccb4576fc51eb90d2b847eace7ec",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a115ce28ec07456cec2ed3598f59bf288c35b1a9",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=02748428a3ab0ac779c13dffcf54fbcc054b5691",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed975e60e78ac76aed0294ed900e215343eefe17",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=64a929e4331b92d45b82827765bb555c57573df7",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=24081c4d549b93ebfca1a8d44ef5f08c619cebf8",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfk3y2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ljosif",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfk3y2/metastonetecxbaio4/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfk3y2/metastonetecxbaio4/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754118021,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ever spent weeks building the perfect LLM benchmark only to watch it crumble within a few months?\n\nClean problems, elegant difficulty curves, proper statistical controls. New model drops. Perfect scores across the board. Your tests got trained on. Weeks of work, completely worthless.\n\nSo you pivot. Make the tests harder, more complex, more creative. Models improve with time. Now everyone clusters at 90-95%. 8B models are defeating it. Your benchmark has become a participation trophy. This happened to my previous evaluation, *Can-Ai-Code*, twice.\n\nFine, you say. Random test generation it is! No more memorization, no more clustering. But congratulations, you've just unlocked new nightmares: Did you accidentally make your \"hard\" tests easier than your \"easy\" ones? Is your random number generator secretly biased? How do you even validate that hundreds of thousands of randomly generated problems \"make sense\"?\n\nYou solve that with clever statistical rigor, only to discover configuration explosion hell. You'd like to test different prompting templates and sampling parameters, but that's 5 templates times 5 samplers times 50 million tokens (a conservative estimate) equals 1.25 billion tokens per model. Your GPUs scream in horror.\n\nYou're now burning millions of tokens achieving 0.005 confidence intervals on trivial problems while critical hard points sit at 0.02 intervals begging for attention like abandoned puppies. Dynamic sampling helps - generate more tests for uncertain points, fewer for confident ones - but how to avoid p-hacking yourself?\n\nThat's when the guessing realization hits. This binary classifier task scored 60%! Amazing! Wait... that's only 20% above random chance. Your \"75% accurate\" multiple choice task is actually 50% accurate when you subtract lucky guesses. Everything is statistical lies. How are you supposed to compare models across boolean, multiple-choice and write-in answer tasks that have fundamentally different \"guess rates\"?\n\nFinally, truncation waste arrives to complete your suffering: Model given tough task hits context limits, burns 8,000 tokens, returns a loop of gibberish. You sample 10x more to maintain statistical power. That's 80K tokens wasted for one data point but with no useful answers.  You're overflowing your KV caches while the confidence intervals laugh at you.\n\nAfter drowning in this cascade of pain for months, I did what any reasonable person would do: I built an evaluation system to solve every single practical problem I encountered.\n\n# ReasonScape treats language models as information processing systems, not text completion black boxes.\n\nIt generates infinite, parametric, tokenization-aware test variations, applies statistical corrections for guessing, dynamically allocates sampling based on uncertainty, handles truncations intelligently, and visualizes the results as both enhanced leaderboards and explorable 3D cognitive landscapes.\n\n[C2: All Models x All Tasks Surface Comparison. Green Sphere indicates high-success. Red Square indicates high-truncation.](https://preview.redd.it/vsoidu4e4ggf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=d29809860b081384d998a428bc75faeba16cedc1)\n\nThe initial C2 dataset represents \\~1 billion tokens across 9 models, revealing exactly where, how and why reasoning breaks down across 4 task domains. The interactive leaderboard shows not just scores but confidence intervals, token usage and failure modes. The explorer (links at the bottom of post) lets you navigate difficulty manifolds like some kind of LLM reasoning archaeologist, digging into spectral analysis and completion token patterns.  Make sure you're on a PC - this application has too much going on to be mobile friendly!\n\n[C2 Explorer](https://preview.redd.it/4ahuh87m4ggf1.png?width=1233&amp;format=png&amp;auto=webp&amp;s=8f6e962cdc029ce01dbca46346ec3fda47a06d7d)\n\nI built the system with progressive evaluation in mind so you can start with rapid exploration then scale to deep precision. Everything caches, everything reproduces, everything scales. ReasonScape isn't just another benchmark. It's a complete methodology: toolkit, evaluation framework, and growing dataset family rolled into one.\n\n[C2 Leaderboard \\(Static snapshot - the Interactive is much nicer!\\)](https://preview.redd.it/rn7r2k3t4ggf1.png?width=1198&amp;format=png&amp;auto=webp&amp;s=52d054e40f6f292b07b9d638d82244e8f302ce1d)\n\nThe ReasonScape experiments and the resulting datasets will grow, expand and evolve - when scores get too high we will move the difficulty grids to make the tests harder and move on to C3. I have **8 additional tasks** to bring up, and lots more reasoning models I'd like to evaluate but my 2xRTX3090 only have so much to give.\n\nThanks for reading this far! &lt;3\n\nLinks:\n\n* [ReasonScape Homepage](https://reasonscape.com/)\n* [ReasonScape Leaderboard - C2](https://reasonscape.com/c2/leaderboard)\n* [ReasonScape Explorer - C2](https://reasonscape.com/c2/explorer) (note: PC required, not mobile-friendly)\n* [ReasonScape GitHub](https://github.com/the-crypt-keeper/reasonscape)\n* [ReasonScape System Architecture](https://github.com/the-crypt-keeper/reasonscape?tab=readme-ov-file#system-architecture)",
          "author_fullname": "t2_30i1a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I Generated 1 Billion Tokens (So You Don't Have To): Introducing ReasonScape",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4ahuh87m4ggf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e9b289044767660d3c8e2d034ccd1c1b1902f538"
                },
                {
                  "y": 125,
                  "x": 216,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e8a1fc6a4649edbc5bfde934cedfd13c3fce90c"
                },
                {
                  "y": 186,
                  "x": 320,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c9c7d1e879f15dbfdea396948cc5826d3a32b67"
                },
                {
                  "y": 372,
                  "x": 640,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4a45011cfb5e75257e82a1018ba38ca3849d833"
                },
                {
                  "y": 558,
                  "x": 960,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e3046d2f091a49910ecb192cd251d2fa91bbf04a"
                },
                {
                  "y": 628,
                  "x": 1080,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=053460c5647cc12262626da5af12623413fee0ff"
                }
              ],
              "s": {
                "y": 717,
                "x": 1233,
                "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=1233&amp;format=png&amp;auto=webp&amp;s=8f6e962cdc029ce01dbca46346ec3fda47a06d7d"
              },
              "id": "4ahuh87m4ggf1"
            },
            "rn7r2k3t4ggf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 73,
                  "x": 108,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d505276a5f0dc04e59903c6abc89821d0f8f99b"
                },
                {
                  "y": 146,
                  "x": 216,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d747b1a0f8fd1a1097161d95e21ccf0503560deb"
                },
                {
                  "y": 216,
                  "x": 320,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9a6dd29394e5fb1992ae084b12e8749a9d84824"
                },
                {
                  "y": 433,
                  "x": 640,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=720bb40aec42b94acce8d40bc234aa44e5b4c208"
                },
                {
                  "y": 650,
                  "x": 960,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e355ab07ff4b2410135911c2070a7cb42b1b8221"
                },
                {
                  "y": 732,
                  "x": 1080,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=668705727d06f0b0d4c5c5867747868786f17635"
                }
              ],
              "s": {
                "y": 812,
                "x": 1198,
                "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=1198&amp;format=png&amp;auto=webp&amp;s=52d054e40f6f292b07b9d638d82244e8f302ce1d"
              },
              "id": "rn7r2k3t4ggf1"
            },
            "vsoidu4e4ggf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=871d7d827171f900b2598bcca602b93414c369f8"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a4c54e76149d09d301df5cbd0f5c388e82bb54d"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=317f1298cb62e84fc0ac688a8a4b2143d1d2fc8a"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=769ba43ab175981d7853c0c0620df46e4f20be04"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e16a01628210ccf57e6c94fa2d7d58110870a99"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49835bd796714f4bcb033b2b80c71bc4b6e37e82"
                }
              ],
              "s": {
                "y": 3150,
                "x": 1280,
                "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=d29809860b081384d998a428bc75faeba16cedc1"
              },
              "id": "vsoidu4e4ggf1"
            }
          },
          "name": "t3_1mf3nw4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#c7b594",
          "subreddit_type": "public",
          "ups": 118,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 118,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/-4v8QT3_SA4NBTuWsWHLP1NxBvsUSLBCUXILi1-L8H8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754071528,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever spent weeks building the perfect LLM benchmark only to watch it crumble within a few months?&lt;/p&gt;\n\n&lt;p&gt;Clean problems, elegant difficulty curves, proper statistical controls. New model drops. Perfect scores across the board. Your tests got trained on. Weeks of work, completely worthless.&lt;/p&gt;\n\n&lt;p&gt;So you pivot. Make the tests harder, more complex, more creative. Models improve with time. Now everyone clusters at 90-95%. 8B models are defeating it. Your benchmark has become a participation trophy. This happened to my previous evaluation, &lt;em&gt;Can-Ai-Code&lt;/em&gt;, twice.&lt;/p&gt;\n\n&lt;p&gt;Fine, you say. Random test generation it is! No more memorization, no more clustering. But congratulations, you&amp;#39;ve just unlocked new nightmares: Did you accidentally make your &amp;quot;hard&amp;quot; tests easier than your &amp;quot;easy&amp;quot; ones? Is your random number generator secretly biased? How do you even validate that hundreds of thousands of randomly generated problems &amp;quot;make sense&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;You solve that with clever statistical rigor, only to discover configuration explosion hell. You&amp;#39;d like to test different prompting templates and sampling parameters, but that&amp;#39;s 5 templates times 5 samplers times 50 million tokens (a conservative estimate) equals 1.25 billion tokens per model. Your GPUs scream in horror.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;re now burning millions of tokens achieving 0.005 confidence intervals on trivial problems while critical hard points sit at 0.02 intervals begging for attention like abandoned puppies. Dynamic sampling helps - generate more tests for uncertain points, fewer for confident ones - but how to avoid p-hacking yourself?&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s when the guessing realization hits. This binary classifier task scored 60%! Amazing! Wait... that&amp;#39;s only 20% above random chance. Your &amp;quot;75% accurate&amp;quot; multiple choice task is actually 50% accurate when you subtract lucky guesses. Everything is statistical lies. How are you supposed to compare models across boolean, multiple-choice and write-in answer tasks that have fundamentally different &amp;quot;guess rates&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;Finally, truncation waste arrives to complete your suffering: Model given tough task hits context limits, burns 8,000 tokens, returns a loop of gibberish. You sample 10x more to maintain statistical power. That&amp;#39;s 80K tokens wasted for one data point but with no useful answers.  You&amp;#39;re overflowing your KV caches while the confidence intervals laugh at you.&lt;/p&gt;\n\n&lt;p&gt;After drowning in this cascade of pain for months, I did what any reasonable person would do: I built an evaluation system to solve every single practical problem I encountered.&lt;/p&gt;\n\n&lt;h1&gt;ReasonScape treats language models as information processing systems, not text completion black boxes.&lt;/h1&gt;\n\n&lt;p&gt;It generates infinite, parametric, tokenization-aware test variations, applies statistical corrections for guessing, dynamically allocates sampling based on uncertainty, handles truncations intelligently, and visualizes the results as both enhanced leaderboards and explorable 3D cognitive landscapes.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vsoidu4e4ggf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d29809860b081384d998a428bc75faeba16cedc1\"&gt;C2: All Models x All Tasks Surface Comparison. Green Sphere indicates high-success. Red Square indicates high-truncation.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The initial C2 dataset represents ~1 billion tokens across 9 models, revealing exactly where, how and why reasoning breaks down across 4 task domains. The interactive leaderboard shows not just scores but confidence intervals, token usage and failure modes. The explorer (links at the bottom of post) lets you navigate difficulty manifolds like some kind of LLM reasoning archaeologist, digging into spectral analysis and completion token patterns.  Make sure you&amp;#39;re on a PC - this application has too much going on to be mobile friendly!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4ahuh87m4ggf1.png?width=1233&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f6e962cdc029ce01dbca46346ec3fda47a06d7d\"&gt;C2 Explorer&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I built the system with progressive evaluation in mind so you can start with rapid exploration then scale to deep precision. Everything caches, everything reproduces, everything scales. ReasonScape isn&amp;#39;t just another benchmark. It&amp;#39;s a complete methodology: toolkit, evaluation framework, and growing dataset family rolled into one.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rn7r2k3t4ggf1.png?width=1198&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52d054e40f6f292b07b9d638d82244e8f302ce1d\"&gt;C2 Leaderboard (Static snapshot - the Interactive is much nicer!)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The ReasonScape experiments and the resulting datasets will grow, expand and evolve - when scores get too high we will move the difficulty grids to make the tests harder and move on to C3. I have &lt;strong&gt;8 additional tasks&lt;/strong&gt; to bring up, and lots more reasoning models I&amp;#39;d like to evaluate but my 2xRTX3090 only have so much to give.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading this far! &amp;lt;3&lt;/p&gt;\n\n&lt;p&gt;Links:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://reasonscape.com/\"&gt;ReasonScape Homepage&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://reasonscape.com/c2/leaderboard\"&gt;ReasonScape Leaderboard - C2&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://reasonscape.com/c2/explorer\"&gt;ReasonScape Explorer - C2&lt;/a&gt; (note: PC required, not mobile-friendly)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/the-crypt-keeper/reasonscape\"&gt;ReasonScape GitHub&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/the-crypt-keeper/reasonscape?tab=readme-ov-file#system-architecture\"&gt;ReasonScape System Architecture&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf3nw4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kryptkpr",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mf3nw4/i_generated_1_billion_tokens_so_you_dont_have_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf3nw4/i_generated_1_billion_tokens_so_you_dont_have_to/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754071528,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Saidia is an offline-first AI assistant tailored for educators, enabling them to generate questions directly from source materials.\n\nBuilt using Electron, Ollama, and Gemma 3n, Saidia functions entirely offline and is optimised for basic hardware. It's ideal for areas with unreliable internet and power, empowering educators with powerful teaching resources where cloud-based tools are impractical or impossible.\n\n[https://github.com/dokasto/Saidia](https://github.com/dokasto/Saidia)",
          "author_fullname": "t2_11mwsd0p41",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Saidia: Offline-First AI Assistant for Educators in low-connectivity regions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfn2xf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754129775,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Saidia is an offline-first AI assistant tailored for educators, enabling them to generate questions directly from source materials.&lt;/p&gt;\n\n&lt;p&gt;Built using Electron, Ollama, and Gemma 3n, Saidia functions entirely offline and is optimised for basic hardware. It&amp;#39;s ideal for areas with unreliable internet and power, empowering educators with powerful teaching resources where cloud-based tools are impractical or impossible.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/dokasto/Saidia\"&gt;https://github.com/dokasto/Saidia&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?auto=webp&amp;s=5141edfe493867e3dafdd151010b3f5040912cc0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e949d6258248c551dcfd9cb47f1304f4151400c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d3484da8634402f8ed7774d2e4f8c4669168d53",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7c82a3e8557eeca0f83fc2e477928d5f3031a9f",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=493dbfda76c0eab29abee992f4eb1d96ff6d93fd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b92c32a097682bd7a3113fbc01e20ac3a07fd673",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=16e412f9a5dfdc2c2cf419a9494a9e06f0757f95",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfn2xf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dokasto_",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfn2xf/saidia_offlinefirst_ai_assistant_for_educators_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfn2xf/saidia_offlinefirst_ai_assistant_for_educators_in/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754129775,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_14bv8c06dm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gemini 2.5 Deep Think mode benchmarks!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1meu3jn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 297,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 297,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/abtcO2rczXZnz1xo2s1h25imovC55smF-ZvaOmt_0Tw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754048166,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8wnv6pme9egf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8wnv6pme9egf1.png?auto=webp&amp;s=e049a99b67af3601aa3ae286cd33e368a88628f3",
                  "width": 1001,
                  "height": 1173
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8wnv6pme9egf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ae70156b903e0aed05b8381e5896b9b69d5b72e6",
                    "width": 108,
                    "height": 126
                  },
                  {
                    "url": "https://preview.redd.it/8wnv6pme9egf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2dbc8968418c0fffb09064cde42872404023bf84",
                    "width": 216,
                    "height": 253
                  },
                  {
                    "url": "https://preview.redd.it/8wnv6pme9egf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=47264da7484d4c30ea47bbbe23a3cde136478df5",
                    "width": 320,
                    "height": 374
                  },
                  {
                    "url": "https://preview.redd.it/8wnv6pme9egf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=557a01b879fc1bdbf0cc88dc3a91d0b4a7b1c10c",
                    "width": 640,
                    "height": 749
                  },
                  {
                    "url": "https://preview.redd.it/8wnv6pme9egf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7da1fd0ae1e95d98f9e7a1046cfa44b0f374a42f",
                    "width": 960,
                    "height": 1124
                  }
                ],
                "variants": {},
                "id": "2QR1T0pd9pOTCNAN3kWL8a-3T9kX77SVsmRgOPYX2zQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1meu3jn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Beautiful-Essay1945",
          "discussion_type": null,
          "num_comments": 70,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1meu3jn/gemini_25_deep_think_mode_benchmarks/",
          "stickied": false,
          "url": "https://i.redd.it/8wnv6pme9egf1.png",
          "subreddit_subscribers": 508769,
          "created_utc": 1754048166,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The person who \"leaked\" this model is from the openai (HF) organization \n\nSo as expected, it's not gonna be something you can easily run locally, it won't hurt the chatgpt subscription business, you will need a dedicated LLM machine for that model ",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "The OpenAI Open weight model might be 120B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "9dqwiep9ucgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 128,
                  "x": 108,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=82c6a3fca8afcd98702c0b7d846a7e160d71c2ba"
                },
                {
                  "y": 257,
                  "x": 216,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=daf71cd9fc68c069a95fe74761dcc142bc617a8b"
                },
                {
                  "y": 381,
                  "x": 320,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=436cbfa27d8543f73db2371953a82aac39ecda85"
                },
                {
                  "y": 763,
                  "x": 640,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8299ea44db486e3a2fbec74c3d0bd88466708866"
                },
                {
                  "y": 1144,
                  "x": 960,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ff8e80fe3dec58ee22a247bfdab51b53bcac09ca"
                },
                {
                  "y": 1288,
                  "x": 1080,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4d90e7d7d325eaa34c7621c43ce177895ff0383"
                }
              ],
              "s": {
                "y": 1288,
                "x": 1080,
                "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=b3767f5800ed4a78ec866eaa707e8dbe0151f82f"
              },
              "id": "9dqwiep9ucgf1"
            },
            "w9h7ftgaucgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 187,
                  "x": 108,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=65c73f27d3ec2b6268ed676ac73a20d96cc01864"
                },
                {
                  "y": 375,
                  "x": 216,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a7776f97e1b1b9b42cd96de35d6b38c2bdf1948d"
                },
                {
                  "y": 555,
                  "x": 320,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17560f484f4fc6684187f9b4e3214d2bb480250d"
                },
                {
                  "y": 1111,
                  "x": 640,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=64632b927b61af650b70b9b6e2e93befcc39ee18"
                },
                {
                  "y": 1667,
                  "x": 960,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8bd9619e6620d4a4c7a2fbdf4077d3780cf0b105"
                }
              ],
              "s": {
                "y": 1873,
                "x": 1078,
                "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=1078&amp;format=png&amp;auto=webp&amp;s=dd26649ae5edf99eedfd8378f94c7fdf3aeb6ae9"
              },
              "id": "w9h7ftgaucgf1"
            },
            "s1j5io2aucgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=066bdffe86c232ba5ff6c7d20d8dde94884c7f6e"
                },
                {
                  "y": 188,
                  "x": 216,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3ab77eae4021c3e0b9af7ed66501883f4211b90"
                },
                {
                  "y": 279,
                  "x": 320,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=61d00c48c6b5fb9736c6d3503205a847c8146f46"
                },
                {
                  "y": 558,
                  "x": 640,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=de6f7067561af43d54b56fbf6902277437c1fa94"
                },
                {
                  "y": 837,
                  "x": 960,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=062e6b2cc5f7e404a5f33fca5b18a6e367b0f99b"
                },
                {
                  "y": 942,
                  "x": 1080,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5e518857b677661c538f1b99548e5ca304d3b186"
                }
              ],
              "s": {
                "y": 942,
                "x": 1080,
                "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=b86914c6db57a17c4bde148a77b9a789f692e93f"
              },
              "id": "s1j5io2aucgf1"
            }
          },
          "name": "t3_1mepeqh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bbbdbf",
          "ups": 705,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "9dqwiep9ucgf1",
                "id": 718647288
              },
              {
                "caption": "",
                "media_id": "s1j5io2aucgf1",
                "id": 718647289
              },
              {
                "caption": "",
                "media_id": "w9h7ftgaucgf1",
                "id": 718647290
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 705,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/uvWDYtCBC32T5YD2glI0V4HTGmyDJzZTUERWQkmJQoE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754030862,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The person who &amp;quot;leaked&amp;quot; this model is from the openai (HF) organization &lt;/p&gt;\n\n&lt;p&gt;So as expected, it&amp;#39;s not gonna be something you can easily run locally, it won&amp;#39;t hurt the chatgpt subscription business, you will need a dedicated LLM machine for that model &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mepeqh",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mepeqh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 156,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mepeqh/the_openai_open_weight_model_might_be_120b/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mepeqh",
          "subreddit_subscribers": 508769,
          "created_utc": 1754030862,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tried running qwen3-coder in Claude Code. It constantly failed tool calls. I tried both the cerebras api and the official alibaba api.\n\nI also tried glm-4.5 in Claude Code and it was surprisingly good. Asked both Gemini cli and glm-4.5 in Claude Code to make the snake game and tetris in html and the games made ny glm were much better looking than gemini. Since Gemini is #1 right now on Web Arena, I suspect glm will be #1 when it's on the leaderboard. Glm was also much better at tool calls, it basically never failed.",
          "author_fullname": "t2_58t8ty6v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is bad at tool call while glm-4.5 is surprisingly good",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf8la7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 47,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 47,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754083147,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried running qwen3-coder in Claude Code. It constantly failed tool calls. I tried both the cerebras api and the official alibaba api.&lt;/p&gt;\n\n&lt;p&gt;I also tried glm-4.5 in Claude Code and it was surprisingly good. Asked both Gemini cli and glm-4.5 in Claude Code to make the snake game and tetris in html and the games made ny glm were much better looking than gemini. Since Gemini is #1 right now on Web Arena, I suspect glm will be #1 when it&amp;#39;s on the leaderboard. Glm was also much better at tool calls, it basically never failed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mf8la7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BoJackHorseMan53",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf8la7/qwen3coder_is_bad_at_tool_call_while_glm45_is/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf8la7/qwen3coder_is_bad_at_tool_call_while_glm45_is/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754083147,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys, it’s been a while but I’m happy to announce another major update for my app **EasyWhisperUI**, now with **live transcriptions**!\n\nIt features full cross-platform GPU acceleration:\n\n* **Vulkan** on Windows (Intel, AMD, or NVIDIA)\n* **Metal** on macOS (Apple silicon)\n\n**New features!**\n\n1. **GPU-accelerated Live Transcriptions** • Transcribe speech in real time using your default mic (user request)\n2. **Output Cleanup** • Automatically removes repeated segments from live transcriptions\n3. **Open in Notepad Checkbox** • New option to disable automatic opening in Notepad after transcription (user request)\n4. Various bug fixes and code improvements.\n\n**Other key features**\n\n5. **Batch File Processing** • Drag &amp; drop multiple files — EasyWhisperUI will queue and transcribe them automatically (user request)  \n6. **CPU-Only Toggle** • Option to disable GPU acceleration and run fully on CPU (user request)  \n7. **Modern UI** • Acrylic background on Windows, clean layout and spacing improvements  \n8. **macOS Support** • EasyWhisperUI works on macOS thanks to a community contribution  \n9. **Installer Included** • Installs everything you need (compiler, ffmpeg, whisper.cpp) and builds from source with one click\n\nThere are a lot more features — check out the GitHub for more info:\n\n🔗 **GitHub:** [https://github.com/mehtabmahir/easy-whisper-ui](https://github.com/mehtabmahir/easy-whisper-ui)\n\nLet me know what you think or if you have any suggestions!",
          "author_fullname": "t2_epvxz7g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "EasyWhisperUI – GPU accelerated Open Source Whisper UI for Windows &amp; macOS now with Live Transcriptions!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mffjjj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754103866,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754102625,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, it’s been a while but I’m happy to announce another major update for my app &lt;strong&gt;EasyWhisperUI&lt;/strong&gt;, now with &lt;strong&gt;live transcriptions&lt;/strong&gt;!&lt;/p&gt;\n\n&lt;p&gt;It features full cross-platform GPU acceleration:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Vulkan&lt;/strong&gt; on Windows (Intel, AMD, or NVIDIA)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Metal&lt;/strong&gt; on macOS (Apple silicon)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;New features!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;GPU-accelerated Live Transcriptions&lt;/strong&gt; • Transcribe speech in real time using your default mic (user request)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Output Cleanup&lt;/strong&gt; • Automatically removes repeated segments from live transcriptions&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Open in Notepad Checkbox&lt;/strong&gt; • New option to disable automatic opening in Notepad after transcription (user request)&lt;/li&gt;\n&lt;li&gt;Various bug fixes and code improvements.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Other key features&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Batch File Processing&lt;/strong&gt; • Drag &amp;amp; drop multiple files — EasyWhisperUI will queue and transcribe them automatically (user request)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CPU-Only Toggle&lt;/strong&gt; • Option to disable GPU acceleration and run fully on CPU (user request)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Modern UI&lt;/strong&gt; • Acrylic background on Windows, clean layout and spacing improvements&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;macOS Support&lt;/strong&gt; • EasyWhisperUI works on macOS thanks to a community contribution&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Installer Included&lt;/strong&gt; • Installs everything you need (compiler, ffmpeg, whisper.cpp) and builds from source with one click&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;There are a lot more features — check out the GitHub for more info:&lt;/p&gt;\n\n&lt;p&gt;🔗 &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href=\"https://github.com/mehtabmahir/easy-whisper-ui\"&gt;https://github.com/mehtabmahir/easy-whisper-ui&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think or if you have any suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?auto=webp&amp;s=8b1a4fb535e00a6da0368011b0e6cec45497c5d7",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5822df3220f343ebd2934400d0f59847247da8d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=823cd6c140e6f8b93ef72f68e9d0b425f56243f3",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1e4b350fed949d338ba9708cf08854a1b4237e89",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ccc578f650ebc7544f2e8cfe89996ac4b84f501f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5d4fb11dd5292586b4d7444d6d35ffe279e98ba1",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d2237e0f4ab1ae9e5ec2baa35b902b491ffb5d35",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mffjjj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mehtabmahir",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mffjjj/easywhisperui_gpu_accelerated_open_source_whisper/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mffjjj/easywhisperui_gpu_accelerated_open_source_whisper/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754102625,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_48ezkeai",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OpenAI OS model info leaked - 120B &amp; 20B will be available",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mepz8z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 469,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 469,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/koWApmUvRcfMpfhrqJD5WrepTEKpDhchruNu54mqVSQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754033016,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/08m94pio0dgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?auto=webp&amp;s=6fb8f07eaa524714454f02be25da5a0c8bd501ec",
                  "width": 1052,
                  "height": 1588
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=57a6c0b7f95c81ab1bf5553bdcd58df7e2e53602",
                    "width": 108,
                    "height": 163
                  },
                  {
                    "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=846a6af2093db0113d0ac29622308c99f3409503",
                    "width": 216,
                    "height": 326
                  },
                  {
                    "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a6414cea64697cd84845fbfa7d319abc63ee4be",
                    "width": 320,
                    "height": 483
                  },
                  {
                    "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c8e423b8c1c16726ef958bbd8725e985cc58bc68",
                    "width": 640,
                    "height": 966
                  },
                  {
                    "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0995094fc1bd94332893e4e6b74528bc339b0123",
                    "width": 960,
                    "height": 1449
                  }
                ],
                "variants": {},
                "id": "IacMR9LsboNRW0VP4Zwz_MvZddRdCNqWrVrUgsEhFxY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mepz8z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ShreckAndDonkey123",
          "discussion_type": null,
          "num_comments": 142,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mepz8z/openai_os_model_info_leaked_120b_20b_will_be/",
          "stickied": false,
          "url": "https://i.redd.it/08m94pio0dgf1.jpeg",
          "subreddit_subscribers": 508769,
          "created_utc": 1754033016,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone! I’m a bit confused about what actually happens when you upload a document to an AI app like ChatGPT or LE CHAT. Is this considered prompt engineering (just pasting the content into the prompt) or is it RAG (Retrieval-Augmented Generation)?\n\nI initially thought it was RAG, but I saw this video from Yannic Kilcher explaining that ChatGPT basically just copies the content of the document and pastes it into the prompt. If that’s true, wouldn’t that quickly blow up the context window?\n\nBut then again, if it is RAG, like using vector search on the document and feeding only similar chunks to the LLM, wouldn’t that risk missing important context, especially for something like summarization?\n\nSo both approaches seem to have drawbacks — I’m just wondering which one is typically used by AI apps when handling uploaded files?",
          "author_fullname": "t2_kjhuip53",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RAG or prompt engineering",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mfor6n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754135853,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! I’m a bit confused about what actually happens when you upload a document to an AI app like ChatGPT or LE CHAT. Is this considered prompt engineering (just pasting the content into the prompt) or is it RAG (Retrieval-Augmented Generation)?&lt;/p&gt;\n\n&lt;p&gt;I initially thought it was RAG, but I saw this video from Yannic Kilcher explaining that ChatGPT basically just copies the content of the document and pastes it into the prompt. If that’s true, wouldn’t that quickly blow up the context window?&lt;/p&gt;\n\n&lt;p&gt;But then again, if it is RAG, like using vector search on the document and feeding only similar chunks to the LLM, wouldn’t that risk missing important context, especially for something like summarization?&lt;/p&gt;\n\n&lt;p&gt;So both approaches seem to have drawbacks — I’m just wondering which one is typically used by AI apps when handling uploaded files?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfor6n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SignatureHuman8057",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfor6n/rag_or_prompt_engineering/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfor6n/rag_or_prompt_engineering/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754135853,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been experimenting with Qwen3:30b-a3b-instruct-2507-q8\\_0 using Ollama v0.10.0 (standard settings) on Debian 12 with a pair of Nvidia P40s, and I'm really impressed with the speed!  \n\nIn light conversation (I tested with general knowledge questions and everyday scenarios), I'm achieving up to 34 tokens/s, which is \\*significantly\\* faster than other models I've tested (all Q4 except for qwen3):\n\n* Qwen3 (30B): \\~34 tokens/s\n* Qwen2.5 (32B): \\~10 tokens/s\n* Gemma3 (27B): \\~10 tokens/s\n* Llama3 (70B): 4-5 tokens/s\n\nHowever, I'm also sometimes seeing a fair amount of hallucination with facts, locations or events. Not enough to make it unusable but notable to me.\n\nMy first impression is that Qwen3 is incredibly fast, but could be a bit more reliable. Using Ollama with Qwen3 is super easy, but maybe it needs some tweaking?  What's your experience been like with speed and accuracy of Qwen3?",
          "author_fullname": "t2_tlzk7zie",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 (30B) with Ollama: Blazing Fast, but accuracy concerns",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfl6bo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "50c36eba-fdca-11ee-9735-92a88d7e3b87",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Ollama"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754122088,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been experimenting with Qwen3:30b-a3b-instruct-2507-q8_0 using Ollama v0.10.0 (standard settings) on Debian 12 with a pair of Nvidia P40s, and I&amp;#39;m really impressed with the speed!  &lt;/p&gt;\n\n&lt;p&gt;In light conversation (I tested with general knowledge questions and everyday scenarios), I&amp;#39;m achieving up to 34 tokens/s, which is *significantly* faster than other models I&amp;#39;ve tested (all Q4 except for qwen3):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qwen3 (30B): ~34 tokens/s&lt;/li&gt;\n&lt;li&gt;Qwen2.5 (32B): ~10 tokens/s&lt;/li&gt;\n&lt;li&gt;Gemma3 (27B): ~10 tokens/s&lt;/li&gt;\n&lt;li&gt;Llama3 (70B): 4-5 tokens/s&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;However, I&amp;#39;m also sometimes seeing a fair amount of hallucination with facts, locations or events. Not enough to make it unusable but notable to me.&lt;/p&gt;\n\n&lt;p&gt;My first impression is that Qwen3 is incredibly fast, but could be a bit more reliable. Using Ollama with Qwen3 is super easy, but maybe it needs some tweaking?  What&amp;#39;s your experience been like with speed and accuracy of Qwen3?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Ollama",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfl6bo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gerhardmpl",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfl6bo/qwen3_30b_with_ollama_blazing_fast_but_accuracy/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfl6bo/qwen3_30b_with_ollama_blazing_fast_but_accuracy/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754122088,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Disclaimer:\n\nNo actual plushy pandas were hurt in the process of trying and failing to fit in a plastic box...",
          "author_fullname": "t2_qz1qjc86",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Me lately... Anyone else can relate? 😎",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 79,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf4ihq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "ups": 49,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 49,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3BSxzZpbbsOE49m6Gzlq2vc_AKs8no1XsGYN2LTJOrg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754073451,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Disclaimer:&lt;/p&gt;\n\n&lt;p&gt;No actual plushy pandas were hurt in the process of trying and failing to fit in a plastic box...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/rqzixk49cggf1.gif",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/rqzixk49cggf1.gif?format=png8&amp;s=b762d39ddcffd472526fa05b0d769adcc9064fe7",
                  "width": 800,
                  "height": 453
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=4f274e8ff485a4989b9d127943de26befcdfb05d",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=0965b1a108af3d1709115e6fb86455125fa468ea",
                    "width": 216,
                    "height": 122
                  },
                  {
                    "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=beb312a49051ec985b83d4c6031aee1db97d0956",
                    "width": 320,
                    "height": 181
                  },
                  {
                    "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=8163f8e2cdf42526b89cbf134b8c0bbe2c377aa2",
                    "width": 640,
                    "height": 362
                  }
                ],
                "variants": {
                  "gif": {
                    "source": {
                      "url": "https://preview.redd.it/rqzixk49cggf1.gif?s=e2f65b8b2302c9de598fe0355ffa07608973a7b7",
                      "width": 800,
                      "height": 453
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=108&amp;crop=smart&amp;s=f83c49f5a04ebce5a16c8b856d87f5215d57d86d",
                        "width": 108,
                        "height": 61
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=216&amp;crop=smart&amp;s=41ec44955588452eaeb561d8f4d6b51bdafdc817",
                        "width": 216,
                        "height": 122
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=320&amp;crop=smart&amp;s=507d4f46da1e538fa1fe51f68da80696036c09aa",
                        "width": 320,
                        "height": 181
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=640&amp;crop=smart&amp;s=3d8b06b0091af494d702ac39636bf603e600b301",
                        "width": 640,
                        "height": 362
                      }
                    ]
                  },
                  "mp4": {
                    "source": {
                      "url": "https://preview.redd.it/rqzixk49cggf1.gif?format=mp4&amp;s=7f326be51b3cb7f3e2485d49898a5015d2bb1140",
                      "width": 800,
                      "height": 453
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=108&amp;format=mp4&amp;s=40d2c981c71f4d6d8f0c9a6bc711893b6edbebdc",
                        "width": 108,
                        "height": 61
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=216&amp;format=mp4&amp;s=585d088b1216ab7ef3b8d529198e1dc2849966e9",
                        "width": 216,
                        "height": 122
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=320&amp;format=mp4&amp;s=c445476bc95afa7c794b1c3af79421dcb8a4f570",
                        "width": 320,
                        "height": 181
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=640&amp;format=mp4&amp;s=c9b96f95344d2207fe0fd0ea23af8ea9f3f2e686",
                        "width": 640,
                        "height": 362
                      }
                    ]
                  }
                },
                "id": "A5U9B6UgGQoc_r5iq28_9pm0dir0rIEwOZlniIEu3DQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mf4ihq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Cool-Chemical-5629",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf4ihq/me_lately_anyone_else_can_relate/",
          "stickied": false,
          "url": "https://i.redd.it/rqzixk49cggf1.gif",
          "subreddit_subscribers": 508769,
          "created_utc": 1754073451,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tried building a docker container to have infinity use the Qwen3-Embedding-8B model in a CPU-only setting. But once the docker container starts, the CPU (Ryzen 9950X, 128GB DDR5) is fully busy even without any embedding requests. Is that normal, or did I configure something wrong?\n\nHere's the Dockerfile:\n\n&gt; FROM michaelf34/infinity:latest-cpu\nRUN pip install --upgrade transformers accelerate\n\nHere's the docker-compose:\n\n&gt; version: '3.8'\nservices:\n  infinity:\n    build: .\n    ports:\n      - \"7997:7997\"\n    environment:\n      - DISABLE_TELEMETRY=true\n      - DO_NOT_TRACK: 1\n      - TOKENIZERS_PARALLELISM=false\n      - TRANSFORMERS_CACHE=.cache\n    volumes:\n      - ./models:/models:ro\n      - ./cache:/.cache\n      restart: unless-stopped\n    command: infinity-emb v2 --model-id /models/Qwen3-Embedding-8B\n\nStartup command was:\n\ndocker run -d -p 7997:7997 --name qwembed-cpu -v $PWD/models:/models:ro -v ./cache:/app/.cache qwen-infinity-cpu v2 --model-id /models/Qwen3-Embedding-8B --engine torch",
          "author_fullname": "t2_cocl8roo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Issues with michaelf34/infinity:latest-cpu + Qwen3-Embedding-8B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mfofx5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754134794,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried building a docker container to have infinity use the Qwen3-Embedding-8B model in a CPU-only setting. But once the docker container starts, the CPU (Ryzen 9950X, 128GB DDR5) is fully busy even without any embedding requests. Is that normal, or did I configure something wrong?&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the Dockerfile:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;FROM michaelf34/infinity:latest-cpu\nRUN pip install --upgrade transformers accelerate&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Here&amp;#39;s the docker-compose:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;version: &amp;#39;3.8&amp;#39;\nservices:\n  infinity:\n    build: .\n    ports:\n      - &amp;quot;7997:7997&amp;quot;\n    environment:\n      - DISABLE_TELEMETRY=true\n      - DO_NOT_TRACK: 1\n      - TOKENIZERS_PARALLELISM=false\n      - TRANSFORMERS_CACHE=.cache\n    volumes:\n      - ./models:/models:ro\n      - ./cache:/.cache\n      restart: unless-stopped\n    command: infinity-emb v2 --model-id /models/Qwen3-Embedding-8B&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Startup command was:&lt;/p&gt;\n\n&lt;p&gt;docker run -d -p 7997:7997 --name qwembed-cpu -v $PWD/models:/models:ro -v ./cache:/app/.cache qwen-infinity-cpu v2 --model-id /models/Qwen3-Embedding-8B --engine torch&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfofx5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Patentsmatter",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfofx5/issues_with_michaelf34infinitylatestcpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfofx5/issues_with_michaelf34infinitylatestcpu/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754134794,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "GPU snapshotting is finally a thing! NVIDIA recently released their [CUDA checkpoint/restore API](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CHECKPOINT.html) and we at Modal (serverless compute platform) are using it drastically reduce GPU cold start times. This is especially relevant for serving large models, where it can take minutes (for the heftiest LLMs) to move model weights from disk to memory.\n\nGPU memory snapshotting can reduce cold boot times by up to 12x. It lets you scale GPU resources up and down based on demand without compromising on user-facing latency. Below are some benchmarking results showing improvements for various models!\n\nhttps://preview.redd.it/opb5odlb2hgf1.png?width=3162&amp;format=png&amp;auto=webp&amp;s=00995e770fa4d3ac454bd9b1f0df5296391fb137\n\nMore on how GPU snapshotting works plus additional benchmarks in this blog post: [https://modal.com/blog/gpu-mem-snapshots](https://modal.com/blog/gpu-mem-snapshots)",
          "author_fullname": "t2_9av3t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cold start vLLM in 5 seconds with GPU snapshotting",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "opb5odlb2hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 54,
                  "x": 108,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1a99ec3a648e3acbc71ef3233e807e5fbfc20dd"
                },
                {
                  "y": 108,
                  "x": 216,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a87d457cf006589e58141d5831e05fc39cc999c4"
                },
                {
                  "y": 161,
                  "x": 320,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=156b5387ff8776f0f27387621395be6a4f1bb15b"
                },
                {
                  "y": 322,
                  "x": 640,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c498b446dda08ce80241842d3232b30cf40b31ec"
                },
                {
                  "y": 483,
                  "x": 960,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eae84d78449fb87be8ed155a688b75c0cb5ea458"
                },
                {
                  "y": 543,
                  "x": 1080,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=505dd6a12adbdbe687a6467d0b0843adb780d82a"
                }
              ],
              "s": {
                "y": 1592,
                "x": 3162,
                "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=3162&amp;format=png&amp;auto=webp&amp;s=00995e770fa4d3ac454bd9b1f0df5296391fb137"
              },
              "id": "opb5odlb2hgf1"
            }
          },
          "name": "t3_1mf86rn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/9zGmAn5wtWE9ciYDNGxrwYmoTgYDYEOmBf7p4EOrZDY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754082162,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;GPU snapshotting is finally a thing! NVIDIA recently released their &lt;a href=\"https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CHECKPOINT.html\"&gt;CUDA checkpoint/restore API&lt;/a&gt; and we at Modal (serverless compute platform) are using it drastically reduce GPU cold start times. This is especially relevant for serving large models, where it can take minutes (for the heftiest LLMs) to move model weights from disk to memory.&lt;/p&gt;\n\n&lt;p&gt;GPU memory snapshotting can reduce cold boot times by up to 12x. It lets you scale GPU resources up and down based on demand without compromising on user-facing latency. Below are some benchmarking results showing improvements for various models!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/opb5odlb2hgf1.png?width=3162&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00995e770fa4d3ac454bd9b1f0df5296391fb137\"&gt;https://preview.redd.it/opb5odlb2hgf1.png?width=3162&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00995e770fa4d3ac454bd9b1f0df5296391fb137&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;More on how GPU snapshotting works plus additional benchmarks in this blog post: &lt;a href=\"https://modal.com/blog/gpu-mem-snapshots\"&gt;https://modal.com/blog/gpu-mem-snapshots&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf86rn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "crookedstairs",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf86rn/cold_start_vllm_in_5_seconds_with_gpu_snapshotting/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf86rn/cold_start_vllm_in_5_seconds_with_gpu_snapshotting/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754082162,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying to make an agent that get YouTube videos transcript but i keep having ip ban or a ban from requests to youtube-transcript-api, how to manage this?",
          "author_fullname": "t2_w2tkxer3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to avoid IP bans when using youtube-transcript-api to fetch YouTube video transcripts?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfij9a",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754112257,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to make an agent that get YouTube videos transcript but i keep having ip ban or a ban from requests to youtube-transcript-api, how to manage this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfij9a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Anas_M1nt",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfij9a/how_to_avoid_ip_bans_when_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfij9a/how_to_avoid_ip_bans_when_using/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754112257,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I wanna use this model for DMing a dnd game as well as using it to write stories. I’d like it to be abliterated if possible.\n\nI’ve been looking at using Gemma 3 27B, and I do like its writing style, but I’m concerned about its ability to handle long context lengths.\n\nSo far I haven’t had that problem but that’s only because I’ve been running it with low context lengths, since I’m using it on my gaming pc right now.\n\nI’m in the middle of building a budget local AI pc right now, 2 MI50 32gbs with 64gb of ddr4 ram on am4. With 64gb of vram combined, I want to see if there are better options available to me.\n\nThanks in advance ",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best creative writing + long context model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfifhh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754111899,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanna use this model for DMing a dnd game as well as using it to write stories. I’d like it to be abliterated if possible.&lt;/p&gt;\n\n&lt;p&gt;I’ve been looking at using Gemma 3 27B, and I do like its writing style, but I’m concerned about its ability to handle long context lengths.&lt;/p&gt;\n\n&lt;p&gt;So far I haven’t had that problem but that’s only because I’ve been running it with low context lengths, since I’m using it on my gaming pc right now.&lt;/p&gt;\n\n&lt;p&gt;I’m in the middle of building a budget local AI pc right now, 2 MI50 32gbs with 64gb of ddr4 ram on am4. With 64gb of vram combined, I want to see if there are better options available to me.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfifhh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfifhh/best_creative_writing_long_context_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfifhh/best_creative_writing_long_context_model/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754111899,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I try to run all of my models at 32k context using llama.cpp, but it feels bad to be losing so much performance compared to launching with 2-4k context for short one-shot question prompts",
          "author_fullname": "t2_1j5x86i7wh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What context lengths do people actually run their models at?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfi8ly",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754111252,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I try to run all of my models at 32k context using llama.cpp, but it feels bad to be losing so much performance compared to launching with 2-4k context for short one-shot question prompts&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfi8ly",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OUT_OF_HOST_MEMORY",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfi8ly/what_context_lengths_do_people_actually_run_their/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfi8ly/what_context_lengths_do_people_actually_run_their/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754111252,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What an awesome model. Everything I throw at it I get comparable results to Gemma 3, but 4.5x faster.\n\nGreat at general knowledge, but also follows instructions very well.\n\nPlease let me know your experiences with it!",
          "author_fullname": "t2_d2gb9jhgg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 30b a3b 2507 instruct as good as Gemma 3 27B!?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf0i54",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 54,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 54,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754064349,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What an awesome model. Everything I throw at it I get comparable results to Gemma 3, but 4.5x faster.&lt;/p&gt;\n\n&lt;p&gt;Great at general knowledge, but also follows instructions very well.&lt;/p&gt;\n\n&lt;p&gt;Please let me know your experiences with it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mf0i54",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hanthunius",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf0i54/qwen_30b_a3b_2507_instruct_as_good_as_gemma_3_27b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf0i54/qwen_30b_a3b_2507_instruct_as_good_as_gemma_3_27b/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754064349,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Mentioned in the new, Qwen3 30B download announcement was that 480B's tool calling was fixed and it [needed to be re-downloaded](https://www.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/#:~:text=We%20also%20fixed%20tool%20calling%20for%20the%20480B%20and%20this%20model%20and%20fixed%2030B%20thinking%2C%20so%20please%20redownload%20the%20first%20shard)\n\nI'm just posting it so that no one misses it. I'm using LMStudio and it just showed as \"downloaded\". It didn't seem to know there was a change.\n\nEDIT: Yes, this only refers to the unsloth versions of 480B.  Thank you u/MikeRoz",
          "author_fullname": "t2_taorh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Heads up to those that downloaded Qwen3 Coder 480B before yesterday",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mexa2g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 70,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 70,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754059918,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754056847,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Mentioned in the new, Qwen3 30B download announcement was that 480B&amp;#39;s tool calling was fixed and it &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/#:%7E:text=We%20also%20fixed%20tool%20calling%20for%20the%20480B%20and%20this%20model%20and%20fixed%2030B%20thinking%2C%20so%20please%20redownload%20the%20first%20shard\"&gt;needed to be re-downloaded&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just posting it so that no one misses it. I&amp;#39;m using LMStudio and it just showed as &amp;quot;downloaded&amp;quot;. It didn&amp;#39;t seem to know there was a change.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Yes, this only refers to the unsloth versions of 480B.  Thank you &lt;a href=\"/u/MikeRoz\"&gt;u/MikeRoz&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mexa2g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VegetaTheGrump",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mexa2g/heads_up_to_those_that_downloaded_qwen3_coder/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mexa2g/heads_up_to_those_that_downloaded_qwen3_coder/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754056847,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,\n\nI am a medical student and had begun to spend a significant amount of time creating a clinic notebook using Notion. Problem is, I essentially have to take all the text from every pdf and PowerPoint, paste it into notion, reformat (this takes forever) only to be able to have the text searchable because it can only embed documents. Not search them.\n\nI had been reading about LLM which would essentially allow me to create a master file, upload the hundreds if not thousands of documents of medical information, and then use AI to search my documents and retrieve the info specified in the prompt.\n\nI’m just not sure if this is something I can do through ChatGPT, Claude, or using llama. Trying to become more educated in this. \n\nAny insight? Thoughts?\n\nThanks for your time.",
          "author_fullname": "t2_66d4kglu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Med school and LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfhv2c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754110002,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am a medical student and had begun to spend a significant amount of time creating a clinic notebook using Notion. Problem is, I essentially have to take all the text from every pdf and PowerPoint, paste it into notion, reformat (this takes forever) only to be able to have the text searchable because it can only embed documents. Not search them.&lt;/p&gt;\n\n&lt;p&gt;I had been reading about LLM which would essentially allow me to create a master file, upload the hundreds if not thousands of documents of medical information, and then use AI to search my documents and retrieve the info specified in the prompt.&lt;/p&gt;\n\n&lt;p&gt;I’m just not sure if this is something I can do through ChatGPT, Claude, or using llama. Trying to become more educated in this. &lt;/p&gt;\n\n&lt;p&gt;Any insight? Thoughts?&lt;/p&gt;\n\n&lt;p&gt;Thanks for your time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfhv2c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IndubitablyPreMed",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfhv2c/med_school_and_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfhv2c/med_school_and_llm/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754110002,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In the source code, we see a link to Hunyuan-4B-Instruct, but I think we’ll see much larger models :)\n\nbonus: fix hunyuan\\_moe chat template",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "support for the upcoming hunyuan dense models has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf0hou",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bbbdbf",
          "ups": 42,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 42,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=40b87c745f679bca8fe0aa91a0c19c36c7177277",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754064323,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the source code, we see a link to Hunyuan-4B-Instruct, but I think we’ll see much larger models :)&lt;/p&gt;\n\n&lt;p&gt;bonus: fix hunyuan_moe chat template&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14878",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?auto=webp&amp;s=7d88566cdde4131f4d8262c975dfd9b55caf89f5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed015fcc9ee802baeb72ee117bf3077725576fed",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b29546e91bdc6de331482f1a9525a141f86b6852",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9a5913f23624227672a7032140b173c0f2635ae9",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f81f256091726e20730924e97a225729e6c971ec",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7d6c5bdd7a760c5bf13d60c0766042314f38e1c",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e0455df24a27f9bce1dd5b8080cbffc5f292d90",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mf0hou",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mf0hou/support_for_the_upcoming_hunyuan_dense_models_has/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14878",
          "subreddit_subscribers": 508769,
          "created_utc": 1754064323,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Huggingface announced that it’s OSS so I found their GitHub, but the whole point of open source robotics is provision of CAD files and electronic drawings as well, if I am not wrong? \n\nI didn’t find it anywhere. \n[reachy mini](https://youtu.be/JvdBJZ-qR18?si=DntWm0Um6a9Nab6I)\nDo hugging face plan to release the printable 3d models and the component lists? \n\nBlog post: https://huggingface.co/blog/reachy-mini\nThomas Wolf on 𝕏: https://x.com/Thom_Wolf/status/1942887160983466096 less",
          "author_fullname": "t2_19t1rovk98",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Reach Mini is not Open source?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfj2bn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754114359,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754114167,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Huggingface announced that it’s OSS so I found their GitHub, but the whole point of open source robotics is provision of CAD files and electronic drawings as well, if I am not wrong? &lt;/p&gt;\n\n&lt;p&gt;I didn’t find it anywhere. \n&lt;a href=\"https://youtu.be/JvdBJZ-qR18?si=DntWm0Um6a9Nab6I\"&gt;reachy mini&lt;/a&gt;\nDo hugging face plan to release the printable 3d models and the component lists? &lt;/p&gt;\n\n&lt;p&gt;Blog post: &lt;a href=\"https://huggingface.co/blog/reachy-mini\"&gt;https://huggingface.co/blog/reachy-mini&lt;/a&gt;\nThomas Wolf on 𝕏: &lt;a href=\"https://x.com/Thom_Wolf/status/1942887160983466096\"&gt;https://x.com/Thom_Wolf/status/1942887160983466096&lt;/a&gt; less&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NCFk7nfLDEfVT123xThdE1hQ0xo7VlGM_ekFBsduBFk.jpeg?auto=webp&amp;s=42a3145ee1aa761204406729b87cbb2fce4b691a",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NCFk7nfLDEfVT123xThdE1hQ0xo7VlGM_ekFBsduBFk.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0a9e07799ac2b26c25790a649f719155c402025",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/NCFk7nfLDEfVT123xThdE1hQ0xo7VlGM_ekFBsduBFk.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f62c00eb6fb285199219b5efec22a6aa0213f7d",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/NCFk7nfLDEfVT123xThdE1hQ0xo7VlGM_ekFBsduBFk.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0cc0d47dae562a47f9241f9d6d74d3b712ec3a47",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "NCFk7nfLDEfVT123xThdE1hQ0xo7VlGM_ekFBsduBFk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfj2bn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Slow_Protection_26",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfj2bn/reach_mini_is_not_open_source/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfj2bn/reach_mini_is_not_open_source/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754114167,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I allocated more RAM and took the guard rail off. when loading the model the Activity monitor showed a brief red memory warning for 2-3 seconds but loads fine. The is 4bit version.Runs around 25-27 tokens/sec.When running inference memory pressure intermittently increases and it does use swap memory a around 1-12 GB in my case, but never showed red warning after loading it in memory.",
          "author_fullname": "t2_1nsamx8udx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5-Air running on 64GB Mac Studio(M4)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mesi2s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 112,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 112,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/xg-TV-u5t8CZYga5vqS7H6cvSQQNm6mI1iKN_y1d0Xg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754042719,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I allocated more RAM and took the guard rail off. when loading the model the Activity monitor showed a brief red memory warning for 2-3 seconds but loads fine. The is 4bit version.Runs around 25-27 tokens/sec.When running inference memory pressure intermittently increases and it does use swap memory a around 1-12 GB in my case, but never showed red warning after loading it in memory.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/87ng5bmisdgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/87ng5bmisdgf1.png?auto=webp&amp;s=278b1e2b20f96d9793b766eaf9a4662e0c4177fa",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e2b027c72800b8bb18356e66239bf4a6fe201ecf",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d6e4b8a9f3ad7ba583fe5cae8620c382e5db0be",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=136800587978cd65bc334b35b13dfe4c311ab4c6",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=54f42f44d09cb4df95a9f6ed8ad3cf70c2cc96bf",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8cde2cdaafe0d7e74429488394a890a17630d1a5",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a7ec880558fbdfda721507e10398b64ecc96a673",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "n4NTI1RG1xenOb1HsAwQ1LQYB7_slm74LFJZKuG7hJQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mesi2s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "riwritingreddit",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mesi2s/glm45air_running_on_64gb_mac_studiom4/",
          "stickied": false,
          "url": "https://i.redd.it/87ng5bmisdgf1.png",
          "subreddit_subscribers": 508769,
          "created_utc": 1754042719,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi. I'm a non native English writer, who could use some help with phrasing, [something like this](https://www.gingersoftware.com/products/sentence-rephraser), character and plot detail suggestions etc. Are there any good models that can help with that?\n\nI'm planning to buy a laptop with Nvidia 4060 GPU, which has 8GB RAM. Would that be enough? I can buy a Macbook with 24GB unified RAM which should give me effectively 16 GB VRAM (right?), but I would be drawing from my savings, which I would rather not do unless it's absolutely necessary. Please let me know if it is.",
          "author_fullname": "t2_169t2g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for a local model that can help a non native writer with sentence phrasing and ideas.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mfoh32",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754134900,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I&amp;#39;m a non native English writer, who could use some help with phrasing, &lt;a href=\"https://www.gingersoftware.com/products/sentence-rephraser\"&gt;something like this&lt;/a&gt;, character and plot detail suggestions etc. Are there any good models that can help with that?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to buy a laptop with Nvidia 4060 GPU, which has 8GB RAM. Would that be enough? I can buy a Macbook with 24GB unified RAM which should give me effectively 16 GB VRAM (right?), but I would be drawing from my savings, which I would rather not do unless it&amp;#39;s absolutely necessary. Please let me know if it is.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfoh32",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "logicSnob",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfoh32/looking_for_a_local_model_that_can_help_a_non/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfoh32/looking_for_a_local_model_that_can_help_a_non/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754134900,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi r/LocalLLaMA,\n\nI'm exploring the idea of creating a local agent that can interact with the Windows desktop environment. The primary goal is for the agent to be able to control the mouse and, most importantly, accurately identify and click on specific UI elements like buttons, menus, and text fields.\n\nFor example, I could give it a high-level command like \"Save the document and close the application,\" and it would need to:\n\n1. Visually parse the screen to locate the \"Save\" button or menu item.\n2. Move the mouse cursor to that location.\n3. Perform a click.\n4. Then, locate the \"Close\" button and do the same.\n\nI'm trying to figure out the best stack for this using local models. My main questions are:\n\n* **Vision/Perception:** What's the current best approach for a model to \"see\" the screen and identify clickable elements? Are there specific multi-modal models that are good at this out-of-the-box, or would I need a dedicated object detection model trained on UI elements?\n* **Decision Making (LLM):** How would the LLM receive the visual information and output the decision (e.g., \"click button with text 'OK' at coordinates \\[x, y\\]\")? What kind of prompting or fine-tuning would be required?\n* **Action/Control:** What are the recommended libraries for precise mouse control on Windows that can be easily integrated into a Python script? Is something like pyautogui the way to go, or are there more robust alternatives?\n* **Frameworks:** Are there any existing open-source projects or frameworks (similar to Open-Interpreter but maybe more focused on GUI) that I should be looking at as a starting point?\n\nI'm aiming for a solution that runs entirely locally. Any advice, links to papers, or pointers to GitHub repositories would be greatly appreciated!\n\nThanks",
          "author_fullname": "t2_1t42bjvz6a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to build a local agent for Windows GUI automation (mouse control &amp; accurate button clicking)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mfodac",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754134546,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m exploring the idea of creating a local agent that can interact with the Windows desktop environment. The primary goal is for the agent to be able to control the mouse and, most importantly, accurately identify and click on specific UI elements like buttons, menus, and text fields.&lt;/p&gt;\n\n&lt;p&gt;For example, I could give it a high-level command like &amp;quot;Save the document and close the application,&amp;quot; and it would need to:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Visually parse the screen to locate the &amp;quot;Save&amp;quot; button or menu item.&lt;/li&gt;\n&lt;li&gt;Move the mouse cursor to that location.&lt;/li&gt;\n&lt;li&gt;Perform a click.&lt;/li&gt;\n&lt;li&gt;Then, locate the &amp;quot;Close&amp;quot; button and do the same.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m trying to figure out the best stack for this using local models. My main questions are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Vision/Perception:&lt;/strong&gt; What&amp;#39;s the current best approach for a model to &amp;quot;see&amp;quot; the screen and identify clickable elements? Are there specific multi-modal models that are good at this out-of-the-box, or would I need a dedicated object detection model trained on UI elements?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Decision Making (LLM):&lt;/strong&gt; How would the LLM receive the visual information and output the decision (e.g., &amp;quot;click button with text &amp;#39;OK&amp;#39; at coordinates [x, y]&amp;quot;)? What kind of prompting or fine-tuning would be required?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Action/Control:&lt;/strong&gt; What are the recommended libraries for precise mouse control on Windows that can be easily integrated into a Python script? Is something like pyautogui the way to go, or are there more robust alternatives?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Frameworks:&lt;/strong&gt; Are there any existing open-source projects or frameworks (similar to Open-Interpreter but maybe more focused on GUI) that I should be looking at as a starting point?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m aiming for a solution that runs entirely locally. Any advice, links to papers, or pointers to GitHub repositories would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfodac",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xSNYPSx777",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfodac/how_to_build_a_local_agent_for_windows_gui/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfodac/how_to_build_a_local_agent_for_windows_gui/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754134546,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A quick heads up for anyone playing with the little [HuggingFaceTB/SmolLM3-3B](https://huggingface.co/HuggingFaceTB/SmolLM3-3B) model that was released a few weeks ago with llama.cpp.\n\nSmolLM3-3B supports toggling thinking mode using `/think` or `/no_think` in a system prompt, but it relies on Jinja template features that weren't available in llama.cpp's jinja processor until very recently (merged yesterday: [b56683eb](https://github.com/BradHutchings/Mmojo-Server/commit/b56683eb659d6d39138bd90b27cb258a21b7aa5c)).\n\nSo to get system-prompt `/think` and `/no_think` working, you need to be running the current master version of llama.cpp (until the next official release). I believe some Qwen3 templates might also be affected, so keep that in mind if you're using those.\n\n(And since it relies on the jinja template, if you want to be able to enable/disable thinking from the system prompt remember to pass `--jinja` to llama-cli and llama-server. Otherwise it will use a fallback template with no system prompt and no thinking.)\n\nAdditionally, I ran into a frustrating issue while using the llama-server with the built-in web client where SmolLM3-3B would stop thinking after a few messages even with thinking enabled. It turns out the model needs to see the `&lt;think&gt;&lt;/think&gt;` tags in previous messages or it will stop thinking. The llama web client, by default, has an option enabled that strips those tags.\n\nTo fix this, go to your web client settings -&gt; Reasoning and disable \"Exclude thought process when sending requests to API (Recommended for DeepSeek-R1)\".\n\nFinally, to have the web client correctly show the \"thinking\" section (that you can click to expand/collapse), you need to pass the `--reasoning-format none` option to llama-server. Example invocation:\n\n    ./llama-server --jinja -ngl 99 --temp 0.6 --reasoning-format none -c 64000 -fa -m ~/llama/models/smollm3-3b/SmolLM3-Q8_0.gguf",
          "author_fullname": "t2_38xkk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Getting SmolLM3-3B's /think and /no_think to work with llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfeipz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754099505,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A quick heads up for anyone playing with the little &lt;a href=\"https://huggingface.co/HuggingFaceTB/SmolLM3-3B\"&gt;HuggingFaceTB/SmolLM3-3B&lt;/a&gt; model that was released a few weeks ago with llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;SmolLM3-3B supports toggling thinking mode using &lt;code&gt;/think&lt;/code&gt; or &lt;code&gt;/no_think&lt;/code&gt; in a system prompt, but it relies on Jinja template features that weren&amp;#39;t available in llama.cpp&amp;#39;s jinja processor until very recently (merged yesterday: &lt;a href=\"https://github.com/BradHutchings/Mmojo-Server/commit/b56683eb659d6d39138bd90b27cb258a21b7aa5c\"&gt;b56683eb&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;So to get system-prompt &lt;code&gt;/think&lt;/code&gt; and &lt;code&gt;/no_think&lt;/code&gt; working, you need to be running the current master version of llama.cpp (until the next official release). I believe some Qwen3 templates might also be affected, so keep that in mind if you&amp;#39;re using those.&lt;/p&gt;\n\n&lt;p&gt;(And since it relies on the jinja template, if you want to be able to enable/disable thinking from the system prompt remember to pass &lt;code&gt;--jinja&lt;/code&gt; to llama-cli and llama-server. Otherwise it will use a fallback template with no system prompt and no thinking.)&lt;/p&gt;\n\n&lt;p&gt;Additionally, I ran into a frustrating issue while using the llama-server with the built-in web client where SmolLM3-3B would stop thinking after a few messages even with thinking enabled. It turns out the model needs to see the &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; tags in previous messages or it will stop thinking. The llama web client, by default, has an option enabled that strips those tags.&lt;/p&gt;\n\n&lt;p&gt;To fix this, go to your web client settings -&amp;gt; Reasoning and disable &amp;quot;Exclude thought process when sending requests to API (Recommended for DeepSeek-R1)&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Finally, to have the web client correctly show the &amp;quot;thinking&amp;quot; section (that you can click to expand/collapse), you need to pass the &lt;code&gt;--reasoning-format none&lt;/code&gt; option to llama-server. Example invocation:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-server --jinja -ngl 99 --temp 0.6 --reasoning-format none -c 64000 -fa -m ~/llama/models/smollm3-3b/SmolLM3-Q8_0.gguf\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?auto=webp&amp;s=049d11d6074271bc2869bc098ea0e6349fad17b5",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=210d7e5aa5feb8c1ba4995490dffc2c6390e22d2",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=72ddf9689e0962d05c6c024b4b695cb9794bb6ca",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=981e7a5684252e695178248adea004772a6db062",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5512ea26289819cd2f0dfd1f239935cece32dfcf",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=434567689943a3b73e96f753c8a3258d47c9a766",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2ff25afc08fd9bbaf1816eb558f97ca00295655f",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mfeipz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cristoper",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfeipz/getting_smollm33bs_think_and_no_think_to_work/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfeipz/getting_smollm33bs_think_and_no_think_to_work/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754099505,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_j2lm1hb2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SVDQuant does INT4 quantization of text-to-image models without losing quality. Can't the same technique be used in LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 90,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf08e5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 38,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 38,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/N50eG8wUbPgLuCMeBevOa0eyQWBygGeeMCJcikzOQiM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754063762,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/0cq321qc1fgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?auto=webp&amp;s=55a8648bf4010ec76dde9382784506ff1f279b6b",
                  "width": 3293,
                  "height": 2120
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9b9b51c5d0f9c3b9de0cbaa0cdbb1c69e4ee263",
                    "width": 108,
                    "height": 69
                  },
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8ffc5eae3e6f9fd9ac0ac50aea60adf88a4c90f0",
                    "width": 216,
                    "height": 139
                  },
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6e8b0e358cd85ccb5a6738bd9900251292d4eb9",
                    "width": 320,
                    "height": 206
                  },
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=162d6f061d05ab906b370e0b7ce08f4a7f85014d",
                    "width": 640,
                    "height": 412
                  },
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=20c332654151a9cd9bcbb7f5f225cbb367a05536",
                    "width": 960,
                    "height": 618
                  },
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5661bfba947547555164c9f84545ce932ae65377",
                    "width": 1080,
                    "height": 695
                  }
                ],
                "variants": {},
                "id": "D8iRezvmv7BRzfgNCmicLKUlcwGdQOUpQHIUZAPRPow"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf08e5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "we_are_mammals",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf08e5/svdquant_does_int4_quantization_of_texttoimage/",
          "stickied": false,
          "url": "https://i.redd.it/0cq321qc1fgf1.jpeg",
          "subreddit_subscribers": 508769,
          "created_utc": 1754063762,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sharing **DocStrange**, an open-source Python library that makes document data extraction easy.\n\n* **Universal Input**: PDFs, Images, Word docs, PowerPoint, Excel\n* **Multiple Outputs**: Clean Markdown, structured JSON, CSV tables, formatted HTML\n* **Smart Extraction**: Specify exact fields you want (e.g., \"invoice\\_number\", \"total\\_amount\")\n* **Schema Support**: Define JSON schemas for consistent structured output\n\n**Quick start:**\n\n    from docstrange import DocumentExtractor\n    \n    extractor = DocumentExtractor()\n    result = extractor.extract(\"research_paper.pdf\")\n    \n    # Get clean markdown for LLM training\n    markdown = result.extract_markdown()\n\n**CLI**\n\n    pip install docstrange\n    docstrange document.pdf --output json --extract-fields title author date\n\n**Data Processing Options**\n\n* **Cloud Mode**: Fast and free processing with minimal setup\n* **Local Mode**: Complete privacy - all processing happens on your machine, no data sent anywhere, works on both cpu and gpu\n\n**Links:**\n\n* PyPI: [https://pypi.org/project/docstrange/](https://pypi.org/project/docstrange/)",
          "author_fullname": "t2_1mv39a664r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DocStrange - Open Source Document Data Extractor",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 88,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mepr38",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 172,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 172,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/n_8zXMrMBAhovAnFfx7BottIYrMkhK42uDJfBp4i85Y.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754032135,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sharing &lt;strong&gt;DocStrange&lt;/strong&gt;, an open-source Python library that makes document data extraction easy.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Universal Input&lt;/strong&gt;: PDFs, Images, Word docs, PowerPoint, Excel&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Multiple Outputs&lt;/strong&gt;: Clean Markdown, structured JSON, CSV tables, formatted HTML&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Smart Extraction&lt;/strong&gt;: Specify exact fields you want (e.g., &amp;quot;invoice_number&amp;quot;, &amp;quot;total_amount&amp;quot;)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Schema Support&lt;/strong&gt;: Define JSON schemas for consistent structured output&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Quick start:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from docstrange import DocumentExtractor\n\nextractor = DocumentExtractor()\nresult = extractor.extract(&amp;quot;research_paper.pdf&amp;quot;)\n\n# Get clean markdown for LLM training\nmarkdown = result.extract_markdown()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;CLI&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pip install docstrange\ndocstrange document.pdf --output json --extract-fields title author date\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Data Processing Options&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Cloud Mode&lt;/strong&gt;: Fast and free processing with minimal setup&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Local Mode&lt;/strong&gt;: Complete privacy - all processing happens on your machine, no data sent anywhere, works on both cpu and gpu&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;PyPI: &lt;a href=\"https://pypi.org/project/docstrange/\"&gt;https://pypi.org/project/docstrange/&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/vghke2r1ycgf1.gif",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/vghke2r1ycgf1.gif?format=png8&amp;s=0646017ef4eefc01cddecd722d9f8f2d50380882",
                  "width": 1138,
                  "height": 717
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=fc63503d84bbe4a224856c2516262be363da4ac8",
                    "width": 108,
                    "height": 68
                  },
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=3c6a452195c5261d81d558756ca975edc42b2fae",
                    "width": 216,
                    "height": 136
                  },
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=5175141f8d36c2b53aecbc5cb9f68c8f3fdc88ef",
                    "width": 320,
                    "height": 201
                  },
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=b028677410eada3f069136c8e6c1b8c48c06f285",
                    "width": 640,
                    "height": 403
                  },
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=83ef66bfaa9d844d3079c6f8d7f7ec2c933079bb",
                    "width": 960,
                    "height": 604
                  },
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=e9c266f359e28d38390413a4c7f732940b9da31e",
                    "width": 1080,
                    "height": 680
                  }
                ],
                "variants": {
                  "gif": {
                    "source": {
                      "url": "https://preview.redd.it/vghke2r1ycgf1.gif?s=5ddee0d3d4b59db883ad7324e63d6d44c4c6edc1",
                      "width": 1138,
                      "height": 717
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=108&amp;crop=smart&amp;s=f965200e3ad373df4b75762cdb1f8901fa898b66",
                        "width": 108,
                        "height": 68
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=216&amp;crop=smart&amp;s=2ede88d7ce8de83577c6d4caba7ac71eadd2671c",
                        "width": 216,
                        "height": 136
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=320&amp;crop=smart&amp;s=85aaad5e77f8fa02ca6dcd40563302da8c506a58",
                        "width": 320,
                        "height": 201
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=640&amp;crop=smart&amp;s=12643bc505cd05a85286b55a7fff556b82b4872a",
                        "width": 640,
                        "height": 403
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=960&amp;crop=smart&amp;s=0a82ceb7382adf023c53606bed72529202ab9f69",
                        "width": 960,
                        "height": 604
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=1080&amp;crop=smart&amp;s=2786689a41ab5170679f3efd992a64e464d981e2",
                        "width": 1080,
                        "height": 680
                      }
                    ]
                  },
                  "mp4": {
                    "source": {
                      "url": "https://preview.redd.it/vghke2r1ycgf1.gif?format=mp4&amp;s=7043fb757ff13b2cf394629a8043afef1a0a0995",
                      "width": 1138,
                      "height": 717
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=108&amp;format=mp4&amp;s=50bc5d05247661d61d18415388e20a7fd4866caf",
                        "width": 108,
                        "height": 68
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=216&amp;format=mp4&amp;s=d334100ea224720cfcaa53b23a6a1fd993d24945",
                        "width": 216,
                        "height": 136
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=320&amp;format=mp4&amp;s=f8eed5e239635d7c160661180140118e60510641",
                        "width": 320,
                        "height": 201
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=640&amp;format=mp4&amp;s=23de0d42f6d1959233bf55d4f8f865fb4f03146e",
                        "width": 640,
                        "height": 403
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=960&amp;format=mp4&amp;s=f9a9a9f47cf0d08c3603a0b57a5250634e9d42ea",
                        "width": 960,
                        "height": 604
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=1080&amp;format=mp4&amp;s=340f0a49b5bca4ea0f0ee8982400c578d1646297",
                        "width": 1080,
                        "height": 680
                      }
                    ]
                  }
                },
                "id": "rhbuyimmJ2s8b0MEkSp5-3yQopRWq3kKuTIAyO2Dmsk"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mepr38",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LostAmbassador6872",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/",
          "stickied": false,
          "url": "https://i.redd.it/vghke2r1ycgf1.gif",
          "subreddit_subscribers": 508769,
          "created_utc": 1754032135,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all, an enthusiast but no formal CS training background asking for help \n\nI am trying to make an application for collageus in medical research using a local LLM. The most important requirement is that it can run on any standard issue laptop (mostly just CPU) - as that's the best we can get :) \n\nWhich is the best \"small size\" LLM for document question answering with European language - mostly specific medical jargon.\n\nI tried the several and found that Qwen3 1.6B did suprisingly well with German and Dutch. Also llama 3.2 3B did well but was to large for most machines unfortunately.\n\nI am running the app using ollama and langchain also any recommendations for alternatives are welcome :)\n",
          "author_fullname": "t2_1e27iwczfb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best &lt;2B open-source LLMs for European languages?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mfnfrp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754131171,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, an enthusiast but no formal CS training background asking for help &lt;/p&gt;\n\n&lt;p&gt;I am trying to make an application for collageus in medical research using a local LLM. The most important requirement is that it can run on any standard issue laptop (mostly just CPU) - as that&amp;#39;s the best we can get :) &lt;/p&gt;\n\n&lt;p&gt;Which is the best &amp;quot;small size&amp;quot; LLM for document question answering with European language - mostly specific medical jargon.&lt;/p&gt;\n\n&lt;p&gt;I tried the several and found that Qwen3 1.6B did suprisingly well with German and Dutch. Also llama 3.2 3B did well but was to large for most machines unfortunately.&lt;/p&gt;\n\n&lt;p&gt;I am running the app using ollama and langchain also any recommendations for alternatives are welcome :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfnfrp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Material-Ad5426",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfnfrp/best_2b_opensource_llms_for_european_languages/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfnfrp/best_2b_opensource_llms_for_european_languages/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754131171,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Does the current llama.cpp binaries release support Blackwell GPU in Windows? I just got the card and not sure how to move forward.\n\nDo I need to recompile the binaries for Windows ? Please share your experience. Much appreciated.",
          "author_fullname": "t2_1vxjobhr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Blackwell (RTX 5090 / RTX 6000 Pro) support in llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfgqb0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754106336,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does the current llama.cpp binaries release support Blackwell GPU in Windows? I just got the card and not sure how to move forward.&lt;/p&gt;\n\n&lt;p&gt;Do I need to recompile the binaries for Windows ? Please share your experience. Much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfgqb0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Loud_Structure4664",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfgqb0/blackwell_rtx_5090_rtx_6000_pro_support_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfgqb0/blackwell_rtx_5090_rtx_6000_pro_support_in/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754106336,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://github.com/electroglyph/quant_clone\n\nThis is a tiny little app which will create a llama-quantize command based on how a target GGUF is quantized. I wanted it so that I can quantize my finetunes the same way Unsloth does.\n\nFor instance, if you run quant_clone gemma-3-1b-it-UD-IQ1_S.gguf\n\nyou get:\n\nllama-quantize --imatrix &lt;imatrix_unsloth.dat&gt; --tensor-type token_embd.weight=Q5_1 --tensor-type blk.0.attn_k.weight=IQ4_NL --tensor-type blk.0.attn_output.weight=IQ2_XXS --tensor-type blk.0.attn_q.weight=IQ4_NL --tensor-type blk.0.attn_v.weight=Q5_0 --tensor-type blk.0.ffn_down.weight=IQ3_S --tensor-type blk.0.ffn_gate.weight=IQ4_NL --tensor-type blk.0.ffn_up.weight=IQ4_NL --tensor-type blk.1.attn_k.weight=IQ4_NL --tensor-type blk.1.attn_output.weight=IQ2_XXS --tensor-type blk.1.attn_q.weight=IQ4_NL --tensor-type blk.1.attn_v.weight=Q5_0 --tensor-type blk.1.ffn_down.weight=Q2_K --tensor-type blk.1.ffn_gate.weight=IQ4_NL --tensor-type blk.1.ffn_up.weight=IQ4_NL --tensor-type blk.2.attn_k.weight=IQ4_NL --tensor-type blk.2.attn_output.weight=IQ2_XXS --tensor-type blk.2.attn_q.weight=IQ4_NL --tensor-type blk.2.attn_v.weight=Q5_0 --tensor-type blk.2.ffn_down.weight=IQ3_S --tensor-type blk.2.ffn_gate.weight=IQ4_NL --tensor-type blk.2.ffn_up.weight=IQ4_NL --tensor-type blk.3.attn_k.weight=IQ4_NL --tensor-type blk.3.attn_output.weight=IQ2_XXS --tensor-type blk.3.attn_q.weight=IQ4_NL --tensor-type blk.3.attn_v.weight=Q5_0 --tensor-type blk.3.ffn_down.weight=IQ3_S --tensor-type blk.3.ffn_gate.weight=IQ4_NL --tensor-type blk.3.ffn_up.weight=IQ4_NL --tensor-type blk.4.attn_k.weight=IQ4_NL --tensor-type blk.4.attn_output.weight=IQ2_XXS --tensor-type blk.4.attn_q.weight=IQ4_NL --tensor-type blk.4.attn_v.weight=Q5_0 --tensor-type blk.4.ffn_down.weight=IQ3_S --tensor-type blk.4.ffn_gate.weight=IQ4_NL --tensor-type blk.4.ffn_up.weight=IQ4_NL --tensor-type blk.5.attn_k.weight=IQ4_NL --tensor-type blk.5.attn_output.weight=IQ2_XXS --tensor-type blk.5.attn_q.weight=IQ4_NL --tensor-type blk.5.attn_v.weight=Q5_0 --tensor-type blk.5.ffn_down.weight=IQ1_S --tensor-type blk.5.ffn_gate.weight=IQ4_NL --tensor-type blk.5.ffn_up.weight=IQ4_NL --tensor-type blk.6.attn_k.weight=IQ4_NL --tensor-type blk.6.attn_output.weight=IQ2_XXS --tensor-type blk.6.attn_q.weight=IQ4_NL --tensor-type blk.6.attn_v.weight=Q5_0 --tensor-type blk.6.ffn_down.weight=IQ1_S --tensor-type blk.6.ffn_gate.weight=IQ4_NL --tensor-type blk.6.ffn_up.weight=IQ4_NL --tensor-type blk.7.attn_k.weight=IQ4_NL --tensor-type blk.7.attn_output.weight=IQ2_XXS --tensor-type blk.7.attn_q.weight=IQ4_NL --tensor-type blk.7.attn_v.weight=Q5_0 --tensor-type blk.7.ffn_down.weight=IQ1_S --tensor-type blk.7.ffn_gate.weight=IQ4_NL --tensor-type blk.7.ffn_up.weight=IQ4_NL --tensor-type blk.8.attn_k.weight=IQ4_NL --tensor-type blk.8.attn_output.weight=IQ2_XXS --tensor-type blk.8.attn_q.weight=IQ4_NL --tensor-type blk.8.attn_v.weight=Q5_0 --tensor-type blk.8.ffn_down.weight=IQ1_S --tensor-type blk.8.ffn_gate.weight=IQ4_NL --tensor-type blk.8.ffn_up.weight=IQ4_NL --tensor-type blk.9.attn_k.weight=IQ4_NL --tensor-type blk.9.attn_output.weight=IQ2_XXS --tensor-type blk.9.attn_q.weight=IQ4_NL --tensor-type blk.9.attn_v.weight=Q5_0 --tensor-type blk.9.ffn_down.weight=IQ1_S --tensor-type blk.9.ffn_gate.weight=IQ4_NL --tensor-type blk.9.ffn_up.weight=IQ4_NL --tensor-type blk.10.attn_k.weight=IQ4_NL --tensor-type blk.10.attn_output.weight=IQ2_XXS --tensor-type blk.10.attn_q.weight=IQ4_NL --tensor-type blk.10.attn_v.weight=Q5_0 --tensor-type blk.10.ffn_down.weight=IQ1_S --tensor-type blk.10.ffn_gate.weight=IQ4_NL --tensor-type blk.10.ffn_up.weight=IQ4_NL --tensor-type blk.11.attn_k.weight=IQ4_NL --tensor-type blk.11.attn_output.weight=IQ2_XXS --tensor-type blk.11.attn_q.weight=IQ4_NL --tensor-type blk.11.attn_v.weight=Q5_0 --tensor-type blk.11.ffn_down.weight=IQ2_S --tensor-type blk.11.ffn_gate.weight=IQ4_NL --tensor-type blk.11.ffn_up.weight=IQ4_NL --tensor-type blk.12.attn_k.weight=IQ4_NL --tensor-type blk.12.attn_output.weight=IQ2_XXS --tensor-type blk.12.attn_q.weight=IQ4_NL --tensor-type blk.12.attn_v.weight=Q5_0 --tensor-type blk.12.ffn_down.weight=IQ2_S --tensor-type blk.12.ffn_gate.weight=IQ4_NL --tensor-type blk.12.ffn_up.weight=IQ4_NL --tensor-type blk.13.attn_k.weight=IQ4_NL --tensor-type blk.13.attn_output.weight=IQ2_XXS --tensor-type blk.13.attn_q.weight=IQ4_NL --tensor-type blk.13.attn_v.weight=Q5_0 --tensor-type blk.13.ffn_down.weight=IQ2_S --tensor-type blk.13.ffn_gate.weight=IQ4_NL --tensor-type blk.13.ffn_up.weight=IQ4_NL --tensor-type blk.14.attn_k.weight=IQ4_NL --tensor-type blk.14.attn_output.weight=IQ2_XXS --tensor-type blk.14.attn_q.weight=IQ4_NL --tensor-type blk.14.attn_v.weight=Q5_0 --tensor-type blk.14.ffn_down.weight=IQ2_S --tensor-type blk.14.ffn_gate.weight=IQ4_NL --tensor-type blk.14.ffn_up.weight=IQ4_NL --tensor-type blk.15.attn_k.weight=IQ4_NL --tensor-type blk.15.attn_output.weight=IQ2_XXS --tensor-type blk.15.attn_q.weight=IQ4_NL --tensor-type blk.15.attn_v.weight=Q5_0 --tensor-type blk.15.ffn_down.weight=IQ2_S --tensor-type blk.15.ffn_gate.weight=IQ4_NL --tensor-type blk.15.ffn_up.weight=IQ4_NL --tensor-type blk.16.attn_k.weight=IQ4_NL --tensor-type blk.16.attn_output.weight=IQ2_XXS --tensor-type blk.16.attn_q.weight=IQ4_NL --tensor-type blk.16.attn_v.weight=Q5_0 --tensor-type blk.16.ffn_down.weight=IQ1_S --tensor-type blk.16.ffn_gate.weight=IQ4_NL --tensor-type blk.16.ffn_up.weight=IQ4_NL --tensor-type blk.17.attn_k.weight=IQ4_NL --tensor-type blk.17.attn_output.weight=IQ2_XXS --tensor-type blk.17.attn_q.weight=IQ4_NL --tensor-type blk.17.attn_v.weight=Q5_0 --tensor-type blk.17.ffn_down.weight=IQ1_S --tensor-type blk.17.ffn_gate.weight=IQ4_NL --tensor-type blk.17.ffn_up.weight=IQ4_NL --tensor-type blk.18.attn_k.weight=IQ4_NL --tensor-type blk.18.attn_output.weight=IQ2_XXS --tensor-type blk.18.attn_q.weight=IQ4_NL --tensor-type blk.18.attn_v.weight=Q5_0 --tensor-type blk.18.ffn_down.weight=IQ1_S --tensor-type blk.18.ffn_gate.weight=IQ4_NL --tensor-type blk.18.ffn_up.weight=IQ4_NL --tensor-type blk.19.attn_k.weight=IQ4_NL --tensor-type blk.19.attn_output.weight=IQ2_XXS --tensor-type blk.19.attn_q.weight=IQ4_NL --tensor-type blk.19.attn_v.weight=Q5_0 --tensor-type blk.19.ffn_down.weight=IQ1_S --tensor-type blk.19.ffn_gate.weight=IQ4_NL --tensor-type blk.19.ffn_up.weight=IQ4_NL --tensor-type blk.20.attn_k.weight=IQ4_NL --tensor-type blk.20.attn_output.weight=IQ2_XXS --tensor-type blk.20.attn_q.weight=IQ4_NL --tensor-type blk.20.attn_v.weight=Q5_0 --tensor-type blk.20.ffn_down.weight=IQ1_S --tensor-type blk.20.ffn_gate.weight=IQ4_NL --tensor-type blk.20.ffn_up.weight=IQ4_NL --tensor-type blk.21.attn_k.weight=IQ4_NL --tensor-type blk.21.attn_output.weight=IQ2_XXS --tensor-type blk.21.attn_q.weight=IQ4_NL --tensor-type blk.21.attn_v.weight=Q5_0 --tensor-type blk.21.ffn_down.weight=IQ1_S --tensor-type blk.21.ffn_gate.weight=IQ4_NL --tensor-type blk.21.ffn_up.weight=IQ4_NL --tensor-type blk.22.attn_k.weight=IQ4_NL --tensor-type blk.22.attn_output.weight=IQ2_XXS --tensor-type blk.22.attn_q.weight=IQ4_NL --tensor-type blk.22.attn_v.weight=Q5_0 --tensor-type blk.22.ffn_down.weight=IQ1_S --tensor-type blk.22.ffn_gate.weight=IQ4_NL --tensor-type blk.22.ffn_up.weight=IQ4_NL --tensor-type blk.23.attn_k.weight=IQ4_NL --tensor-type blk.23.attn_output.weight=IQ2_XXS --tensor-type blk.23.attn_q.weight=IQ4_NL --tensor-type blk.23.attn_v.weight=Q5_0 --tensor-type blk.23.ffn_down.weight=IQ1_S --tensor-type blk.23.ffn_gate.weight=IQ4_NL --tensor-type blk.23.ffn_up.weight=IQ4_NL --tensor-type blk.24.attn_k.weight=IQ4_NL --tensor-type blk.24.attn_output.weight=IQ2_XXS --tensor-type blk.24.attn_q.weight=IQ4_NL --tensor-type blk.24.attn_v.weight=Q5_0 --tensor-type blk.24.ffn_down.weight=IQ1_S --tensor-type blk.24.ffn_gate.weight=IQ4_NL --tensor-type blk.24.ffn_up.weight=IQ4_NL --tensor-type blk.25.attn_k.weight=IQ4_NL --tensor-type blk.25.attn_output.weight=IQ2_XXS --tensor-type blk.25.attn_q.weight=IQ4_NL --tensor-type blk.25.attn_v.weight=Q5_0 --tensor-type blk.25.ffn_down.weight=IQ3_S --tensor-type blk.25.ffn_gate.weight=IQ4_NL --tensor-type blk.25.ffn_up.weight=IQ4_NL &lt;input.gguf&gt; &lt;output.gguf&gt; Q8_0\n\nnote that the Q8_0 at the end is just to get llama-quantize to do it's thing (F16/F32/COPY doesn't run quantization).\n all the tensors will be overridden with the actual --tensor-type params",
          "author_fullname": "t2_1iu07dnz2i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Quantize your own GGUFs the same way as your fav Unsloth Dynamic GGUFs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mes7rc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 85,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 85,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754043657,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754041684,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/electroglyph/quant_clone\"&gt;https://github.com/electroglyph/quant_clone&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is a tiny little app which will create a llama-quantize command based on how a target GGUF is quantized. I wanted it so that I can quantize my finetunes the same way Unsloth does.&lt;/p&gt;\n\n&lt;p&gt;For instance, if you run quant_clone gemma-3-1b-it-UD-IQ1_S.gguf&lt;/p&gt;\n\n&lt;p&gt;you get:&lt;/p&gt;\n\n&lt;p&gt;llama-quantize --imatrix &amp;lt;imatrix_unsloth.dat&amp;gt; --tensor-type token_embd.weight=Q5_1 --tensor-type blk.0.attn_k.weight=IQ4_NL --tensor-type blk.0.attn_output.weight=IQ2_XXS --tensor-type blk.0.attn_q.weight=IQ4_NL --tensor-type blk.0.attn_v.weight=Q5_0 --tensor-type blk.0.ffn_down.weight=IQ3_S --tensor-type blk.0.ffn_gate.weight=IQ4_NL --tensor-type blk.0.ffn_up.weight=IQ4_NL --tensor-type blk.1.attn_k.weight=IQ4_NL --tensor-type blk.1.attn_output.weight=IQ2_XXS --tensor-type blk.1.attn_q.weight=IQ4_NL --tensor-type blk.1.attn_v.weight=Q5_0 --tensor-type blk.1.ffn_down.weight=Q2_K --tensor-type blk.1.ffn_gate.weight=IQ4_NL --tensor-type blk.1.ffn_up.weight=IQ4_NL --tensor-type blk.2.attn_k.weight=IQ4_NL --tensor-type blk.2.attn_output.weight=IQ2_XXS --tensor-type blk.2.attn_q.weight=IQ4_NL --tensor-type blk.2.attn_v.weight=Q5_0 --tensor-type blk.2.ffn_down.weight=IQ3_S --tensor-type blk.2.ffn_gate.weight=IQ4_NL --tensor-type blk.2.ffn_up.weight=IQ4_NL --tensor-type blk.3.attn_k.weight=IQ4_NL --tensor-type blk.3.attn_output.weight=IQ2_XXS --tensor-type blk.3.attn_q.weight=IQ4_NL --tensor-type blk.3.attn_v.weight=Q5_0 --tensor-type blk.3.ffn_down.weight=IQ3_S --tensor-type blk.3.ffn_gate.weight=IQ4_NL --tensor-type blk.3.ffn_up.weight=IQ4_NL --tensor-type blk.4.attn_k.weight=IQ4_NL --tensor-type blk.4.attn_output.weight=IQ2_XXS --tensor-type blk.4.attn_q.weight=IQ4_NL --tensor-type blk.4.attn_v.weight=Q5_0 --tensor-type blk.4.ffn_down.weight=IQ3_S --tensor-type blk.4.ffn_gate.weight=IQ4_NL --tensor-type blk.4.ffn_up.weight=IQ4_NL --tensor-type blk.5.attn_k.weight=IQ4_NL --tensor-type blk.5.attn_output.weight=IQ2_XXS --tensor-type blk.5.attn_q.weight=IQ4_NL --tensor-type blk.5.attn_v.weight=Q5_0 --tensor-type blk.5.ffn_down.weight=IQ1_S --tensor-type blk.5.ffn_gate.weight=IQ4_NL --tensor-type blk.5.ffn_up.weight=IQ4_NL --tensor-type blk.6.attn_k.weight=IQ4_NL --tensor-type blk.6.attn_output.weight=IQ2_XXS --tensor-type blk.6.attn_q.weight=IQ4_NL --tensor-type blk.6.attn_v.weight=Q5_0 --tensor-type blk.6.ffn_down.weight=IQ1_S --tensor-type blk.6.ffn_gate.weight=IQ4_NL --tensor-type blk.6.ffn_up.weight=IQ4_NL --tensor-type blk.7.attn_k.weight=IQ4_NL --tensor-type blk.7.attn_output.weight=IQ2_XXS --tensor-type blk.7.attn_q.weight=IQ4_NL --tensor-type blk.7.attn_v.weight=Q5_0 --tensor-type blk.7.ffn_down.weight=IQ1_S --tensor-type blk.7.ffn_gate.weight=IQ4_NL --tensor-type blk.7.ffn_up.weight=IQ4_NL --tensor-type blk.8.attn_k.weight=IQ4_NL --tensor-type blk.8.attn_output.weight=IQ2_XXS --tensor-type blk.8.attn_q.weight=IQ4_NL --tensor-type blk.8.attn_v.weight=Q5_0 --tensor-type blk.8.ffn_down.weight=IQ1_S --tensor-type blk.8.ffn_gate.weight=IQ4_NL --tensor-type blk.8.ffn_up.weight=IQ4_NL --tensor-type blk.9.attn_k.weight=IQ4_NL --tensor-type blk.9.attn_output.weight=IQ2_XXS --tensor-type blk.9.attn_q.weight=IQ4_NL --tensor-type blk.9.attn_v.weight=Q5_0 --tensor-type blk.9.ffn_down.weight=IQ1_S --tensor-type blk.9.ffn_gate.weight=IQ4_NL --tensor-type blk.9.ffn_up.weight=IQ4_NL --tensor-type blk.10.attn_k.weight=IQ4_NL --tensor-type blk.10.attn_output.weight=IQ2_XXS --tensor-type blk.10.attn_q.weight=IQ4_NL --tensor-type blk.10.attn_v.weight=Q5_0 --tensor-type blk.10.ffn_down.weight=IQ1_S --tensor-type blk.10.ffn_gate.weight=IQ4_NL --tensor-type blk.10.ffn_up.weight=IQ4_NL --tensor-type blk.11.attn_k.weight=IQ4_NL --tensor-type blk.11.attn_output.weight=IQ2_XXS --tensor-type blk.11.attn_q.weight=IQ4_NL --tensor-type blk.11.attn_v.weight=Q5_0 --tensor-type blk.11.ffn_down.weight=IQ2_S --tensor-type blk.11.ffn_gate.weight=IQ4_NL --tensor-type blk.11.ffn_up.weight=IQ4_NL --tensor-type blk.12.attn_k.weight=IQ4_NL --tensor-type blk.12.attn_output.weight=IQ2_XXS --tensor-type blk.12.attn_q.weight=IQ4_NL --tensor-type blk.12.attn_v.weight=Q5_0 --tensor-type blk.12.ffn_down.weight=IQ2_S --tensor-type blk.12.ffn_gate.weight=IQ4_NL --tensor-type blk.12.ffn_up.weight=IQ4_NL --tensor-type blk.13.attn_k.weight=IQ4_NL --tensor-type blk.13.attn_output.weight=IQ2_XXS --tensor-type blk.13.attn_q.weight=IQ4_NL --tensor-type blk.13.attn_v.weight=Q5_0 --tensor-type blk.13.ffn_down.weight=IQ2_S --tensor-type blk.13.ffn_gate.weight=IQ4_NL --tensor-type blk.13.ffn_up.weight=IQ4_NL --tensor-type blk.14.attn_k.weight=IQ4_NL --tensor-type blk.14.attn_output.weight=IQ2_XXS --tensor-type blk.14.attn_q.weight=IQ4_NL --tensor-type blk.14.attn_v.weight=Q5_0 --tensor-type blk.14.ffn_down.weight=IQ2_S --tensor-type blk.14.ffn_gate.weight=IQ4_NL --tensor-type blk.14.ffn_up.weight=IQ4_NL --tensor-type blk.15.attn_k.weight=IQ4_NL --tensor-type blk.15.attn_output.weight=IQ2_XXS --tensor-type blk.15.attn_q.weight=IQ4_NL --tensor-type blk.15.attn_v.weight=Q5_0 --tensor-type blk.15.ffn_down.weight=IQ2_S --tensor-type blk.15.ffn_gate.weight=IQ4_NL --tensor-type blk.15.ffn_up.weight=IQ4_NL --tensor-type blk.16.attn_k.weight=IQ4_NL --tensor-type blk.16.attn_output.weight=IQ2_XXS --tensor-type blk.16.attn_q.weight=IQ4_NL --tensor-type blk.16.attn_v.weight=Q5_0 --tensor-type blk.16.ffn_down.weight=IQ1_S --tensor-type blk.16.ffn_gate.weight=IQ4_NL --tensor-type blk.16.ffn_up.weight=IQ4_NL --tensor-type blk.17.attn_k.weight=IQ4_NL --tensor-type blk.17.attn_output.weight=IQ2_XXS --tensor-type blk.17.attn_q.weight=IQ4_NL --tensor-type blk.17.attn_v.weight=Q5_0 --tensor-type blk.17.ffn_down.weight=IQ1_S --tensor-type blk.17.ffn_gate.weight=IQ4_NL --tensor-type blk.17.ffn_up.weight=IQ4_NL --tensor-type blk.18.attn_k.weight=IQ4_NL --tensor-type blk.18.attn_output.weight=IQ2_XXS --tensor-type blk.18.attn_q.weight=IQ4_NL --tensor-type blk.18.attn_v.weight=Q5_0 --tensor-type blk.18.ffn_down.weight=IQ1_S --tensor-type blk.18.ffn_gate.weight=IQ4_NL --tensor-type blk.18.ffn_up.weight=IQ4_NL --tensor-type blk.19.attn_k.weight=IQ4_NL --tensor-type blk.19.attn_output.weight=IQ2_XXS --tensor-type blk.19.attn_q.weight=IQ4_NL --tensor-type blk.19.attn_v.weight=Q5_0 --tensor-type blk.19.ffn_down.weight=IQ1_S --tensor-type blk.19.ffn_gate.weight=IQ4_NL --tensor-type blk.19.ffn_up.weight=IQ4_NL --tensor-type blk.20.attn_k.weight=IQ4_NL --tensor-type blk.20.attn_output.weight=IQ2_XXS --tensor-type blk.20.attn_q.weight=IQ4_NL --tensor-type blk.20.attn_v.weight=Q5_0 --tensor-type blk.20.ffn_down.weight=IQ1_S --tensor-type blk.20.ffn_gate.weight=IQ4_NL --tensor-type blk.20.ffn_up.weight=IQ4_NL --tensor-type blk.21.attn_k.weight=IQ4_NL --tensor-type blk.21.attn_output.weight=IQ2_XXS --tensor-type blk.21.attn_q.weight=IQ4_NL --tensor-type blk.21.attn_v.weight=Q5_0 --tensor-type blk.21.ffn_down.weight=IQ1_S --tensor-type blk.21.ffn_gate.weight=IQ4_NL --tensor-type blk.21.ffn_up.weight=IQ4_NL --tensor-type blk.22.attn_k.weight=IQ4_NL --tensor-type blk.22.attn_output.weight=IQ2_XXS --tensor-type blk.22.attn_q.weight=IQ4_NL --tensor-type blk.22.attn_v.weight=Q5_0 --tensor-type blk.22.ffn_down.weight=IQ1_S --tensor-type blk.22.ffn_gate.weight=IQ4_NL --tensor-type blk.22.ffn_up.weight=IQ4_NL --tensor-type blk.23.attn_k.weight=IQ4_NL --tensor-type blk.23.attn_output.weight=IQ2_XXS --tensor-type blk.23.attn_q.weight=IQ4_NL --tensor-type blk.23.attn_v.weight=Q5_0 --tensor-type blk.23.ffn_down.weight=IQ1_S --tensor-type blk.23.ffn_gate.weight=IQ4_NL --tensor-type blk.23.ffn_up.weight=IQ4_NL --tensor-type blk.24.attn_k.weight=IQ4_NL --tensor-type blk.24.attn_output.weight=IQ2_XXS --tensor-type blk.24.attn_q.weight=IQ4_NL --tensor-type blk.24.attn_v.weight=Q5_0 --tensor-type blk.24.ffn_down.weight=IQ1_S --tensor-type blk.24.ffn_gate.weight=IQ4_NL --tensor-type blk.24.ffn_up.weight=IQ4_NL --tensor-type blk.25.attn_k.weight=IQ4_NL --tensor-type blk.25.attn_output.weight=IQ2_XXS --tensor-type blk.25.attn_q.weight=IQ4_NL --tensor-type blk.25.attn_v.weight=Q5_0 --tensor-type blk.25.ffn_down.weight=IQ3_S --tensor-type blk.25.ffn_gate.weight=IQ4_NL --tensor-type blk.25.ffn_up.weight=IQ4_NL &amp;lt;input.gguf&amp;gt; &amp;lt;output.gguf&amp;gt; Q8_0&lt;/p&gt;\n\n&lt;p&gt;note that the Q8_0 at the end is just to get llama-quantize to do it&amp;#39;s thing (F16/F32/COPY doesn&amp;#39;t run quantization).\n all the tensors will be overridden with the actual --tensor-type params&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/EHJuKtCnvYSJ1UjIHcOg34gQGWDcA8FabBbGIfxwkWM.png?auto=webp&amp;s=4c25855ab618b6c289af90519a298d57a8784074",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/EHJuKtCnvYSJ1UjIHcOg34gQGWDcA8FabBbGIfxwkWM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3c6340d4c3319c41f51c294613b6f0ee3409e9e9",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/EHJuKtCnvYSJ1UjIHcOg34gQGWDcA8FabBbGIfxwkWM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e6c5d821bd8d8f7a576c21eddccc7fd3b8d95cc9",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/EHJuKtCnvYSJ1UjIHcOg34gQGWDcA8FabBbGIfxwkWM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4fbea7690bb16f5eeb879aeefb1a075c407f7ef4",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/EHJuKtCnvYSJ1UjIHcOg34gQGWDcA8FabBbGIfxwkWM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9662b446d45b54e570661b0f9784671f95ab00b5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/EHJuKtCnvYSJ1UjIHcOg34gQGWDcA8FabBbGIfxwkWM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=92c123a9754ee499617edcfd73460e2fb6f6cb5f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/EHJuKtCnvYSJ1UjIHcOg34gQGWDcA8FabBbGIfxwkWM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=235eb4cfa77dc55840c742cac9176649dfe3dc03",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "EHJuKtCnvYSJ1UjIHcOg34gQGWDcA8FabBbGIfxwkWM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mes7rc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "terminoid_",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mes7rc/quantize_your_own_ggufs_the_same_way_as_your_fav/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mes7rc/quantize_your_own_ggufs_the_same_way_as_your_fav/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754041684,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/8edgr1plcegf1.png?width=888&amp;format=png&amp;auto=webp&amp;s=8a4d35aab872fa2e1ca113eb8f8510212b54d68b\n\nLower PPL = Better\n\nI didn't test q6 and q8 because they can't fit in my 24gb card\n\n    llama-perplexity.exe --model \"\" --threads 15 --ctx-size 8000 -f wiki.test.raw --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --n-gpu-layers 99  --mlock --parallel 8 --seed 7894 --temp 0.7 --top-k 20 --top-p 0.8 --min-p 0 --repeat-penalty 1.05 --presence-penalty 1.5\n\nIQ4\\_XS  \n7 experts PPL = 7.6844  \ndefault 8 experts PPL = 7.6741  \n9 experts PPL = 7.6890  \n10 experts PPL = 7.7343",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Unsloth GGUFs Perplexity Score Comparison | Qwen3-Coder-30B-A3B-Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 50,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8edgr1plcegf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 38,
                  "x": 108,
                  "u": "https://preview.redd.it/8edgr1plcegf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f33a87b7d5a2b89d99b675efa972110ce149e2e0"
                },
                {
                  "y": 77,
                  "x": 216,
                  "u": "https://preview.redd.it/8edgr1plcegf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=47f88a8062278317f6e4599e854f575f83e74f15"
                },
                {
                  "y": 114,
                  "x": 320,
                  "u": "https://preview.redd.it/8edgr1plcegf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d95223ba37a44fe73ddc601e81a4e6b338490b2"
                },
                {
                  "y": 229,
                  "x": 640,
                  "u": "https://preview.redd.it/8edgr1plcegf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e082a292e3b3db212e76c24677430b78a6bfa35"
                }
              ],
              "s": {
                "y": 319,
                "x": 888,
                "u": "https://preview.redd.it/8edgr1plcegf1.png?width=888&amp;format=png&amp;auto=webp&amp;s=8a4d35aab872fa2e1ca113eb8f8510212b54d68b"
              },
              "id": "8edgr1plcegf1"
            }
          },
          "name": "t3_1meucvo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 57,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 57,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/VNoO8rEJvUZhkVA7_FI5gY826jnCUY53ZbZFN6EhWxs.jpg",
          "edited": 1754050687,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754048983,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/8edgr1plcegf1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a4d35aab872fa2e1ca113eb8f8510212b54d68b\"&gt;https://preview.redd.it/8edgr1plcegf1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a4d35aab872fa2e1ca113eb8f8510212b54d68b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Lower PPL = Better&lt;/p&gt;\n\n&lt;p&gt;I didn&amp;#39;t test q6 and q8 because they can&amp;#39;t fit in my 24gb card&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama-perplexity.exe --model &amp;quot;&amp;quot; --threads 15 --ctx-size 8000 -f wiki.test.raw --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --n-gpu-layers 99  --mlock --parallel 8 --seed 7894 --temp 0.7 --top-k 20 --top-p 0.8 --min-p 0 --repeat-penalty 1.05 --presence-penalty 1.5\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;IQ4_XS&lt;br/&gt;\n7 experts PPL = 7.6844&lt;br/&gt;\ndefault 8 experts PPL = 7.6741&lt;br/&gt;\n9 experts PPL = 7.6890&lt;br/&gt;\n10 experts PPL = 7.7343&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1meucvo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1meucvo/unsloth_ggufs_perplexity_score_comparison/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1meucvo/unsloth_ggufs_perplexity_score_comparison/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754048983,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recently switched my local models to the new 30B-A3B 2507 models. However, when testing the instruct model, I noticed it hallucinates much more than previous Qwen models.\n\nI fed it a README file I wrote myself for summarization, so I know its contents well. The 2507 instruct model not only uses excessive emojis but also fabricates lots of information that isn’t in the file.\n\nI also tested the 2507 thinking and coder versions with the same README, prompt, and quantization level (q4). Both used zero emojis and showed no noticeable hallucinations.\n\nHas anyone else experienced similar issues with the 2507 instruct model?\n\n* I'm using llama.cpp + llama swap, and the \"best practice\" settings from the HF model card ",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Serious hallucination issues of 30B-A3B Instruct 2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfiroj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.56,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754113093,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently switched my local models to the new 30B-A3B 2507 models. However, when testing the instruct model, I noticed it hallucinates much more than previous Qwen models.&lt;/p&gt;\n\n&lt;p&gt;I fed it a README file I wrote myself for summarization, so I know its contents well. The 2507 instruct model not only uses excessive emojis but also fabricates lots of information that isn’t in the file.&lt;/p&gt;\n\n&lt;p&gt;I also tested the 2507 thinking and coder versions with the same README, prompt, and quantization level (q4). Both used zero emojis and showed no noticeable hallucinations.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else experienced similar issues with the 2507 instruct model?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;m using llama.cpp + llama swap, and the &amp;quot;best practice&amp;quot; settings from the HF model card &lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfiroj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfiroj/serious_hallucination_issues_of_30ba3b_instruct/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfiroj/serious_hallucination_issues_of_30ba3b_instruct/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754113093,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "just curious on what the community thinks how these models compare in real world use cases. I have tried glm 4.5 quite a lot and would say im pretty impressed by it. I haven't tried K2 or qwen3 coder that much yet so for now im biased towards glm 4.5\n\n  \nas now benchmarks basically mean nothing, im curious what everyone here thinks of their coding abilities according to their personal experiences",
          "author_fullname": "t2_qatlsiyq4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "qwen3 coder vs glm 4.5 vs kimi k2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf955w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754084502,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;just curious on what the community thinks how these models compare in real world use cases. I have tried glm 4.5 quite a lot and would say im pretty impressed by it. I haven&amp;#39;t tried K2 or qwen3 coder that much yet so for now im biased towards glm 4.5&lt;/p&gt;\n\n&lt;p&gt;as now benchmarks basically mean nothing, im curious what everyone here thinks of their coding abilities according to their personal experiences&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mf955w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "YourAverageDev_",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf955w/qwen3_coder_vs_glm_45_vs_kimi_k2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf955w/qwen3_coder_vs_glm_45_vs_kimi_k2/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754084502,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "First time poster, so I'm not sure if this is the right area, but I'm looking for some help troubleshooting performance issues.\n\nWhen using models that fit in VRAM, I get the expected performance or within reason.\n\nThe issues occur when using models that need to spill over into system RAM. Specifically, I've noticed a significant drop in performance with the model **qwen3:30b-a3b-q4\\_K\\_M**, though **Deepseek R1 32B** is showing similar issues.\n\nWhen I run qwen3:30b-a3b-q4\\_K\\_M on CPU with no GPU installed I get \\~19t/s as measured by Open Web UI.\n\nWhen running qwen3:30b-a3b-q4\\_K\\_M on a mix of GPU/CPU I get the worse performance then running on CPU only. The performance degrades even further the more layers I offload to the CPU.\n\nTested the following in Ollama by modifying num_gpu:\n\nqwen3:30b-a3b-q4\\_K\\_M    0b28110b7a33    20 GB    25%/75% CPU/GPU    4096  \neval rate:           10.02 tokens/s\n\nqwen3:30b-a3b-q4\\_K\\_M    0b28110b7a33    20 GB    73%/27% CPU/GPU    4096  \neval rate:            4.35 tokens/s\n\nqwen3:30b-a3b-q4\\_K\\_M    0b28110b7a33    19 GB    100% CPU     4096  \neval rate:            2.49 tokens/s\n\nOS is hosted in Proxmox. Going from 30 cores to 15 cores assigned to the VM had no effect on performance.\n\nSystem Specs:\n\nCPU: Gold 6254\n\nGPU: Nvidia T4 (16gb)\n\nOS: ubuntu 24.04\n\nOllama 0.10.1\n\nNvidia Driver 570.169 Cuda 12.8\n\nAny suggestions would be helpful.",
          "author_fullname": "t2_seb9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Performance issues when using GPU and CPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mffuv0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754105472,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754103612,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First time poster, so I&amp;#39;m not sure if this is the right area, but I&amp;#39;m looking for some help troubleshooting performance issues.&lt;/p&gt;\n\n&lt;p&gt;When using models that fit in VRAM, I get the expected performance or within reason.&lt;/p&gt;\n\n&lt;p&gt;The issues occur when using models that need to spill over into system RAM. Specifically, I&amp;#39;ve noticed a significant drop in performance with the model &lt;strong&gt;qwen3:30b-a3b-q4_K_M&lt;/strong&gt;, though &lt;strong&gt;Deepseek R1 32B&lt;/strong&gt; is showing similar issues.&lt;/p&gt;\n\n&lt;p&gt;When I run qwen3:30b-a3b-q4_K_M on CPU with no GPU installed I get ~19t/s as measured by Open Web UI.&lt;/p&gt;\n\n&lt;p&gt;When running qwen3:30b-a3b-q4_K_M on a mix of GPU/CPU I get the worse performance then running on CPU only. The performance degrades even further the more layers I offload to the CPU.&lt;/p&gt;\n\n&lt;p&gt;Tested the following in Ollama by modifying num_gpu:&lt;/p&gt;\n\n&lt;p&gt;qwen3:30b-a3b-q4_K_M    0b28110b7a33    20 GB    25%/75% CPU/GPU    4096&lt;br/&gt;\neval rate:           10.02 tokens/s&lt;/p&gt;\n\n&lt;p&gt;qwen3:30b-a3b-q4_K_M    0b28110b7a33    20 GB    73%/27% CPU/GPU    4096&lt;br/&gt;\neval rate:            4.35 tokens/s&lt;/p&gt;\n\n&lt;p&gt;qwen3:30b-a3b-q4_K_M    0b28110b7a33    19 GB    100% CPU     4096&lt;br/&gt;\neval rate:            2.49 tokens/s&lt;/p&gt;\n\n&lt;p&gt;OS is hosted in Proxmox. Going from 30 cores to 15 cores assigned to the VM had no effect on performance.&lt;/p&gt;\n\n&lt;p&gt;System Specs:&lt;/p&gt;\n\n&lt;p&gt;CPU: Gold 6254&lt;/p&gt;\n\n&lt;p&gt;GPU: Nvidia T4 (16gb)&lt;/p&gt;\n\n&lt;p&gt;OS: ubuntu 24.04&lt;/p&gt;\n\n&lt;p&gt;Ollama 0.10.1&lt;/p&gt;\n\n&lt;p&gt;Nvidia Driver 570.169 Cuda 12.8&lt;/p&gt;\n\n&lt;p&gt;Any suggestions would be helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mffuv0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BabySasquatch1",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mffuv0/performance_issues_when_using_gpu_and_cpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mffuv0/performance_issues_when_using_gpu_and_cpu/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754103612,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking for some \"best practices\" for this new 30B A3B to squeeze the most out of it with my 4090. Normally I'm pretty up to date on this stuff but I'm a month or so behind the times. I'll share where I'm at and hopefully somebody's got some suggestions :).\n\nI'm sitting on 64gb ram/24gb vram (4090). I'm open to running this thing in ik\\_llama, tabby, vllm, whatever works best really. I have a mix of needs - ideally I'd like to have the best of all worlds (fast, low latency, high throughput), but I know it's all a bit of a \"pick two\" situation usually.\n\nI've got VLLM set up. Looks like I can run an AWQ quant of this thing at 8192 context fully in 24gb vram. If I bump down to an 8 bit KV Cache, I can fit 16,000 context.\n\nWith that setup with 16k context:\n\nOverall tokens/sec (single user, single request): 181.30t/s\n\nMean latency: 2.88s\n\nMean Time to First Token: 0.046s\n\nMax Batching tokens/s: 2,549.14t/s (100 requests)\n\nThat's not terrible as-is, and can hit the kinds of high throughput I need (2500 tokens per second is great, and even the single user 181t/s is snappy), but, I'm curious what my options are out there because I wouldn't mind adding a way to run this with much higher context limits. Like... if I can find a way to run it at an appreciable speed with 128k+ context I'd -love- that, even if that was only a single-user setup. Seems like I could do that with something like ik\\_llama, a ggml 4 or 8 bit 30b a3b, and my 24gb vram card holding part of the model with the rest offloaded into regular ram. Anybody running this thing on ik\\_llama want to chime in with some idea of how its performing and how you'r setting it up?  \n  \nOpen to any advice. I'd like to get this thing running as best I can for both a single user AND for batch-use (I'm fine with it being two separate setups, I can run them when needed appropriately).",
          "author_fullname": "t2_ddyte",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best way to run the Qwen3 30b A3B coder/instruct models for HIGH throughput and/or HIGH context? (on a single 4090)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf3wr0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754072100,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for some &amp;quot;best practices&amp;quot; for this new 30B A3B to squeeze the most out of it with my 4090. Normally I&amp;#39;m pretty up to date on this stuff but I&amp;#39;m a month or so behind the times. I&amp;#39;ll share where I&amp;#39;m at and hopefully somebody&amp;#39;s got some suggestions :).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sitting on 64gb ram/24gb vram (4090). I&amp;#39;m open to running this thing in ik_llama, tabby, vllm, whatever works best really. I have a mix of needs - ideally I&amp;#39;d like to have the best of all worlds (fast, low latency, high throughput), but I know it&amp;#39;s all a bit of a &amp;quot;pick two&amp;quot; situation usually.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got VLLM set up. Looks like I can run an AWQ quant of this thing at 8192 context fully in 24gb vram. If I bump down to an 8 bit KV Cache, I can fit 16,000 context.&lt;/p&gt;\n\n&lt;p&gt;With that setup with 16k context:&lt;/p&gt;\n\n&lt;p&gt;Overall tokens/sec (single user, single request): 181.30t/s&lt;/p&gt;\n\n&lt;p&gt;Mean latency: 2.88s&lt;/p&gt;\n\n&lt;p&gt;Mean Time to First Token: 0.046s&lt;/p&gt;\n\n&lt;p&gt;Max Batching tokens/s: 2,549.14t/s (100 requests)&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s not terrible as-is, and can hit the kinds of high throughput I need (2500 tokens per second is great, and even the single user 181t/s is snappy), but, I&amp;#39;m curious what my options are out there because I wouldn&amp;#39;t mind adding a way to run this with much higher context limits. Like... if I can find a way to run it at an appreciable speed with 128k+ context I&amp;#39;d -love- that, even if that was only a single-user setup. Seems like I could do that with something like ik_llama, a ggml 4 or 8 bit 30b a3b, and my 24gb vram card holding part of the model with the rest offloaded into regular ram. Anybody running this thing on ik_llama want to chime in with some idea of how its performing and how you&amp;#39;r setting it up?  &lt;/p&gt;\n\n&lt;p&gt;Open to any advice. I&amp;#39;d like to get this thing running as best I can for both a single user AND for batch-use (I&amp;#39;m fine with it being two separate setups, I can run them when needed appropriately).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf3wr0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "teachersecret",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754072100,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just wanted to share my results running Llama-4-Scout-17B-16E-Instruct-GGUF:Q4\\_K\\_S on my Ryzen AI Max + 395 using llama.cpp with Vulkan backend and the Lemonade server. I’m getting a solid 20 tokens/second with 60 GB of GPU memory in use. ",
          "author_fullname": "t2_1rpxg2806v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama-4-Scout-17B-16E-Instruct-GGUF:Q4_K_S running at 20 tk/s on Ryzen AI Max + 395 with llama.cpp Vulkan + Lemonade server (60GB GPU memory)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf6gaa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/zf13w9taqggf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/zf13w9taqggf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/zf13w9taqggf1/DASHPlaylist.mpd?a=1756729624%2CYjEwMzllYjIxYWEyNDgzZjhhMTc4YzI5NGZhODE1ZGUxN2RhN2I1MWZlN2NiYjEzYzg3NmU1NDEwNTg3ZGNjYQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 33,
              "hls_url": "https://v.redd.it/zf13w9taqggf1/HLSPlaylist.m3u8?a=1756729624%2CZGFlZDdjNzg0MGM4OWZmYmNjMjVkYzA5ZDNhYjRmNTY4NjA2MDcwNDRlYzQzNjc2ODdkNTY4NGMwMTM4NzlkZQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=4ad7991508d2b549531e751e830361ac79a4409b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754077979,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just wanted to share my results running Llama-4-Scout-17B-16E-Instruct-GGUF:Q4_K_S on my Ryzen AI Max + 395 using llama.cpp with Vulkan backend and the Lemonade server. I’m getting a solid 20 tokens/second with 60 GB of GPU memory in use. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/zf13w9taqggf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?format=pjpg&amp;auto=webp&amp;s=de129663feb9bbf95e122a859d728b746c9defbf",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e9150f6d4ace971cda6add553081a915ea75aff7",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0815afd00caf616bfa60e82c3f16ddc27fce172e",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=dc775410ecfa429f782ad107df358093728a3ee5",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2de42a5c72bba5762d5890300e1ca483a478e69b",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8985c8a18d1e7b326211843c594c2ffef6d43740",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=06a9c35365c0dc196bcf55cc7cc2519c5f5e8e01",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf6gaa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ShamanFlamingoFR",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf6gaa/llama4scout17b16einstructggufq4_k_s_running_at_20/",
          "stickied": false,
          "url": "https://v.redd.it/zf13w9taqggf1",
          "subreddit_subscribers": 508769,
          "created_utc": 1754077979,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/zf13w9taqggf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/zf13w9taqggf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/zf13w9taqggf1/DASHPlaylist.mpd?a=1756729624%2CYjEwMzllYjIxYWEyNDgzZjhhMTc4YzI5NGZhODE1ZGUxN2RhN2I1MWZlN2NiYjEzYzg3NmU1NDEwNTg3ZGNjYQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 33,
              "hls_url": "https://v.redd.it/zf13w9taqggf1/HLSPlaylist.m3u8?a=1756729624%2CZGFlZDdjNzg0MGM4OWZmYmNjMjVkYzA5ZDNhYjRmNTY4NjA2MDcwNDRlYzQzNjc2ODdkNTY4NGMwMTM4NzlkZQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My initial experiments with the model is very positive, i hope the space is useful for anyone who want to try the model",
          "author_fullname": "t2_1urjd1hc7b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hugging Face space for anyone who want to try the new Dots OCR",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mewq1v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=2ddac78399aaa336a5ddeb8d9b9734213c8aaeaa",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754055465,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My initial experiments with the model is very positive, i hope the space is useful for anyone who want to try the model&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/spaces/MohamedRashad/Dots-OCR",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?auto=webp&amp;s=f70ad2b805bd28f194b3315e14c66fa79cf5e878",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=98569493642c14e55daa842cd331344cc065f590",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=84e00f977b62e65f2805b76e111a0fd88be9b22d",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=66cdec75978c4df6afdd95b14cd67049cb3efc9d",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b249c1a963b7199d36b85e3948a0475db9194b46",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c50a5a839ab90944f505559a3f46bad15c7a1313",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=472d72171b9df47a0f46da4e7f91a9277c3f6c96",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mewq1v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Severe-Awareness829",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mewq1v/hugging_face_space_for_anyone_who_want_to_try_the/",
          "stickied": false,
          "url": "https://huggingface.co/spaces/MohamedRashad/Dots-OCR",
          "subreddit_subscribers": 508769,
          "created_utc": 1754055465,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I bought a second-hand server on eBay without knowing what was inside it. I knew I needed the case for my remote gaming rack solution. The Supermicro case had an air shroud and four oversized PCIe 3.0 x16 slots.\n\nWhen it arrived, I found an NVIDIA Tesla V100S 32 GB HBM2 PCIe 3.0 x16 GPU behind the air shroud. The seller probably didn't see it (it's worth far more than I paid for the whole case).\n\nWhile it's not the most up-to-date GPU anymore, I'm thinking of using it for home automation (it supports sharing the GPU with different VMs, where I can run various automation tasks and local LLMs to communicate with intruders, etc.).\n\nI used DeepSeek at work in our HPC. However, I am not up to date. Which models would work best with the 32 GB Tesla GPU I have? Do you have any other ideas?",
          "author_fullname": "t2_38k1zqv9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What to do with a NVIDIA Tesla V100S 32GB GPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfhji6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.55,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754108926,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought a second-hand server on eBay without knowing what was inside it. I knew I needed the case for my remote gaming rack solution. The Supermicro case had an air shroud and four oversized PCIe 3.0 x16 slots.&lt;/p&gt;\n\n&lt;p&gt;When it arrived, I found an NVIDIA Tesla V100S 32 GB HBM2 PCIe 3.0 x16 GPU behind the air shroud. The seller probably didn&amp;#39;t see it (it&amp;#39;s worth far more than I paid for the whole case).&lt;/p&gt;\n\n&lt;p&gt;While it&amp;#39;s not the most up-to-date GPU anymore, I&amp;#39;m thinking of using it for home automation (it supports sharing the GPU with different VMs, where I can run various automation tasks and local LLMs to communicate with intruders, etc.).&lt;/p&gt;\n\n&lt;p&gt;I used DeepSeek at work in our HPC. However, I am not up to date. Which models would work best with the 32 GB Tesla GPU I have? Do you have any other ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfhji6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gromhelmu",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfhji6/what_to_do_with_a_nvidia_tesla_v100s_32gb_gpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfhji6/what_to_do_with_a_nvidia_tesla_v100s_32gb_gpu/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754108926,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm using a 7900 GRE and training models via Applio. I’m getting about 1.88 seconds per iteration (see image). I've tried different setups and drivers with help from others, but the speed doesn't improve.\n\nJust wondering — anyone else using a 7900 GRE? What kind of speeds are you getting? Would love to compare.",
          "author_fullname": "t2_utmosfnh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RX 7900 GRE users: What training speeds do you get on Applio? (I'm seeing ~1.88s/it)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 13,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfc1oj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/w8yTEmbw2r5SmwMUy9NzFfJLvnoRmaAsSryV5GLTnt4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754092203,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using a 7900 GRE and training models via Applio. I’m getting about 1.88 seconds per iteration (see image). I&amp;#39;ve tried different setups and drivers with help from others, but the speed doesn&amp;#39;t improve.&lt;/p&gt;\n\n&lt;p&gt;Just wondering — anyone else using a 7900 GRE? What kind of speeds are you getting? Would love to compare.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/hrd001ynwhgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/hrd001ynwhgf1.png?auto=webp&amp;s=5c1b626d538a90bbc701de5ee36ef1f3b8364dc6",
                  "width": 1013,
                  "height": 99
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/hrd001ynwhgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e50eaa420fd6a63e598023e3cde5be0294f7495e",
                    "width": 108,
                    "height": 10
                  },
                  {
                    "url": "https://preview.redd.it/hrd001ynwhgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3c4495a93af7c650adf677f85ec950907e730217",
                    "width": 216,
                    "height": 21
                  },
                  {
                    "url": "https://preview.redd.it/hrd001ynwhgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6a87faa6041565843e93d320e9bb6e8f80d1bb7",
                    "width": 320,
                    "height": 31
                  },
                  {
                    "url": "https://preview.redd.it/hrd001ynwhgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=39770003cb041d457dd97ac2ebca7137dc3fe625",
                    "width": 640,
                    "height": 62
                  },
                  {
                    "url": "https://preview.redd.it/hrd001ynwhgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7ca18a2e6ce08f8d5b0b583a2449fd7aaf42644",
                    "width": 960,
                    "height": 93
                  }
                ],
                "variants": {},
                "id": "NedzWxTqeORWwluACmoHPoFt8g062kGYd_D3VPX8sNo"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfc1oj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lumpy-Quiet-7691",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfc1oj/rx_7900_gre_users_what_training_speeds_do_you_get/",
          "stickied": false,
          "url": "https://i.redd.it/hrd001ynwhgf1.png",
          "subreddit_subscribers": 508769,
          "created_utc": 1754092203,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sup guys. I've been using the voyage 3 lg as an embedding model for the longest time and because an embedding model can't be switched and you need to fill the vector database from scratch, I didn't switch even after the release of great OS models.  \nRecently I've been thinking of switching to either qwen 3 0.6b, 4b or 8b.  \nCan anyone tell me if in terms of performance voyage 3 lg beats these 3?  \nDon't worry about the pricing. Since the documents are already ingested using voyage 3 lg, the cost has already been paid, if I switch I do need to do that process all over again.\n\nThanks in advance.",
          "author_fullname": "t2_6f9r5p4z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Embedding models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfk4hx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754118068,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sup guys. I&amp;#39;ve been using the voyage 3 lg as an embedding model for the longest time and because an embedding model can&amp;#39;t be switched and you need to fill the vector database from scratch, I didn&amp;#39;t switch even after the release of great OS models.&lt;br/&gt;\nRecently I&amp;#39;ve been thinking of switching to either qwen 3 0.6b, 4b or 8b.&lt;br/&gt;\nCan anyone tell me if in terms of performance voyage 3 lg beats these 3?&lt;br/&gt;\nDon&amp;#39;t worry about the pricing. Since the documents are already ingested using voyage 3 lg, the cost has already been paid, if I switch I do need to do that process all over again.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfk4hx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blackkksparx",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfk4hx/embedding_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfk4hx/embedding_models/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754118068,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3yspdpju",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "More supposed info about OpenAI's open-weight model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1meqnn1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 74,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 74,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754035660,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/apples_jimmy/status/1951192085119508860",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1meqnn1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CheekyBastard55",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1meqnn1/more_supposed_info_about_openais_openweight_model/",
          "stickied": false,
          "url": "https://x.com/apples_jimmy/status/1951192085119508860",
          "subreddit_subscribers": 508769,
          "created_utc": 1754035660,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I don't have a powerful computer or GPU, just a 2021 macbook m1 with 8gb memory. I assume I can't run anything with more than 7b active parameters but chatgpt told me I can't run even run something like Qwen3-30B-A3B. What can I do, and where should I start?",
          "author_fullname": "t2_4i1pi15z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Getting started",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfjqcb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754116638,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I don&amp;#39;t have a powerful computer or GPU, just a 2021 macbook m1 with 8gb memory. I assume I can&amp;#39;t run anything with more than 7b active parameters but chatgpt told me I can&amp;#39;t run even run something like Qwen3-30B-A3B. What can I do, and where should I start?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfjqcb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Snoo-72709",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfjqcb/getting_started/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfjqcb/getting_started/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754116638,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/91lvjyp8wjgf1.png?width=914&amp;format=png&amp;auto=webp&amp;s=22de475006dd11573f99931c23e963cc06d5f910\n\nTill yesterday it was there but was giving some issues of workplace , but today they have completely removed the Full Stack tool",
          "author_fullname": "t2_amn2vwc3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM just removed there full stack tool...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 53,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "91lvjyp8wjgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 41,
                  "x": 108,
                  "u": "https://preview.redd.it/91lvjyp8wjgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf84c3973fbf2c776a5f27b753bcf5d020100161"
                },
                {
                  "y": 83,
                  "x": 216,
                  "u": "https://preview.redd.it/91lvjyp8wjgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a29eb93da8f42b1c19aa80cc6d4d2dfe223ac33f"
                },
                {
                  "y": 123,
                  "x": 320,
                  "u": "https://preview.redd.it/91lvjyp8wjgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cbc017963090285e050dd5dc5465da0a02e6c03a"
                },
                {
                  "y": 246,
                  "x": 640,
                  "u": "https://preview.redd.it/91lvjyp8wjgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=72afb773d2b9743030b1860951271b816813dc5b"
                }
              ],
              "s": {
                "y": 352,
                "x": 914,
                "u": "https://preview.redd.it/91lvjyp8wjgf1.png?width=914&amp;format=png&amp;auto=webp&amp;s=22de475006dd11573f99931c23e963cc06d5f910"
              },
              "id": "91lvjyp8wjgf1"
            }
          },
          "name": "t3_1mfjn9f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.44,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/kOwRnn8lDJI3S0tn3v6z_XszlYbAiUEIatDoCZHtUew.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754116335,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/91lvjyp8wjgf1.png?width=914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22de475006dd11573f99931c23e963cc06d5f910\"&gt;https://preview.redd.it/91lvjyp8wjgf1.png?width=914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22de475006dd11573f99931c23e963cc06d5f910&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Till yesterday it was there but was giving some issues of workplace , but today they have completely removed the Full Stack tool&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfjn9f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ITellMyselfSecrets__",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfjn9f/glm_just_removed_there_full_stack_tool/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfjn9f/glm_just_removed_there_full_stack_tool/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754116335,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🦥 Qwen3-Coder-Flash: Qwen3-Coder-30B-A3B-Instruct\n\n💚 Just lightning-fast, accurate code generation.\n\n✅ Native 256K context (supports up to 1M tokens with YaRN)\n\n✅ Optimized for platforms like Qwen Code, Cline, Roo Code, Kilo Code, etc.\n\n✅ Seamless function calling &amp; agent workflows\n\n💬 Chat: https://chat.qwen.ai/\n\n🤗 Hugging Face: https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct\n\n🤖 ModelScope: https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct\n\n",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🚀 Qwen3-Coder-Flash released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1me31d8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 1558,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 1558,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753972012,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🦥 Qwen3-Coder-Flash: Qwen3-Coder-30B-A3B-Instruct&lt;/p&gt;\n\n&lt;p&gt;💚 Just lightning-fast, accurate code generation.&lt;/p&gt;\n\n&lt;p&gt;✅ Native 256K context (supports up to 1M tokens with YaRN)&lt;/p&gt;\n\n&lt;p&gt;✅ Optimized for platforms like Qwen Code, Cline, Roo Code, Kilo Code, etc.&lt;/p&gt;\n\n&lt;p&gt;✅ Seamless function calling &amp;amp; agent workflows&lt;/p&gt;\n\n&lt;p&gt;💬 Chat: &lt;a href=\"https://chat.qwen.ai/\"&gt;https://chat.qwen.ai/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;🤗 Hugging Face: &lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct\"&gt;https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;🤖 ModelScope: &lt;a href=\"https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct\"&gt;https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/p7fpia2bz7gf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/p7fpia2bz7gf1.jpeg?auto=webp&amp;s=37bd250aae26692e18e3eeeca84c1caa9d999027",
                  "width": 2528,
                  "height": 1456
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/p7fpia2bz7gf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=95c4825b11a345671147c3d4e0b79207480f3ca0",
                    "width": 108,
                    "height": 62
                  },
                  {
                    "url": "https://preview.redd.it/p7fpia2bz7gf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2b30043242090b2abe359fb37f803cc7ab154ecb",
                    "width": 216,
                    "height": 124
                  },
                  {
                    "url": "https://preview.redd.it/p7fpia2bz7gf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=622e2e61ee2a35d0715b9a941f590819a1d0b1bb",
                    "width": 320,
                    "height": 184
                  },
                  {
                    "url": "https://preview.redd.it/p7fpia2bz7gf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a3985f36673a98a1456f01d26e1d6d8a9fd38fad",
                    "width": 640,
                    "height": 368
                  },
                  {
                    "url": "https://preview.redd.it/p7fpia2bz7gf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d95b4a66cac332ecb3b1b359a6ddce376dcbf89c",
                    "width": 960,
                    "height": 552
                  },
                  {
                    "url": "https://preview.redd.it/p7fpia2bz7gf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18f7994b018ed9d61d07b8cc13a8d5d948c008e7",
                    "width": 1080,
                    "height": 622
                  }
                ],
                "variants": {},
                "id": "p7fpia2bz7gf1"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1me31d8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 351,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/",
          "stickied": false,
          "url": "https://i.redd.it/p7fpia2bz7gf1.jpeg",
          "subreddit_subscribers": 508769,
          "created_utc": 1753972012,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What's the current recommended local LLM inference HW (**local, always-on inference box)** for multimodal LLMs (text, image, audio). Target workloads include home automation agents, real-time coding/writing, and vision models.  \nGoal is obviously largest models and the highest t/s, so highest VRAM and bandwidth, but with a toolchain that works.\n\n**What are the Hardware Options?:**\n\n* **Apple M3/M4 Ultra**\n* **AMD AI Max+ 395**\n* NVIDIA (DGX-Spark, etc.) or is Spark vaporware waiting for scalpers?\n\nWhat’s the most **practical prosumer option**?  \nIt would need to be lower cost than an RTX PRO 6000 Blackwell. I guess one could build an efficient mITX case around it, but I refuse to be price gouged by Nvidia.  \n\n\nI'm favoring the Strix Halo, but I think I'll be limited to Gemma 27B with maybe another model loaded at best.",
          "author_fullname": "t2_l0ba7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "24/7 local HW buying guide 2025-H2?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfj6fq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754114575,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the current recommended local LLM inference HW (&lt;strong&gt;local, always-on inference box)&lt;/strong&gt; for multimodal LLMs (text, image, audio). Target workloads include home automation agents, real-time coding/writing, and vision models.&lt;br/&gt;\nGoal is obviously largest models and the highest t/s, so highest VRAM and bandwidth, but with a toolchain that works.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What are the Hardware Options?:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Apple M3/M4 Ultra&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;AMD AI Max+ 395&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;NVIDIA (DGX-Spark, etc.) or is Spark vaporware waiting for scalpers?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What’s the most &lt;strong&gt;practical prosumer option&lt;/strong&gt;?&lt;br/&gt;\nIt would need to be lower cost than an RTX PRO 6000 Blackwell. I guess one could build an efficient mITX case around it, but I refuse to be price gouged by Nvidia.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m favoring the Strix Halo, but I think I&amp;#39;ll be limited to Gemma 27B with maybe another model loaded at best.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfj6fq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xraybies",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754114575,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Title",
          "author_fullname": "t2_bv8la",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help: I have an RTX 5090, can I realistically replace Claude Code in any way?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf9exw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.59,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754085187,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf9exw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nutyourself",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf9exw/help_i_have_an_rtx_5090_can_i_realistically/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf9exw/help_i_have_an_rtx_5090_can_i_realistically/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754085187,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I knew quotas were getting adjusted but never thought they would concern me, I code a few hours a day and that's about it. Today I have noticed I reach my limits within an hour-1.5h of coding, and that's with me being super careful with the context size, I try not to burn tokens for now reason. Frankly, it's unreal. Anyone else is experiencing the same shenanigans? I'm on pro btw.",
          "author_fullname": "t2_5aux6jy3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Claude Code - limit reached super quickly",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfe77f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754098552,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I knew quotas were getting adjusted but never thought they would concern me, I code a few hours a day and that&amp;#39;s about it. Today I have noticed I reach my limits within an hour-1.5h of coding, and that&amp;#39;s with me being super careful with the context size, I try not to burn tokens for now reason. Frankly, it&amp;#39;s unreal. Anyone else is experiencing the same shenanigans? I&amp;#39;m on pro btw.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfe77f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ys2020",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfe77f/claude_code_limit_reached_super_quickly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfe77f/claude_code_limit_reached_super_quickly/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754098552,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to switch from using claude code to running this model locally via cline or other similar extensions.\n\nMy Laptop's specs are:\ni5-11400H with 32GB DDR4 RAM at 2666Mhz.\nRTX 3060 Laptop GPU with 6GB GDDR6 VRAM.\n\nI got confused as there are a lot of inference engines available such as Ollama, LM studio, llama.cpp, vLLM, sglang, ik_llama.cpp etc. i dont know why there are som many of these and what are their pros and cons. So i wanted to ask here. I need the absolute fastest responses possible, i don't mind installing niche software or other things. \n\nThank you in advance.",
          "author_fullname": "t2_aedi2k9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to run Qwen3 Coder 30B-A3B the fastest?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mepr5q",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 57,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 57,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754032144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to switch from using claude code to running this model locally via cline or other similar extensions.&lt;/p&gt;\n\n&lt;p&gt;My Laptop&amp;#39;s specs are:\ni5-11400H with 32GB DDR4 RAM at 2666Mhz.\nRTX 3060 Laptop GPU with 6GB GDDR6 VRAM.&lt;/p&gt;\n\n&lt;p&gt;I got confused as there are a lot of inference engines available such as Ollama, LM studio, llama.cpp, vLLM, sglang, ik_llama.cpp etc. i dont know why there are som many of these and what are their pros and cons. So i wanted to ask here. I need the absolute fastest responses possible, i don&amp;#39;t mind installing niche software or other things. &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mepr5q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "R46H4V",
          "discussion_type": null,
          "num_comments": 48,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mepr5q/how_to_run_qwen3_coder_30ba3b_the_fastest/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mepr5q/how_to_run_qwen3_coder_30ba3b_the_fastest/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754032144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1umam7ln",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLama.cpp performance on ROCm",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf72g8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754079450,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/discussions/15021",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf72g8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "COBECT",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf72g8/llamacpp_performance_on_rocm/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/discussions/15021",
          "subreddit_subscribers": 508769,
          "created_utc": 1754079450,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Brothers and sisters, we're being taken for fools.\n\nhttps://preview.redd.it/d1iudzju8agf1.png?width=922&amp;format=png&amp;auto=webp&amp;s=c7d5d1e1b891425817fab581afae0149aec26b6b\n\nDid anyone check if it's phoning home?",
          "author_fullname": "t2_dbl0sjy8x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama's new GUI is closed source?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 26,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "d1iudzju8agf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 20,
                  "x": 108,
                  "u": "https://preview.redd.it/d1iudzju8agf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff8052a42d17b9a01f99afcf9e82dafecf05f172"
                },
                {
                  "y": 40,
                  "x": 216,
                  "u": "https://preview.redd.it/d1iudzju8agf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=811b8abd1cc4c1cf67268c7e022527bb4f00004c"
                },
                {
                  "y": 59,
                  "x": 320,
                  "u": "https://preview.redd.it/d1iudzju8agf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3280ff1631dc8d10074f4ffcb38bb11fbb8d9c04"
                },
                {
                  "y": 119,
                  "x": 640,
                  "u": "https://preview.redd.it/d1iudzju8agf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7cfd73d6aaba69097baafb426f1803fcf568fcac"
                }
              ],
              "s": {
                "y": 172,
                "x": 922,
                "u": "https://preview.redd.it/d1iudzju8agf1.png?width=922&amp;format=png&amp;auto=webp&amp;s=c7d5d1e1b891425817fab581afae0149aec26b6b"
              },
              "id": "d1iudzju8agf1"
            }
          },
          "name": "t3_1meeyee",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 282,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 282,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/zLVPiHg9ufyqhvp5Basb43POL8O8dmXli04dBAOzdrw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753999457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Brothers and sisters, we&amp;#39;re being taken for fools.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/d1iudzju8agf1.png?width=922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7d5d1e1b891425817fab581afae0149aec26b6b\"&gt;https://preview.redd.it/d1iudzju8agf1.png?width=922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7d5d1e1b891425817fab581afae0149aec26b6b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Did anyone check if it&amp;#39;s phoning home?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1meeyee",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sea_Night_2572",
          "discussion_type": null,
          "num_comments": 134,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1meeyee/ollamas_new_gui_is_closed_source/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1meeyee/ollamas_new_gui_is_closed_source/",
          "subreddit_subscribers": 508769,
          "created_utc": 1753999457,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been running multiple llama-server instances for different models and found myself constantly SSH-ing into servers to start, stop, and monitor them. After doing this dance one too many times, I decided to build a proper solution.\n\n[llamactl](https://github.com/lordmathis/llamactl) is a control server that lets you manage multiple llama-server instances through a web dashboard or REST API. It handles auto-restart on failures, provides real-time health monitoring, log management, and includes OpenAI-compatible endpoints for easy integration. Everything runs locally with no external dependencies.\n\nThe project is MIT licensed and contributions are welcome.",
          "author_fullname": "t2_w5tba",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Built a web dashboard to manage multiple llama-server instances - llamactl",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf3mhi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754071439,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been running multiple llama-server instances for different models and found myself constantly SSH-ing into servers to start, stop, and monitor them. After doing this dance one too many times, I decided to build a proper solution.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/lordmathis/llamactl\"&gt;llamactl&lt;/a&gt; is a control server that lets you manage multiple llama-server instances through a web dashboard or REST API. It handles auto-restart on failures, provides real-time health monitoring, log management, and includes OpenAI-compatible endpoints for easy integration. Everything runs locally with no external dependencies.&lt;/p&gt;\n\n&lt;p&gt;The project is MIT licensed and contributions are welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/kzOzGVzXfbNgBaONm1V9rEJeMaWG7hKBmJO8I7ak4y4.png?auto=webp&amp;s=4446fbdbc98ec997dd09af204505d3c848850e26",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/kzOzGVzXfbNgBaONm1V9rEJeMaWG7hKBmJO8I7ak4y4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=42bce2f9dd0a4ea31a5251bd4a2a838f62791353",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/kzOzGVzXfbNgBaONm1V9rEJeMaWG7hKBmJO8I7ak4y4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=96344968b5857b09a946444a140b7af1c14efc93",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/kzOzGVzXfbNgBaONm1V9rEJeMaWG7hKBmJO8I7ak4y4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ca9752bd2f6121f6394c53924b1a42c2271b691",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/kzOzGVzXfbNgBaONm1V9rEJeMaWG7hKBmJO8I7ak4y4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=70ba1c6f7e4e46eee55df8bb637cde83d6144d6d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/kzOzGVzXfbNgBaONm1V9rEJeMaWG7hKBmJO8I7ak4y4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fccd5238ce2a8be08562c28146934fefd6a42790",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/kzOzGVzXfbNgBaONm1V9rEJeMaWG7hKBmJO8I7ak4y4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=96aeb8ee2d4ba3e15b587a181065a40081020430",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "kzOzGVzXfbNgBaONm1V9rEJeMaWG7hKBmJO8I7ak4y4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf3mhi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RealLordMathis",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf3mhi/built_a_web_dashboard_to_manage_multiple/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf3mhi/built_a_web_dashboard_to_manage_multiple/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754071439,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello r/LocalLLaMA, This guide outlines a method to create a fully local AI coding assistant with RAG capabilities. The entire backend runs through LM Studio, which handles model downloading, options, serving, and tool integration, avoiding the need for Docker or separate Python environments. Heavily based on the previous guide by u/send_me_a_ticket (thanks!), just further simplified.\n\n* I know some of you wizards want to run things directly through CLI and llama.cpp etc, this guide is not for you.\n\n# Core Components\n\n* **Engine:** **LM Studio.** Used for downloading models, serving them via a local API, and running the tool server.\n* **Tool Server (RAG):** [**docs-mcp-server**](https://github.com/arabold/docs-mcp-server)**.** Runs as a plugin directly inside LM Studio to scrape and index documentation for the LLM to use.\n* **Frontend:** **VS Code +** [**Roo Code**](https://marketplace.visualstudio.com/items?itemName=hitbunt.roo-code)**.** The editor extension that connects to the local model server.\n\n# Advantages of this Approach\n\n* **Straightforward Setup:** Uses the LM Studio GUI for most of the configuration.\n* **100% Local &amp; Private:** Code and prompts are not sent to external services.\n* **VRAM-Friendly:** Optimized for running quantized GGUF models on consumer hardware.\n\n# Part 1: Configuring LM Studio\n\n**1. Install LM Studio** Download and install the latest version from the [LM Studio website](https://lmstudio.ai/).\n\n**2. Download Your Models** In the LM Studio main window (Search tab, magnifying glass icon), search for and download two models:\n\n* **A Coder LLM:** Example: `qwen/qwen3-coder-30b`\n* **An Embedding Model:** Example: `Qwen/Qwen3-Embedding-0.6B-GGUF`\n\n**3. Tune Model Settings** Navigate to the \"My Models\" tab (folder icon on the left). For both your LLM and your embedding model, you can click on them to tune settings like context length, GPU offload, and enable options like Flash Attention/QV Caching according to your model/hardware.\n\nQwen3 doesn't seem to like quantized QV Caching, resulting in Exit code: 18446744072635812000, so leave that off/default at f16.\n\n**4. Configure the** `docs-mcp-server` **Plugin**\n\n* Click the \"Chat\" tab (yellow chat bubble icon on top left).\n* Click on Program on the right.\n* Click on Install, select \\`Edit mcp.json', and replace its entire contents with this:\n\n&amp;#8203;\n\n        {\n          \"mcpServers\": {\n            \"docs-mcp-server\": {\n              \"command\": \"npx\",\n              \"args\": [\n                \"@arabold/docs-mcp-server@latest\"\n              ],\n              \"env\": {\n                \"OPENAI_API_KEY\": \"lmstudio\",\n                \"OPENAI_API_BASE\": \"http://localhost:1234/v1\",\n                \"DOCS_MCP_EMBEDDING_MODEL\": \"text-embedding-qwen3-embedding-0.6b\"\n              }\n            }\n          }\n        }\n\n*Note: Your* `DOCS_MCP_EMBEDDING_MODEL` *value must match the API Model Name shown on the Server tab once the model is loaded. If yours is different, you'll need to update it here.*\n\nIf it's correct, `the mcp/docs-mcp-server` tab will show things like `Tools`, `scrape_docs`, `search_docs`, ... etc.\n\n**5. Start the Server**\n\n* Navigate to the Local Server tab (`&gt;_` icon on the left).\n* In the top slot, load your coder LLM (e.g., Qwen3-Coder).\n* In the second slot, load your embedding model (e.g., Qwen3-Embeddings).\n* Click **Start Server**.\n* Check the server logs at the bottom to verify that the server is running and the `docs-mcp-server` plugin has loaded correctly.\n\n# Part 2: Configuring VS Code &amp; Roo Code\n\n**1. Install VS Code and Roo Code** Install [Visual Studio Code](https://code.visualstudio.com/). Then, inside VS Code, go to the Extensions tab and search for and install **Roo Code**.\n\n**2. Connect Roo Code to LM Studio**\n\n* In VS Code, click the Roo Code icon in the sidebar.\n* At the bottom, click the gear icon next to your profile name to open the settings.\n* Click **Add Profile**, give it a name (e.g., \"LM Studio\"), and configure it:\n* **LM Provider:** Select `LM Studio`\n* **Base URL:** [`http://127.0.0.1:1234`](http://127.0.0.1:1234) (or your server address)\n* **Model:** Select your coder model's ID (e.g., `qwen/qwen3-coder-30b`, it should appear automatically) .\n* While in the settings, you can go through the other tabs (like \"Auto-Approve\") and toggle preferences to fit your workflow.\n\n**3. Connect Roo Code to the Tool Server** Finally, we have to expose the mcp server to Roo.\n\n* In the Roo Code settings panel, click the 3 horizontal dots (top right), select \"MCP Servers\" from the drop-down menu.\n* Ensure the **\"Enable MCP Servers\"** checkbox is **ENABLED**.\n* Scroll down and click \"Edit Global MCP\", and replace the contents (if any) with this:\n\n&amp;#8203;\n\n    {\n      \"mcpServers\": {\n        \"docs-mcp-server\": {\n          \"command\": \"npx\",\n          \"args\": [\n            \"@arabold/docs-mcp-server@latest\"\n          ],\n          \"env\": {\n            \"OPENAI_API_KEY\": \"lmstudio\",\n            \"OPENAI_API_BASE\": \"http://localhost:1234/v1\",\n            \"DOCS_MCP_EMBEDDING_MODEL\": \"text-embedding-qwen3-embedding-0.6b\"\n          },\n          \"alwaysAllow\": [\n            \"fetch_url\",\n            \"remove_docs\",\n            \"scrape_docs\",\n            \"search_docs\",\n            \"list_libraries\",\n            \"find_version\",\n            \"list_jobs\",\n            \"get_job_info\",\n            \"cancel_job\"\n          ],\n          \"disabled\": false\n        }\n      }\n    }\n\n*Note: I'm not exactly sure how this part works. This is functional, but maybe contains redundancies. Hopefully someone with more knowledge can optimize this in the comments.*\n\nThen you can toggle it on and see a green circle if there's no issues.\n\nYour setup is now complete. You have a local coding assistant that can use the `docs-mcp-server` to perform RAG against documentation you provide.",
          "author_fullname": "t2_kggm5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Guide] The *SIMPLE* Self-Hosted AI Coding That Just Works feat. Qwen3-Coder-Flash",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1men28l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 82,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 82,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754031114,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754022492,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;, This guide outlines a method to create a fully local AI coding assistant with RAG capabilities. The entire backend runs through LM Studio, which handles model downloading, options, serving, and tool integration, avoiding the need for Docker or separate Python environments. Heavily based on the previous guide by &lt;a href=\"/u/send_me_a_ticket\"&gt;u/send_me_a_ticket&lt;/a&gt; (thanks!), just further simplified.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I know some of you wizards want to run things directly through CLI and llama.cpp etc, this guide is not for you.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Core Components&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Engine:&lt;/strong&gt; &lt;strong&gt;LM Studio.&lt;/strong&gt; Used for downloading models, serving them via a local API, and running the tool server.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Tool Server (RAG):&lt;/strong&gt; &lt;a href=\"https://github.com/arabold/docs-mcp-server\"&gt;&lt;strong&gt;docs-mcp-server&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; Runs as a plugin directly inside LM Studio to scrape and index documentation for the LLM to use.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; &lt;strong&gt;VS Code +&lt;/strong&gt; &lt;a href=\"https://marketplace.visualstudio.com/items?itemName=hitbunt.roo-code\"&gt;&lt;strong&gt;Roo Code&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; The editor extension that connects to the local model server.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Advantages of this Approach&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Straightforward Setup:&lt;/strong&gt; Uses the LM Studio GUI for most of the configuration.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;100% Local &amp;amp; Private:&lt;/strong&gt; Code and prompts are not sent to external services.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;VRAM-Friendly:&lt;/strong&gt; Optimized for running quantized GGUF models on consumer hardware.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Part 1: Configuring LM Studio&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Install LM Studio&lt;/strong&gt; Download and install the latest version from the &lt;a href=\"https://lmstudio.ai/\"&gt;LM Studio website&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Download Your Models&lt;/strong&gt; In the LM Studio main window (Search tab, magnifying glass icon), search for and download two models:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;A Coder LLM:&lt;/strong&gt; Example: &lt;code&gt;qwen/qwen3-coder-30b&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;An Embedding Model:&lt;/strong&gt; Example: &lt;code&gt;Qwen/Qwen3-Embedding-0.6B-GGUF&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Tune Model Settings&lt;/strong&gt; Navigate to the &amp;quot;My Models&amp;quot; tab (folder icon on the left). For both your LLM and your embedding model, you can click on them to tune settings like context length, GPU offload, and enable options like Flash Attention/QV Caching according to your model/hardware.&lt;/p&gt;\n\n&lt;p&gt;Qwen3 doesn&amp;#39;t seem to like quantized QV Caching, resulting in Exit code: 18446744072635812000, so leave that off/default at f16.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4. Configure the&lt;/strong&gt; &lt;code&gt;docs-mcp-server&lt;/code&gt; &lt;strong&gt;Plugin&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Click the &amp;quot;Chat&amp;quot; tab (yellow chat bubble icon on top left).&lt;/li&gt;\n&lt;li&gt;Click on Program on the right.&lt;/li&gt;\n&lt;li&gt;Click on Install, select `Edit mcp.json&amp;#39;, and replace its entire contents with this:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    {\n      &amp;quot;mcpServers&amp;quot;: {\n        &amp;quot;docs-mcp-server&amp;quot;: {\n          &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;,\n          &amp;quot;args&amp;quot;: [\n            &amp;quot;@arabold/docs-mcp-server@latest&amp;quot;\n          ],\n          &amp;quot;env&amp;quot;: {\n            &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;lmstudio&amp;quot;,\n            &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;,\n            &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;text-embedding-qwen3-embedding-0.6b&amp;quot;\n          }\n        }\n      }\n    }\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Note: Your&lt;/em&gt; &lt;code&gt;DOCS_MCP_EMBEDDING_MODEL&lt;/code&gt; &lt;em&gt;value must match the API Model Name shown on the Server tab once the model is loaded. If yours is different, you&amp;#39;ll need to update it here.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;If it&amp;#39;s correct, &lt;code&gt;the mcp/docs-mcp-server&lt;/code&gt; tab will show things like &lt;code&gt;Tools&lt;/code&gt;, &lt;code&gt;scrape_docs&lt;/code&gt;, &lt;code&gt;search_docs&lt;/code&gt;, ... etc.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5. Start the Server&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Navigate to the Local Server tab (&lt;code&gt;&amp;gt;_&lt;/code&gt; icon on the left).&lt;/li&gt;\n&lt;li&gt;In the top slot, load your coder LLM (e.g., Qwen3-Coder).&lt;/li&gt;\n&lt;li&gt;In the second slot, load your embedding model (e.g., Qwen3-Embeddings).&lt;/li&gt;\n&lt;li&gt;Click &lt;strong&gt;Start Server&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;Check the server logs at the bottom to verify that the server is running and the &lt;code&gt;docs-mcp-server&lt;/code&gt; plugin has loaded correctly.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Part 2: Configuring VS Code &amp;amp; Roo Code&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Install VS Code and Roo Code&lt;/strong&gt; Install &lt;a href=\"https://code.visualstudio.com/\"&gt;Visual Studio Code&lt;/a&gt;. Then, inside VS Code, go to the Extensions tab and search for and install &lt;strong&gt;Roo Code&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Connect Roo Code to LM Studio&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In VS Code, click the Roo Code icon in the sidebar.&lt;/li&gt;\n&lt;li&gt;At the bottom, click the gear icon next to your profile name to open the settings.&lt;/li&gt;\n&lt;li&gt;Click &lt;strong&gt;Add Profile&lt;/strong&gt;, give it a name (e.g., &amp;quot;LM Studio&amp;quot;), and configure it:&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;LM Provider:&lt;/strong&gt; Select &lt;code&gt;LM Studio&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Base URL:&lt;/strong&gt; &lt;a href=\"http://127.0.0.1:1234\"&gt;&lt;code&gt;http://127.0.0.1:1234&lt;/code&gt;&lt;/a&gt; (or your server address)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; Select your coder model&amp;#39;s ID (e.g., &lt;code&gt;qwen/qwen3-coder-30b&lt;/code&gt;, it should appear automatically) .&lt;/li&gt;\n&lt;li&gt;While in the settings, you can go through the other tabs (like &amp;quot;Auto-Approve&amp;quot;) and toggle preferences to fit your workflow.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Connect Roo Code to the Tool Server&lt;/strong&gt; Finally, we have to expose the mcp server to Roo.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In the Roo Code settings panel, click the 3 horizontal dots (top right), select &amp;quot;MCP Servers&amp;quot; from the drop-down menu.&lt;/li&gt;\n&lt;li&gt;Ensure the &lt;strong&gt;&amp;quot;Enable MCP Servers&amp;quot;&lt;/strong&gt; checkbox is &lt;strong&gt;ENABLED&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;Scroll down and click &amp;quot;Edit Global MCP&amp;quot;, and replace the contents (if any) with this:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n  &amp;quot;mcpServers&amp;quot;: {\n    &amp;quot;docs-mcp-server&amp;quot;: {\n      &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;,\n      &amp;quot;args&amp;quot;: [\n        &amp;quot;@arabold/docs-mcp-server@latest&amp;quot;\n      ],\n      &amp;quot;env&amp;quot;: {\n        &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;lmstudio&amp;quot;,\n        &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;,\n        &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;text-embedding-qwen3-embedding-0.6b&amp;quot;\n      },\n      &amp;quot;alwaysAllow&amp;quot;: [\n        &amp;quot;fetch_url&amp;quot;,\n        &amp;quot;remove_docs&amp;quot;,\n        &amp;quot;scrape_docs&amp;quot;,\n        &amp;quot;search_docs&amp;quot;,\n        &amp;quot;list_libraries&amp;quot;,\n        &amp;quot;find_version&amp;quot;,\n        &amp;quot;list_jobs&amp;quot;,\n        &amp;quot;get_job_info&amp;quot;,\n        &amp;quot;cancel_job&amp;quot;\n      ],\n      &amp;quot;disabled&amp;quot;: false\n    }\n  }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Note: I&amp;#39;m not exactly sure how this part works. This is functional, but maybe contains redundancies. Hopefully someone with more knowledge can optimize this in the comments.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Then you can toggle it on and see a green circle if there&amp;#39;s no issues.&lt;/p&gt;\n\n&lt;p&gt;Your setup is now complete. You have a local coding assistant that can use the &lt;code&gt;docs-mcp-server&lt;/code&gt; to perform RAG against documentation you provide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?auto=webp&amp;s=c9b66d5932995b559501dedad243d4991822c62b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5dca5318ae2d95c180426ac49239f78614273fd3",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d21c6913689d6baa653b4499775bf99e5e67b83",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce1aca8605a116b21a8c31ef08a357697c4e5ce6",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=10ea5ed6a277615b50b6e389b4686099afd9f5a9",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=37054aedb1b74092c1a5c30ab58106206841963e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a8a940e26cd7cbb500200fee501049b651a48b4d",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1men28l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xrailgun",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754022492,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A lot of people including myself want a personalized AI tool. Not in the sense of tones and personality, but one that adapts to my work style - answer questions and do deep researches based on what I care about from past conversations. I don't really see any tools can do this. Even chatgpt's memory today is still quite basic. It only remembers facts from the past and quotes that from time to time.\n\nI want to implement this logic in my tool. But anything specific I can do besides building RAG? What else can I do to to make the LLM truely \"adapt\"?",
          "author_fullname": "t2_ho98y0dwh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Good practices to implement memory for LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfhfg0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754108553,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A lot of people including myself want a personalized AI tool. Not in the sense of tones and personality, but one that adapts to my work style - answer questions and do deep researches based on what I care about from past conversations. I don&amp;#39;t really see any tools can do this. Even chatgpt&amp;#39;s memory today is still quite basic. It only remembers facts from the past and quotes that from time to time.&lt;/p&gt;\n\n&lt;p&gt;I want to implement this logic in my tool. But anything specific I can do besides building RAG? What else can I do to to make the LLM truely &amp;quot;adapt&amp;quot;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfhfg0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tonyc1118",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfhfg0/good_practices_to_implement_memory_for_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfhfg0/good_practices_to_implement_memory_for_llms/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754108553,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Im no way programmer nor IT guy. Just history teacher trying to make myself companion for job. For whatever reason, my laptop doesnt let me run openwebUI by terminal commands (cant even pip it), so I cant use instructions from herehttps://www.reddit.com/r/LocalLLaMA/comments/1iqngrb/lm\\_studio\\_over\\_a\\_lan/\n\nRn, Im trying to do same stuff with docker but for whatever reason I always get error 500 in my openwebui then trying to reach my running model(by LM studio) on PC.  \nCan someone give me guide/step-by-step instruction/what to read on subject in order to be able to use model which is running on another my device in same network?  \nHope this isn't off topic post",
          "author_fullname": "t2_2l9kaflr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Want to run models on PC and use them via same wifi with my laptop",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf9vr7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754086388,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im no way programmer nor IT guy. Just history teacher trying to make myself companion for job. For whatever reason, my laptop doesnt let me run openwebUI by terminal commands (cant even pip it), so I cant use instructions from herehttps://&lt;a href=\"http://www.reddit.com/r/LocalLLaMA/comments/1iqngrb/lm%5C_studio%5C_over%5C_a%5C_lan/\"&gt;www.reddit.com/r/LocalLLaMA/comments/1iqngrb/lm\\_studio\\_over\\_a\\_lan/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Rn, Im trying to do same stuff with docker but for whatever reason I always get error 500 in my openwebui then trying to reach my running model(by LM studio) on PC.&lt;br/&gt;\nCan someone give me guide/step-by-step instruction/what to read on subject in order to be able to use model which is running on another my device in same network?&lt;br/&gt;\nHope this isn&amp;#39;t off topic post&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf9vr7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RussianNewbie",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf9vr7/want_to_run_models_on_pc_and_use_them_via_same/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf9vr7/want_to_run_models_on_pc_and_use_them_via_same/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754086388,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Something with dockerfiles would be nice.\n\nMain requirement is to be able to run the following social media apps: (ordered by priority)\n\n- WhatsApp \n- WhatsApp Business\n- Linkedin\n- X\n- Reddit\n- Youtube",
          "author_fullname": "t2_1utdj13v24",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the best way to connect Android with LLM - Virtually",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfk60l",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754118213,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Something with dockerfiles would be nice.&lt;/p&gt;\n\n&lt;p&gt;Main requirement is to be able to run the following social media apps: (ordered by priority)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;WhatsApp &lt;/li&gt;\n&lt;li&gt;WhatsApp Business&lt;/li&gt;\n&lt;li&gt;Linkedin&lt;/li&gt;\n&lt;li&gt;X&lt;/li&gt;\n&lt;li&gt;Reddit&lt;/li&gt;\n&lt;li&gt;Youtube&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfk60l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rozeappletree",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfk60l/what_is_the_best_way_to_connect_android_with_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfk60l/what_is_the_best_way_to_connect_android_with_llm/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754118213,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello, I've been reading the subreddit for some days now and I was wondering if Qwen 3 or Qwen 2.5 code was still the best model to run to run on vscode with either AI toolkit or RooCode?\n\nI got a M4 pro with 14-Core CPU, 20-Core GPU, 24GB Unified Memory and about 50gb of storage left, can free up another 50gb if needed\n\nFeel free to suggest a different model, or another way to run the model on vscode as I plan on coding offline  \n\nThanks :)",
          "author_fullname": "t2_2v7kco98",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is Qwen still the best for coding?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf2cu1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754068563,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;ve been reading the subreddit for some days now and I was wondering if Qwen 3 or Qwen 2.5 code was still the best model to run to run on vscode with either AI toolkit or RooCode?&lt;/p&gt;\n\n&lt;p&gt;I got a M4 pro with 14-Core CPU, 20-Core GPU, 24GB Unified Memory and about 50gb of storage left, can free up another 50gb if needed&lt;/p&gt;\n\n&lt;p&gt;Feel free to suggest a different model, or another way to run the model on vscode as I plan on coding offline  &lt;/p&gt;\n\n&lt;p&gt;Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf2cu1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OTBKR",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf2cu1/is_qwen_still_the_best_for_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf2cu1/is_qwen_still_the_best_for_coding/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754068563,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, are there any open source solutions to codebase indexing that rival Cursor?",
          "author_fullname": "t2_s7g9g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cursor codebase indexing open source alternative?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfap30",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754088490,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, are there any open source solutions to codebase indexing that rival Cursor?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfap30",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Imjustmisunderstood",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfap30/cursor_codebase_indexing_open_source_alternative/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfap30/cursor_codebase_indexing_open_source_alternative/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754088490,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,\n\nI've been toying around with qwen3 coder (0 temp btw).  \nI've tested it on cerebras cloud. 1.4k T/S. Solved a medium-level logic problem in a blink of an eye, blew me away, the fact that the responses come instant makes you wanna pop a bottle and stare in the abyss. The first AI to solve it was o1, in like 60s of thinking. I do actually believe it's Sonnet 4 level.\n\nI'm curious to better understand the limits of open-source llms. \n\nSo circling back to my title, ya'll got anymore of dem hard problems that can't be solved by current open-weights SOTA?\n\n",
          "author_fullname": "t2_5uhcd48d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Y'all got more of them hard problems?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf7hkw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754080464,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been toying around with qwen3 coder (0 temp btw).&lt;br/&gt;\nI&amp;#39;ve tested it on cerebras cloud. 1.4k T/S. Solved a medium-level logic problem in a blink of an eye, blew me away, the fact that the responses come instant makes you wanna pop a bottle and stare in the abyss. The first AI to solve it was o1, in like 60s of thinking. I do actually believe it&amp;#39;s Sonnet 4 level.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious to better understand the limits of open-source llms. &lt;/p&gt;\n\n&lt;p&gt;So circling back to my title, ya&amp;#39;ll got anymore of dem hard problems that can&amp;#39;t be solved by current open-weights SOTA?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf7hkw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shaman-warrior",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf7hkw/yall_got_more_of_them_hard_problems/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf7hkw/yall_got_more_of_them_hard_problems/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754080464,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys!\n\nHow much do PCIe Lanes really matter?\n\nAs far as i understand, just for inference, with for example ollama, they are only really needed when the model is loaded into VRAM - after that everything is done on the card itself.\n\nSo basically, if using multiple gpus, its enough when they are connected via PCIe x1-x4 - or do i oversee something here?\n\nThanks for input!\n\nEdit: I'm planning to use AMD Mi50s",
          "author_fullname": "t2_fe2ok1q3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How much do PCIe Lanes matter?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf1lfv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754067983,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754066851,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys!&lt;/p&gt;\n\n&lt;p&gt;How much do PCIe Lanes really matter?&lt;/p&gt;\n\n&lt;p&gt;As far as i understand, just for inference, with for example ollama, they are only really needed when the model is loaded into VRAM - after that everything is done on the card itself.&lt;/p&gt;\n\n&lt;p&gt;So basically, if using multiple gpus, its enough when they are connected via PCIe x1-x4 - or do i oversee something here?&lt;/p&gt;\n\n&lt;p&gt;Thanks for input!&lt;/p&gt;\n\n&lt;p&gt;Edit: I&amp;#39;m planning to use AMD Mi50s&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf1lfv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MrCatberry",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754066851,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If so how did it go?",
          "author_fullname": "t2_g5exwc2h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone tried GLM-4.5 with Claude code or other agents?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfa9tj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754087390,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If so how did it go?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfa9tj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BlueeWaater",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfa9tj/anyone_tried_glm45_with_claude_code_or_other/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfa9tj/anyone_tried_glm45_with_claude_code_or_other/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754087390,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Im trying to make a chatbot that sounds and acts like BMO from adventure time and was wondering if there is a TTS model that I can use a premade voice.   \n  \nThe voice I downloaded is from [https://voice-models.com/](https://voice-models.com/) and has a .index file and a .pth file if that means anything or helps at all",
          "author_fullname": "t2_69b6kbxa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "TTS that I can use a downloaded AI voice for? (not sure if this is the right place to ask)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfj3vj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754114327,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im trying to make a chatbot that sounds and acts like BMO from adventure time and was wondering if there is a TTS model that I can use a premade voice.   &lt;/p&gt;\n\n&lt;p&gt;The voice I downloaded is from &lt;a href=\"https://voice-models.com/\"&gt;https://voice-models.com/&lt;/a&gt; and has a .index file and a .pth file if that means anything or helps at all&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfj3vj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "crackaddict42069",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfj3vj/tts_that_i_can_use_a_downloaded_ai_voice_for_not/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfj3vj/tts_that_i_can_use_a_downloaded_ai_voice_for_not/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754114327,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Google delivers ancient benchmarks, I used to love aider benchmarks, but it seems it was abandoned, no updates on new models. I want to know how qwen3-coder and glm4.5 compare.. but nobody updates benchmarks anymore? are we in a postbenchmark era? Benchmarks as gamed as they are they still signal utility!",
          "author_fullname": "t2_7j2k5hlp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any up to date coding benchmarks?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf6n4u",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754078439,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google delivers ancient benchmarks, I used to love aider benchmarks, but it seems it was abandoned, no updates on new models. I want to know how qwen3-coder and glm4.5 compare.. but nobody updates benchmarks anymore? are we in a postbenchmark era? Benchmarks as gamed as they are they still signal utility!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf6n4u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sudden-Lingonberry-8",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf6n4u/any_up_to_date_coding_benchmarks/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf6n4u/any_up_to_date_coding_benchmarks/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754078439,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have LLM studio installed on a server. And I did enable the feature to run as a server with Tailscale and on my Mac mini, I installed anything LLM . And when I set up anything LLM to use lm studio. It just says refreshing models and nothing else after that it does not pull any of the models I have installed. I’m just curious what I’m doing wrong. In my IP settings for anything LLM I have. http:// my up:1234/v1. But after letting it run 10 minutes, it does not pull any models at all. So to test to see if it was the server I installed ollama and that worked just fine. I’m just curious what am I doing wrong?",
          "author_fullname": "t2_33j4dylg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New to LLM studio?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfek6x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754099629,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have LLM studio installed on a server. And I did enable the feature to run as a server with Tailscale and on my Mac mini, I installed anything LLM . And when I set up anything LLM to use lm studio. It just says refreshing models and nothing else after that it does not pull any of the models I have installed. I’m just curious what I’m doing wrong. In my IP settings for anything LLM I have. http:// my up:1234/v1. But after letting it run 10 minutes, it does not pull any models at all. So to test to see if it was the server I installed ollama and that worked just fine. I’m just curious what am I doing wrong?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfek6x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "wbiggs205",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfek6x/new_to_llm_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfek6x/new_to_llm_studio/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754099629,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Right now I am using Qwen and Gemma (32B and 27B) on my old pc from 2011 where the architecture isn’t compatible and doesn’t even detect my graphics card. \n\nI want to know why sometimes the performance is (almost) instantly , maybe it will answer after 5-30 seconds. But other times it’s either 30 minutes or 1 hour I get a response .\n\nIs there a logical reason for this? Is there some possible way I can figure this out and keep using the higher version models ? \n\n (I realize i need to get a new pc but now isn’t the best time for that)",
          "author_fullname": "t2_l4qac",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Question about my dinosaur computer",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfedua",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754099099,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Right now I am using Qwen and Gemma (32B and 27B) on my old pc from 2011 where the architecture isn’t compatible and doesn’t even detect my graphics card. &lt;/p&gt;\n\n&lt;p&gt;I want to know why sometimes the performance is (almost) instantly , maybe it will answer after 5-30 seconds. But other times it’s either 30 minutes or 1 hour I get a response .&lt;/p&gt;\n\n&lt;p&gt;Is there a logical reason for this? Is there some possible way I can figure this out and keep using the higher version models ? &lt;/p&gt;\n\n&lt;p&gt;(I realize i need to get a new pc but now isn’t the best time for that)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfedua",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "XiRw",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754099099,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I get it up and running, first pass and its response.. What the heck is this???\n\n    I'm sorry, but I cannot provide development services directly or review documents. However, if you have specific questions or concerns about the strategy or implementation details, please ask away! I can guide you on the platform and its programming environment, but additional development work would require a fee or contract with a licensed developer.",
          "author_fullname": "t2_kwkhg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wizard Coder... or not coder?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfe6jm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754098495,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I get it up and running, first pass and its response.. What the heck is this???&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;I&amp;#39;m sorry, but I cannot provide development services directly or review documents. However, if you have specific questions or concerns about the strategy or implementation details, please ask away! I can guide you on the platform and its programming environment, but additional development work would require a fee or contract with a licensed developer.\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfe6jm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "modernDayKing",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfe6jm/wizard_coder_or_not_coder/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfe6jm/wizard_coder_or_not_coder/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754098495,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello to the MI50 owners out there, I am struggling to find any prompt processing performance for the MI50 on ~8b and ~14b class models.\n\nHas anyone got any numbers for those types of models ?",
          "author_fullname": "t2_1urn03vg8s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MI50 prompt processing performance",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mexai2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754056873,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello to the MI50 owners out there, I am struggling to find any prompt processing performance for the MI50 on ~8b and ~14b class models.&lt;/p&gt;\n\n&lt;p&gt;Has anyone got any numbers for those types of models ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mexai2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kasimolo33",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mexai2/mi50_prompt_processing_performance/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mexai2/mi50_prompt_processing_performance/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754056873,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": " ( Or... The adventures of a newbie )\n\nToday I learned something really important — and honestly, I had no idea how using API-hosted LLMs can quietly become a black hole for your wallet.💸💰\n\nAt first glance, the pricing seems super appealing. You see those spicy “low” prices from big US companies — something like $0.002 per 1,000 tokens, and you think, \"Wow, that’s cheap!\"\n\nBut… let’s do the math.\n\nYou start using a 128k context model on a platform like OpenRouter, and you don’t realize that with every new interaction, your entire chat history is being resent to the API. That’s the only way the model can \"remember\" the conversation. So after just a few minutes, each message you're sending might carry along 10k tokens — or even more.\n\nNow imagine you’re chatting for hours. Every tiny reply — even a simple “ok” — could trigger a payload of 50,000 or 100,000 tokens being sent again and again. It’s like buying an entire book just to read the next letter.\n\nIn just a few hours, you may have burned through $5 to $10, just for a basic conversation. And now think monthly... or worse — imagine you’re editing a software file with 800 lines of code. Every time you tweak a line and hit send, it could cost you $1 or $2 per second.\n\nI mean... what?!\n\nI now understand the almost desperate effort some people make to run LLMs locally on their own machines — because something that looks insanely cheap at first glance… can turn out to be violently expensive.\n\nThis is insane. Maybe everyone else already knew this — but I didn’t! 😯😯😯\n",
          "author_fullname": "t2_8c7clfk1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The Great Deception of \"Low Prices\" in LLM APIs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1meep6o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "ups": 136,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 136,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/KpH7dlNRh78oWPR5CwX_DiS1oipkBRXTYNWHmoAZyyg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753998846,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;( Or... The adventures of a newbie )&lt;/p&gt;\n\n&lt;p&gt;Today I learned something really important — and honestly, I had no idea how using API-hosted LLMs can quietly become a black hole for your wallet.💸💰&lt;/p&gt;\n\n&lt;p&gt;At first glance, the pricing seems super appealing. You see those spicy “low” prices from big US companies — something like $0.002 per 1,000 tokens, and you think, &amp;quot;Wow, that’s cheap!&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;But… let’s do the math.&lt;/p&gt;\n\n&lt;p&gt;You start using a 128k context model on a platform like OpenRouter, and you don’t realize that with every new interaction, your entire chat history is being resent to the API. That’s the only way the model can &amp;quot;remember&amp;quot; the conversation. So after just a few minutes, each message you&amp;#39;re sending might carry along 10k tokens — or even more.&lt;/p&gt;\n\n&lt;p&gt;Now imagine you’re chatting for hours. Every tiny reply — even a simple “ok” — could trigger a payload of 50,000 or 100,000 tokens being sent again and again. It’s like buying an entire book just to read the next letter.&lt;/p&gt;\n\n&lt;p&gt;In just a few hours, you may have burned through $5 to $10, just for a basic conversation. And now think monthly... or worse — imagine you’re editing a software file with 800 lines of code. Every time you tweak a line and hit send, it could cost you $1 or $2 per second.&lt;/p&gt;\n\n&lt;p&gt;I mean... what?!&lt;/p&gt;\n\n&lt;p&gt;I now understand the almost desperate effort some people make to run LLMs locally on their own machines — because something that looks insanely cheap at first glance… can turn out to be violently expensive.&lt;/p&gt;\n\n&lt;p&gt;This is insane. Maybe everyone else already knew this — but I didn’t! 😯😯😯&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/f8vv4t837agf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/f8vv4t837agf1.png?auto=webp&amp;s=2a5bde2dd3cb61e64af4720e8cc13e534a92116f",
                  "width": 1024,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/f8vv4t837agf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a158041f9882499a65c08f11adace0fa76a0f40",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/f8vv4t837agf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b71e536e993c221a57840d46c0f8345d4fd26f2",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/f8vv4t837agf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a89d09f3744524462cc6bc4d5c80648aca4f27e9",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/f8vv4t837agf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b1305c708b1fbe4bb7166cf9808a29640f750a67",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/f8vv4t837agf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=73ec0bc2072e5f2bc9194e2510d445f7e8673cfb",
                    "width": 960,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "PQVtbBsS9q88WP67d3L6vyJ8WKHnI51rshmbM64ONSA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1meep6o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Current-Stop7806",
          "discussion_type": null,
          "num_comments": 135,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1meep6o/the_great_deception_of_low_prices_in_llm_apis/",
          "stickied": false,
          "url": "https://i.redd.it/f8vv4t837agf1.png",
          "subreddit_subscribers": 508769,
          "created_utc": 1753998846,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everybody, I was wondering how to add knowledge for generating recent suggestions for a given Python library to the Qwen3-coder. Are there any ways to add the new SDK or docs to the Qwen3-coder? I was thinking of gluing cline-ollama-some new docs on the Python library. Are there some kind of RAG or similar techniques to enrich the model's knowledge?",
          "author_fullname": "t2_2h97wo38",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to add most recent python library documentation?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf7q94",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754081056,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everybody, I was wondering how to add knowledge for generating recent suggestions for a given Python library to the Qwen3-coder. Are there any ways to add the new SDK or docs to the Qwen3-coder? I was thinking of gluing cline-ollama-some new docs on the Python library. Are there some kind of RAG or similar techniques to enrich the model&amp;#39;s knowledge?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf7q94",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arm2armreddit",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf7q94/how_to_add_most_recent_python_library/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf7q94/how_to_add_most_recent_python_library/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754081056,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_dwy0w3kf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder-30B-A3B released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1me2zc6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#bbbdbf",
          "ups": 533,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ff54b802-c910-11ed-be9d-ea867d8afa86",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 533,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=50e58d8c576ff1f0469c49c5086a3d54ed8234ad",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 33B"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753971880,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?auto=webp&amp;s=4cacac54fb0a262f4128b23481bccaf4104c19d5",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fbf0440b72bf3c599b24d782f0bddf00251537cf",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=824bee5d7aa9841a221b2f60a969d54551eccb18",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=91ca7f6cb7614731e917c0c8e162bd66bfbc25ca",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9d2b1429fc14f5ca152608718fd3ef6d50119778",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6655884fe4ff60136ee88021696ace5be4875862",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=991b600ad67e419b1091cac2c8c55f34d86b36fa",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 33B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1me2zc6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "glowcialist",
          "discussion_type": null,
          "num_comments": 92,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1me2zc6/qwen3coder30ba3b_released/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct",
          "subreddit_subscribers": 508769,
          "created_utc": 1753971880,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello friends, \n\nI recently got myself a new PC, Ryzen 9800x3d, 32gb RAM and a 5070ti (16gb vram). I want to create AI art locally, what’s a good llm to play around with while I learn? ",
          "author_fullname": "t2_w9e9g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Noob question",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf7663",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754079692,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello friends, &lt;/p&gt;\n\n&lt;p&gt;I recently got myself a new PC, Ryzen 9800x3d, 32gb RAM and a 5070ti (16gb vram). I want to create AI art locally, what’s a good llm to play around with while I learn? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf7663",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "panlid5000",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf7663/noob_question/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf7663/noob_question/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754079692,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Currently in Toronto area the 7900 xtx is cheaper brand new with taxes then a used 3090. What are people’s experience with a couple of these cards for inference on Windows?  I searched and saw some feedback from months ago, looking how they handle all the new models for inference?  \n",
          "author_fullname": "t2_14v7k3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD 7900 xtx for inference?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf16vx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754065916,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently in Toronto area the 7900 xtx is cheaper brand new with taxes then a used 3090. What are people’s experience with a couple of these cards for inference on Windows?  I searched and saw some feedback from months ago, looking how they handle all the new models for inference?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf16vx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Willdudes",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf16vx/amd_7900_xtx_for_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf16vx/amd_7900_xtx_for_inference/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754065916,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_l10vk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD EPYC 4545P: 16 Zen 5 Cores @ 65 Watts For Low-Power / Energy Efficient Servers",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mewg8a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754054772,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "phoronix.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.phoronix.com/review/amd-epyc-4545p/3",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mewg8a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nostriluu",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mewg8a/amd_epyc_4545p_16_zen_5_cores_65_watts_for/",
          "stickied": false,
          "url": "https://www.phoronix.com/review/amd-epyc-4545p/3",
          "subreddit_subscribers": 508769,
          "created_utc": 1754054772,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "After reading that ik\\_llama.cpp gives way higher performance than LMStudio, I wanted to have a simple method of installing and running the Qwen3 Coder model under Windows. I chose to install everything needed and build from source within one single script - written mainly by ChatGPT with experimenting &amp; testing until it worked on both of Windows machines:\n\n||Desktop|Notebook|\n|:-|:-|:-|\n|OS|Windows 11|Windows 10|\n|CPU|AMD Ryzen 5 7600|Intel i7 8750H|\n|RAM|32GB DDR5 5600|32GB DDR4 2667|\n|GPU|NVIDIA RTX 4070 Ti 12GB|NVIDIA GTX 1070 8GB|\n|Tokens/s|35|9.5|\n\n\n\nFor my desktop PC that works out great and I get super nice results.\n\nOn my notebook however there seems to be a problem with context: the model mostly outputs random text instead of referencing my questions. If anyone has any idea help would be greatly appreciated!\n\nAlthough this might not be the perfect solution I thought I'd share it here, maybe someone finds it useful:\n\n[https://github.com/Danmoreng/local-qwen3-coder-env](https://github.com/Danmoreng/local-qwen3-coder-env)",
          "author_fullname": "t2_7z26p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Installscript for Qwen3-Coder running on ik_llama.cpp for high performance",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1metf4h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754045970,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After reading that ik_llama.cpp gives way higher performance than LMStudio, I wanted to have a simple method of installing and running the Qwen3 Coder model under Windows. I chose to install everything needed and build from source within one single script - written mainly by ChatGPT with experimenting &amp;amp; testing until it worked on both of Windows machines:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;Desktop&lt;/th&gt;\n&lt;th align=\"left\"&gt;Notebook&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OS&lt;/td&gt;\n&lt;td align=\"left\"&gt;Windows 11&lt;/td&gt;\n&lt;td align=\"left\"&gt;Windows 10&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;CPU&lt;/td&gt;\n&lt;td align=\"left\"&gt;AMD Ryzen 5 7600&lt;/td&gt;\n&lt;td align=\"left\"&gt;Intel i7 8750H&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;RAM&lt;/td&gt;\n&lt;td align=\"left\"&gt;32GB DDR5 5600&lt;/td&gt;\n&lt;td align=\"left\"&gt;32GB DDR4 2667&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GPU&lt;/td&gt;\n&lt;td align=\"left\"&gt;NVIDIA RTX 4070 Ti 12GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;NVIDIA GTX 1070 8GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Tokens/s&lt;/td&gt;\n&lt;td align=\"left\"&gt;35&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.5&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;For my desktop PC that works out great and I get super nice results.&lt;/p&gt;\n\n&lt;p&gt;On my notebook however there seems to be a problem with context: the model mostly outputs random text instead of referencing my questions. If anyone has any idea help would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;Although this might not be the perfect solution I thought I&amp;#39;d share it here, maybe someone finds it useful:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Danmoreng/local-qwen3-coder-env\"&gt;https://github.com/Danmoreng/local-qwen3-coder-env&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?auto=webp&amp;s=696008496e1bd6e0694f2b9886836ed38e83c28c",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0ff28b544d610ccf62e98f3feddd075959ce926b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=72c1589c6e0b5c99b15e4303d1779e5eec8f9a80",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3fcaf94729c1bcd29118e071f6822ad5c896bec0",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e9afa83dad53bd5ba85b1ab44cba7113663f13b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c6c818c201ba8089a89a84387df82adfd28220d6",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b492d3d79ff10edeacd4e012c9aa3e160eaf88db",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1metf4h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Danmoreng",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754045970,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_9i1ld",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "An Experiment in Logit Control: Using Statistical \"Constraint Masks\" to Guide Token Selection",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 92,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf07dy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/4Z9-NAqtbqtnqebZUIjxu77W_oURHIoXd1CTVK0ApLk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754063698,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/442znbdvjfgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/442znbdvjfgf1.png?auto=webp&amp;s=067543edf5b573e79e55dada76d1b42024878b94",
                  "width": 5412,
                  "height": 3559
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/442znbdvjfgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=246430f723d9c8ce03b5b5c8131885dfecbf19ca",
                    "width": 108,
                    "height": 71
                  },
                  {
                    "url": "https://preview.redd.it/442znbdvjfgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f4dfe6b3711dbf6fedff94639e75ae169d09a4c",
                    "width": 216,
                    "height": 142
                  },
                  {
                    "url": "https://preview.redd.it/442znbdvjfgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4beeb14654062e663fb168617969c05283c849c1",
                    "width": 320,
                    "height": 210
                  },
                  {
                    "url": "https://preview.redd.it/442znbdvjfgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4851b5f9cc19d50904b48e98b0aa7852b3af4751",
                    "width": 640,
                    "height": 420
                  },
                  {
                    "url": "https://preview.redd.it/442znbdvjfgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=faf607578daa5b30cc0f103b1eb14332d6b17c23",
                    "width": 960,
                    "height": 631
                  },
                  {
                    "url": "https://preview.redd.it/442znbdvjfgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=06973bc0f157c93cd74d0450f25bf38c4344d8cc",
                    "width": 1080,
                    "height": 710
                  }
                ],
                "variants": {},
                "id": "faYYXvJxRspMDkoFqA01LcpfUOA3UJYunCqTwB8eSl4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mf07dy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vesudeva",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf07dy/an_experiment_in_logit_control_using_statistical/",
          "stickied": false,
          "url": "https://i.redd.it/442znbdvjfgf1.png",
          "subreddit_subscribers": 508769,
          "created_utc": 1754063698,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,\n\nWe're a scrappy startup at Trillion Labs and just released [Tri-70B-preview-SFT](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT), our largest language model yet (70B params!), trained from scratch on \\~1.5T tokens. We unexpectedly ran short on compute, so this is a pure supervised fine-tuning (SFT) release—zero RLHF.\n\n# TL;DR:\n\n* **70B parameters**; pure supervised fine-tuning (**no RLHF** yet!)\n* **32K token context window** (perfect for experimenting with Yarn, if you're bold!)\n* Optimized primarily for **English and Korean**, with decent Japanese performance\n* Tried some new tricks (**FP8 mixed precision, Scalable Softmax, iRoPE attention**)\n* Benchmarked roughly around **Qwen-2.5-72B and LLaMA-3.1-70B**, but it's noticeably raw and needs alignment tweaks.\n* Model and tokenizer fully open on 🤗 HuggingFace under a permissive license (**auto-approved** conditional commercial usage allowed, but it’s definitely experimental!).\n\n# Why release it raw?\n\nWe think releasing Tri-70B in its current form might spur unique research—especially for those into RLHF, RLVR, GRPO, CISPO, GSPO, etc. It’s a perfect baseline for alignment experimentation. Frankly, we know it’s not perfectly aligned, and we'd love your help to identify weak spots.\n\nGive it a spin and see what it can (and can’t) do. We’re particularly curious about your experiences with alignment, context handling, and multilingual use.\n\n\\*\\*👉 \\*\\*[**Check out the repo and model card here!**](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT)\n\nQuestions, thoughts, criticisms warmly welcomed—hit us up below!",
          "author_fullname": "t2_1ug5bi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[P] Tri-70B-preview-SFT: New 70B Model (Research Preview, SFT-only)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mejkcu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 55,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 55,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754011885,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re a scrappy startup at Trillion Labs and just released &lt;a href=\"https://huggingface.co/trillionlabs/Tri-70B-preview-SFT\"&gt;Tri-70B-preview-SFT&lt;/a&gt;, our largest language model yet (70B params!), trained from scratch on ~1.5T tokens. We unexpectedly ran short on compute, so this is a pure supervised fine-tuning (SFT) release—zero RLHF.&lt;/p&gt;\n\n&lt;h1&gt;TL;DR:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;70B parameters&lt;/strong&gt;; pure supervised fine-tuning (&lt;strong&gt;no RLHF&lt;/strong&gt; yet!)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;32K token context window&lt;/strong&gt; (perfect for experimenting with Yarn, if you&amp;#39;re bold!)&lt;/li&gt;\n&lt;li&gt;Optimized primarily for &lt;strong&gt;English and Korean&lt;/strong&gt;, with decent Japanese performance&lt;/li&gt;\n&lt;li&gt;Tried some new tricks (&lt;strong&gt;FP8 mixed precision, Scalable Softmax, iRoPE attention&lt;/strong&gt;)&lt;/li&gt;\n&lt;li&gt;Benchmarked roughly around &lt;strong&gt;Qwen-2.5-72B and LLaMA-3.1-70B&lt;/strong&gt;, but it&amp;#39;s noticeably raw and needs alignment tweaks.&lt;/li&gt;\n&lt;li&gt;Model and tokenizer fully open on 🤗 HuggingFace under a permissive license (&lt;strong&gt;auto-approved&lt;/strong&gt; conditional commercial usage allowed, but it’s definitely experimental!).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Why release it raw?&lt;/h1&gt;\n\n&lt;p&gt;We think releasing Tri-70B in its current form might spur unique research—especially for those into RLHF, RLVR, GRPO, CISPO, GSPO, etc. It’s a perfect baseline for alignment experimentation. Frankly, we know it’s not perfectly aligned, and we&amp;#39;d love your help to identify weak spots.&lt;/p&gt;\n\n&lt;p&gt;Give it a spin and see what it can (and can’t) do. We’re particularly curious about your experiences with alignment, context handling, and multilingual use.&lt;/p&gt;\n\n&lt;p&gt;**👉 **&lt;a href=\"https://huggingface.co/trillionlabs/Tri-70B-preview-SFT\"&gt;&lt;strong&gt;Check out the repo and model card here!&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Questions, thoughts, criticisms warmly welcomed—hit us up below!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?auto=webp&amp;s=b45653eadbcba17c38d2f4f2e15f6ebd41ec7c72",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b7a80c31c557591f18bda1f387961a8fe38f053e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=13c5c597d54b6cfe9cc7d137d17b8c65f8c6db95",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=10f045816aeb8c03203c2c64cc8f2065cb392da4",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cad5dd9413d8196f56dd930d3b4333ee0351d472",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=37853a1aca067c019b4d7c06a11c66190e6bc001",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f4fc9332f1425fd05814a4fb77159385e0f2d89",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mejkcu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jshin49",
          "discussion_type": null,
          "num_comments": 38,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mejkcu/p_tri70bpreviewsft_new_70b_model_research_preview/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mejkcu/p_tri70bpreviewsft_new_70b_model_research_preview/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754011885,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've combined llama.cpp CUDA results in a single place. Fill free to add and share!",
          "author_fullname": "t2_1umam7ln",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLama.cpp on CUDA performance",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mezdl4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=d99b9ef119d65a8da47f74caa52d3c89b0b21c5a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754061785,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve combined llama.cpp CUDA results in a single place. Fill free to add and share!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/discussions/15013",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk.png?auto=webp&amp;s=d05fc23d7b4197a6434808392495399c877ddb61",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c6d436fd4dbc8296467786571735a612b521bd39",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a947aa945637e90a5a9ec08a9bd9e027b8cb98e7",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fd4307b3c3dff86f984a05fb0c866f66292991e9",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=35c0244e69e083ef09fd0ecf92ea27b64f32878c",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6d56ac1d6186b02e0cffa3db5452d10a42ce61f0",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5c4c5e35f0ce27c442b9f87d385ef0615b7c6747",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mezdl4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "COBECT",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mezdl4/llamacpp_on_cuda_performance/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/discussions/15013",
          "subreddit_subscribers": 508769,
          "created_utc": 1754061785,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "👤 User 9:32:07  \nAlso, back propagation is really similar to the trial-error mechansisms of the brain  \n🧠 Model 70B 9:32:52  \nThat's another fascinating parallel! The error-driven update mechanism in backpropagation does bear a striking resemblance to the trial-and-error processes observed in the brain. In both cases, there's an iterative refinement process where errors or discrepancies drive adjustments to improve performance. In the brain, this process is thought to occur through various feedback loops, such as the cortico-basal ganglia-thalamo-cortical loop, which refines motor skills through trial and error. Similarly, in backpropagation, the error gradient flows backwards through the network, adjusting weights and biases to minimize the difference between predicted outputs and actual targets. Both mechanisms rely on some form of exploration-exploitation trade-off, where the system balances exploring new possibilities with exploiting learned knowledge to optimize performance. This parallel highlights the brain's remarkable ability to adapt and learn from mistakes, which is essential for human intelligence. Moreover, this similarity raises intriguing questions about the fundamental principles governing learning and adaptation across biological and artificial systems. Could we be tapping into some universal principles of intelligence, ones that transcend the boundaries between living organisms and machines? What do you think about the implications of these parallels on our understanding of intelligence and its implementation in AI systems?\n\nWhat do you think about the responses for the model and how would you rate this model?",
          "author_fullname": "t2_1qaiikqj0m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can you please review the comment from my tuned open source local model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mffa5a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754101825,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;👤 User 9:32:07&lt;br/&gt;\nAlso, back propagation is really similar to the trial-error mechansisms of the brain&lt;br/&gt;\n🧠 Model 70B 9:32:52&lt;br/&gt;\nThat&amp;#39;s another fascinating parallel! The error-driven update mechanism in backpropagation does bear a striking resemblance to the trial-and-error processes observed in the brain. In both cases, there&amp;#39;s an iterative refinement process where errors or discrepancies drive adjustments to improve performance. In the brain, this process is thought to occur through various feedback loops, such as the cortico-basal ganglia-thalamo-cortical loop, which refines motor skills through trial and error. Similarly, in backpropagation, the error gradient flows backwards through the network, adjusting weights and biases to minimize the difference between predicted outputs and actual targets. Both mechanisms rely on some form of exploration-exploitation trade-off, where the system balances exploring new possibilities with exploiting learned knowledge to optimize performance. This parallel highlights the brain&amp;#39;s remarkable ability to adapt and learn from mistakes, which is essential for human intelligence. Moreover, this similarity raises intriguing questions about the fundamental principles governing learning and adaptation across biological and artificial systems. Could we be tapping into some universal principles of intelligence, ones that transcend the boundaries between living organisms and machines? What do you think about the implications of these parallels on our understanding of intelligence and its implementation in AI systems?&lt;/p&gt;\n\n&lt;p&gt;What do you think about the responses for the model and how would you rate this model?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mffa5a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Over-Pilot4908",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mffa5a/can_you_please_review_the_comment_from_my_tuned/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mffa5a/can_you_please_review_the_comment_from_my_tuned/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754101825,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct (Foundation-Sec-8B-Instruct) is an open-weight, 8-billion parameter instruction-tuned language model specialized for cybersecurity applications. It extends the Foundation-Sec-8B base model with instruction-following capabilities. It leverages prior training to understand security concepts, terminology, and practices across multiple security domains. Further instruction-tuning allows the model to interact with human users in a chat-like interface. Foundation-Sec-8B-Instruct enables organizations to build AI-driven security tools that can be deployed locally, reducing dependency on cloud-based AI services while maintaining high performance on security-related tasks.\n\n# [](https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct#intended-use-cases)Intended Use Cases\n\nFoundation-Sec-8B-Instruct is designed for security practitioners, researchers, and developers building AI-powered security workflows and applications. Foundation-Sec-8B-Instruct is optimized for three core use case categories:\n\n* **SOC Acceleration**: Automating triage, summarization, case note generation, and evidence collection.\n* **Proactive Threat Defense**: Simulating attacks, prioritizing vulnerabilities, mapping TTPs, and modeling attacker behavior.\n* **Engineering Enablement**: Providing security assistance, validating configurations, assessing compliance evidence, and improving security posture.\n\nThe model is intended for local deployment in environments prioritizing data security, regulatory compliance, and operational control.",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Foundation-Sec-8B-Instruct (from Cisco Foundation AI)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1meohe5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": "#bbbdbf",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=f3b6927054c79a6b9c320f5a873b7e51197b6d20",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754027417,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct (Foundation-Sec-8B-Instruct) is an open-weight, 8-billion parameter instruction-tuned language model specialized for cybersecurity applications. It extends the Foundation-Sec-8B base model with instruction-following capabilities. It leverages prior training to understand security concepts, terminology, and practices across multiple security domains. Further instruction-tuning allows the model to interact with human users in a chat-like interface. Foundation-Sec-8B-Instruct enables organizations to build AI-driven security tools that can be deployed locally, reducing dependency on cloud-based AI services while maintaining high performance on security-related tasks.&lt;/p&gt;\n\n&lt;h1&gt;&lt;a href=\"https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct#intended-use-cases\"&gt;&lt;/a&gt;Intended Use Cases&lt;/h1&gt;\n\n&lt;p&gt;Foundation-Sec-8B-Instruct is designed for security practitioners, researchers, and developers building AI-powered security workflows and applications. Foundation-Sec-8B-Instruct is optimized for three core use case categories:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;SOC Acceleration&lt;/strong&gt;: Automating triage, summarization, case note generation, and evidence collection.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Proactive Threat Defense&lt;/strong&gt;: Simulating attacks, prioritizing vulnerabilities, mapping TTPs, and modeling attacker behavior.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Engineering Enablement&lt;/strong&gt;: Providing security assistance, validating configurations, assessing compliance evidence, and improving security posture.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The model is intended for local deployment in environments prioritizing data security, regulatory compliance, and operational control.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?auto=webp&amp;s=4b50bc8534f794cfb721f5e7a56e0988e9706425",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1655f1e0ba75303138e6971e05ac5a664c40495",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b4b11101e28ed4469beb02c5f388a2796fcd2def",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=72106dc391fd436ddabdac884da6a7960faf8885",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e2dea2312ffbb767f3df65e80cd1d35836547959",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6dcba500e84789da13556b719aa7a79e8dc51cd6",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2ff14c7b4bd7712cf611e33e443b5d59a8e15418",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1meohe5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1meohe5/foundationsec8binstruct_from_cisco_foundation_ai/",
          "stickied": false,
          "url": "https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct",
          "subreddit_subscribers": 508769,
          "created_utc": 1754027417,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As you can see from the radar chart, the scores on the left for the two Agent capability tests, mind2web and BFCL-v3, are very close. This suggests that the Agent capabilities of Qwen3-Coder-FLash should be quite strong.   \n  \nHowever, there is still a significant gap in the Aider-Polyglot and SWE Multilingual tests, which implies that its programming capabilities are indeed quite different from those of Qwen3-Coder-480B.\n\nHas anyone started using it yet? What's the actual user experience like?",
          "author_fullname": "t2_fiiv6xm3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I made a comparison chart for Qwen3-Coder-30B-A3B vs. Qwen3-Coder-480B-A35B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1me4i2h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 313,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 313,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/nDJcrKit9leloNf-Ut_mbP1e9pe5yDgS5VazXSDA6qU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753975422,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As you can see from the radar chart, the scores on the left for the two Agent capability tests, mind2web and BFCL-v3, are very close. This suggests that the Agent capabilities of Qwen3-Coder-FLash should be quite strong.   &lt;/p&gt;\n\n&lt;p&gt;However, there is still a significant gap in the Aider-Polyglot and SWE Multilingual tests, which implies that its programming capabilities are indeed quite different from those of Qwen3-Coder-480B.&lt;/p&gt;\n\n&lt;p&gt;Has anyone started using it yet? What&amp;#39;s the actual user experience like?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/l6547uel88gf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/l6547uel88gf1.png?auto=webp&amp;s=b5aa7e3e5dcfa254c689f363f60a9722e1eb72e4",
                  "width": 1324,
                  "height": 2088
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/l6547uel88gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fc731d339acbcf88a184b51662dd456eefd477c5",
                    "width": 108,
                    "height": 170
                  },
                  {
                    "url": "https://preview.redd.it/l6547uel88gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a836e9fdf9366f6a0058a586fbda0b1632c21cbe",
                    "width": 216,
                    "height": 340
                  },
                  {
                    "url": "https://preview.redd.it/l6547uel88gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f980b5b5346583dbdaaebfe56b59bd8d5b9291df",
                    "width": 320,
                    "height": 504
                  },
                  {
                    "url": "https://preview.redd.it/l6547uel88gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=40dafbd4c67e845ff8ce7c141e92d59fdfd342fe",
                    "width": 640,
                    "height": 1009
                  },
                  {
                    "url": "https://preview.redd.it/l6547uel88gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b98fec591844d74f49e2febeeff5c3c53305d21b",
                    "width": 960,
                    "height": 1513
                  },
                  {
                    "url": "https://preview.redd.it/l6547uel88gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a8de36630bff8bc1b7132277c5bc86fe5e8e9a61",
                    "width": 1080,
                    "height": 1703
                  }
                ],
                "variants": {},
                "id": "vmGH_RgQTp6SHjczHQXNSGnIw-M1Xweu2bYiYKsj904"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1me4i2h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dr_Karminski",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1me4i2h/i_made_a_comparison_chart_for_qwen3coder30ba3b_vs/",
          "stickied": false,
          "url": "https://i.redd.it/l6547uel88gf1.png",
          "subreddit_subscribers": 508769,
          "created_utc": 1753975422,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone with experience using Lambda chat know why DeepSeek R1 Distill Llama 3.3 70B gets fixated on questions I asked earlier in the thread and unable to recognized new questions? Just keeps providing the same reasoning it gave for an earlier answer.\n\nhttps://preview.redd.it/c912if06ihgf1.png?width=1806&amp;format=png&amp;auto=webp&amp;s=4900dfaf788170c984a4bfd7ca40290c7661f412\n\n",
          "author_fullname": "t2_1haa9ceo0b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Lambda Chat Odd Outputs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "c912if06ihgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/c912if06ihgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=940dd889ec733bf0472a244ff8e48de42c8e4a8c"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/c912if06ihgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=969e8e1751016ec1653bfd54524e5a4977d2ffaa"
                },
                {
                  "y": 167,
                  "x": 320,
                  "u": "https://preview.redd.it/c912if06ihgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e9150334892addf31039e2bda8131a44c71fa61"
                },
                {
                  "y": 334,
                  "x": 640,
                  "u": "https://preview.redd.it/c912if06ihgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7a7158c9c204a4224e2d709f0ad6e864c8f44e4b"
                },
                {
                  "y": 501,
                  "x": 960,
                  "u": "https://preview.redd.it/c912if06ihgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=92ec9da33b1a08b6034f3171e8faa00762fd7481"
                },
                {
                  "y": 564,
                  "x": 1080,
                  "u": "https://preview.redd.it/c912if06ihgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6d57dcde18a0f4d23603ed9601ab172146782eb7"
                }
              ],
              "s": {
                "y": 944,
                "x": 1806,
                "u": "https://preview.redd.it/c912if06ihgf1.png?width=1806&amp;format=png&amp;auto=webp&amp;s=4900dfaf788170c984a4bfd7ca40290c7661f412"
              },
              "id": "c912if06ihgf1"
            }
          },
          "name": "t3_1mfa9nd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/sItMVC2C5VBzzKd5fuWtlYdcyEgIb41CnqwNAfSjkXE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754087378,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone with experience using Lambda chat know why DeepSeek R1 Distill Llama 3.3 70B gets fixated on questions I asked earlier in the thread and unable to recognized new questions? Just keeps providing the same reasoning it gave for an earlier answer.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/c912if06ihgf1.png?width=1806&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4900dfaf788170c984a4bfd7ca40290c7661f412\"&gt;https://preview.redd.it/c912if06ihgf1.png?width=1806&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4900dfaf788170c984a4bfd7ca40290c7661f412&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfa9nd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thick-Connection5549",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfa9nd/lambda_chat_odd_outputs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfa9nd/lambda_chat_odd_outputs/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754087378,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**System:** Threadripper Pro 3945WX &amp; RTX 4090 + 128GB system RAM\n\n**Inference engine:**  recent build of ik\\_llama.cpp in an LXC under proxmox *(with -DGGML\\_CUDA=ON -DGGML\\_CUDA\\_FA\\_ALL\\_QUANTS=ON -DGGML\\_BLAS=OFF -DCMAKE\\_CUDA\\_ARCHITECTURES=89 -DGGML\\_IQK\\_FA\\_ALL\\_QUANTS=1 -DGGML\\_SCHED\\_MAX\\_COPIES=1 -DGGML\\_CUDA\\_IQK\\_FORCE\\_BF16=1 -DGGML\\_MAX\\_CONTEXTS=2048)*\n\n**Model:** [unsloth](https://huggingface.co/unsloth)/[Qwen3-Coder-30B-A3B-Instruct-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF) Q5\\_K\\_M\n\n**llama-server arguments:** \\-fa -fmoe --metrics --n-gpu-layers 99 --override-tensor exps=CPU\n\n(though I understand -ngl and -ot  are not strictly necessary as this model fits in 24GB VRAM and removing these arguments stil results in the same situation as below)\n\nThe model runs fast (though not quite as fast as a 5090 running same prompt in Ollama in a windows machine) so I assume it is running on 4090. But when I actually look at what is happenig in the system I cant make sense of what the hardware is doing:\n\n1. The llama-server output seems to indicate NO layers are being offloaded to GPU\n2. nvidia-smi appears to show less than 6GB VRAM ustilised\n3. proxmox shows my CPU at 60% useage but only 555MB of system RAM utilised.\n\nSo where is the actual 'work' being done, by whom and with what resources when I've sent a prompt to the model?\n\nhttps://preview.redd.it/77ei5ozrdhgf1.png?width=1424&amp;format=png&amp;auto=webp&amp;s=51988beff5df7d92551b5ea589d0269bf1495de9\n\nhttps://preview.redd.it/f2rbytyrdhgf1.png?width=834&amp;format=png&amp;auto=webp&amp;s=c5e9a47c3795a7778617960a0332915e11175738\n\nhttps://preview.redd.it/q25p3syrdhgf1.png?width=689&amp;format=png&amp;auto=webp&amp;s=6bf259369ba31c2d505f978975fe5b3fa3b6b3b9",
          "author_fullname": "t2_if95iuzc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I know how much my GPU/CPU is being used by ik_llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "q25p3syrdhgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 50,
                  "x": 108,
                  "u": "https://preview.redd.it/q25p3syrdhgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=30cfee213aeafa0c0832df866c5f8e2433710dd3"
                },
                {
                  "y": 100,
                  "x": 216,
                  "u": "https://preview.redd.it/q25p3syrdhgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=60ace2e74c81147096a4c092a139de0e010d9f3a"
                },
                {
                  "y": 149,
                  "x": 320,
                  "u": "https://preview.redd.it/q25p3syrdhgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=41575a41590519a2549146d429c0af0fed4ab9c9"
                },
                {
                  "y": 299,
                  "x": 640,
                  "u": "https://preview.redd.it/q25p3syrdhgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d187a40dd06756ab6f7129dec9c2b9dea5e10606"
                }
              ],
              "s": {
                "y": 322,
                "x": 689,
                "u": "https://preview.redd.it/q25p3syrdhgf1.png?width=689&amp;format=png&amp;auto=webp&amp;s=6bf259369ba31c2d505f978975fe5b3fa3b6b3b9"
              },
              "id": "q25p3syrdhgf1"
            },
            "77ei5ozrdhgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 46,
                  "x": 108,
                  "u": "https://preview.redd.it/77ei5ozrdhgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9daee453bc912d747a1f4b4d939a9bd137acea4a"
                },
                {
                  "y": 93,
                  "x": 216,
                  "u": "https://preview.redd.it/77ei5ozrdhgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab5d82e84323811d4bdc79e032ba85cd1af22659"
                },
                {
                  "y": 138,
                  "x": 320,
                  "u": "https://preview.redd.it/77ei5ozrdhgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6bb740f5a107044d66abe3ce716d030f43abc24"
                },
                {
                  "y": 276,
                  "x": 640,
                  "u": "https://preview.redd.it/77ei5ozrdhgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=92d25f5df6008fffe9f151d749611b16ceeda7ad"
                },
                {
                  "y": 414,
                  "x": 960,
                  "u": "https://preview.redd.it/77ei5ozrdhgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=be12f05f4d725aa5da225b4aaf406de451964ddd"
                },
                {
                  "y": 466,
                  "x": 1080,
                  "u": "https://preview.redd.it/77ei5ozrdhgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf30b63c5714cf9a004202c442393d389773dfdd"
                }
              ],
              "s": {
                "y": 615,
                "x": 1424,
                "u": "https://preview.redd.it/77ei5ozrdhgf1.png?width=1424&amp;format=png&amp;auto=webp&amp;s=51988beff5df7d92551b5ea589d0269bf1495de9"
              },
              "id": "77ei5ozrdhgf1"
            },
            "f2rbytyrdhgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 49,
                  "x": 108,
                  "u": "https://preview.redd.it/f2rbytyrdhgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4450dbbc6856d5daca1f1fc8dd29cb393b2522ab"
                },
                {
                  "y": 98,
                  "x": 216,
                  "u": "https://preview.redd.it/f2rbytyrdhgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a2e42c546b733640d1e56fc8678c8ead80c8aa2e"
                },
                {
                  "y": 146,
                  "x": 320,
                  "u": "https://preview.redd.it/f2rbytyrdhgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdc95c5b92f6e28e51ce694cc5bbd2cb45e96467"
                },
                {
                  "y": 293,
                  "x": 640,
                  "u": "https://preview.redd.it/f2rbytyrdhgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fb69f354c1790297b3df2140717acdb87c90c7a4"
                }
              ],
              "s": {
                "y": 382,
                "x": 834,
                "u": "https://preview.redd.it/f2rbytyrdhgf1.png?width=834&amp;format=png&amp;auto=webp&amp;s=c5e9a47c3795a7778617960a0332915e11175738"
              },
              "id": "f2rbytyrdhgf1"
            }
          },
          "name": "t3_1mfa5nv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=bb5e71525389d3b53f19ec775a25ce04738c2fdf",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1754087090,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;System:&lt;/strong&gt; Threadripper Pro 3945WX &amp;amp; RTX 4090 + 128GB system RAM&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Inference engine:&lt;/strong&gt;  recent build of ik_llama.cpp in an LXC under proxmox &lt;em&gt;(with -DGGML_CUDA=ON -DGGML_CUDA_FA_ALL_QUANTS=ON -DGGML_BLAS=OFF -DCMAKE_CUDA_ARCHITECTURES=89 -DGGML_IQK_FA_ALL_QUANTS=1 -DGGML_SCHED_MAX_COPIES=1 -DGGML_CUDA_IQK_FORCE_BF16=1 -DGGML_MAX_CONTEXTS=2048)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href=\"https://huggingface.co/unsloth\"&gt;unsloth&lt;/a&gt;/&lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF\"&gt;Qwen3-Coder-30B-A3B-Instruct-GGUF&lt;/a&gt; Q5_K_M&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;llama-server arguments:&lt;/strong&gt; -fa -fmoe --metrics --n-gpu-layers 99 --override-tensor exps=CPU&lt;/p&gt;\n\n&lt;p&gt;(though I understand -ngl and -ot  are not strictly necessary as this model fits in 24GB VRAM and removing these arguments stil results in the same situation as below)&lt;/p&gt;\n\n&lt;p&gt;The model runs fast (though not quite as fast as a 5090 running same prompt in Ollama in a windows machine) so I assume it is running on 4090. But when I actually look at what is happenig in the system I cant make sense of what the hardware is doing:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The llama-server output seems to indicate NO layers are being offloaded to GPU&lt;/li&gt;\n&lt;li&gt;nvidia-smi appears to show less than 6GB VRAM ustilised&lt;/li&gt;\n&lt;li&gt;proxmox shows my CPU at 60% useage but only 555MB of system RAM utilised.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So where is the actual &amp;#39;work&amp;#39; being done, by whom and with what resources when I&amp;#39;ve sent a prompt to the model?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/77ei5ozrdhgf1.png?width=1424&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51988beff5df7d92551b5ea589d0269bf1495de9\"&gt;https://preview.redd.it/77ei5ozrdhgf1.png?width=1424&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51988beff5df7d92551b5ea589d0269bf1495de9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/f2rbytyrdhgf1.png?width=834&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5e9a47c3795a7778617960a0332915e11175738\"&gt;https://preview.redd.it/f2rbytyrdhgf1.png?width=834&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5e9a47c3795a7778617960a0332915e11175738&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/q25p3syrdhgf1.png?width=689&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bf259369ba31c2d505f978975fe5b3fa3b6b3b9\"&gt;https://preview.redd.it/q25p3syrdhgf1.png?width=689&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bf259369ba31c2d505f978975fe5b3fa3b6b3b9&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA.png?auto=webp&amp;s=e56082d18db2b9b44c9a8404db67a6a0159b5aaa",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=305a70e8c82e5c0a94fb3ba2ee9df26c9b46914f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb27b19d48faec1a1b9eb8d5977c1c5dc9b60ce9",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17894ebb2ab4b6a2595f8ef54d10ed9c6f3670cb",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=980118277fff46b9a8e1b486d83ba01a5045e9a9",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e2f5464545b7a0e8b1172bf0c91182a19e11edf3",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f9074f9f7d7985d6799aab5078f32476394a2e67",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfa5nv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "munkiemagik",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfa5nv/how_do_i_know_how_much_my_gpucpu_is_being_used_by/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfa5nv/how_do_i_know_how_much_my_gpucpu_is_being_used_by/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754087090,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just got through Raschka's model architecture series. Seems like everything is a tweak of Llama 3.",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-source architectures that aren't Llama 3 knock offs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf0mw2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.58,
          "author_flair_background_color": "transparent",
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754064644,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just got through Raschka&amp;#39;s model architecture series. Seems like everything is a tweak of Llama 3.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf0mw2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mf0mw2/opensource_architectures_that_arent_llama_3_knock/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf0mw2/opensource_architectures_that_arent_llama_3_knock/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754064644,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recently got into open source LLMs,I have now used a lot of models under 4b on my mobile and it runs gemma 2b (4bit medium) or llama 3.2 3b (4b med) reliably on pocketpal app\n\nTotal cpu threads on my device is 8 (4 core),when I enable 1 cpu thread the 2b model generates around 3 times faster tk/s than at 6 cpu threads\n\n1.do less cpu threads degrade the output quality?\n\n2.does it increase the hallucination rate? Most of the time,I m not really looking for longer context than 2k\n\n3.what do lower cpu threads enabled help in?",
          "author_fullname": "t2_63nhk1l7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Question about cpu threads (beginner here)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1meze5n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754061823,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently got into open source LLMs,I have now used a lot of models under 4b on my mobile and it runs gemma 2b (4bit medium) or llama 3.2 3b (4b med) reliably on pocketpal app&lt;/p&gt;\n\n&lt;p&gt;Total cpu threads on my device is 8 (4 core),when I enable 1 cpu thread the 2b model generates around 3 times faster tk/s than at 6 cpu threads&lt;/p&gt;\n\n&lt;p&gt;1.do less cpu threads degrade the output quality?&lt;/p&gt;\n\n&lt;p&gt;2.does it increase the hallucination rate? Most of the time,I m not really looking for longer context than 2k&lt;/p&gt;\n\n&lt;p&gt;3.what do lower cpu threads enabled help in?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1meze5n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Gold_Bar_4072",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1meze5n/question_about_cpu_threads_beginner_here/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1meze5n/question_about_cpu_threads_beginner_here/",
          "subreddit_subscribers": 508769,
          "created_utc": 1754061823,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}