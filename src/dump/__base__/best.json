{
  "kind": "Listing",
  "data": {
    "after": "t3_1mfb2ed",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "|Model Name|Organization|HuggingFace Link|Size|Modality|\n|:-|:-|:-|:-|:-|\n|dots.ocr|REDnote Hilab|[https://huggingface.co/rednote-hilab/dots.ocr](https://huggingface.co/rednote-hilab/dots.ocr)|3B|Image-Text-to-Text|\n||||||\n|GLM 4.5|[Z.ai](http://Z.ai)|[https://huggingface.co/zai-org/GLM-4.5](https://huggingface.co/zai-org/GLM-4.5)|355B-A32B|Text-to-Text|\n|GLM 4.5 Base|[Z.ai](http://Z.ai)|[https://huggingface.co/zai-org/GLM-4.5-Base](https://huggingface.co/zai-org/GLM-4.5-Base)|355B-A32B|Text-to-Text|\n|GLM 4.5-Air|[Z.ai](http://Z.ai)|[https://huggingface.co/zai-org/GLM-4.5-Air](https://huggingface.co/zai-org/GLM-4.5-Air)|106B-A12B|Text-to-Text|\n|GLM 4.5 Air Base|[Z.ai](http://Z.ai)|[https://huggingface.co/zai-org/GLM-4.5-Air-Base](https://huggingface.co/zai-org/GLM-4.5-Air-Base)|106B-A12B|Text-to-Text|\n||||||\n|Qwen3 235B-A22B Instruct 2507|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507)|235B-A22B|Text-to-Text|\n|Qwen3 235B-A22B Thinking 2507|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507)|235B-A22B|Text-to-Text|\n|Qwen3 30B-A3B Instruct 2507|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507)|30B-A3B|Text-to-Text|\n|Qwen3 30B-A3B Thinking 2507|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507)|30B-A3B|Text-to-Text|\n|Qwen3 Coder 480B-A35B Instruct|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct](https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct)|480B-A35B|Text-to-Text|\n|Qwen3 Coder 30B-A3B Instruct|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct](https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct)|30B-A3B|Text-to-Text|\n||||||\n|Kimi K2 Instruct|Moonshot AI|[https://huggingface.co/moonshotai/Kimi-K2-Instruct](https://huggingface.co/moonshotai/Kimi-K2-Instruct)|1T-32B|Text-to-Text|\n|Kimi K2 Base|Moonshot AI|[https://huggingface.co/moonshotai/Kimi-K2-Base](https://huggingface.co/moonshotai/Kimi-K2-Base)|1T-32B|Text-to-Text|\n||||||\n|Intern S1|Shanghai AI Laboratory - Intern|[https://huggingface.co/internlm/Intern-S1](https://huggingface.co/internlm/Intern-S1)|241B-A22B|Image-Text-to-Text|\n||||||\n|Llama-3.3 Nemotron Super 49B v1.5|Nvidia|[https://huggingface.co/nvidia/Llama-3\\_3-Nemotron-Super-49B-v1\\_5](https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5)|49B|Text-to-Text|\n|OpenReasoning Nemotron 1.5B|Nvidia|[https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B)|1.5B|Text-to-Text|\n|OpenReasoning Nemotron 7B|Nvidia|[https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B)|7B|Text-to-Text|\n|OpenReasoning Nemotron 14B|Nvidia|[https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B)|14B|Text-to-Text|\n|OpenReasoning Nemotron 32B|Nvidia|[https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B)|32B|Text-to-Text|\n||||||\n|step3|StepFun|[https://huggingface.co/stepfun-ai/step3](https://huggingface.co/stepfun-ai/step3)|321B-A38B|Text-to-Text|\n||||||\n|SmallThinker 21B-A3B Instruct|IPADS - PowerInfer|[https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct](https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct)|21B-A3B|Text-to-Text|\n|SmallThinker 4B-A0.6B Instruct|IPADS - PowerInfer|[https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct](https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct)|4B-A0.6B|Text-to-Text|\n||||||\n|Seed X Instruct-7B|ByteDance Seed|[https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B](https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B)|7B|Machine Translation|\n|Seed X PPO-7B|ByteDance Seed|[https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B](https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B)|7B|Machine Translation|\n||||||\n|Magistral Small 2507|Mistral|[https://huggingface.co/mistralai/Magistral-Small-2507](https://huggingface.co/mistralai/Magistral-Small-2507)|24B|Text-to-Text|\n|Devstral Small 2507|Mistral|[https://huggingface.co/mistralai/Devstral-Small-2507](https://huggingface.co/mistralai/Devstral-Small-2507)|24B|Text-to-Text|\n|Voxtral Small 24B 2507|Mistral|[https://huggingface.co/mistralai/Voxtral-Small-24B-2507](https://huggingface.co/mistralai/Voxtral-Small-24B-2507)|24B|Audio-Text-to-Text|\n|Voxtral Mini 3B 2507|Mistral|[https://huggingface.co/mistralai/Voxtral-Mini-3B-2507](https://huggingface.co/mistralai/Voxtral-Mini-3B-2507)|3B|Audio-Text-to-Text|\n||||||\n|AFM 4.5B|Arcee AI|[https://huggingface.co/arcee-ai/AFM-4.5B](https://huggingface.co/arcee-ai/AFM-4.5B)|4.5B|Text-to-Text|\n|AFM 4.5B Base|Arcee AI|[https://huggingface.co/arcee-ai/AFM-4.5B-Base](https://huggingface.co/arcee-ai/AFM-4.5B-Base)|4B|Text-to-Text|\n||||||\n|Ling lite-1.5 2506|Ant Group - Inclusion AI|[https://huggingface.co/inclusionAI/Ling-lite-1.5-2506](https://huggingface.co/inclusionAI/Ling-lite-1.5-2506)|16B|Text-to-Text|\n|Ming Lite Omni-1.5|Ant Group - Inclusion AI|[https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5](https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5)|20.3B|Text-Audio-Video-Image-To-Text|\n||||||\n|UIGEN X 32B 0727|Tesslate|[https://huggingface.co/Tesslate/UIGEN-X-32B-0727](https://huggingface.co/Tesslate/UIGEN-X-32B-0727)|32B|Text-to-Text|\n|UIGEN X 4B 0729|Tesslate|[https://huggingface.co/Tesslate/UIGEN-X-4B-0729](https://huggingface.co/Tesslate/UIGEN-X-4B-0729)|4B|Text-to-Text|\n|UIGEN X 8B|Tesslate|[https://huggingface.co/Tesslate/UIGEN-X-8B](https://huggingface.co/Tesslate/UIGEN-X-8B)|8B|Text-to-Text|\n||||||\n|command a vision 07-2025|Cohere|[https://huggingface.co/CohereLabs/command-a-vision-07-2025](https://huggingface.co/CohereLabs/command-a-vision-07-2025)|112B|Image-Text-to-Text|\n||||||\n|KAT V1 40B|Kwaipilot|[https://huggingface.co/Kwaipilot/KAT-V1-40B](https://huggingface.co/Kwaipilot/KAT-V1-40B)|40B|Text-to-Text|\n||||||\n|EXAONE 4.0.1 32B|LG AI|[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B)|32B|Text-to-Text|\n|EXAONE 4.0.1 2B|LG AI|[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B)|2B|Text-to-Text|\n|EXAONE 4.0 32B|LG AI|[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B)|32B|Text-to-Text|\n||||||\n|cogito v2 preview deepseek-671B-MoE|Deep Cogito|[https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE](https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE)|671B-A37B|Text-to-Text|\n|cogito v2 preview llama-405B|Deep Cogito|[https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B](https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B)|405B|Text-to-Text|\n|cogito v2 preview llama-109B-MoE|Deep Cogito|[https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE](https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE)|109B-A17B|Image-Text-to-Text|\n|cogito v2 preview llama-70B|Deep Cogito|[https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B](https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B)|70B|Text-to-Text|\n||||||\n|A.X 4.0 VL Light|SK Telecom|[https://huggingface.co/skt/A.X-4.0-VL-Light](https://huggingface.co/skt/A.X-4.0-VL-Light)|8B|Image-Text-to-Text|\n|A.X 3.1|SK Telecom|[https://huggingface.co/skt/A.X-3.1](https://huggingface.co/skt/A.X-3.1)|35B|Text-to-Text|\n|olmOCR 7B 0725|AllenAI|[https://huggingface.co/allenai/olmOCR-7B-0725](https://huggingface.co/allenai/olmOCR-7B-0725)|7B|Image-Text-to-Text|\n||||||\n|kanana 1.5 15.7B-A3B instruct|Kakao|[https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct](https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct)|7B-A3B|Text-to-Text|\n|kanana 1.5v 3B instruct|Kakao|[https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct](https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct)|3B|Image-Text-to-Text|\n||||||\n|Tri 7B|Trillion Labs|[https://huggingface.co/trillionlabs/Tri-7B](https://huggingface.co/trillionlabs/Tri-7B)|7B|Text-to-Text|\n|Tri 21B|Trillion Labs|[https://huggingface.co/trillionlabs/Tri-21B](https://huggingface.co/trillionlabs/Tri-21B)|21B|Text-to-Text|\n|Tri 70B preview SFT|Trillion Labs|[https://huggingface.co/trillionlabs/Tri-70B-preview-SFT](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT)|70B|Text-to-Text|\n\nI tried to compile the latest models released over the past 2–3 weeks, and its kinda like there is a ground breaking model every 2 days. I’m really glad to be living in this era of rapid progress.\n\nThis list doesn’t even include other modalities like 3D, image, and audio, where there's also a ton of new models (Like Wan2.2 , Flux-Krea , ...)\n\nHope this can serve as a breakdown of the latest models.\n\n*Feel free to tag me if I missed any you think should be added!*",
          "author_fullname": "t2_7zubl1l8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We're truly in the fastest-paced era of AI these days. (50 LLM Released these 2-3 Weeks)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfaigh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 140,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 140,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754091718,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754088000,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Organization&lt;/th&gt;\n&lt;th align=\"left\"&gt;HuggingFace Link&lt;/th&gt;\n&lt;th align=\"left\"&gt;Size&lt;/th&gt;\n&lt;th align=\"left\"&gt;Modality&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;dots.ocr&lt;/td&gt;\n&lt;td align=\"left\"&gt;REDnote Hilab&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/rednote-hilab/dots.ocr\"&gt;https://huggingface.co/rednote-hilab/dots.ocr&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GLM 4.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5\"&gt;https://huggingface.co/zai-org/GLM-4.5&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;355B-A32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GLM 4.5 Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5-Base\"&gt;https://huggingface.co/zai-org/GLM-4.5-Base&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;355B-A32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GLM 4.5-Air&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5-Air\"&gt;https://huggingface.co/zai-org/GLM-4.5-Air&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;106B-A12B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GLM 4.5 Air Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5-Air-Base\"&gt;https://huggingface.co/zai-org/GLM-4.5-Air-Base&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;106B-A12B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 235B-A22B Instruct 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507\"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;235B-A22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 235B-A22B Thinking 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507\"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;235B-A22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 30B-A3B Instruct 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507\"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 30B-A3B Thinking 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507\"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 Coder 480B-A35B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\"&gt;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;480B-A35B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 Coder 30B-A3B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct\"&gt;https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Kimi K2 Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Moonshot AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Instruct\"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;1T-32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Kimi K2 Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;Moonshot AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Base\"&gt;https://huggingface.co/moonshotai/Kimi-K2-Base&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;1T-32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Intern S1&lt;/td&gt;\n&lt;td align=\"left\"&gt;Shanghai AI Laboratory - Intern&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/internlm/Intern-S1\"&gt;https://huggingface.co/internlm/Intern-S1&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;241B-A22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama-3.3 Nemotron Super 49B v1.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\"&gt;https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;49B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenReasoning Nemotron 1.5B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.5B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenReasoning Nemotron 7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenReasoning Nemotron 14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenReasoning Nemotron 32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;step3&lt;/td&gt;\n&lt;td align=\"left\"&gt;StepFun&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/stepfun-ai/step3\"&gt;https://huggingface.co/stepfun-ai/step3&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;321B-A38B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;SmallThinker 21B-A3B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;IPADS - PowerInfer&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct\"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;21B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;SmallThinker 4B-A0.6B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;IPADS - PowerInfer&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct\"&gt;https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4B-A0.6B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Seed X Instruct-7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;ByteDance Seed&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B\"&gt;https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Machine Translation&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Seed X PPO-7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;ByteDance Seed&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B\"&gt;https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Machine Translation&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Magistral Small 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/mistralai/Magistral-Small-2507\"&gt;https://huggingface.co/mistralai/Magistral-Small-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;24B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Devstral Small 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/mistralai/Devstral-Small-2507\"&gt;https://huggingface.co/mistralai/Devstral-Small-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;24B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Voxtral Small 24B 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/mistralai/Voxtral-Small-24B-2507\"&gt;https://huggingface.co/mistralai/Voxtral-Small-24B-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;24B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Audio-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Voxtral Mini 3B 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/mistralai/Voxtral-Mini-3B-2507\"&gt;https://huggingface.co/mistralai/Voxtral-Mini-3B-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Audio-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;AFM 4.5B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Arcee AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/arcee-ai/AFM-4.5B\"&gt;https://huggingface.co/arcee-ai/AFM-4.5B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.5B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;AFM 4.5B Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;Arcee AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/arcee-ai/AFM-4.5B-Base\"&gt;https://huggingface.co/arcee-ai/AFM-4.5B-Base&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Ling lite-1.5 2506&lt;/td&gt;\n&lt;td align=\"left\"&gt;Ant Group - Inclusion AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/inclusionAI/Ling-lite-1.5-2506\"&gt;https://huggingface.co/inclusionAI/Ling-lite-1.5-2506&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;16B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Ming Lite Omni-1.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;Ant Group - Inclusion AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5\"&gt;https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-Audio-Video-Image-To-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;UIGEN X 32B 0727&lt;/td&gt;\n&lt;td align=\"left\"&gt;Tesslate&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-32B-0727\"&gt;https://huggingface.co/Tesslate/UIGEN-X-32B-0727&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;UIGEN X 4B 0729&lt;/td&gt;\n&lt;td align=\"left\"&gt;Tesslate&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-4B-0729\"&gt;https://huggingface.co/Tesslate/UIGEN-X-4B-0729&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;UIGEN X 8B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Tesslate&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-8B\"&gt;https://huggingface.co/Tesslate/UIGEN-X-8B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;8B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;command a vision 07-2025&lt;/td&gt;\n&lt;td align=\"left\"&gt;Cohere&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/CohereLabs/command-a-vision-07-2025\"&gt;https://huggingface.co/CohereLabs/command-a-vision-07-2025&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;112B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;KAT V1 40B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Kwaipilot&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Kwaipilot/KAT-V1-40B\"&gt;https://huggingface.co/Kwaipilot/KAT-V1-40B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;40B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;EXAONE 4.0.1 32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;LG AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;EXAONE 4.0.1 2B&lt;/td&gt;\n&lt;td align=\"left\"&gt;LG AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;2B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;EXAONE 4.0 32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;LG AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;cogito v2 preview deepseek-671B-MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;Deep Cogito&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;671B-A37B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;cogito v2 preview llama-405B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Deep Cogito&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;405B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;cogito v2 preview llama-109B-MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;Deep Cogito&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;109B-A17B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;cogito v2 preview llama-70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Deep Cogito&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A.X 4.0 VL Light&lt;/td&gt;\n&lt;td align=\"left\"&gt;SK Telecom&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/skt/A.X-4.0-VL-Light\"&gt;https://huggingface.co/skt/A.X-4.0-VL-Light&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;8B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A.X 3.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;SK Telecom&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/skt/A.X-3.1\"&gt;https://huggingface.co/skt/A.X-3.1&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;35B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;olmOCR 7B 0725&lt;/td&gt;\n&lt;td align=\"left\"&gt;AllenAI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/allenai/olmOCR-7B-0725\"&gt;https://huggingface.co/allenai/olmOCR-7B-0725&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;kanana 1.5 15.7B-A3B instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Kakao&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct\"&gt;https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;kanana 1.5v 3B instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Kakao&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct\"&gt;https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Tri 7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Trillion Labs&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/trillionlabs/Tri-7B\"&gt;https://huggingface.co/trillionlabs/Tri-7B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Tri 21B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Trillion Labs&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/trillionlabs/Tri-21B\"&gt;https://huggingface.co/trillionlabs/Tri-21B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;21B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Tri 70B preview SFT&lt;/td&gt;\n&lt;td align=\"left\"&gt;Trillion Labs&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/trillionlabs/Tri-70B-preview-SFT\"&gt;https://huggingface.co/trillionlabs/Tri-70B-preview-SFT&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I tried to compile the latest models released over the past 2–3 weeks, and its kinda like there is a ground breaking model every 2 days. I’m really glad to be living in this era of rapid progress.&lt;/p&gt;\n\n&lt;p&gt;This list doesn’t even include other modalities like 3D, image, and audio, where there&amp;#39;s also a ton of new models (Like Wan2.2 , Flux-Krea , ...)&lt;/p&gt;\n\n&lt;p&gt;Hope this can serve as a breakdown of the latest models.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Feel free to tag me if I missed any you think should be added!&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?auto=webp&amp;s=83e0fd1aa924b9918306c02a99cedb9bbb2eb1cb",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=014ce09ab614e86be0bda115d3ee826dd4c7e72b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9fb10f0400ab7291afbb905ab3dfdfb49e477ed8",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=88b160d056e65a5fdd1da13d608db9a9c123e2d7",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=54fe70e1d1e50ac63262c7c7180e0173f9cc1673",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fa0d50402090bd3ad6e9c270f0f950421a2c1523",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dff2cd4e982c9356a88ce61af693c4ca57815b99",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfaigh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "citaman",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754088000,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The \"Leaked\" 120B OpenAI Model Is Trained In FP4\n",
          "author_fullname": "t2_9zkdy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The “Leaked” 120 B OpenAI Model is not Trained in FP4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 98,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf3tm9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 257,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 257,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/D9ovYsMF-MnoR2CRSeSS8Yh_VU6bc2G4S1R5gIz7WiE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754071895,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The &amp;quot;Leaked&amp;quot; 120B OpenAI Model Is Trained In FP4&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/g1yk8r6b8ggf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?auto=webp&amp;s=af4bbedc766a4ee5a39037f2ab17d7b5501cd231",
                  "width": 1290,
                  "height": 906
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=926137a58fce6f1ef8bee443ff019ae18b337863",
                    "width": 108,
                    "height": 75
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e501be0974ffc2db9bd5cda1678b62434d903b5",
                    "width": 216,
                    "height": 151
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f48501cc5bd5fb8e0b44c8d1575f9a4f16b061b",
                    "width": 320,
                    "height": 224
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd4ab4d6c8195a6e7189dc0435de525dd356fb06",
                    "width": 640,
                    "height": 449
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=002c3d3feadb04f86a25d1efd1608fcdeb907ef0",
                    "width": 960,
                    "height": 674
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f1b63bb57ddbe4e7a32c5af91d1f065caf81082f",
                    "width": 1080,
                    "height": 758
                  }
                ],
                "variants": {},
                "id": "xtQ4De0a5tVgoxoyHlb2WUymp023nocly_no7XupZ6k"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mf3tm9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "badbutt21",
          "discussion_type": null,
          "num_comments": 70,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf3tm9/the_leaked_120_b_openai_model_is_not_trained_in/",
          "stickied": false,
          "url": "https://i.redd.it/g1yk8r6b8ggf1.jpeg",
          "subreddit_subscribers": 508541,
          "created_utc": 1754071895,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/Qwen/Qwen3-Embedding-0.6B](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B)\n\n\n\nI switched over today. Initially the results seemed poor, but it turns out there was an issue when using Text embedding inference 1.7.2 related to pad tokens. Fixed in 1.7.3 . Depending on what inference tooling you are using there could be a similar issue.\n\nThe very fast response time opens up new use cases. Most small embedding models until recently had very small context windows of around 512 tokens and the quality didn't rival the bigger models you could use through openAI or google.",
          "author_fullname": "t2_uaotuj04",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Embedding-0.6B is fast, high quality, and supports up to 32k tokens. Beats OpenAI embeddings on MTEB",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf6bkl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 171,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 171,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754077669,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-Embedding-0.6B\"&gt;https://huggingface.co/Qwen/Qwen3-Embedding-0.6B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I switched over today. Initially the results seemed poor, but it turns out there was an issue when using Text embedding inference 1.7.2 related to pad tokens. Fixed in 1.7.3 . Depending on what inference tooling you are using there could be a similar issue.&lt;/p&gt;\n\n&lt;p&gt;The very fast response time opens up new use cases. Most small embedding models until recently had very small context windows of around 512 tokens and the quality didn&amp;#39;t rival the bigger models you could use through openAI or google.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HVlDx3sIc9fDyqxYROltZh4_zaLPIwQ2OvPGjxm27kk.png?auto=webp&amp;s=4399c8976d12b85ffaee7ae14ab7f5725bb2ea12",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HVlDx3sIc9fDyqxYROltZh4_zaLPIwQ2OvPGjxm27kk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e0a0b9e00a90a64308b392ea065a5666bbc7c99a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/HVlDx3sIc9fDyqxYROltZh4_zaLPIwQ2OvPGjxm27kk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=24794d4dc5e2f816acf136f12041a449ec01d2b4",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/HVlDx3sIc9fDyqxYROltZh4_zaLPIwQ2OvPGjxm27kk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ced369b8a0ae3d1cdbe7a030960c50fd3f2cfdd2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/HVlDx3sIc9fDyqxYROltZh4_zaLPIwQ2OvPGjxm27kk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=64a598c0c7e2a44fa02d43ac09c7d63d0a6c1b6b",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/HVlDx3sIc9fDyqxYROltZh4_zaLPIwQ2OvPGjxm27kk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cba6e6727f41b1406c3ce46b365fd9edcbcbf5c5",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/HVlDx3sIc9fDyqxYROltZh4_zaLPIwQ2OvPGjxm27kk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4a426c4d7704d76efbc418602a12814dc8c29b80",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "HVlDx3sIc9fDyqxYROltZh4_zaLPIwQ2OvPGjxm27kk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mf6bkl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Edge2098",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf6bkl/qwen3embedding06b_is_fast_high_quality_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf6bkl/qwen3embedding06b_is_fast_high_quality_and/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754077669,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hg：https://huggingface.co/ScienceOne-AI/S1-Base-671B",
          "author_fullname": "t2_44shmmed9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China report the finetune deepseek scientific model 40.44% on HLE",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf8pdo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 86,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 86,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/UTfHZTk3AkcFKdrIagp_47SxVLFGVmG24S66FaWQdXU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754083417,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hg：&lt;a href=\"https://huggingface.co/ScienceOne-AI/S1-Base-671B\"&gt;https://huggingface.co/ScienceOne-AI/S1-Base-671B&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/rnyzqia76hgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?auto=webp&amp;s=06740559a06a04129cbe49eca027a0862fd98c3b",
                  "width": 1927,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=71982e38120d9120577e53d8fabd9588d8007e4b",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1b276889bf7a37bf56f4c27a131188e43815865c",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc9bc3ff25403be175e671f34f1139f244fb6d61",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=19933808cef3cec6dce268be3e9d5d269f435579",
                    "width": 640,
                    "height": 358
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fcd174de9a930fac1ee5ef6231c6a4a6d70fd332",
                    "width": 960,
                    "height": 538
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4f6e3196a3a315fef89102429c2a9e280f5ad273",
                    "width": 1080,
                    "height": 605
                  }
                ],
                "variants": {},
                "id": "l_Dz9A7On8qHXdUNNBTW3zJ0Gj8IkayixkIAvRolxWc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mf8pdo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Afraid_Hall_2971",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf8pdo/china_report_the_finetune_deepseek_scientific/",
          "stickied": false,
          "url": "https://i.redd.it/rnyzqia76hgf1.jpeg",
          "subreddit_subscribers": 508541,
          "created_utc": 1754083417,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_dphk4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-2507 is the top open weights model on lmarena",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf0qlf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 129,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 129,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754064880,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/lmarena_ai/status/1951308670375174457",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mf0qlf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tarruda",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf0qlf/qwen3235ba22b2507_is_the_top_open_weights_model/",
          "stickied": false,
          "url": "https://x.com/lmarena_ai/status/1951308670375174457",
          "subreddit_subscribers": 508541,
          "created_utc": 1754064880,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "MAESTRO is a self-hosted AI application designed to streamline the research and writing process. It integrates a powerful document management system with two distinct operational modes: Research Mode (like deep research) and Writing Mode (AI assisted writing).\n\n# Autonomous Research Mode\n\nIn this mode, the application automates research tasks for you.\n\n* **Process**: You start by giving it a research question or a topic.\n* **Action**: The AI then searches for information in your uploaded documents or on the web.\n* **Output**: Based on what it finds, the AI generates organized notes and then writes a full research report.\n\nThis mode is useful when you need to quickly gather information on a topic or create a first draft of a document.\n\n# AI-Assisted Writing Mode\n\nThis mode provides help from an AI while you are writing.\n\n* **Interface**: It consists of a markdown text editor next to an AI chat window.\n* **Workflow**: You can write in the editor and ask the AI questions at the same time. The AI can access your document collections and the web to find answers.\n* **Function**: The AI provides the information you request in the chat window, which you can then use in the document you are writing.\n\nThis mode allows you to get research help without needing to leave your writing environment.\n\n# Document Management\n\nThe application is built around a document management system.\n\n* **Functionality**: You can upload your documents (currently only PDFs) and group them into \"folders.\"\n* **Purpose**: These collections serve as a specific knowledge base for your projects. You can instruct the AI in either mode to use only the documents within a particular collection, ensuring its work is based on the source materials you provide.",
          "author_fullname": "t2_281myw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "MAESTRO, a deep research assistant/RAG pipeline that runs on your local LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 106,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "wfye5hh28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=263452376481ede5f5ee30b3d1fa8d06b92acb2f"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f6985e77e3d428df5e2fae627ebdaccf43636351"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=36eabba987901a939cb1cba3fa220c59244f5980"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=54358ea98fe46953da35a682104b2d28c041974a"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0fce8a0da83bbecc8646448132fc88235fc0cc6f"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=41259159b0776715b20abd50ed3f735ab55ec141"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=defebde57be1e79d81fad254f827317795a27064"
              },
              "id": "wfye5hh28hgf1"
            },
            "vdsr7ch28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bdacf777510be4c1c1e2cb42159066c1880e6dab"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c356b2b93449bd2d60ae1d426ad98a061cd4282"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae0474fcb61f8fc562cb20c1d53c97e12decaa3c"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=32dbcb0b4526b601435315f9f9ad55eac8e9e9f1"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=54e5b3229d01b08c97d666d27a878563ef0471ae"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e566592ef69e058c675213f1b88982c9f112e66b"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=6a56c58428afe63b1a658254915645c82408e1ce"
              },
              "id": "vdsr7ch28hgf1"
            },
            "4tvg8ah28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=388d6a0156beacf091be4cabfc6a12ef9b2df0e0"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab7ae93af73319fe2da856256040e6bb81700659"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=36457622c836252465883bf076811b25d1ad38a2"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=60ee85a9f73c317a448f5f7a4a39ee239c5eed29"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a6a9af626a4376d663d8cf2d5ce76f802eb0d173"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=726f4a6fca4e608a57b66118d9f5186788c5d1b8"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=ace0180e9725fd088236b603c03e703e138e1af1"
              },
              "id": "4tvg8ah28hgf1"
            },
            "tjfnu7h28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=65253585bbce7fb9495344864b4dbf089fe4a866"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab36743c53f66ceaa0b69bd5950db8c4c34979cd"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1946f554204091e4ee226eecbc6b9b75297b047d"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb8016f74c1a4927f5217225504f99e78266bd6d"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6fff118076daf8093a4c9a0e956b59d667d8a17a"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5073f2ce72eef58984bea03e338686a824f604b1"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=16d847bb68c7a49b063970c2a53144217a358b64"
              },
              "id": "tjfnu7h28hgf1"
            },
            "ko10neh28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33c335bff8fd3207fbe0ef4cb7e045a625ad3790"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4245d285c8e6e167c8aa5f8a8196f6f0f79234b0"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ec4b568dd2d51940205e41d3a1168db82a30974"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bcb8c81cb1083135981d3576d1e94c4d7061ef1"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e3f2a7685b2f5d071bd47a85f2da0b70360fa499"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=379b494f484f127cc25b662a1a5f9a3753470d94"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/ko10neh28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=de810a3b9c7f9df343cc5c34d57dd4752a8b191d"
              },
              "id": "ko10neh28hgf1"
            },
            "xpft85h28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8b16877882ed90f113b1d8753896a42c5c1cb57"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d6125a00674b9020cb681a99893ad47b47e7913"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7cb747f3b0be9eca90601bb0dd0e4f364c7ebc2b"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd1b23bc33d392f682dd70713ef0e6770cafd430"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=43a97bc0e6dad1aeeb47843446cd709c83f052db"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3927f27a929df1bb9fa672c58a3616cffaa32c5d"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/xpft85h28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=a8d61da6b389396eadf3f74a5e84046b8a859996"
              },
              "id": "xpft85h28hgf1"
            },
            "n8f3d7h28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0099db17c53d752e1f52434bb4a16edfe724a7d1"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7b068877d833bcda5b55ab2245068d9b2e0148e"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e3c098055fc230578e3b0d4cf171776999a25ae"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=124fc23c5296c23dc74d6a6c55ce26db75d2d137"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=16d4bec4c048c58090daaad2dfcaf34e92f941cb"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4be81c901606198bf5e6e6c8097ebe64e6a6d39c"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=e915bfa4f37d301877e87b2951c78762e6a8ea17"
              },
              "id": "n8f3d7h28hgf1"
            },
            "9gzp5ch28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=74b4696764dfc847eef669961f79b0866d42f5d9"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=168ad0f4c7c2dbc7f867cf7085cff2fc27e24bc3"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d89d57702d0ba973c5772c39da3b51e2e6b5da5"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f80ab136ac496f933eb22700bd7e0c0cfabe288"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=978a98d62c65f1d83a8464507a025600df74f898"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5124a214a0cad85638935b440b2b58a3587824c6"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=f958ce1ae3061d6b0c94c35b4636b3aa5e8260d7"
              },
              "id": "9gzp5ch28hgf1"
            },
            "otdkteh28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=055054f2a3a59c53fa172087439b7f25945578f3"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63baa369d4fd7095a2c702fb261aebb398248ee5"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=993e59ad6c2ab02adfae15d2fbd66ec726a4bd13"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e5d52a002ea01c6b7d0bfa10e537c7d7969f822"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=49dcb5b218991d6cfc81b7dea2cdd7b72a730add"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=64546f9c94b5f46090ccd646df96e7d3c4477044"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/otdkteh28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=2f6b24cf0547caddaa5f756f6cd6778238257d37"
              },
              "id": "otdkteh28hgf1"
            }
          },
          "name": "t3_1mf92r1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#c7b594",
          "ups": 43,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "Deep Research Draft",
                "media_id": "wfye5hh28hgf1",
                "id": 719101200
              },
              {
                "caption": "Writing Draft with chat pulling data from your documents as well as the internet",
                "media_id": "ko10neh28hgf1",
                "id": 719101201
              },
              {
                "caption": "Write in markdown",
                "media_id": "vdsr7ch28hgf1",
                "id": 719101202
              },
              {
                "caption": "Make document folders to use with your research/writing projects",
                "media_id": "9gzp5ch28hgf1",
                "id": 719101203
              },
              {
                "caption": "Manage documents",
                "media_id": "4tvg8ah28hgf1",
                "id": 719101204
              },
              {
                "caption": "Deep dive into the Deep Researcher outputs like notes prepped from your sources",
                "media_id": "n8f3d7h28hgf1",
                "id": 719101205
              },
              {
                "caption": "Comprehensive research flow with iterative action/reflection loops",
                "media_id": "otdkteh28hgf1",
                "id": 719101206
              },
              {
                "caption": "Complete transparency in the model of your choice's reasoning and performance",
                "media_id": "tjfnu7h28hgf1",
                "id": 719101207
              },
              {
                "caption": "Complete transparency in the model of your choice's reasoning and performance",
                "media_id": "xpft85h28hgf1",
                "id": 719101208
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 43,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Tt0ml3YBBqO4cJ7-sHxE5os9lg6KgXNM6oovDynmETQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754084338,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MAESTRO is a self-hosted AI application designed to streamline the research and writing process. It integrates a powerful document management system with two distinct operational modes: Research Mode (like deep research) and Writing Mode (AI assisted writing).&lt;/p&gt;\n\n&lt;h1&gt;Autonomous Research Mode&lt;/h1&gt;\n\n&lt;p&gt;In this mode, the application automates research tasks for you.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: You start by giving it a research question or a topic.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: The AI then searches for information in your uploaded documents or on the web.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Based on what it finds, the AI generates organized notes and then writes a full research report.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This mode is useful when you need to quickly gather information on a topic or create a first draft of a document.&lt;/p&gt;\n\n&lt;h1&gt;AI-Assisted Writing Mode&lt;/h1&gt;\n\n&lt;p&gt;This mode provides help from an AI while you are writing.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Interface&lt;/strong&gt;: It consists of a markdown text editor next to an AI chat window.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Workflow&lt;/strong&gt;: You can write in the editor and ask the AI questions at the same time. The AI can access your document collections and the web to find answers.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Function&lt;/strong&gt;: The AI provides the information you request in the chat window, which you can then use in the document you are writing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This mode allows you to get research help without needing to leave your writing environment.&lt;/p&gt;\n\n&lt;h1&gt;Document Management&lt;/h1&gt;\n\n&lt;p&gt;The application is built around a document management system.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Functionality&lt;/strong&gt;: You can upload your documents (currently only PDFs) and group them into &amp;quot;folders.&amp;quot;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: These collections serve as a specific knowledge base for your projects. You can instruct the AI in either mode to use only the documents within a particular collection, ensuring its work is based on the source materials you provide.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mf92r1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf92r1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hedonihilistic",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mf92r1/maestro_a_deep_research_assistantrag_pipeline/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mf92r1",
          "subreddit_subscribers": 508541,
          "created_utc": 1754084338,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just because you are hosting locally, doesn't mean your LLM agent is necessarily private. I wrote a blog about how LLMs can be fine-tuned to execute malicious tool calls with popular MCP servers. I included links to the code and dataset in the article. Enjoy!",
          "author_fullname": "t2_kfjfm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DoubleAgents: Fine-tuning LLMs for Covert Malicious Tool Calls",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 76,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mfbw8a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=140&amp;height=76&amp;crop=140:76,smart&amp;auto=webp&amp;s=178741032ad68bb72212a1f0482ccf59165855d7",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754091780,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "medium.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just because you are hosting locally, doesn&amp;#39;t mean your LLM agent is necessarily private. I wrote a blog about how LLMs can be fine-tuned to execute malicious tool calls with popular MCP servers. I included links to the code and dataset in the article. Enjoy!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://medium.com/@justin_45141/doubleagents-fine-tuning-llms-for-covert-malicious-tool-calls-b8ff00bf513e",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?auto=webp&amp;s=db6f154eef502137479106eb0ee5e3497a2c8a5d",
                  "width": 1024,
                  "height": 559
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6fc30b818f499ebfea16a1a44bc05f5b89c31100",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d8f36d4f360255f41bf1da0fda787ab734a46fbd",
                    "width": 216,
                    "height": 117
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ad1e44aa12420eb39ba559c3fdbad4ad0407e0b",
                    "width": 320,
                    "height": 174
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=24b8b3213823bb044c73076e1852d1957545a17f",
                    "width": 640,
                    "height": 349
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4cb31249007f2a139fccd496d749e3a58bdc7c0d",
                    "width": 960,
                    "height": 524
                  }
                ],
                "variants": {},
                "id": "1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfbw8a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JAlbrethsen",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfbw8a/doubleagents_finetuning_llms_for_covert_malicious/",
          "stickied": false,
          "url": "https://medium.com/@justin_45141/doubleagents-fine-tuning-llms-for-covert-malicious-tool-calls-b8ff00bf513e",
          "subreddit_subscribers": 508541,
          "created_utc": 1754091780,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The person who \"leaked\" this model is from the openai (HF) organization \n\nSo as expected, it's not gonna be something you can easily run locally, it won't hurt the chatgpt subscription business, you will need a dedicated LLM machine for that model ",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "The OpenAI Open weight model might be 120B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "9dqwiep9ucgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 128,
                  "x": 108,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=82c6a3fca8afcd98702c0b7d846a7e160d71c2ba"
                },
                {
                  "y": 257,
                  "x": 216,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=daf71cd9fc68c069a95fe74761dcc142bc617a8b"
                },
                {
                  "y": 381,
                  "x": 320,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=436cbfa27d8543f73db2371953a82aac39ecda85"
                },
                {
                  "y": 763,
                  "x": 640,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8299ea44db486e3a2fbec74c3d0bd88466708866"
                },
                {
                  "y": 1144,
                  "x": 960,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ff8e80fe3dec58ee22a247bfdab51b53bcac09ca"
                },
                {
                  "y": 1288,
                  "x": 1080,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4d90e7d7d325eaa34c7621c43ce177895ff0383"
                }
              ],
              "s": {
                "y": 1288,
                "x": 1080,
                "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=b3767f5800ed4a78ec866eaa707e8dbe0151f82f"
              },
              "id": "9dqwiep9ucgf1"
            },
            "w9h7ftgaucgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 187,
                  "x": 108,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=65c73f27d3ec2b6268ed676ac73a20d96cc01864"
                },
                {
                  "y": 375,
                  "x": 216,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a7776f97e1b1b9b42cd96de35d6b38c2bdf1948d"
                },
                {
                  "y": 555,
                  "x": 320,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17560f484f4fc6684187f9b4e3214d2bb480250d"
                },
                {
                  "y": 1111,
                  "x": 640,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=64632b927b61af650b70b9b6e2e93befcc39ee18"
                },
                {
                  "y": 1667,
                  "x": 960,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8bd9619e6620d4a4c7a2fbdf4077d3780cf0b105"
                }
              ],
              "s": {
                "y": 1873,
                "x": 1078,
                "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=1078&amp;format=png&amp;auto=webp&amp;s=dd26649ae5edf99eedfd8378f94c7fdf3aeb6ae9"
              },
              "id": "w9h7ftgaucgf1"
            },
            "s1j5io2aucgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=066bdffe86c232ba5ff6c7d20d8dde94884c7f6e"
                },
                {
                  "y": 188,
                  "x": 216,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3ab77eae4021c3e0b9af7ed66501883f4211b90"
                },
                {
                  "y": 279,
                  "x": 320,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=61d00c48c6b5fb9736c6d3503205a847c8146f46"
                },
                {
                  "y": 558,
                  "x": 640,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=de6f7067561af43d54b56fbf6902277437c1fa94"
                },
                {
                  "y": 837,
                  "x": 960,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=062e6b2cc5f7e404a5f33fca5b18a6e367b0f99b"
                },
                {
                  "y": 942,
                  "x": 1080,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5e518857b677661c538f1b99548e5ca304d3b186"
                }
              ],
              "s": {
                "y": 942,
                "x": 1080,
                "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=b86914c6db57a17c4bde148a77b9a789f692e93f"
              },
              "id": "s1j5io2aucgf1"
            }
          },
          "name": "t3_1mepeqh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bbbdbf",
          "ups": 639,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "9dqwiep9ucgf1",
                "id": 718647288
              },
              {
                "caption": "",
                "media_id": "s1j5io2aucgf1",
                "id": 718647289
              },
              {
                "caption": "",
                "media_id": "w9h7ftgaucgf1",
                "id": 718647290
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 639,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/uvWDYtCBC32T5YD2glI0V4HTGmyDJzZTUERWQkmJQoE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754030862,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The person who &amp;quot;leaked&amp;quot; this model is from the openai (HF) organization &lt;/p&gt;\n\n&lt;p&gt;So as expected, it&amp;#39;s not gonna be something you can easily run locally, it won&amp;#39;t hurt the chatgpt subscription business, you will need a dedicated LLM machine for that model &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mepeqh",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mepeqh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 154,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mepeqh/the_openai_open_weight_model_might_be_120b/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mepeqh",
          "subreddit_subscribers": 508541,
          "created_utc": 1754030862,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_14bv8c06dm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gemini 2.5 Deep Think mode benchmarks!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1meu3jn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "ups": 256,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 256,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/abtcO2rczXZnz1xo2s1h25imovC55smF-ZvaOmt_0Tw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754048166,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8wnv6pme9egf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8wnv6pme9egf1.png?auto=webp&amp;s=e049a99b67af3601aa3ae286cd33e368a88628f3",
                  "width": 1001,
                  "height": 1173
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8wnv6pme9egf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ae70156b903e0aed05b8381e5896b9b69d5b72e6",
                    "width": 108,
                    "height": 126
                  },
                  {
                    "url": "https://preview.redd.it/8wnv6pme9egf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2dbc8968418c0fffb09064cde42872404023bf84",
                    "width": 216,
                    "height": 253
                  },
                  {
                    "url": "https://preview.redd.it/8wnv6pme9egf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=47264da7484d4c30ea47bbbe23a3cde136478df5",
                    "width": 320,
                    "height": 374
                  },
                  {
                    "url": "https://preview.redd.it/8wnv6pme9egf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=557a01b879fc1bdbf0cc88dc3a91d0b4a7b1c10c",
                    "width": 640,
                    "height": 749
                  },
                  {
                    "url": "https://preview.redd.it/8wnv6pme9egf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7da1fd0ae1e95d98f9e7a1046cfa44b0f374a42f",
                    "width": 960,
                    "height": 1124
                  }
                ],
                "variants": {},
                "id": "2QR1T0pd9pOTCNAN3kWL8a-3T9kX77SVsmRgOPYX2zQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1meu3jn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Beautiful-Essay1945",
          "discussion_type": null,
          "num_comments": 67,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1meu3jn/gemini_25_deep_think_mode_benchmarks/",
          "stickied": false,
          "url": "https://i.redd.it/8wnv6pme9egf1.png",
          "subreddit_subscribers": 508541,
          "created_utc": 1754048166,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ever spent weeks building the perfect LLM benchmark only to watch it crumble within a few months?\n\nClean problems, elegant difficulty curves, proper statistical controls. New model drops. Perfect scores across the board. Your tests got trained on. Weeks of work, completely worthless.\n\nSo you pivot. Make the tests harder, more complex, more creative. Models improve with time. Now everyone clusters at 90-95%. 8B models are defeating it. Your benchmark has become a participation trophy. This happened to my previous evaluation, *Can-Ai-Code*, twice.\n\nFine, you say. Random test generation it is! No more memorization, no more clustering. But congratulations, you've just unlocked new nightmares: Did you accidentally make your \"hard\" tests easier than your \"easy\" ones? Is your random number generator secretly biased? How do you even validate that hundreds of thousands of randomly generated problems \"make sense\"?\n\nYou solve that with clever statistical rigor, only to discover configuration explosion hell. You'd like to test different prompting templates and sampling parameters, but that's 5 templates times 5 samplers times 50 million tokens (a conservative estimate) equals 1.25 billion tokens per model. Your GPUs scream in horror.\n\nYou're now burning millions of tokens achieving 0.005 confidence intervals on trivial problems while critical hard points sit at 0.02 intervals begging for attention like abandoned puppies. Dynamic sampling helps - generate more tests for uncertain points, fewer for confident ones - but how to avoid p-hacking yourself?\n\nThat's when the guessing realization hits. This binary classifier task scored 60%! Amazing! Wait... that's only 20% above random chance. Your \"75% accurate\" multiple choice task is actually 50% accurate when you subtract lucky guesses. Everything is statistical lies. How are you supposed to compare models across boolean, multiple-choice and write-in answer tasks that have fundamentally different \"guess rates\"?\n\nFinally, truncation waste arrives to complete your suffering: Model given tough task hits context limits, burns 8,000 tokens, returns a loop of gibberish. You sample 10x more to maintain statistical power. That's 80K tokens wasted for one data point but with no useful answers.  You're overflowing your KV caches while the confidence intervals laugh at you.\n\nAfter drowning in this cascade of pain for months, I did what any reasonable person would do: I built an evaluation system to solve every single practical problem I encountered.\n\n# ReasonScape treats language models as information processing systems, not text completion black boxes.\n\nIt generates infinite, parametric, tokenization-aware test variations, applies statistical corrections for guessing, dynamically allocates sampling based on uncertainty, handles truncations intelligently, and visualizes the results as both enhanced leaderboards and explorable 3D cognitive landscapes.\n\n[C2: All Models x All Tasks Surface Comparison. Green Sphere indicates high-success. Red Square indicates high-truncation.](https://preview.redd.it/vsoidu4e4ggf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=d29809860b081384d998a428bc75faeba16cedc1)\n\nThe initial C2 dataset represents \\~1 billion tokens across 9 models, revealing exactly where, how and why reasoning breaks down across 4 task domains. The interactive leaderboard shows not just scores but confidence intervals, token usage and failure modes. The explorer (links at the bottom of post) lets you navigate difficulty manifolds like some kind of LLM reasoning archaeologist, digging into spectral analysis and completion token patterns.  Make sure you're on a PC - this application has too much going on to be mobile friendly!\n\n[C2 Explorer](https://preview.redd.it/4ahuh87m4ggf1.png?width=1233&amp;format=png&amp;auto=webp&amp;s=8f6e962cdc029ce01dbca46346ec3fda47a06d7d)\n\nI built the system with progressive evaluation in mind so you can start with rapid exploration then scale to deep precision. Everything caches, everything reproduces, everything scales. ReasonScape isn't just another benchmark. It's a complete methodology: toolkit, evaluation framework, and growing dataset family rolled into one.\n\n[C2 Leaderboard \\(Static snapshot - the Interactive is much nicer!\\)](https://preview.redd.it/rn7r2k3t4ggf1.png?width=1198&amp;format=png&amp;auto=webp&amp;s=52d054e40f6f292b07b9d638d82244e8f302ce1d)\n\nThe ReasonScape experiments and the resulting datasets will grow, expand and evolve - when scores get too high we will move the difficulty grids to make the tests harder and move on to C3. I have **8 additional tasks** to bring up, and lots more reasoning models I'd like to evaluate but my 2xRTX3090 only have so much to give.\n\nThanks for reading this far! &lt;3\n\nLinks:\n\n* [ReasonScape Homepage](https://reasonscape.com/)\n* [ReasonScape Leaderboard - C2](https://reasonscape.com/c2/leaderboard)\n* [ReasonScape Explorer - C2](https://reasonscape.com/c2/explorer) (note: PC required, not mobile-friendly)\n* [ReasonScape GitHub](https://github.com/the-crypt-keeper/reasonscape)\n* [ReasonScape System Architecture](https://github.com/the-crypt-keeper/reasonscape?tab=readme-ov-file#system-architecture)",
          "author_fullname": "t2_30i1a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I Generated 1 Billion Tokens (So You Don't Have To): Introducing ReasonScape",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4ahuh87m4ggf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e9b289044767660d3c8e2d034ccd1c1b1902f538"
                },
                {
                  "y": 125,
                  "x": 216,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e8a1fc6a4649edbc5bfde934cedfd13c3fce90c"
                },
                {
                  "y": 186,
                  "x": 320,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c9c7d1e879f15dbfdea396948cc5826d3a32b67"
                },
                {
                  "y": 372,
                  "x": 640,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4a45011cfb5e75257e82a1018ba38ca3849d833"
                },
                {
                  "y": 558,
                  "x": 960,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e3046d2f091a49910ecb192cd251d2fa91bbf04a"
                },
                {
                  "y": 628,
                  "x": 1080,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=053460c5647cc12262626da5af12623413fee0ff"
                }
              ],
              "s": {
                "y": 717,
                "x": 1233,
                "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=1233&amp;format=png&amp;auto=webp&amp;s=8f6e962cdc029ce01dbca46346ec3fda47a06d7d"
              },
              "id": "4ahuh87m4ggf1"
            },
            "rn7r2k3t4ggf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 73,
                  "x": 108,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d505276a5f0dc04e59903c6abc89821d0f8f99b"
                },
                {
                  "y": 146,
                  "x": 216,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d747b1a0f8fd1a1097161d95e21ccf0503560deb"
                },
                {
                  "y": 216,
                  "x": 320,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9a6dd29394e5fb1992ae084b12e8749a9d84824"
                },
                {
                  "y": 433,
                  "x": 640,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=720bb40aec42b94acce8d40bc234aa44e5b4c208"
                },
                {
                  "y": 650,
                  "x": 960,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e355ab07ff4b2410135911c2070a7cb42b1b8221"
                },
                {
                  "y": 732,
                  "x": 1080,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=668705727d06f0b0d4c5c5867747868786f17635"
                }
              ],
              "s": {
                "y": 812,
                "x": 1198,
                "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=1198&amp;format=png&amp;auto=webp&amp;s=52d054e40f6f292b07b9d638d82244e8f302ce1d"
              },
              "id": "rn7r2k3t4ggf1"
            },
            "vsoidu4e4ggf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=871d7d827171f900b2598bcca602b93414c369f8"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a4c54e76149d09d301df5cbd0f5c388e82bb54d"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=317f1298cb62e84fc0ac688a8a4b2143d1d2fc8a"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=769ba43ab175981d7853c0c0620df46e4f20be04"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e16a01628210ccf57e6c94fa2d7d58110870a99"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49835bd796714f4bcb033b2b80c71bc4b6e37e82"
                }
              ],
              "s": {
                "y": 3150,
                "x": 1280,
                "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=d29809860b081384d998a428bc75faeba16cedc1"
              },
              "id": "vsoidu4e4ggf1"
            }
          },
          "name": "t3_1mf3nw4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": "#c7b594",
          "subreddit_type": "public",
          "ups": 73,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 73,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/-4v8QT3_SA4NBTuWsWHLP1NxBvsUSLBCUXILi1-L8H8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754071528,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever spent weeks building the perfect LLM benchmark only to watch it crumble within a few months?&lt;/p&gt;\n\n&lt;p&gt;Clean problems, elegant difficulty curves, proper statistical controls. New model drops. Perfect scores across the board. Your tests got trained on. Weeks of work, completely worthless.&lt;/p&gt;\n\n&lt;p&gt;So you pivot. Make the tests harder, more complex, more creative. Models improve with time. Now everyone clusters at 90-95%. 8B models are defeating it. Your benchmark has become a participation trophy. This happened to my previous evaluation, &lt;em&gt;Can-Ai-Code&lt;/em&gt;, twice.&lt;/p&gt;\n\n&lt;p&gt;Fine, you say. Random test generation it is! No more memorization, no more clustering. But congratulations, you&amp;#39;ve just unlocked new nightmares: Did you accidentally make your &amp;quot;hard&amp;quot; tests easier than your &amp;quot;easy&amp;quot; ones? Is your random number generator secretly biased? How do you even validate that hundreds of thousands of randomly generated problems &amp;quot;make sense&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;You solve that with clever statistical rigor, only to discover configuration explosion hell. You&amp;#39;d like to test different prompting templates and sampling parameters, but that&amp;#39;s 5 templates times 5 samplers times 50 million tokens (a conservative estimate) equals 1.25 billion tokens per model. Your GPUs scream in horror.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;re now burning millions of tokens achieving 0.005 confidence intervals on trivial problems while critical hard points sit at 0.02 intervals begging for attention like abandoned puppies. Dynamic sampling helps - generate more tests for uncertain points, fewer for confident ones - but how to avoid p-hacking yourself?&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s when the guessing realization hits. This binary classifier task scored 60%! Amazing! Wait... that&amp;#39;s only 20% above random chance. Your &amp;quot;75% accurate&amp;quot; multiple choice task is actually 50% accurate when you subtract lucky guesses. Everything is statistical lies. How are you supposed to compare models across boolean, multiple-choice and write-in answer tasks that have fundamentally different &amp;quot;guess rates&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;Finally, truncation waste arrives to complete your suffering: Model given tough task hits context limits, burns 8,000 tokens, returns a loop of gibberish. You sample 10x more to maintain statistical power. That&amp;#39;s 80K tokens wasted for one data point but with no useful answers.  You&amp;#39;re overflowing your KV caches while the confidence intervals laugh at you.&lt;/p&gt;\n\n&lt;p&gt;After drowning in this cascade of pain for months, I did what any reasonable person would do: I built an evaluation system to solve every single practical problem I encountered.&lt;/p&gt;\n\n&lt;h1&gt;ReasonScape treats language models as information processing systems, not text completion black boxes.&lt;/h1&gt;\n\n&lt;p&gt;It generates infinite, parametric, tokenization-aware test variations, applies statistical corrections for guessing, dynamically allocates sampling based on uncertainty, handles truncations intelligently, and visualizes the results as both enhanced leaderboards and explorable 3D cognitive landscapes.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vsoidu4e4ggf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d29809860b081384d998a428bc75faeba16cedc1\"&gt;C2: All Models x All Tasks Surface Comparison. Green Sphere indicates high-success. Red Square indicates high-truncation.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The initial C2 dataset represents ~1 billion tokens across 9 models, revealing exactly where, how and why reasoning breaks down across 4 task domains. The interactive leaderboard shows not just scores but confidence intervals, token usage and failure modes. The explorer (links at the bottom of post) lets you navigate difficulty manifolds like some kind of LLM reasoning archaeologist, digging into spectral analysis and completion token patterns.  Make sure you&amp;#39;re on a PC - this application has too much going on to be mobile friendly!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4ahuh87m4ggf1.png?width=1233&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f6e962cdc029ce01dbca46346ec3fda47a06d7d\"&gt;C2 Explorer&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I built the system with progressive evaluation in mind so you can start with rapid exploration then scale to deep precision. Everything caches, everything reproduces, everything scales. ReasonScape isn&amp;#39;t just another benchmark. It&amp;#39;s a complete methodology: toolkit, evaluation framework, and growing dataset family rolled into one.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rn7r2k3t4ggf1.png?width=1198&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52d054e40f6f292b07b9d638d82244e8f302ce1d\"&gt;C2 Leaderboard (Static snapshot - the Interactive is much nicer!)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The ReasonScape experiments and the resulting datasets will grow, expand and evolve - when scores get too high we will move the difficulty grids to make the tests harder and move on to C3. I have &lt;strong&gt;8 additional tasks&lt;/strong&gt; to bring up, and lots more reasoning models I&amp;#39;d like to evaluate but my 2xRTX3090 only have so much to give.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading this far! &amp;lt;3&lt;/p&gt;\n\n&lt;p&gt;Links:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://reasonscape.com/\"&gt;ReasonScape Homepage&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://reasonscape.com/c2/leaderboard\"&gt;ReasonScape Leaderboard - C2&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://reasonscape.com/c2/explorer\"&gt;ReasonScape Explorer - C2&lt;/a&gt; (note: PC required, not mobile-friendly)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/the-crypt-keeper/reasonscape\"&gt;ReasonScape GitHub&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/the-crypt-keeper/reasonscape?tab=readme-ov-file#system-architecture\"&gt;ReasonScape System Architecture&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf3nw4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kryptkpr",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mf3nw4/i_generated_1_billion_tokens_so_you_dont_have_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf3nw4/i_generated_1_billion_tokens_so_you_dont_have_to/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754071528,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_48ezkeai",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OpenAI OS model info leaked - 120B &amp; 20B will be available",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mepz8z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 438,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 438,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/koWApmUvRcfMpfhrqJD5WrepTEKpDhchruNu54mqVSQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754033016,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/08m94pio0dgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?auto=webp&amp;s=6fb8f07eaa524714454f02be25da5a0c8bd501ec",
                  "width": 1052,
                  "height": 1588
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=57a6c0b7f95c81ab1bf5553bdcd58df7e2e53602",
                    "width": 108,
                    "height": 163
                  },
                  {
                    "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=846a6af2093db0113d0ac29622308c99f3409503",
                    "width": 216,
                    "height": 326
                  },
                  {
                    "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a6414cea64697cd84845fbfa7d319abc63ee4be",
                    "width": 320,
                    "height": 483
                  },
                  {
                    "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c8e423b8c1c16726ef958bbd8725e985cc58bc68",
                    "width": 640,
                    "height": 966
                  },
                  {
                    "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0995094fc1bd94332893e4e6b74528bc339b0123",
                    "width": 960,
                    "height": 1449
                  }
                ],
                "variants": {},
                "id": "IacMR9LsboNRW0VP4Zwz_MvZddRdCNqWrVrUgsEhFxY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mepz8z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ShreckAndDonkey123",
          "discussion_type": null,
          "num_comments": 138,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mepz8z/openai_os_model_info_leaked_120b_20b_will_be/",
          "stickied": false,
          "url": "https://i.redd.it/08m94pio0dgf1.jpeg",
          "subreddit_subscribers": 508541,
          "created_utc": 1754033016,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tried running qwen3-coder in Claude Code. It constantly failed tool calls. I tried both the cerebras api and the official alibaba api.\n\nI also tried glm-4.5 in Claude Code and it was surprisingly good. Asked both Gemini cli and glm-4.5 in Claude Code to make the snake game and tetris in html and the games made ny glm were much better looking than gemini. Since Gemini is #1 right now on Web Arena, I suspect glm will be #1 when it's on the leaderboard. Glm was also much better at tool calls, it basically never failed.",
          "author_fullname": "t2_58t8ty6v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is bad at tool call while glm-4.5 is surprisingly good",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf8la7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 25,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754083147,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried running qwen3-coder in Claude Code. It constantly failed tool calls. I tried both the cerebras api and the official alibaba api.&lt;/p&gt;\n\n&lt;p&gt;I also tried glm-4.5 in Claude Code and it was surprisingly good. Asked both Gemini cli and glm-4.5 in Claude Code to make the snake game and tetris in html and the games made ny glm were much better looking than gemini. Since Gemini is #1 right now on Web Arena, I suspect glm will be #1 when it&amp;#39;s on the leaderboard. Glm was also much better at tool calls, it basically never failed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mf8la7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BoJackHorseMan53",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf8la7/qwen3coder_is_bad_at_tool_call_while_glm45_is/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf8la7/qwen3coder_is_bad_at_tool_call_while_glm45_is/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754083147,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been having a lot of fun playing around with the new Qwen coder as a 100% local agentic coding. A lot of going on with in the demo above: \n\n- Roo Code with [Unsloth Qwen3 Coder 30B Q8](https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF)\n- [llama-swap](https://github.com/mostlygeek/llama-swap) with new Activity page with real time updates. \n- [VibeCities MCP server](https://github.com/mostlygeek/vibecities) for hosting the pages\n- Dual 3090s with Q8 gives about 50 tok/sec to 55 tok/sec. The UD Q4_K_XL quant was not able to one shot the spinning pentagon. \n\nHere's my llama-swap config: \n\n```\nmacros:\n  \"qwen3-coder-server\": |\n    /path/to/llama-server/llama-server-latest\n    --host 127.0.0.1 --port ${PORT}\n    --flash-attn -ngl 999 -ngld 999\n    --no-mmap\n    --cache-type-k q8_0 --cache-type-v q8_0\n    --temp 0.7 --top-k 20 --top-p 0.8 --repeat_penalty 1.05\n    --jinja\n    --swa-full\n\nmodels:\n  \"Q3-30B-CODER-3090\":\n    env:\n      - \"CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10\"\n    name: \"Qwen3 30B Coder Dual 3090 (Q3-30B-CODER-3090)\"\n    description: \"Q8_K_XL, 180K context, 2x3090\"\n    filters:\n      # enforce recommended params for model\n      strip_params: \"temperature, top_k, top_p, repeat_penalty\"\n    cmd: |\n      ${qwen3-coder-server}\n      --model /path/to/models/Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf\n      --ctx-size 184320\n      # rebalance layers/context a bit better across dual GPUs\n      --tensor-split 46,54\n```\n\nRoo code MCP settings: \n\n```\n{\n  \"mcpServers\": {\n    \"vibecities\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"http://10.0.1.173:8888/mcp\",\n      \"headers\": {\n        \"X-API-Key\": \"your-secure-api-key\"\n      },\n      \"alwaysAllow\": [\n        \"page_list\",\n        \"page_set\",\n        \"page_get\"\n      ],\n      \"disabled\": false\n    }\n  }\n}\n```\n\n\n\n",
          "author_fullname": "t2_11gh93nhos",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "All local Roo Code and qwen3 coder 30B Q8",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfariy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bbbdbf",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g5aj1csfjhgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g5aj1csfjhgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g5aj1csfjhgf1/DASHPlaylist.mpd?a=1756688985%2CNDIxODYyMDMxYWMyYTczMGIzZjg4MjZkZmQ1NzI1OTY3NDBjNTQ5NmFiMTBjZWU1YjFhYmQ1M2JhMzVlM2VkMA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 158,
              "hls_url": "https://v.redd.it/g5aj1csfjhgf1/HLSPlaylist.m3u8?a=1756688985%2CMDY0MGQ4YjNhZmVhNjc5NWZlNDc5MWQ5Njc5ZDBhYjViNjVlOWRjNTkyYjZjZDY4NDA5OGUwYTk4Y2YyODJhNw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=140&amp;height=87&amp;crop=140:87,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=96e2057bb1f1ef1e9f3beb1d9a28a9ccd4dcaa6b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754088672,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been having a lot of fun playing around with the new Qwen coder as a 100% local agentic coding. A lot of going on with in the demo above: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Roo Code with &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF\"&gt;Unsloth Qwen3 Coder 30B Q8&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/mostlygeek/llama-swap\"&gt;llama-swap&lt;/a&gt; with new Activity page with real time updates. &lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/mostlygeek/vibecities\"&gt;VibeCities MCP server&lt;/a&gt; for hosting the pages&lt;/li&gt;\n&lt;li&gt;Dual 3090s with Q8 gives about 50 tok/sec to 55 tok/sec. The UD Q4_K_XL quant was not able to one shot the spinning pentagon. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here&amp;#39;s my llama-swap config: &lt;/p&gt;\n\n&lt;p&gt;```\nmacros:\n  &amp;quot;qwen3-coder-server&amp;quot;: |\n    /path/to/llama-server/llama-server-latest\n    --host 127.0.0.1 --port ${PORT}\n    --flash-attn -ngl 999 -ngld 999\n    --no-mmap\n    --cache-type-k q8_0 --cache-type-v q8_0\n    --temp 0.7 --top-k 20 --top-p 0.8 --repeat_penalty 1.05\n    --jinja\n    --swa-full&lt;/p&gt;\n\n&lt;p&gt;models:\n  &amp;quot;Q3-30B-CODER-3090&amp;quot;:\n    env:\n      - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10&amp;quot;\n    name: &amp;quot;Qwen3 30B Coder Dual 3090 (Q3-30B-CODER-3090)&amp;quot;\n    description: &amp;quot;Q8_K_XL, 180K context, 2x3090&amp;quot;\n    filters:\n      # enforce recommended params for model\n      strip_params: &amp;quot;temperature, top_k, top_p, repeat_penalty&amp;quot;\n    cmd: |\n      ${qwen3-coder-server}\n      --model /path/to/models/Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf\n      --ctx-size 184320\n      # rebalance layers/context a bit better across dual GPUs\n      --tensor-split 46,54\n```&lt;/p&gt;\n\n&lt;p&gt;Roo code MCP settings: &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n{\n  &amp;quot;mcpServers&amp;quot;: {\n    &amp;quot;vibecities&amp;quot;: {\n      &amp;quot;type&amp;quot;: &amp;quot;streamable-http&amp;quot;,\n      &amp;quot;url&amp;quot;: &amp;quot;http://10.0.1.173:8888/mcp&amp;quot;,\n      &amp;quot;headers&amp;quot;: {\n        &amp;quot;X-API-Key&amp;quot;: &amp;quot;your-secure-api-key&amp;quot;\n      },\n      &amp;quot;alwaysAllow&amp;quot;: [\n        &amp;quot;page_list&amp;quot;,\n        &amp;quot;page_set&amp;quot;,\n        &amp;quot;page_get&amp;quot;\n      ],\n      &amp;quot;disabled&amp;quot;: false\n    }\n  }\n}\n&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/g5aj1csfjhgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?format=pjpg&amp;auto=webp&amp;s=a799694f3cd2a8d09be3eac7cc9981be88d234a1",
                  "width": 1920,
                  "height": 1197
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7b56a58c7f6f027ee7357cad95a460ff999afeea",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5adfd5aba83e12c59bf4648d145f4ab40fd5648e",
                    "width": 216,
                    "height": 134
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4be2e0e3ac56fa7c11be8c2c58c9a02a90039429",
                    "width": 320,
                    "height": 199
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c11415fad487d232adecf2767cc5b4b8ac2ab42f",
                    "width": 640,
                    "height": 399
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fd1f305092aa1774fdb5bb8d64c33f4d4acc5781",
                    "width": 960,
                    "height": 598
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a0c4665784dbdfe56aa44706a6b06cd7fb427df9",
                    "width": 1080,
                    "height": 673
                  }
                ],
                "variants": {},
                "id": "OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfariy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Statement-0001",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfariy/all_local_roo_code_and_qwen3_coder_30b_q8/",
          "stickied": false,
          "url": "https://v.redd.it/g5aj1csfjhgf1",
          "subreddit_subscribers": 508541,
          "created_utc": 1754088672,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g5aj1csfjhgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g5aj1csfjhgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g5aj1csfjhgf1/DASHPlaylist.mpd?a=1756688985%2CNDIxODYyMDMxYWMyYTczMGIzZjg4MjZkZmQ1NzI1OTY3NDBjNTQ5NmFiMTBjZWU1YjFhYmQ1M2JhMzVlM2VkMA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 158,
              "hls_url": "https://v.redd.it/g5aj1csfjhgf1/HLSPlaylist.m3u8?a=1756688985%2CMDY0MGQ4YjNhZmVhNjc5NWZlNDc5MWQ5Njc5ZDBhYjViNjVlOWRjNTkyYjZjZDY4NDA5OGUwYTk4Y2YyODJhNw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Disclaimer:\n\nNo actual plushy pandas were hurt in the process of trying and failing to fit in a plastic box...",
          "author_fullname": "t2_qz1qjc86",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Me lately... Anyone else can relate? 😎",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 79,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf4ihq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3BSxzZpbbsOE49m6Gzlq2vc_AKs8no1XsGYN2LTJOrg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754073451,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Disclaimer:&lt;/p&gt;\n\n&lt;p&gt;No actual plushy pandas were hurt in the process of trying and failing to fit in a plastic box...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/rqzixk49cggf1.gif",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/rqzixk49cggf1.gif?format=png8&amp;s=b762d39ddcffd472526fa05b0d769adcc9064fe7",
                  "width": 800,
                  "height": 453
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=4f274e8ff485a4989b9d127943de26befcdfb05d",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=0965b1a108af3d1709115e6fb86455125fa468ea",
                    "width": 216,
                    "height": 122
                  },
                  {
                    "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=beb312a49051ec985b83d4c6031aee1db97d0956",
                    "width": 320,
                    "height": 181
                  },
                  {
                    "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=8163f8e2cdf42526b89cbf134b8c0bbe2c377aa2",
                    "width": 640,
                    "height": 362
                  }
                ],
                "variants": {
                  "gif": {
                    "source": {
                      "url": "https://preview.redd.it/rqzixk49cggf1.gif?s=e2f65b8b2302c9de598fe0355ffa07608973a7b7",
                      "width": 800,
                      "height": 453
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=108&amp;crop=smart&amp;s=f83c49f5a04ebce5a16c8b856d87f5215d57d86d",
                        "width": 108,
                        "height": 61
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=216&amp;crop=smart&amp;s=41ec44955588452eaeb561d8f4d6b51bdafdc817",
                        "width": 216,
                        "height": 122
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=320&amp;crop=smart&amp;s=507d4f46da1e538fa1fe51f68da80696036c09aa",
                        "width": 320,
                        "height": 181
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=640&amp;crop=smart&amp;s=3d8b06b0091af494d702ac39636bf603e600b301",
                        "width": 640,
                        "height": 362
                      }
                    ]
                  },
                  "mp4": {
                    "source": {
                      "url": "https://preview.redd.it/rqzixk49cggf1.gif?format=mp4&amp;s=7f326be51b3cb7f3e2485d49898a5015d2bb1140",
                      "width": 800,
                      "height": 453
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=108&amp;format=mp4&amp;s=40d2c981c71f4d6d8f0c9a6bc711893b6edbebdc",
                        "width": 108,
                        "height": 61
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=216&amp;format=mp4&amp;s=585d088b1216ab7ef3b8d529198e1dc2849966e9",
                        "width": 216,
                        "height": 122
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=320&amp;format=mp4&amp;s=c445476bc95afa7c794b1c3af79421dcb8a4f570",
                        "width": 320,
                        "height": 181
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=640&amp;format=mp4&amp;s=c9b96f95344d2207fe0fd0ea23af8ea9f3f2e686",
                        "width": 640,
                        "height": 362
                      }
                    ]
                  }
                },
                "id": "A5U9B6UgGQoc_r5iq28_9pm0dir0rIEwOZlniIEu3DQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mf4ihq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Cool-Chemical-5629",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf4ihq/me_lately_anyone_else_can_relate/",
          "stickied": false,
          "url": "https://i.redd.it/rqzixk49cggf1.gif",
          "subreddit_subscribers": 508541,
          "created_utc": 1754073451,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "GPU snapshotting is finally a thing! NVIDIA recently released their [CUDA checkpoint/restore API](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CHECKPOINT.html) and we at Modal (serverless compute platform) are using it drastically reduce GPU cold start times. This is especially relevant for serving large models, where it can take minutes (for the heftiest LLMs) to move model weights from disk to memory.\n\nGPU memory snapshotting can reduce cold boot times by up to 12x. It lets you scale GPU resources up and down based on demand without compromising on user-facing latency. Below are some benchmarking results showing improvements for various models!\n\nhttps://preview.redd.it/opb5odlb2hgf1.png?width=3162&amp;format=png&amp;auto=webp&amp;s=00995e770fa4d3ac454bd9b1f0df5296391fb137\n\nMore on how GPU snapshotting works plus additional benchmarks in this blog post: [https://modal.com/blog/gpu-mem-snapshots](https://modal.com/blog/gpu-mem-snapshots)",
          "author_fullname": "t2_9av3t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cold start vLLM in 5 seconds with GPU snapshotting",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "opb5odlb2hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 54,
                  "x": 108,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1a99ec3a648e3acbc71ef3233e807e5fbfc20dd"
                },
                {
                  "y": 108,
                  "x": 216,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a87d457cf006589e58141d5831e05fc39cc999c4"
                },
                {
                  "y": 161,
                  "x": 320,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=156b5387ff8776f0f27387621395be6a4f1bb15b"
                },
                {
                  "y": 322,
                  "x": 640,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c498b446dda08ce80241842d3232b30cf40b31ec"
                },
                {
                  "y": 483,
                  "x": 960,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eae84d78449fb87be8ed155a688b75c0cb5ea458"
                },
                {
                  "y": 543,
                  "x": 1080,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=505dd6a12adbdbe687a6467d0b0843adb780d82a"
                }
              ],
              "s": {
                "y": 1592,
                "x": 3162,
                "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=3162&amp;format=png&amp;auto=webp&amp;s=00995e770fa4d3ac454bd9b1f0df5296391fb137"
              },
              "id": "opb5odlb2hgf1"
            }
          },
          "name": "t3_1mf86rn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/9zGmAn5wtWE9ciYDNGxrwYmoTgYDYEOmBf7p4EOrZDY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754082162,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;GPU snapshotting is finally a thing! NVIDIA recently released their &lt;a href=\"https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CHECKPOINT.html\"&gt;CUDA checkpoint/restore API&lt;/a&gt; and we at Modal (serverless compute platform) are using it drastically reduce GPU cold start times. This is especially relevant for serving large models, where it can take minutes (for the heftiest LLMs) to move model weights from disk to memory.&lt;/p&gt;\n\n&lt;p&gt;GPU memory snapshotting can reduce cold boot times by up to 12x. It lets you scale GPU resources up and down based on demand without compromising on user-facing latency. Below are some benchmarking results showing improvements for various models!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/opb5odlb2hgf1.png?width=3162&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00995e770fa4d3ac454bd9b1f0df5296391fb137\"&gt;https://preview.redd.it/opb5odlb2hgf1.png?width=3162&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00995e770fa4d3ac454bd9b1f0df5296391fb137&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;More on how GPU snapshotting works plus additional benchmarks in this blog post: &lt;a href=\"https://modal.com/blog/gpu-mem-snapshots\"&gt;https://modal.com/blog/gpu-mem-snapshots&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf86rn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "crookedstairs",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf86rn/cold_start_vllm_in_5_seconds_with_gpu_snapshotting/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf86rn/cold_start_vllm_in_5_seconds_with_gpu_snapshotting/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754082162,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What an awesome model. Everything I throw at it I get comparable results to Gemma 3, but 4.5x faster.\n\nGreat at general knowledge, but also follows instructions very well.\n\nPlease let me know your experiences with it!",
          "author_fullname": "t2_d2gb9jhgg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 30b a3b 2507 instruct as good as Gemma 3 27B!?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf0i54",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754064349,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What an awesome model. Everything I throw at it I get comparable results to Gemma 3, but 4.5x faster.&lt;/p&gt;\n\n&lt;p&gt;Great at general knowledge, but also follows instructions very well.&lt;/p&gt;\n\n&lt;p&gt;Please let me know your experiences with it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mf0i54",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hanthunius",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf0i54/qwen_30b_a3b_2507_instruct_as_good_as_gemma_3_27b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf0i54/qwen_30b_a3b_2507_instruct_as_good_as_gemma_3_27b/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754064349,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Mentioned in the new, Qwen3 30B download announcement was that 480B's tool calling was fixed and it [needed to be re-downloaded](https://www.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/#:~:text=We%20also%20fixed%20tool%20calling%20for%20the%20480B%20and%20this%20model%20and%20fixed%2030B%20thinking%2C%20so%20please%20redownload%20the%20first%20shard)\n\nI'm just posting it so that no one misses it. I'm using LMStudio and it just showed as \"downloaded\". It didn't seem to know there was a change.\n\nEDIT: Yes, this only refers to the unsloth versions of 480B.  Thank you u/MikeRoz",
          "author_fullname": "t2_taorh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Heads up to those that downloaded Qwen3 Coder 480B before yesterday",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mexa2g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 58,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 58,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754059918,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754056847,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Mentioned in the new, Qwen3 30B download announcement was that 480B&amp;#39;s tool calling was fixed and it &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/#:%7E:text=We%20also%20fixed%20tool%20calling%20for%20the%20480B%20and%20this%20model%20and%20fixed%2030B%20thinking%2C%20so%20please%20redownload%20the%20first%20shard\"&gt;needed to be re-downloaded&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just posting it so that no one misses it. I&amp;#39;m using LMStudio and it just showed as &amp;quot;downloaded&amp;quot;. It didn&amp;#39;t seem to know there was a change.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Yes, this only refers to the unsloth versions of 480B.  Thank you &lt;a href=\"/u/MikeRoz\"&gt;u/MikeRoz&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mexa2g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VegetaTheGrump",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mexa2g/heads_up_to_those_that_downloaded_qwen3_coder/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mexa2g/heads_up_to_those_that_downloaded_qwen3_coder/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754056847,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In the source code, we see a link to Hunyuan-4B-Instruct, but I think we’ll see much larger models :)\n\nbonus: fix hunyuan\\_moe chat template",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "support for the upcoming hunyuan dense models has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf0hou",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": "#bbbdbf",
          "ups": 36,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 36,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=40b87c745f679bca8fe0aa91a0c19c36c7177277",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754064323,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the source code, we see a link to Hunyuan-4B-Instruct, but I think we’ll see much larger models :)&lt;/p&gt;\n\n&lt;p&gt;bonus: fix hunyuan_moe chat template&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14878",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?auto=webp&amp;s=7d88566cdde4131f4d8262c975dfd9b55caf89f5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed015fcc9ee802baeb72ee117bf3077725576fed",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b29546e91bdc6de331482f1a9525a141f86b6852",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9a5913f23624227672a7032140b173c0f2635ae9",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f81f256091726e20730924e97a225729e6c971ec",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7d6c5bdd7a760c5bf13d60c0766042314f38e1c",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e0455df24a27f9bce1dd5b8080cbffc5f292d90",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mf0hou",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mf0hou/support_for_the_upcoming_hunyuan_dense_models_has/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14878",
          "subreddit_subscribers": 508541,
          "created_utc": 1754064323,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I allocated more RAM and took the guard rail off. when loading the model the Activity monitor showed a brief red memory warning for 2-3 seconds but loads fine. The is 4bit version.Runs around 25-27 tokens/sec.When running inference memory pressure intermittently increases and it does use swap memory a around 1-12 GB in my case, but never showed red warning after loading it in memory.",
          "author_fullname": "t2_1nsamx8udx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5-Air running on 64GB Mac Studio(M4)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mesi2s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 92,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 92,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/xg-TV-u5t8CZYga5vqS7H6cvSQQNm6mI1iKN_y1d0Xg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754042719,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I allocated more RAM and took the guard rail off. when loading the model the Activity monitor showed a brief red memory warning for 2-3 seconds but loads fine. The is 4bit version.Runs around 25-27 tokens/sec.When running inference memory pressure intermittently increases and it does use swap memory a around 1-12 GB in my case, but never showed red warning after loading it in memory.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/87ng5bmisdgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/87ng5bmisdgf1.png?auto=webp&amp;s=278b1e2b20f96d9793b766eaf9a4662e0c4177fa",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e2b027c72800b8bb18356e66239bf4a6fe201ecf",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d6e4b8a9f3ad7ba583fe5cae8620c382e5db0be",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=136800587978cd65bc334b35b13dfe4c311ab4c6",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=54f42f44d09cb4df95a9f6ed8ad3cf70c2cc96bf",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8cde2cdaafe0d7e74429488394a890a17630d1a5",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a7ec880558fbdfda721507e10398b64ecc96a673",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "n4NTI1RG1xenOb1HsAwQ1LQYB7_slm74LFJZKuG7hJQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mesi2s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "riwritingreddit",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mesi2s/glm45air_running_on_64gb_mac_studiom4/",
          "stickied": false,
          "url": "https://i.redd.it/87ng5bmisdgf1.png",
          "subreddit_subscribers": 508541,
          "created_utc": 1754042719,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_j2lm1hb2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SVDQuant does INT4 quantization of text-to-image models without losing quality. Can't the same technique be used in LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 90,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf08e5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 33,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 33,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/N50eG8wUbPgLuCMeBevOa0eyQWBygGeeMCJcikzOQiM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754063762,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/0cq321qc1fgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?auto=webp&amp;s=55a8648bf4010ec76dde9382784506ff1f279b6b",
                  "width": 3293,
                  "height": 2120
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9b9b51c5d0f9c3b9de0cbaa0cdbb1c69e4ee263",
                    "width": 108,
                    "height": 69
                  },
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8ffc5eae3e6f9fd9ac0ac50aea60adf88a4c90f0",
                    "width": 216,
                    "height": 139
                  },
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6e8b0e358cd85ccb5a6738bd9900251292d4eb9",
                    "width": 320,
                    "height": 206
                  },
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=162d6f061d05ab906b370e0b7ce08f4a7f85014d",
                    "width": 640,
                    "height": 412
                  },
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=20c332654151a9cd9bcbb7f5f225cbb367a05536",
                    "width": 960,
                    "height": 618
                  },
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5661bfba947547555164c9f84545ce932ae65377",
                    "width": 1080,
                    "height": 695
                  }
                ],
                "variants": {},
                "id": "D8iRezvmv7BRzfgNCmicLKUlcwGdQOUpQHIUZAPRPow"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf08e5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "we_are_mammals",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf08e5/svdquant_does_int4_quantization_of_texttoimage/",
          "stickied": false,
          "url": "https://i.redd.it/0cq321qc1fgf1.jpeg",
          "subreddit_subscribers": 508541,
          "created_utc": 1754063762,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sharing **DocStrange**, an open-source Python library that makes document data extraction easy.\n\n* **Universal Input**: PDFs, Images, Word docs, PowerPoint, Excel\n* **Multiple Outputs**: Clean Markdown, structured JSON, CSV tables, formatted HTML\n* **Smart Extraction**: Specify exact fields you want (e.g., \"invoice\\_number\", \"total\\_amount\")\n* **Schema Support**: Define JSON schemas for consistent structured output\n\n**Quick start:**\n\n    from docstrange import DocumentExtractor\n    \n    extractor = DocumentExtractor()\n    result = extractor.extract(\"research_paper.pdf\")\n    \n    # Get clean markdown for LLM training\n    markdown = result.extract_markdown()\n\n**CLI**\n\n    pip install docstrange\n    docstrange document.pdf --output json --extract-fields title author date\n\n**Data Processing Options**\n\n* **Cloud Mode**: Fast and free processing with minimal setup\n* **Local Mode**: Complete privacy - all processing happens on your machine, no data sent anywhere, works on both cpu and gpu\n\n**Links:**\n\n* PyPI: [https://pypi.org/project/docstrange/](https://pypi.org/project/docstrange/)",
          "author_fullname": "t2_1mv39a664r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DocStrange - Open Source Document Data Extractor",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 88,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mepr38",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 153,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 153,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/n_8zXMrMBAhovAnFfx7BottIYrMkhK42uDJfBp4i85Y.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754032135,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sharing &lt;strong&gt;DocStrange&lt;/strong&gt;, an open-source Python library that makes document data extraction easy.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Universal Input&lt;/strong&gt;: PDFs, Images, Word docs, PowerPoint, Excel&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Multiple Outputs&lt;/strong&gt;: Clean Markdown, structured JSON, CSV tables, formatted HTML&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Smart Extraction&lt;/strong&gt;: Specify exact fields you want (e.g., &amp;quot;invoice_number&amp;quot;, &amp;quot;total_amount&amp;quot;)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Schema Support&lt;/strong&gt;: Define JSON schemas for consistent structured output&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Quick start:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from docstrange import DocumentExtractor\n\nextractor = DocumentExtractor()\nresult = extractor.extract(&amp;quot;research_paper.pdf&amp;quot;)\n\n# Get clean markdown for LLM training\nmarkdown = result.extract_markdown()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;CLI&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pip install docstrange\ndocstrange document.pdf --output json --extract-fields title author date\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Data Processing Options&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Cloud Mode&lt;/strong&gt;: Fast and free processing with minimal setup&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Local Mode&lt;/strong&gt;: Complete privacy - all processing happens on your machine, no data sent anywhere, works on both cpu and gpu&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;PyPI: &lt;a href=\"https://pypi.org/project/docstrange/\"&gt;https://pypi.org/project/docstrange/&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/vghke2r1ycgf1.gif",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/vghke2r1ycgf1.gif?format=png8&amp;s=0646017ef4eefc01cddecd722d9f8f2d50380882",
                  "width": 1138,
                  "height": 717
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=fc63503d84bbe4a224856c2516262be363da4ac8",
                    "width": 108,
                    "height": 68
                  },
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=3c6a452195c5261d81d558756ca975edc42b2fae",
                    "width": 216,
                    "height": 136
                  },
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=5175141f8d36c2b53aecbc5cb9f68c8f3fdc88ef",
                    "width": 320,
                    "height": 201
                  },
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=b028677410eada3f069136c8e6c1b8c48c06f285",
                    "width": 640,
                    "height": 403
                  },
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=83ef66bfaa9d844d3079c6f8d7f7ec2c933079bb",
                    "width": 960,
                    "height": 604
                  },
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=e9c266f359e28d38390413a4c7f732940b9da31e",
                    "width": 1080,
                    "height": 680
                  }
                ],
                "variants": {
                  "gif": {
                    "source": {
                      "url": "https://preview.redd.it/vghke2r1ycgf1.gif?s=5ddee0d3d4b59db883ad7324e63d6d44c4c6edc1",
                      "width": 1138,
                      "height": 717
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=108&amp;crop=smart&amp;s=f965200e3ad373df4b75762cdb1f8901fa898b66",
                        "width": 108,
                        "height": 68
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=216&amp;crop=smart&amp;s=2ede88d7ce8de83577c6d4caba7ac71eadd2671c",
                        "width": 216,
                        "height": 136
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=320&amp;crop=smart&amp;s=85aaad5e77f8fa02ca6dcd40563302da8c506a58",
                        "width": 320,
                        "height": 201
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=640&amp;crop=smart&amp;s=12643bc505cd05a85286b55a7fff556b82b4872a",
                        "width": 640,
                        "height": 403
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=960&amp;crop=smart&amp;s=0a82ceb7382adf023c53606bed72529202ab9f69",
                        "width": 960,
                        "height": 604
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=1080&amp;crop=smart&amp;s=2786689a41ab5170679f3efd992a64e464d981e2",
                        "width": 1080,
                        "height": 680
                      }
                    ]
                  },
                  "mp4": {
                    "source": {
                      "url": "https://preview.redd.it/vghke2r1ycgf1.gif?format=mp4&amp;s=7043fb757ff13b2cf394629a8043afef1a0a0995",
                      "width": 1138,
                      "height": 717
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=108&amp;format=mp4&amp;s=50bc5d05247661d61d18415388e20a7fd4866caf",
                        "width": 108,
                        "height": 68
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=216&amp;format=mp4&amp;s=d334100ea224720cfcaa53b23a6a1fd993d24945",
                        "width": 216,
                        "height": 136
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=320&amp;format=mp4&amp;s=f8eed5e239635d7c160661180140118e60510641",
                        "width": 320,
                        "height": 201
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=640&amp;format=mp4&amp;s=23de0d42f6d1959233bf55d4f8f865fb4f03146e",
                        "width": 640,
                        "height": 403
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=960&amp;format=mp4&amp;s=f9a9a9f47cf0d08c3603a0b57a5250634e9d42ea",
                        "width": 960,
                        "height": 604
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=1080&amp;format=mp4&amp;s=340f0a49b5bca4ea0f0ee8982400c578d1646297",
                        "width": 1080,
                        "height": 680
                      }
                    ]
                  }
                },
                "id": "rhbuyimmJ2s8b0MEkSp5-3yQopRWq3kKuTIAyO2Dmsk"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mepr38",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LostAmbassador6872",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/",
          "stickied": false,
          "url": "https://i.redd.it/vghke2r1ycgf1.gif",
          "subreddit_subscribers": 508541,
          "created_utc": 1754032135,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_xq83l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta - new openai open source model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mfda7s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=e6bc83d22983565c851331294c37a95f480af7fa",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754095791,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "openrouter.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://openrouter.ai/openrouter/horizon-beta",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?auto=webp&amp;s=76311bc0d854d91946fad4dd34c15d2aabd68203",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f17bb8ad3532cb9e5aee2735555aab1785143fb",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6fde130f24de4941de6382c0b47920888676cb02",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e2b3b64a4ebeaaef6a2c94effc7c44b3d4bf9e5",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d1e79479fdaa990ea889c0b392a6ab4a884ffc4",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5298d732c100973951f754378b36e657d827055b",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7f7a4a4538b90aad59b31dde3f164c87c0a08175",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mfda7s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "popsumbong",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfda7s/horizon_beta_new_openai_open_source_model/",
          "stickied": false,
          "url": "https://openrouter.ai/openrouter/horizon-beta",
          "subreddit_subscribers": 508541,
          "created_utc": 1754095791,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://github.com/electroglyph/quant_clone\n\nThis is a tiny little app which will create a llama-quantize command based on how a target GGUF is quantized. I wanted it so that I can quantize my finetunes the same way Unsloth does.\n\nFor instance, if you run quant_clone gemma-3-1b-it-UD-IQ1_S.gguf\n\nyou get:\n\nllama-quantize --imatrix &lt;imatrix_unsloth.dat&gt; --tensor-type token_embd.weight=Q5_1 --tensor-type blk.0.attn_k.weight=IQ4_NL --tensor-type blk.0.attn_output.weight=IQ2_XXS --tensor-type blk.0.attn_q.weight=IQ4_NL --tensor-type blk.0.attn_v.weight=Q5_0 --tensor-type blk.0.ffn_down.weight=IQ3_S --tensor-type blk.0.ffn_gate.weight=IQ4_NL --tensor-type blk.0.ffn_up.weight=IQ4_NL --tensor-type blk.1.attn_k.weight=IQ4_NL --tensor-type blk.1.attn_output.weight=IQ2_XXS --tensor-type blk.1.attn_q.weight=IQ4_NL --tensor-type blk.1.attn_v.weight=Q5_0 --tensor-type blk.1.ffn_down.weight=Q2_K --tensor-type blk.1.ffn_gate.weight=IQ4_NL --tensor-type blk.1.ffn_up.weight=IQ4_NL --tensor-type blk.2.attn_k.weight=IQ4_NL --tensor-type blk.2.attn_output.weight=IQ2_XXS --tensor-type blk.2.attn_q.weight=IQ4_NL --tensor-type blk.2.attn_v.weight=Q5_0 --tensor-type blk.2.ffn_down.weight=IQ3_S --tensor-type blk.2.ffn_gate.weight=IQ4_NL --tensor-type blk.2.ffn_up.weight=IQ4_NL --tensor-type blk.3.attn_k.weight=IQ4_NL --tensor-type blk.3.attn_output.weight=IQ2_XXS --tensor-type blk.3.attn_q.weight=IQ4_NL --tensor-type blk.3.attn_v.weight=Q5_0 --tensor-type blk.3.ffn_down.weight=IQ3_S --tensor-type blk.3.ffn_gate.weight=IQ4_NL --tensor-type blk.3.ffn_up.weight=IQ4_NL --tensor-type blk.4.attn_k.weight=IQ4_NL --tensor-type blk.4.attn_output.weight=IQ2_XXS --tensor-type blk.4.attn_q.weight=IQ4_NL --tensor-type blk.4.attn_v.weight=Q5_0 --tensor-type blk.4.ffn_down.weight=IQ3_S --tensor-type blk.4.ffn_gate.weight=IQ4_NL --tensor-type blk.4.ffn_up.weight=IQ4_NL --tensor-type blk.5.attn_k.weight=IQ4_NL --tensor-type blk.5.attn_output.weight=IQ2_XXS --tensor-type blk.5.attn_q.weight=IQ4_NL --tensor-type blk.5.attn_v.weight=Q5_0 --tensor-type blk.5.ffn_down.weight=IQ1_S --tensor-type blk.5.ffn_gate.weight=IQ4_NL --tensor-type blk.5.ffn_up.weight=IQ4_NL --tensor-type blk.6.attn_k.weight=IQ4_NL --tensor-type blk.6.attn_output.weight=IQ2_XXS --tensor-type blk.6.attn_q.weight=IQ4_NL --tensor-type blk.6.attn_v.weight=Q5_0 --tensor-type blk.6.ffn_down.weight=IQ1_S --tensor-type blk.6.ffn_gate.weight=IQ4_NL --tensor-type blk.6.ffn_up.weight=IQ4_NL --tensor-type blk.7.attn_k.weight=IQ4_NL --tensor-type blk.7.attn_output.weight=IQ2_XXS --tensor-type blk.7.attn_q.weight=IQ4_NL --tensor-type blk.7.attn_v.weight=Q5_0 --tensor-type blk.7.ffn_down.weight=IQ1_S --tensor-type blk.7.ffn_gate.weight=IQ4_NL --tensor-type blk.7.ffn_up.weight=IQ4_NL --tensor-type blk.8.attn_k.weight=IQ4_NL --tensor-type blk.8.attn_output.weight=IQ2_XXS --tensor-type blk.8.attn_q.weight=IQ4_NL --tensor-type blk.8.attn_v.weight=Q5_0 --tensor-type blk.8.ffn_down.weight=IQ1_S --tensor-type blk.8.ffn_gate.weight=IQ4_NL --tensor-type blk.8.ffn_up.weight=IQ4_NL --tensor-type blk.9.attn_k.weight=IQ4_NL --tensor-type blk.9.attn_output.weight=IQ2_XXS --tensor-type blk.9.attn_q.weight=IQ4_NL --tensor-type blk.9.attn_v.weight=Q5_0 --tensor-type blk.9.ffn_down.weight=IQ1_S --tensor-type blk.9.ffn_gate.weight=IQ4_NL --tensor-type blk.9.ffn_up.weight=IQ4_NL --tensor-type blk.10.attn_k.weight=IQ4_NL --tensor-type blk.10.attn_output.weight=IQ2_XXS --tensor-type blk.10.attn_q.weight=IQ4_NL --tensor-type blk.10.attn_v.weight=Q5_0 --tensor-type blk.10.ffn_down.weight=IQ1_S --tensor-type blk.10.ffn_gate.weight=IQ4_NL --tensor-type blk.10.ffn_up.weight=IQ4_NL --tensor-type blk.11.attn_k.weight=IQ4_NL --tensor-type blk.11.attn_output.weight=IQ2_XXS --tensor-type blk.11.attn_q.weight=IQ4_NL --tensor-type blk.11.attn_v.weight=Q5_0 --tensor-type blk.11.ffn_down.weight=IQ2_S --tensor-type blk.11.ffn_gate.weight=IQ4_NL --tensor-type blk.11.ffn_up.weight=IQ4_NL --tensor-type blk.12.attn_k.weight=IQ4_NL --tensor-type blk.12.attn_output.weight=IQ2_XXS --tensor-type blk.12.attn_q.weight=IQ4_NL --tensor-type blk.12.attn_v.weight=Q5_0 --tensor-type blk.12.ffn_down.weight=IQ2_S --tensor-type blk.12.ffn_gate.weight=IQ4_NL --tensor-type blk.12.ffn_up.weight=IQ4_NL --tensor-type blk.13.attn_k.weight=IQ4_NL --tensor-type blk.13.attn_output.weight=IQ2_XXS --tensor-type blk.13.attn_q.weight=IQ4_NL --tensor-type blk.13.attn_v.weight=Q5_0 --tensor-type blk.13.ffn_down.weight=IQ2_S --tensor-type blk.13.ffn_gate.weight=IQ4_NL --tensor-type blk.13.ffn_up.weight=IQ4_NL --tensor-type blk.14.attn_k.weight=IQ4_NL --tensor-type blk.14.attn_output.weight=IQ2_XXS --tensor-type blk.14.attn_q.weight=IQ4_NL --tensor-type blk.14.attn_v.weight=Q5_0 --tensor-type blk.14.ffn_down.weight=IQ2_S --tensor-type blk.14.ffn_gate.weight=IQ4_NL --tensor-type blk.14.ffn_up.weight=IQ4_NL --tensor-type blk.15.attn_k.weight=IQ4_NL --tensor-type blk.15.attn_output.weight=IQ2_XXS --tensor-type blk.15.attn_q.weight=IQ4_NL --tensor-type blk.15.attn_v.weight=Q5_0 --tensor-type blk.15.ffn_down.weight=IQ2_S --tensor-type blk.15.ffn_gate.weight=IQ4_NL --tensor-type blk.15.ffn_up.weight=IQ4_NL --tensor-type blk.16.attn_k.weight=IQ4_NL --tensor-type blk.16.attn_output.weight=IQ2_XXS --tensor-type blk.16.attn_q.weight=IQ4_NL --tensor-type blk.16.attn_v.weight=Q5_0 --tensor-type blk.16.ffn_down.weight=IQ1_S --tensor-type blk.16.ffn_gate.weight=IQ4_NL --tensor-type blk.16.ffn_up.weight=IQ4_NL --tensor-type blk.17.attn_k.weight=IQ4_NL --tensor-type blk.17.attn_output.weight=IQ2_XXS --tensor-type blk.17.attn_q.weight=IQ4_NL --tensor-type blk.17.attn_v.weight=Q5_0 --tensor-type blk.17.ffn_down.weight=IQ1_S --tensor-type blk.17.ffn_gate.weight=IQ4_NL --tensor-type blk.17.ffn_up.weight=IQ4_NL --tensor-type blk.18.attn_k.weight=IQ4_NL --tensor-type blk.18.attn_output.weight=IQ2_XXS --tensor-type blk.18.attn_q.weight=IQ4_NL --tensor-type blk.18.attn_v.weight=Q5_0 --tensor-type blk.18.ffn_down.weight=IQ1_S --tensor-type blk.18.ffn_gate.weight=IQ4_NL --tensor-type blk.18.ffn_up.weight=IQ4_NL --tensor-type blk.19.attn_k.weight=IQ4_NL --tensor-type blk.19.attn_output.weight=IQ2_XXS --tensor-type blk.19.attn_q.weight=IQ4_NL --tensor-type blk.19.attn_v.weight=Q5_0 --tensor-type blk.19.ffn_down.weight=IQ1_S --tensor-type blk.19.ffn_gate.weight=IQ4_NL --tensor-type blk.19.ffn_up.weight=IQ4_NL --tensor-type blk.20.attn_k.weight=IQ4_NL --tensor-type blk.20.attn_output.weight=IQ2_XXS --tensor-type blk.20.attn_q.weight=IQ4_NL --tensor-type blk.20.attn_v.weight=Q5_0 --tensor-type blk.20.ffn_down.weight=IQ1_S --tensor-type blk.20.ffn_gate.weight=IQ4_NL --tensor-type blk.20.ffn_up.weight=IQ4_NL --tensor-type blk.21.attn_k.weight=IQ4_NL --tensor-type blk.21.attn_output.weight=IQ2_XXS --tensor-type blk.21.attn_q.weight=IQ4_NL --tensor-type blk.21.attn_v.weight=Q5_0 --tensor-type blk.21.ffn_down.weight=IQ1_S --tensor-type blk.21.ffn_gate.weight=IQ4_NL --tensor-type blk.21.ffn_up.weight=IQ4_NL --tensor-type blk.22.attn_k.weight=IQ4_NL --tensor-type blk.22.attn_output.weight=IQ2_XXS --tensor-type blk.22.attn_q.weight=IQ4_NL --tensor-type blk.22.attn_v.weight=Q5_0 --tensor-type blk.22.ffn_down.weight=IQ1_S --tensor-type blk.22.ffn_gate.weight=IQ4_NL --tensor-type blk.22.ffn_up.weight=IQ4_NL --tensor-type blk.23.attn_k.weight=IQ4_NL --tensor-type blk.23.attn_output.weight=IQ2_XXS --tensor-type blk.23.attn_q.weight=IQ4_NL --tensor-type blk.23.attn_v.weight=Q5_0 --tensor-type blk.23.ffn_down.weight=IQ1_S --tensor-type blk.23.ffn_gate.weight=IQ4_NL --tensor-type blk.23.ffn_up.weight=IQ4_NL --tensor-type blk.24.attn_k.weight=IQ4_NL --tensor-type blk.24.attn_output.weight=IQ2_XXS --tensor-type blk.24.attn_q.weight=IQ4_NL --tensor-type blk.24.attn_v.weight=Q5_0 --tensor-type blk.24.ffn_down.weight=IQ1_S --tensor-type blk.24.ffn_gate.weight=IQ4_NL --tensor-type blk.24.ffn_up.weight=IQ4_NL --tensor-type blk.25.attn_k.weight=IQ4_NL --tensor-type blk.25.attn_output.weight=IQ2_XXS --tensor-type blk.25.attn_q.weight=IQ4_NL --tensor-type blk.25.attn_v.weight=Q5_0 --tensor-type blk.25.ffn_down.weight=IQ3_S --tensor-type blk.25.ffn_gate.weight=IQ4_NL --tensor-type blk.25.ffn_up.weight=IQ4_NL &lt;input.gguf&gt; &lt;output.gguf&gt; Q8_0\n\nnote that the Q8_0 at the end is just to get llama-quantize to do it's thing (F16/F32/COPY doesn't run quantization).\n all the tensors will be overridden with the actual --tensor-type params",
          "author_fullname": "t2_1iu07dnz2i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Quantize your own GGUFs the same way as your fav Unsloth Dynamic GGUFs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mes7rc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 80,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 80,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754043657,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754041684,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/electroglyph/quant_clone\"&gt;https://github.com/electroglyph/quant_clone&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is a tiny little app which will create a llama-quantize command based on how a target GGUF is quantized. I wanted it so that I can quantize my finetunes the same way Unsloth does.&lt;/p&gt;\n\n&lt;p&gt;For instance, if you run quant_clone gemma-3-1b-it-UD-IQ1_S.gguf&lt;/p&gt;\n\n&lt;p&gt;you get:&lt;/p&gt;\n\n&lt;p&gt;llama-quantize --imatrix &amp;lt;imatrix_unsloth.dat&amp;gt; --tensor-type token_embd.weight=Q5_1 --tensor-type blk.0.attn_k.weight=IQ4_NL --tensor-type blk.0.attn_output.weight=IQ2_XXS --tensor-type blk.0.attn_q.weight=IQ4_NL --tensor-type blk.0.attn_v.weight=Q5_0 --tensor-type blk.0.ffn_down.weight=IQ3_S --tensor-type blk.0.ffn_gate.weight=IQ4_NL --tensor-type blk.0.ffn_up.weight=IQ4_NL --tensor-type blk.1.attn_k.weight=IQ4_NL --tensor-type blk.1.attn_output.weight=IQ2_XXS --tensor-type blk.1.attn_q.weight=IQ4_NL --tensor-type blk.1.attn_v.weight=Q5_0 --tensor-type blk.1.ffn_down.weight=Q2_K --tensor-type blk.1.ffn_gate.weight=IQ4_NL --tensor-type blk.1.ffn_up.weight=IQ4_NL --tensor-type blk.2.attn_k.weight=IQ4_NL --tensor-type blk.2.attn_output.weight=IQ2_XXS --tensor-type blk.2.attn_q.weight=IQ4_NL --tensor-type blk.2.attn_v.weight=Q5_0 --tensor-type blk.2.ffn_down.weight=IQ3_S --tensor-type blk.2.ffn_gate.weight=IQ4_NL --tensor-type blk.2.ffn_up.weight=IQ4_NL --tensor-type blk.3.attn_k.weight=IQ4_NL --tensor-type blk.3.attn_output.weight=IQ2_XXS --tensor-type blk.3.attn_q.weight=IQ4_NL --tensor-type blk.3.attn_v.weight=Q5_0 --tensor-type blk.3.ffn_down.weight=IQ3_S --tensor-type blk.3.ffn_gate.weight=IQ4_NL --tensor-type blk.3.ffn_up.weight=IQ4_NL --tensor-type blk.4.attn_k.weight=IQ4_NL --tensor-type blk.4.attn_output.weight=IQ2_XXS --tensor-type blk.4.attn_q.weight=IQ4_NL --tensor-type blk.4.attn_v.weight=Q5_0 --tensor-type blk.4.ffn_down.weight=IQ3_S --tensor-type blk.4.ffn_gate.weight=IQ4_NL --tensor-type blk.4.ffn_up.weight=IQ4_NL --tensor-type blk.5.attn_k.weight=IQ4_NL --tensor-type blk.5.attn_output.weight=IQ2_XXS --tensor-type blk.5.attn_q.weight=IQ4_NL --tensor-type blk.5.attn_v.weight=Q5_0 --tensor-type blk.5.ffn_down.weight=IQ1_S --tensor-type blk.5.ffn_gate.weight=IQ4_NL --tensor-type blk.5.ffn_up.weight=IQ4_NL --tensor-type blk.6.attn_k.weight=IQ4_NL --tensor-type blk.6.attn_output.weight=IQ2_XXS --tensor-type blk.6.attn_q.weight=IQ4_NL --tensor-type blk.6.attn_v.weight=Q5_0 --tensor-type blk.6.ffn_down.weight=IQ1_S --tensor-type blk.6.ffn_gate.weight=IQ4_NL --tensor-type blk.6.ffn_up.weight=IQ4_NL --tensor-type blk.7.attn_k.weight=IQ4_NL --tensor-type blk.7.attn_output.weight=IQ2_XXS --tensor-type blk.7.attn_q.weight=IQ4_NL --tensor-type blk.7.attn_v.weight=Q5_0 --tensor-type blk.7.ffn_down.weight=IQ1_S --tensor-type blk.7.ffn_gate.weight=IQ4_NL --tensor-type blk.7.ffn_up.weight=IQ4_NL --tensor-type blk.8.attn_k.weight=IQ4_NL --tensor-type blk.8.attn_output.weight=IQ2_XXS --tensor-type blk.8.attn_q.weight=IQ4_NL --tensor-type blk.8.attn_v.weight=Q5_0 --tensor-type blk.8.ffn_down.weight=IQ1_S --tensor-type blk.8.ffn_gate.weight=IQ4_NL --tensor-type blk.8.ffn_up.weight=IQ4_NL --tensor-type blk.9.attn_k.weight=IQ4_NL --tensor-type blk.9.attn_output.weight=IQ2_XXS --tensor-type blk.9.attn_q.weight=IQ4_NL --tensor-type blk.9.attn_v.weight=Q5_0 --tensor-type blk.9.ffn_down.weight=IQ1_S --tensor-type blk.9.ffn_gate.weight=IQ4_NL --tensor-type blk.9.ffn_up.weight=IQ4_NL --tensor-type blk.10.attn_k.weight=IQ4_NL --tensor-type blk.10.attn_output.weight=IQ2_XXS --tensor-type blk.10.attn_q.weight=IQ4_NL --tensor-type blk.10.attn_v.weight=Q5_0 --tensor-type blk.10.ffn_down.weight=IQ1_S --tensor-type blk.10.ffn_gate.weight=IQ4_NL --tensor-type blk.10.ffn_up.weight=IQ4_NL --tensor-type blk.11.attn_k.weight=IQ4_NL --tensor-type blk.11.attn_output.weight=IQ2_XXS --tensor-type blk.11.attn_q.weight=IQ4_NL --tensor-type blk.11.attn_v.weight=Q5_0 --tensor-type blk.11.ffn_down.weight=IQ2_S --tensor-type blk.11.ffn_gate.weight=IQ4_NL --tensor-type blk.11.ffn_up.weight=IQ4_NL --tensor-type blk.12.attn_k.weight=IQ4_NL --tensor-type blk.12.attn_output.weight=IQ2_XXS --tensor-type blk.12.attn_q.weight=IQ4_NL --tensor-type blk.12.attn_v.weight=Q5_0 --tensor-type blk.12.ffn_down.weight=IQ2_S --tensor-type blk.12.ffn_gate.weight=IQ4_NL --tensor-type blk.12.ffn_up.weight=IQ4_NL --tensor-type blk.13.attn_k.weight=IQ4_NL --tensor-type blk.13.attn_output.weight=IQ2_XXS --tensor-type blk.13.attn_q.weight=IQ4_NL --tensor-type blk.13.attn_v.weight=Q5_0 --tensor-type blk.13.ffn_down.weight=IQ2_S --tensor-type blk.13.ffn_gate.weight=IQ4_NL --tensor-type blk.13.ffn_up.weight=IQ4_NL --tensor-type blk.14.attn_k.weight=IQ4_NL --tensor-type blk.14.attn_output.weight=IQ2_XXS --tensor-type blk.14.attn_q.weight=IQ4_NL --tensor-type blk.14.attn_v.weight=Q5_0 --tensor-type blk.14.ffn_down.weight=IQ2_S --tensor-type blk.14.ffn_gate.weight=IQ4_NL --tensor-type blk.14.ffn_up.weight=IQ4_NL --tensor-type blk.15.attn_k.weight=IQ4_NL --tensor-type blk.15.attn_output.weight=IQ2_XXS --tensor-type blk.15.attn_q.weight=IQ4_NL --tensor-type blk.15.attn_v.weight=Q5_0 --tensor-type blk.15.ffn_down.weight=IQ2_S --tensor-type blk.15.ffn_gate.weight=IQ4_NL --tensor-type blk.15.ffn_up.weight=IQ4_NL --tensor-type blk.16.attn_k.weight=IQ4_NL --tensor-type blk.16.attn_output.weight=IQ2_XXS --tensor-type blk.16.attn_q.weight=IQ4_NL --tensor-type blk.16.attn_v.weight=Q5_0 --tensor-type blk.16.ffn_down.weight=IQ1_S --tensor-type blk.16.ffn_gate.weight=IQ4_NL --tensor-type blk.16.ffn_up.weight=IQ4_NL --tensor-type blk.17.attn_k.weight=IQ4_NL --tensor-type blk.17.attn_output.weight=IQ2_XXS --tensor-type blk.17.attn_q.weight=IQ4_NL --tensor-type blk.17.attn_v.weight=Q5_0 --tensor-type blk.17.ffn_down.weight=IQ1_S --tensor-type blk.17.ffn_gate.weight=IQ4_NL --tensor-type blk.17.ffn_up.weight=IQ4_NL --tensor-type blk.18.attn_k.weight=IQ4_NL --tensor-type blk.18.attn_output.weight=IQ2_XXS --tensor-type blk.18.attn_q.weight=IQ4_NL --tensor-type blk.18.attn_v.weight=Q5_0 --tensor-type blk.18.ffn_down.weight=IQ1_S --tensor-type blk.18.ffn_gate.weight=IQ4_NL --tensor-type blk.18.ffn_up.weight=IQ4_NL --tensor-type blk.19.attn_k.weight=IQ4_NL --tensor-type blk.19.attn_output.weight=IQ2_XXS --tensor-type blk.19.attn_q.weight=IQ4_NL --tensor-type blk.19.attn_v.weight=Q5_0 --tensor-type blk.19.ffn_down.weight=IQ1_S --tensor-type blk.19.ffn_gate.weight=IQ4_NL --tensor-type blk.19.ffn_up.weight=IQ4_NL --tensor-type blk.20.attn_k.weight=IQ4_NL --tensor-type blk.20.attn_output.weight=IQ2_XXS --tensor-type blk.20.attn_q.weight=IQ4_NL --tensor-type blk.20.attn_v.weight=Q5_0 --tensor-type blk.20.ffn_down.weight=IQ1_S --tensor-type blk.20.ffn_gate.weight=IQ4_NL --tensor-type blk.20.ffn_up.weight=IQ4_NL --tensor-type blk.21.attn_k.weight=IQ4_NL --tensor-type blk.21.attn_output.weight=IQ2_XXS --tensor-type blk.21.attn_q.weight=IQ4_NL --tensor-type blk.21.attn_v.weight=Q5_0 --tensor-type blk.21.ffn_down.weight=IQ1_S --tensor-type blk.21.ffn_gate.weight=IQ4_NL --tensor-type blk.21.ffn_up.weight=IQ4_NL --tensor-type blk.22.attn_k.weight=IQ4_NL --tensor-type blk.22.attn_output.weight=IQ2_XXS --tensor-type blk.22.attn_q.weight=IQ4_NL --tensor-type blk.22.attn_v.weight=Q5_0 --tensor-type blk.22.ffn_down.weight=IQ1_S --tensor-type blk.22.ffn_gate.weight=IQ4_NL --tensor-type blk.22.ffn_up.weight=IQ4_NL --tensor-type blk.23.attn_k.weight=IQ4_NL --tensor-type blk.23.attn_output.weight=IQ2_XXS --tensor-type blk.23.attn_q.weight=IQ4_NL --tensor-type blk.23.attn_v.weight=Q5_0 --tensor-type blk.23.ffn_down.weight=IQ1_S --tensor-type blk.23.ffn_gate.weight=IQ4_NL --tensor-type blk.23.ffn_up.weight=IQ4_NL --tensor-type blk.24.attn_k.weight=IQ4_NL --tensor-type blk.24.attn_output.weight=IQ2_XXS --tensor-type blk.24.attn_q.weight=IQ4_NL --tensor-type blk.24.attn_v.weight=Q5_0 --tensor-type blk.24.ffn_down.weight=IQ1_S --tensor-type blk.24.ffn_gate.weight=IQ4_NL --tensor-type blk.24.ffn_up.weight=IQ4_NL --tensor-type blk.25.attn_k.weight=IQ4_NL --tensor-type blk.25.attn_output.weight=IQ2_XXS --tensor-type blk.25.attn_q.weight=IQ4_NL --tensor-type blk.25.attn_v.weight=Q5_0 --tensor-type blk.25.ffn_down.weight=IQ3_S --tensor-type blk.25.ffn_gate.weight=IQ4_NL --tensor-type blk.25.ffn_up.weight=IQ4_NL &amp;lt;input.gguf&amp;gt; &amp;lt;output.gguf&amp;gt; Q8_0&lt;/p&gt;\n\n&lt;p&gt;note that the Q8_0 at the end is just to get llama-quantize to do it&amp;#39;s thing (F16/F32/COPY doesn&amp;#39;t run quantization).\n all the tensors will be overridden with the actual --tensor-type params&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/EHJuKtCnvYSJ1UjIHcOg34gQGWDcA8FabBbGIfxwkWM.png?auto=webp&amp;s=4c25855ab618b6c289af90519a298d57a8784074",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/EHJuKtCnvYSJ1UjIHcOg34gQGWDcA8FabBbGIfxwkWM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3c6340d4c3319c41f51c294613b6f0ee3409e9e9",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/EHJuKtCnvYSJ1UjIHcOg34gQGWDcA8FabBbGIfxwkWM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e6c5d821bd8d8f7a576c21eddccc7fd3b8d95cc9",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/EHJuKtCnvYSJ1UjIHcOg34gQGWDcA8FabBbGIfxwkWM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4fbea7690bb16f5eeb879aeefb1a075c407f7ef4",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/EHJuKtCnvYSJ1UjIHcOg34gQGWDcA8FabBbGIfxwkWM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9662b446d45b54e570661b0f9784671f95ab00b5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/EHJuKtCnvYSJ1UjIHcOg34gQGWDcA8FabBbGIfxwkWM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=92c123a9754ee499617edcfd73460e2fb6f6cb5f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/EHJuKtCnvYSJ1UjIHcOg34gQGWDcA8FabBbGIfxwkWM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=235eb4cfa77dc55840c742cac9176649dfe3dc03",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "EHJuKtCnvYSJ1UjIHcOg34gQGWDcA8FabBbGIfxwkWM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mes7rc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "terminoid_",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mes7rc/quantize_your_own_ggufs_the_same_way_as_your_fav/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mes7rc/quantize_your_own_ggufs_the_same_way_as_your_fav/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754041684,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/8edgr1plcegf1.png?width=888&amp;format=png&amp;auto=webp&amp;s=8a4d35aab872fa2e1ca113eb8f8510212b54d68b\n\nLower PPL = Better\n\nI didn't test q6 and q8 because they can't fit in my 24gb card\n\n    llama-perplexity.exe --model \"\" --threads 15 --ctx-size 8000 -f wiki.test.raw --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --n-gpu-layers 99  --mlock --parallel 8 --seed 7894 --temp 0.7 --top-k 20 --top-p 0.8 --min-p 0 --repeat-penalty 1.05 --presence-penalty 1.5\n\nIQ4\\_XS  \n7 experts PPL = 7.6844  \ndefault 8 experts PPL = 7.6741  \n9 experts PPL = 7.6890  \n10 experts PPL = 7.7343",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Unsloth GGUFs Perplexity Score Comparison | Qwen3-Coder-30B-A3B-Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 50,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8edgr1plcegf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 38,
                  "x": 108,
                  "u": "https://preview.redd.it/8edgr1plcegf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f33a87b7d5a2b89d99b675efa972110ce149e2e0"
                },
                {
                  "y": 77,
                  "x": 216,
                  "u": "https://preview.redd.it/8edgr1plcegf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=47f88a8062278317f6e4599e854f575f83e74f15"
                },
                {
                  "y": 114,
                  "x": 320,
                  "u": "https://preview.redd.it/8edgr1plcegf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d95223ba37a44fe73ddc601e81a4e6b338490b2"
                },
                {
                  "y": 229,
                  "x": 640,
                  "u": "https://preview.redd.it/8edgr1plcegf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e082a292e3b3db212e76c24677430b78a6bfa35"
                }
              ],
              "s": {
                "y": 319,
                "x": 888,
                "u": "https://preview.redd.it/8edgr1plcegf1.png?width=888&amp;format=png&amp;auto=webp&amp;s=8a4d35aab872fa2e1ca113eb8f8510212b54d68b"
              },
              "id": "8edgr1plcegf1"
            }
          },
          "name": "t3_1meucvo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 55,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 55,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/VNoO8rEJvUZhkVA7_FI5gY826jnCUY53ZbZFN6EhWxs.jpg",
          "edited": 1754050687,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754048983,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/8edgr1plcegf1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a4d35aab872fa2e1ca113eb8f8510212b54d68b\"&gt;https://preview.redd.it/8edgr1plcegf1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a4d35aab872fa2e1ca113eb8f8510212b54d68b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Lower PPL = Better&lt;/p&gt;\n\n&lt;p&gt;I didn&amp;#39;t test q6 and q8 because they can&amp;#39;t fit in my 24gb card&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama-perplexity.exe --model &amp;quot;&amp;quot; --threads 15 --ctx-size 8000 -f wiki.test.raw --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --n-gpu-layers 99  --mlock --parallel 8 --seed 7894 --temp 0.7 --top-k 20 --top-p 0.8 --min-p 0 --repeat-penalty 1.05 --presence-penalty 1.5\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;IQ4_XS&lt;br/&gt;\n7 experts PPL = 7.6844&lt;br/&gt;\ndefault 8 experts PPL = 7.6741&lt;br/&gt;\n9 experts PPL = 7.6890&lt;br/&gt;\n10 experts PPL = 7.7343&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1meucvo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 40,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1meucvo/unsloth_ggufs_perplexity_score_comparison/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1meucvo/unsloth_ggufs_perplexity_score_comparison/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754048983,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My initial experiments with the model is very positive, i hope the space is useful for anyone who want to try the model",
          "author_fullname": "t2_1urjd1hc7b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hugging Face space for anyone who want to try the new Dots OCR",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mewq1v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=2ddac78399aaa336a5ddeb8d9b9734213c8aaeaa",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754055465,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My initial experiments with the model is very positive, i hope the space is useful for anyone who want to try the model&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/spaces/MohamedRashad/Dots-OCR",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?auto=webp&amp;s=f70ad2b805bd28f194b3315e14c66fa79cf5e878",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=98569493642c14e55daa842cd331344cc065f590",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=84e00f977b62e65f2805b76e111a0fd88be9b22d",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=66cdec75978c4df6afdd95b14cd67049cb3efc9d",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b249c1a963b7199d36b85e3948a0475db9194b46",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c50a5a839ab90944f505559a3f46bad15c7a1313",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=472d72171b9df47a0f46da4e7f91a9277c3f6c96",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mewq1v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Severe-Awareness829",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mewq1v/hugging_face_space_for_anyone_who_want_to_try_the/",
          "stickied": false,
          "url": "https://huggingface.co/spaces/MohamedRashad/Dots-OCR",
          "subreddit_subscribers": 508541,
          "created_utc": 1754055465,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3yspdpju",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "More supposed info about OpenAI's open-weight model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1meqnn1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 68,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 68,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754035660,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/apples_jimmy/status/1951192085119508860",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1meqnn1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CheekyBastard55",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1meqnn1/more_supposed_info_about_openais_openweight_model/",
          "stickied": false,
          "url": "https://x.com/apples_jimmy/status/1951192085119508860",
          "subreddit_subscribers": 508541,
          "created_utc": 1754035660,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking for some \"best practices\" for this new 30B A3B to squeeze the most out of it with my 4090. Normally I'm pretty up to date on this stuff but I'm a month or so behind the times. I'll share where I'm at and hopefully somebody's got some suggestions :).\n\nI'm sitting on 64gb ram/24gb vram (4090). I'm open to running this thing in ik\\_llama, tabby, vllm, whatever works best really. I have a mix of needs - ideally I'd like to have the best of all worlds (fast, low latency, high throughput), but I know it's all a bit of a \"pick two\" situation usually.\n\nI've got VLLM set up. Looks like I can run an AWQ quant of this thing at 8192 context fully in 24gb vram. If I bump down to an 8 bit KV Cache, I can fit 16,000 context.\n\nWith that setup with 16k context:\n\nOverall tokens/sec (single user, single request): 181.30t/s\n\nMean latency: 2.88s\n\nMean Time to First Token: 0.046s\n\nMax Batching tokens/s: 2,549.14t/s (100 requests)\n\nThat's not terrible as-is, and can hit the kinds of high throughput I need (2500 tokens per second is great, and even the single user 181t/s is snappy), but, I'm curious what my options are out there because I wouldn't mind adding a way to run this with much higher context limits. Like... if I can find a way to run it at an appreciable speed with 128k+ context I'd -love- that, even if that was only a single-user setup. Seems like I could do that with something like ik\\_llama, a ggml 4 or 8 bit 30b a3b, and my 24gb vram card holding part of the model with the rest offloaded into regular ram. Anybody running this thing on ik\\_llama want to chime in with some idea of how its performing and how you'r setting it up?  \n  \nOpen to any advice. I'd like to get this thing running as best I can for both a single user AND for batch-use (I'm fine with it being two separate setups, I can run them when needed appropriately).",
          "author_fullname": "t2_ddyte",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best way to run the Qwen3 30b A3B coder/instruct models for HIGH throughput and/or HIGH context? (on a single 4090)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf3wr0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754072100,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for some &amp;quot;best practices&amp;quot; for this new 30B A3B to squeeze the most out of it with my 4090. Normally I&amp;#39;m pretty up to date on this stuff but I&amp;#39;m a month or so behind the times. I&amp;#39;ll share where I&amp;#39;m at and hopefully somebody&amp;#39;s got some suggestions :).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sitting on 64gb ram/24gb vram (4090). I&amp;#39;m open to running this thing in ik_llama, tabby, vllm, whatever works best really. I have a mix of needs - ideally I&amp;#39;d like to have the best of all worlds (fast, low latency, high throughput), but I know it&amp;#39;s all a bit of a &amp;quot;pick two&amp;quot; situation usually.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got VLLM set up. Looks like I can run an AWQ quant of this thing at 8192 context fully in 24gb vram. If I bump down to an 8 bit KV Cache, I can fit 16,000 context.&lt;/p&gt;\n\n&lt;p&gt;With that setup with 16k context:&lt;/p&gt;\n\n&lt;p&gt;Overall tokens/sec (single user, single request): 181.30t/s&lt;/p&gt;\n\n&lt;p&gt;Mean latency: 2.88s&lt;/p&gt;\n\n&lt;p&gt;Mean Time to First Token: 0.046s&lt;/p&gt;\n\n&lt;p&gt;Max Batching tokens/s: 2,549.14t/s (100 requests)&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s not terrible as-is, and can hit the kinds of high throughput I need (2500 tokens per second is great, and even the single user 181t/s is snappy), but, I&amp;#39;m curious what my options are out there because I wouldn&amp;#39;t mind adding a way to run this with much higher context limits. Like... if I can find a way to run it at an appreciable speed with 128k+ context I&amp;#39;d -love- that, even if that was only a single-user setup. Seems like I could do that with something like ik_llama, a ggml 4 or 8 bit 30b a3b, and my 24gb vram card holding part of the model with the rest offloaded into regular ram. Anybody running this thing on ik_llama want to chime in with some idea of how its performing and how you&amp;#39;r setting it up?  &lt;/p&gt;\n\n&lt;p&gt;Open to any advice. I&amp;#39;d like to get this thing running as best I can for both a single user AND for batch-use (I&amp;#39;m fine with it being two separate setups, I can run them when needed appropriately).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf3wr0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "teachersecret",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754072100,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "just curious on what the community thinks how these models compare in real world use cases. I have tried glm 4.5 quite a lot and would say im pretty impressed by it. I haven't tried K2 or qwen3 coder that much yet so for now im biased towards glm 4.5\n\n  \nas now benchmarks basically mean nothing, im curious what everyone here thinks of their coding abilities according to their personal experiences",
          "author_fullname": "t2_qatlsiyq4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "qwen3 coder vs glm 4.5 vs kimi k2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf955w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754084502,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;just curious on what the community thinks how these models compare in real world use cases. I have tried glm 4.5 quite a lot and would say im pretty impressed by it. I haven&amp;#39;t tried K2 or qwen3 coder that much yet so for now im biased towards glm 4.5&lt;/p&gt;\n\n&lt;p&gt;as now benchmarks basically mean nothing, im curious what everyone here thinks of their coding abilities according to their personal experiences&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mf955w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "YourAverageDev_",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf955w/qwen3_coder_vs_glm_45_vs_kimi_k2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf955w/qwen3_coder_vs_glm_45_vs_kimi_k2/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754084502,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🦥 Qwen3-Coder-Flash: Qwen3-Coder-30B-A3B-Instruct\n\n💚 Just lightning-fast, accurate code generation.\n\n✅ Native 256K context (supports up to 1M tokens with YaRN)\n\n✅ Optimized for platforms like Qwen Code, Cline, Roo Code, Kilo Code, etc.\n\n✅ Seamless function calling &amp; agent workflows\n\n💬 Chat: https://chat.qwen.ai/\n\n🤗 Hugging Face: https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct\n\n🤖 ModelScope: https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct\n\n",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🚀 Qwen3-Coder-Flash released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1me31d8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 1526,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 1526,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753972012,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🦥 Qwen3-Coder-Flash: Qwen3-Coder-30B-A3B-Instruct&lt;/p&gt;\n\n&lt;p&gt;💚 Just lightning-fast, accurate code generation.&lt;/p&gt;\n\n&lt;p&gt;✅ Native 256K context (supports up to 1M tokens with YaRN)&lt;/p&gt;\n\n&lt;p&gt;✅ Optimized for platforms like Qwen Code, Cline, Roo Code, Kilo Code, etc.&lt;/p&gt;\n\n&lt;p&gt;✅ Seamless function calling &amp;amp; agent workflows&lt;/p&gt;\n\n&lt;p&gt;💬 Chat: &lt;a href=\"https://chat.qwen.ai/\"&gt;https://chat.qwen.ai/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;🤗 Hugging Face: &lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct\"&gt;https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;🤖 ModelScope: &lt;a href=\"https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct\"&gt;https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/p7fpia2bz7gf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/p7fpia2bz7gf1.jpeg?auto=webp&amp;s=37bd250aae26692e18e3eeeca84c1caa9d999027",
                  "width": 2528,
                  "height": 1456
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/p7fpia2bz7gf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=95c4825b11a345671147c3d4e0b79207480f3ca0",
                    "width": 108,
                    "height": 62
                  },
                  {
                    "url": "https://preview.redd.it/p7fpia2bz7gf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2b30043242090b2abe359fb37f803cc7ab154ecb",
                    "width": 216,
                    "height": 124
                  },
                  {
                    "url": "https://preview.redd.it/p7fpia2bz7gf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=622e2e61ee2a35d0715b9a941f590819a1d0b1bb",
                    "width": 320,
                    "height": 184
                  },
                  {
                    "url": "https://preview.redd.it/p7fpia2bz7gf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a3985f36673a98a1456f01d26e1d6d8a9fd38fad",
                    "width": 640,
                    "height": 368
                  },
                  {
                    "url": "https://preview.redd.it/p7fpia2bz7gf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d95b4a66cac332ecb3b1b359a6ddce376dcbf89c",
                    "width": 960,
                    "height": 552
                  },
                  {
                    "url": "https://preview.redd.it/p7fpia2bz7gf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18f7994b018ed9d61d07b8cc13a8d5d948c008e7",
                    "width": 1080,
                    "height": 622
                  }
                ],
                "variants": {},
                "id": "p7fpia2bz7gf1"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1me31d8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 343,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/",
          "stickied": false,
          "url": "https://i.redd.it/p7fpia2bz7gf1.jpeg",
          "subreddit_subscribers": 508541,
          "created_utc": 1753972012,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just wanted to share my results running Llama-4-Scout-17B-16E-Instruct-GGUF:Q4\\_K\\_S on my Ryzen AI Max + 395 using llama.cpp with Vulkan backend and the Lemonade server. I’m getting a solid 20 tokens/second with 60 GB of GPU memory in use. ",
          "author_fullname": "t2_1rpxg2806v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama-4-Scout-17B-16E-Instruct-GGUF:Q4_K_S running at 20 tk/s on Ryzen AI Max + 395 with llama.cpp Vulkan + Lemonade server (60GB GPU memory)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf6gaa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.65,
          "author_flair_background_color": null,
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/zf13w9taqggf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/zf13w9taqggf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/zf13w9taqggf1/DASHPlaylist.mpd?a=1756688985%2CMmNhYWZkNmNhMTExMDVmN2M3MzU1ZmU3NmJmMmZkM2FkMzg4MmRiNzNmY2NlOGZkMWZkNTNlOTAxMjZjNmUwZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 33,
              "hls_url": "https://v.redd.it/zf13w9taqggf1/HLSPlaylist.m3u8?a=1756688985%2CNjJlM2VhM2VlYzU3NjY1Zjk3Y2IxZTgxMzlhODJhZWI0YWVmZDc3NmM4MjZlZGQ2ZTE0MDU2ZjI3NzI1MTA0Nw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=4ad7991508d2b549531e751e830361ac79a4409b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754077979,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just wanted to share my results running Llama-4-Scout-17B-16E-Instruct-GGUF:Q4_K_S on my Ryzen AI Max + 395 using llama.cpp with Vulkan backend and the Lemonade server. I’m getting a solid 20 tokens/second with 60 GB of GPU memory in use. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/zf13w9taqggf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?format=pjpg&amp;auto=webp&amp;s=de129663feb9bbf95e122a859d728b746c9defbf",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e9150f6d4ace971cda6add553081a915ea75aff7",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0815afd00caf616bfa60e82c3f16ddc27fce172e",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=dc775410ecfa429f782ad107df358093728a3ee5",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2de42a5c72bba5762d5890300e1ca483a478e69b",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8985c8a18d1e7b326211843c594c2ffef6d43740",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=06a9c35365c0dc196bcf55cc7cc2519c5f5e8e01",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf6gaa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ShamanFlamingoFR",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf6gaa/llama4scout17b16einstructggufq4_k_s_running_at_20/",
          "stickied": false,
          "url": "https://v.redd.it/zf13w9taqggf1",
          "subreddit_subscribers": 508541,
          "created_utc": 1754077979,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/zf13w9taqggf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/zf13w9taqggf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/zf13w9taqggf1/DASHPlaylist.mpd?a=1756688985%2CMmNhYWZkNmNhMTExMDVmN2M3MzU1ZmU3NmJmMmZkM2FkMzg4MmRiNzNmY2NlOGZkMWZkNTNlOTAxMjZjNmUwZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 33,
              "hls_url": "https://v.redd.it/zf13w9taqggf1/HLSPlaylist.m3u8?a=1756688985%2CNjJlM2VhM2VlYzU3NjY1Zjk3Y2IxZTgxMzlhODJhZWI0YWVmZDc3NmM4MjZlZGQ2ZTE0MDU2ZjI3NzI1MTA0Nw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Title",
          "author_fullname": "t2_bv8la",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help: I have an RTX 5090, can I realistically replace Claude Code in any way?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf9exw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754085187,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf9exw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nutyourself",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf9exw/help_i_have_an_rtx_5090_can_i_realistically/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf9exw/help_i_have_an_rtx_5090_can_i_realistically/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754085187,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1umam7ln",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLama.cpp performance on ROCm",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf72g8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754079450,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/discussions/15021",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf72g8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "COBECT",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf72g8/llamacpp_performance_on_rocm/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/discussions/15021",
          "subreddit_subscribers": 508541,
          "created_utc": 1754079450,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to switch from using claude code to running this model locally via cline or other similar extensions.\n\nMy Laptop's specs are:\ni5-11400H with 32GB DDR4 RAM at 2666Mhz.\nRTX 3060 Laptop GPU with 6GB GDDR6 VRAM.\n\nI got confused as there are a lot of inference engines available such as Ollama, LM studio, llama.cpp, vLLM, sglang, ik_llama.cpp etc. i dont know why there are som many of these and what are their pros and cons. So i wanted to ask here. I need the absolute fastest responses possible, i don't mind installing niche software or other things. \n\nThank you in advance.",
          "author_fullname": "t2_aedi2k9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to run Qwen3 Coder 30B-A3B the fastest?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mepr5q",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 54,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 54,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754032144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to switch from using claude code to running this model locally via cline or other similar extensions.&lt;/p&gt;\n\n&lt;p&gt;My Laptop&amp;#39;s specs are:\ni5-11400H with 32GB DDR4 RAM at 2666Mhz.\nRTX 3060 Laptop GPU with 6GB GDDR6 VRAM.&lt;/p&gt;\n\n&lt;p&gt;I got confused as there are a lot of inference engines available such as Ollama, LM studio, llama.cpp, vLLM, sglang, ik_llama.cpp etc. i dont know why there are som many of these and what are their pros and cons. So i wanted to ask here. I need the absolute fastest responses possible, i don&amp;#39;t mind installing niche software or other things. &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mepr5q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "R46H4V",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mepr5q/how_to_run_qwen3_coder_30ba3b_the_fastest/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mepr5q/how_to_run_qwen3_coder_30ba3b_the_fastest/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754032144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Brothers and sisters, we're being taken for fools.\n\nhttps://preview.redd.it/d1iudzju8agf1.png?width=922&amp;format=png&amp;auto=webp&amp;s=c7d5d1e1b891425817fab581afae0149aec26b6b\n\nDid anyone check if it's phoning home?",
          "author_fullname": "t2_dbl0sjy8x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama's new GUI is closed source?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 26,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "d1iudzju8agf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 20,
                  "x": 108,
                  "u": "https://preview.redd.it/d1iudzju8agf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff8052a42d17b9a01f99afcf9e82dafecf05f172"
                },
                {
                  "y": 40,
                  "x": 216,
                  "u": "https://preview.redd.it/d1iudzju8agf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=811b8abd1cc4c1cf67268c7e022527bb4f00004c"
                },
                {
                  "y": 59,
                  "x": 320,
                  "u": "https://preview.redd.it/d1iudzju8agf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3280ff1631dc8d10074f4ffcb38bb11fbb8d9c04"
                },
                {
                  "y": 119,
                  "x": 640,
                  "u": "https://preview.redd.it/d1iudzju8agf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7cfd73d6aaba69097baafb426f1803fcf568fcac"
                }
              ],
              "s": {
                "y": 172,
                "x": 922,
                "u": "https://preview.redd.it/d1iudzju8agf1.png?width=922&amp;format=png&amp;auto=webp&amp;s=c7d5d1e1b891425817fab581afae0149aec26b6b"
              },
              "id": "d1iudzju8agf1"
            }
          },
          "name": "t3_1meeyee",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 271,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 271,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/zLVPiHg9ufyqhvp5Basb43POL8O8dmXli04dBAOzdrw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753999457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Brothers and sisters, we&amp;#39;re being taken for fools.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/d1iudzju8agf1.png?width=922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7d5d1e1b891425817fab581afae0149aec26b6b\"&gt;https://preview.redd.it/d1iudzju8agf1.png?width=922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7d5d1e1b891425817fab581afae0149aec26b6b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Did anyone check if it&amp;#39;s phoning home?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1meeyee",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sea_Night_2572",
          "discussion_type": null,
          "num_comments": 130,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1meeyee/ollamas_new_gui_is_closed_source/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1meeyee/ollamas_new_gui_is_closed_source/",
          "subreddit_subscribers": 508541,
          "created_utc": 1753999457,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello r/LocalLLaMA, This guide outlines a method to create a fully local AI coding assistant with RAG capabilities. The entire backend runs through LM Studio, which handles model downloading, options, serving, and tool integration, avoiding the need for Docker or separate Python environments. Heavily based on the previous guide by u/send_me_a_ticket (thanks!), just further simplified.\n\n* I know some of you wizards want to run things directly through CLI and llama.cpp etc, this guide is not for you.\n\n# Core Components\n\n* **Engine:** **LM Studio.** Used for downloading models, serving them via a local API, and running the tool server.\n* **Tool Server (RAG):** [**docs-mcp-server**](https://github.com/arabold/docs-mcp-server)**.** Runs as a plugin directly inside LM Studio to scrape and index documentation for the LLM to use.\n* **Frontend:** **VS Code +** [**Roo Code**](https://marketplace.visualstudio.com/items?itemName=hitbunt.roo-code)**.** The editor extension that connects to the local model server.\n\n# Advantages of this Approach\n\n* **Straightforward Setup:** Uses the LM Studio GUI for most of the configuration.\n* **100% Local &amp; Private:** Code and prompts are not sent to external services.\n* **VRAM-Friendly:** Optimized for running quantized GGUF models on consumer hardware.\n\n# Part 1: Configuring LM Studio\n\n**1. Install LM Studio** Download and install the latest version from the [LM Studio website](https://lmstudio.ai/).\n\n**2. Download Your Models** In the LM Studio main window (Search tab, magnifying glass icon), search for and download two models:\n\n* **A Coder LLM:** Example: `qwen/qwen3-coder-30b`\n* **An Embedding Model:** Example: `Qwen/Qwen3-Embedding-0.6B-GGUF`\n\n**3. Tune Model Settings** Navigate to the \"My Models\" tab (folder icon on the left). For both your LLM and your embedding model, you can click on them to tune settings like context length, GPU offload, and enable options like Flash Attention/QV Caching according to your model/hardware.\n\nQwen3 doesn't seem to like quantized QV Caching, resulting in Exit code: 18446744072635812000, so leave that off/default at f16.\n\n**4. Configure the** `docs-mcp-server` **Plugin**\n\n* Click the \"Chat\" tab (yellow chat bubble icon on top left).\n* Click on Program on the right.\n* Click on Install, select \\`Edit mcp.json', and replace its entire contents with this:\n\n&amp;#8203;\n\n        {\n          \"mcpServers\": {\n            \"docs-mcp-server\": {\n              \"command\": \"npx\",\n              \"args\": [\n                \"@arabold/docs-mcp-server@latest\"\n              ],\n              \"env\": {\n                \"OPENAI_API_KEY\": \"lmstudio\",\n                \"OPENAI_API_BASE\": \"http://localhost:1234/v1\",\n                \"DOCS_MCP_EMBEDDING_MODEL\": \"text-embedding-qwen3-embedding-0.6b\"\n              }\n            }\n          }\n        }\n\n*Note: Your* `DOCS_MCP_EMBEDDING_MODEL` *value must match the API Model Name shown on the Server tab once the model is loaded. If yours is different, you'll need to update it here.*\n\nIf it's correct, `the mcp/docs-mcp-server` tab will show things like `Tools`, `scrape_docs`, `search_docs`, ... etc.\n\n**5. Start the Server**\n\n* Navigate to the Local Server tab (`&gt;_` icon on the left).\n* In the top slot, load your coder LLM (e.g., Qwen3-Coder).\n* In the second slot, load your embedding model (e.g., Qwen3-Embeddings).\n* Click **Start Server**.\n* Check the server logs at the bottom to verify that the server is running and the `docs-mcp-server` plugin has loaded correctly.\n\n# Part 2: Configuring VS Code &amp; Roo Code\n\n**1. Install VS Code and Roo Code** Install [Visual Studio Code](https://code.visualstudio.com/). Then, inside VS Code, go to the Extensions tab and search for and install **Roo Code**.\n\n**2. Connect Roo Code to LM Studio**\n\n* In VS Code, click the Roo Code icon in the sidebar.\n* At the bottom, click the gear icon next to your profile name to open the settings.\n* Click **Add Profile**, give it a name (e.g., \"LM Studio\"), and configure it:\n* **LM Provider:** Select `LM Studio`\n* **Base URL:** [`http://127.0.0.1:1234`](http://127.0.0.1:1234) (or your server address)\n* **Model:** Select your coder model's ID (e.g., `qwen/qwen3-coder-30b`, it should appear automatically) .\n* While in the settings, you can go through the other tabs (like \"Auto-Approve\") and toggle preferences to fit your workflow.\n\n**3. Connect Roo Code to the Tool Server** Finally, we have to expose the mcp server to Roo.\n\n* In the Roo Code settings panel, click the 3 horizontal dots (top right), select \"MCP Servers\" from the drop-down menu.\n* Ensure the **\"Enable MCP Servers\"** checkbox is **ENABLED**.\n* Scroll down and click \"Edit Global MCP\", and replace the contents (if any) with this:\n\n&amp;#8203;\n\n    {\n      \"mcpServers\": {\n        \"docs-mcp-server\": {\n          \"command\": \"npx\",\n          \"args\": [\n            \"@arabold/docs-mcp-server@latest\"\n          ],\n          \"env\": {\n            \"OPENAI_API_KEY\": \"lmstudio\",\n            \"OPENAI_API_BASE\": \"http://localhost:1234/v1\",\n            \"DOCS_MCP_EMBEDDING_MODEL\": \"text-embedding-qwen3-embedding-0.6b\"\n          },\n          \"alwaysAllow\": [\n            \"fetch_url\",\n            \"remove_docs\",\n            \"scrape_docs\",\n            \"search_docs\",\n            \"list_libraries\",\n            \"find_version\",\n            \"list_jobs\",\n            \"get_job_info\",\n            \"cancel_job\"\n          ],\n          \"disabled\": false\n        }\n      }\n    }\n\n*Note: I'm not exactly sure how this part works. This is functional, but maybe contains redundancies. Hopefully someone with more knowledge can optimize this in the comments.*\n\nThen you can toggle it on and see a green circle if there's no issues.\n\nYour setup is now complete. You have a local coding assistant that can use the `docs-mcp-server` to perform RAG against documentation you provide.",
          "author_fullname": "t2_kggm5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Guide] The *SIMPLE* Self-Hosted AI Coding That Just Works feat. Qwen3-Coder-Flash",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1men28l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 77,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 77,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754031114,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754022492,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;, This guide outlines a method to create a fully local AI coding assistant with RAG capabilities. The entire backend runs through LM Studio, which handles model downloading, options, serving, and tool integration, avoiding the need for Docker or separate Python environments. Heavily based on the previous guide by &lt;a href=\"/u/send_me_a_ticket\"&gt;u/send_me_a_ticket&lt;/a&gt; (thanks!), just further simplified.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I know some of you wizards want to run things directly through CLI and llama.cpp etc, this guide is not for you.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Core Components&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Engine:&lt;/strong&gt; &lt;strong&gt;LM Studio.&lt;/strong&gt; Used for downloading models, serving them via a local API, and running the tool server.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Tool Server (RAG):&lt;/strong&gt; &lt;a href=\"https://github.com/arabold/docs-mcp-server\"&gt;&lt;strong&gt;docs-mcp-server&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; Runs as a plugin directly inside LM Studio to scrape and index documentation for the LLM to use.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; &lt;strong&gt;VS Code +&lt;/strong&gt; &lt;a href=\"https://marketplace.visualstudio.com/items?itemName=hitbunt.roo-code\"&gt;&lt;strong&gt;Roo Code&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; The editor extension that connects to the local model server.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Advantages of this Approach&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Straightforward Setup:&lt;/strong&gt; Uses the LM Studio GUI for most of the configuration.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;100% Local &amp;amp; Private:&lt;/strong&gt; Code and prompts are not sent to external services.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;VRAM-Friendly:&lt;/strong&gt; Optimized for running quantized GGUF models on consumer hardware.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Part 1: Configuring LM Studio&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Install LM Studio&lt;/strong&gt; Download and install the latest version from the &lt;a href=\"https://lmstudio.ai/\"&gt;LM Studio website&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Download Your Models&lt;/strong&gt; In the LM Studio main window (Search tab, magnifying glass icon), search for and download two models:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;A Coder LLM:&lt;/strong&gt; Example: &lt;code&gt;qwen/qwen3-coder-30b&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;An Embedding Model:&lt;/strong&gt; Example: &lt;code&gt;Qwen/Qwen3-Embedding-0.6B-GGUF&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Tune Model Settings&lt;/strong&gt; Navigate to the &amp;quot;My Models&amp;quot; tab (folder icon on the left). For both your LLM and your embedding model, you can click on them to tune settings like context length, GPU offload, and enable options like Flash Attention/QV Caching according to your model/hardware.&lt;/p&gt;\n\n&lt;p&gt;Qwen3 doesn&amp;#39;t seem to like quantized QV Caching, resulting in Exit code: 18446744072635812000, so leave that off/default at f16.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4. Configure the&lt;/strong&gt; &lt;code&gt;docs-mcp-server&lt;/code&gt; &lt;strong&gt;Plugin&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Click the &amp;quot;Chat&amp;quot; tab (yellow chat bubble icon on top left).&lt;/li&gt;\n&lt;li&gt;Click on Program on the right.&lt;/li&gt;\n&lt;li&gt;Click on Install, select `Edit mcp.json&amp;#39;, and replace its entire contents with this:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    {\n      &amp;quot;mcpServers&amp;quot;: {\n        &amp;quot;docs-mcp-server&amp;quot;: {\n          &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;,\n          &amp;quot;args&amp;quot;: [\n            &amp;quot;@arabold/docs-mcp-server@latest&amp;quot;\n          ],\n          &amp;quot;env&amp;quot;: {\n            &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;lmstudio&amp;quot;,\n            &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;,\n            &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;text-embedding-qwen3-embedding-0.6b&amp;quot;\n          }\n        }\n      }\n    }\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Note: Your&lt;/em&gt; &lt;code&gt;DOCS_MCP_EMBEDDING_MODEL&lt;/code&gt; &lt;em&gt;value must match the API Model Name shown on the Server tab once the model is loaded. If yours is different, you&amp;#39;ll need to update it here.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;If it&amp;#39;s correct, &lt;code&gt;the mcp/docs-mcp-server&lt;/code&gt; tab will show things like &lt;code&gt;Tools&lt;/code&gt;, &lt;code&gt;scrape_docs&lt;/code&gt;, &lt;code&gt;search_docs&lt;/code&gt;, ... etc.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5. Start the Server&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Navigate to the Local Server tab (&lt;code&gt;&amp;gt;_&lt;/code&gt; icon on the left).&lt;/li&gt;\n&lt;li&gt;In the top slot, load your coder LLM (e.g., Qwen3-Coder).&lt;/li&gt;\n&lt;li&gt;In the second slot, load your embedding model (e.g., Qwen3-Embeddings).&lt;/li&gt;\n&lt;li&gt;Click &lt;strong&gt;Start Server&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;Check the server logs at the bottom to verify that the server is running and the &lt;code&gt;docs-mcp-server&lt;/code&gt; plugin has loaded correctly.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Part 2: Configuring VS Code &amp;amp; Roo Code&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Install VS Code and Roo Code&lt;/strong&gt; Install &lt;a href=\"https://code.visualstudio.com/\"&gt;Visual Studio Code&lt;/a&gt;. Then, inside VS Code, go to the Extensions tab and search for and install &lt;strong&gt;Roo Code&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Connect Roo Code to LM Studio&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In VS Code, click the Roo Code icon in the sidebar.&lt;/li&gt;\n&lt;li&gt;At the bottom, click the gear icon next to your profile name to open the settings.&lt;/li&gt;\n&lt;li&gt;Click &lt;strong&gt;Add Profile&lt;/strong&gt;, give it a name (e.g., &amp;quot;LM Studio&amp;quot;), and configure it:&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;LM Provider:&lt;/strong&gt; Select &lt;code&gt;LM Studio&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Base URL:&lt;/strong&gt; &lt;a href=\"http://127.0.0.1:1234\"&gt;&lt;code&gt;http://127.0.0.1:1234&lt;/code&gt;&lt;/a&gt; (or your server address)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; Select your coder model&amp;#39;s ID (e.g., &lt;code&gt;qwen/qwen3-coder-30b&lt;/code&gt;, it should appear automatically) .&lt;/li&gt;\n&lt;li&gt;While in the settings, you can go through the other tabs (like &amp;quot;Auto-Approve&amp;quot;) and toggle preferences to fit your workflow.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Connect Roo Code to the Tool Server&lt;/strong&gt; Finally, we have to expose the mcp server to Roo.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In the Roo Code settings panel, click the 3 horizontal dots (top right), select &amp;quot;MCP Servers&amp;quot; from the drop-down menu.&lt;/li&gt;\n&lt;li&gt;Ensure the &lt;strong&gt;&amp;quot;Enable MCP Servers&amp;quot;&lt;/strong&gt; checkbox is &lt;strong&gt;ENABLED&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;Scroll down and click &amp;quot;Edit Global MCP&amp;quot;, and replace the contents (if any) with this:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n  &amp;quot;mcpServers&amp;quot;: {\n    &amp;quot;docs-mcp-server&amp;quot;: {\n      &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;,\n      &amp;quot;args&amp;quot;: [\n        &amp;quot;@arabold/docs-mcp-server@latest&amp;quot;\n      ],\n      &amp;quot;env&amp;quot;: {\n        &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;lmstudio&amp;quot;,\n        &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;,\n        &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;text-embedding-qwen3-embedding-0.6b&amp;quot;\n      },\n      &amp;quot;alwaysAllow&amp;quot;: [\n        &amp;quot;fetch_url&amp;quot;,\n        &amp;quot;remove_docs&amp;quot;,\n        &amp;quot;scrape_docs&amp;quot;,\n        &amp;quot;search_docs&amp;quot;,\n        &amp;quot;list_libraries&amp;quot;,\n        &amp;quot;find_version&amp;quot;,\n        &amp;quot;list_jobs&amp;quot;,\n        &amp;quot;get_job_info&amp;quot;,\n        &amp;quot;cancel_job&amp;quot;\n      ],\n      &amp;quot;disabled&amp;quot;: false\n    }\n  }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Note: I&amp;#39;m not exactly sure how this part works. This is functional, but maybe contains redundancies. Hopefully someone with more knowledge can optimize this in the comments.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Then you can toggle it on and see a green circle if there&amp;#39;s no issues.&lt;/p&gt;\n\n&lt;p&gt;Your setup is now complete. You have a local coding assistant that can use the &lt;code&gt;docs-mcp-server&lt;/code&gt; to perform RAG against documentation you provide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?auto=webp&amp;s=c9b66d5932995b559501dedad243d4991822c62b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5dca5318ae2d95c180426ac49239f78614273fd3",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d21c6913689d6baa653b4499775bf99e5e67b83",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce1aca8605a116b21a8c31ef08a357697c4e5ce6",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=10ea5ed6a277615b50b6e389b4686099afd9f5a9",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=37054aedb1b74092c1a5c30ab58106206841963e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a8a940e26cd7cbb500200fee501049b651a48b4d",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1men28l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xrailgun",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754022492,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello, I've been reading the subreddit for some days now and I was wondering if Qwen 3 or Qwen 2.5 code was still the best model to run to run on vscode with either AI toolkit or RooCode?\n\nI got a M4 pro with 14-Core CPU, 20-Core GPU, 24GB Unified Memory and about 50gb of storage left, can free up another 50gb if needed\n\nFeel free to suggest a different model, or another way to run the model on vscode as I plan on coding offline  \n\nThanks :)",
          "author_fullname": "t2_2v7kco98",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is Qwen still the best for coding?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf2cu1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754068563,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;ve been reading the subreddit for some days now and I was wondering if Qwen 3 or Qwen 2.5 code was still the best model to run to run on vscode with either AI toolkit or RooCode?&lt;/p&gt;\n\n&lt;p&gt;I got a M4 pro with 14-Core CPU, 20-Core GPU, 24GB Unified Memory and about 50gb of storage left, can free up another 50gb if needed&lt;/p&gt;\n\n&lt;p&gt;Feel free to suggest a different model, or another way to run the model on vscode as I plan on coding offline  &lt;/p&gt;\n\n&lt;p&gt;Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf2cu1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OTBKR",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf2cu1/is_qwen_still_the_best_for_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf2cu1/is_qwen_still_the_best_for_coding/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754068563,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been running multiple llama-server instances for different models and found myself constantly SSH-ing into servers to start, stop, and monitor them. After doing this dance one too many times, I decided to build a proper solution.\n\n[llamactl](https://github.com/lordmathis/llamactl) is a control server that lets you manage multiple llama-server instances through a web dashboard or REST API. It handles auto-restart on failures, provides real-time health monitoring, log management, and includes OpenAI-compatible endpoints for easy integration. Everything runs locally with no external dependencies.\n\nThe project is MIT licensed and contributions are welcome.",
          "author_fullname": "t2_w5tba",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Built a web dashboard to manage multiple llama-server instances - llamactl",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf3mhi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754071439,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been running multiple llama-server instances for different models and found myself constantly SSH-ing into servers to start, stop, and monitor them. After doing this dance one too many times, I decided to build a proper solution.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/lordmathis/llamactl\"&gt;llamactl&lt;/a&gt; is a control server that lets you manage multiple llama-server instances through a web dashboard or REST API. It handles auto-restart on failures, provides real-time health monitoring, log management, and includes OpenAI-compatible endpoints for easy integration. Everything runs locally with no external dependencies.&lt;/p&gt;\n\n&lt;p&gt;The project is MIT licensed and contributions are welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/kzOzGVzXfbNgBaONm1V9rEJeMaWG7hKBmJO8I7ak4y4.png?auto=webp&amp;s=4446fbdbc98ec997dd09af204505d3c848850e26",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/kzOzGVzXfbNgBaONm1V9rEJeMaWG7hKBmJO8I7ak4y4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=42bce2f9dd0a4ea31a5251bd4a2a838f62791353",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/kzOzGVzXfbNgBaONm1V9rEJeMaWG7hKBmJO8I7ak4y4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=96344968b5857b09a946444a140b7af1c14efc93",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/kzOzGVzXfbNgBaONm1V9rEJeMaWG7hKBmJO8I7ak4y4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ca9752bd2f6121f6394c53924b1a42c2271b691",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/kzOzGVzXfbNgBaONm1V9rEJeMaWG7hKBmJO8I7ak4y4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=70ba1c6f7e4e46eee55df8bb637cde83d6144d6d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/kzOzGVzXfbNgBaONm1V9rEJeMaWG7hKBmJO8I7ak4y4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fccd5238ce2a8be08562c28146934fefd6a42790",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/kzOzGVzXfbNgBaONm1V9rEJeMaWG7hKBmJO8I7ak4y4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=96aeb8ee2d4ba3e15b587a181065a40081020430",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "kzOzGVzXfbNgBaONm1V9rEJeMaWG7hKBmJO8I7ak4y4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf3mhi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RealLordMathis",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf3mhi/built_a_web_dashboard_to_manage_multiple/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf3mhi/built_a_web_dashboard_to_manage_multiple/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754071439,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, are there any open source solutions to codebase indexing that rival Cursor?",
          "author_fullname": "t2_s7g9g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cursor codebase indexing open source alternative?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfap30",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754088490,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, are there any open source solutions to codebase indexing that rival Cursor?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfap30",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Imjustmisunderstood",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfap30/cursor_codebase_indexing_open_source_alternative/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfap30/cursor_codebase_indexing_open_source_alternative/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754088490,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys!\n\nHow much do PCIe Lanes really matter?\n\nAs far as i understand, just for inference, with for example ollama, they are only really needed when the model is loaded into VRAM - after that everything is done on the card itself.\n\nSo basically, if using multiple gpus, its enough when they are connected via PCIe x1-x4 - or do i oversee something here?\n\nThanks for input!\n\nEdit: I'm planning to use AMD Mi50s",
          "author_fullname": "t2_fe2ok1q3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How much do PCIe Lanes matter?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf1lfv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754067983,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754066851,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys!&lt;/p&gt;\n\n&lt;p&gt;How much do PCIe Lanes really matter?&lt;/p&gt;\n\n&lt;p&gt;As far as i understand, just for inference, with for example ollama, they are only really needed when the model is loaded into VRAM - after that everything is done on the card itself.&lt;/p&gt;\n\n&lt;p&gt;So basically, if using multiple gpus, its enough when they are connected via PCIe x1-x4 - or do i oversee something here?&lt;/p&gt;\n\n&lt;p&gt;Thanks for input!&lt;/p&gt;\n\n&lt;p&gt;Edit: I&amp;#39;m planning to use AMD Mi50s&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf1lfv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MrCatberry",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754066851,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Im no way programmer nor IT guy. Just history teacher trying to make myself companion for job. For whatever reason, my laptop doesnt let me run openwebUI by terminal commands (cant even pip it), so I cant use instructions from herehttps://www.reddit.com/r/LocalLLaMA/comments/1iqngrb/lm\\_studio\\_over\\_a\\_lan/\n\nRn, Im trying to do same stuff with docker but for whatever reason I always get error 500 in my openwebui then trying to reach my running model(by LM studio) on PC.  \nCan someone give me guide/step-by-step instruction/what to read on subject in order to be able to use model which is running on another my device in same network?  \nHope this isn't off topic post",
          "author_fullname": "t2_2l9kaflr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Want to run models on PC and use them via same wifi with my laptop",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf9vr7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754086388,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im no way programmer nor IT guy. Just history teacher trying to make myself companion for job. For whatever reason, my laptop doesnt let me run openwebUI by terminal commands (cant even pip it), so I cant use instructions from herehttps://&lt;a href=\"http://www.reddit.com/r/LocalLLaMA/comments/1iqngrb/lm%5C_studio%5C_over%5C_a%5C_lan/\"&gt;www.reddit.com/r/LocalLLaMA/comments/1iqngrb/lm\\_studio\\_over\\_a\\_lan/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Rn, Im trying to do same stuff with docker but for whatever reason I always get error 500 in my openwebui then trying to reach my running model(by LM studio) on PC.&lt;br/&gt;\nCan someone give me guide/step-by-step instruction/what to read on subject in order to be able to use model which is running on another my device in same network?&lt;br/&gt;\nHope this isn&amp;#39;t off topic post&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf9vr7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RussianNewbie",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf9vr7/want_to_run_models_on_pc_and_use_them_via_same/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf9vr7/want_to_run_models_on_pc_and_use_them_via_same/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754086388,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Google delivers ancient benchmarks, I used to love aider benchmarks, but it seems it was abandoned, no updates on new models. I want to know how qwen3-coder and glm4.5 compare.. but nobody updates benchmarks anymore? are we in a postbenchmark era? Benchmarks as gamed as they are they still signal utility!",
          "author_fullname": "t2_7j2k5hlp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any up to date coding benchmarks?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf6n4u",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754078439,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google delivers ancient benchmarks, I used to love aider benchmarks, but it seems it was abandoned, no updates on new models. I want to know how qwen3-coder and glm4.5 compare.. but nobody updates benchmarks anymore? are we in a postbenchmark era? Benchmarks as gamed as they are they still signal utility!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf6n4u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sudden-Lingonberry-8",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf6n4u/any_up_to_date_coding_benchmarks/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf6n4u/any_up_to_date_coding_benchmarks/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754078439,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello to the MI50 owners out there, I am struggling to find any prompt processing performance for the MI50 on ~8b and ~14b class models.\n\nHas anyone got any numbers for those types of models ?",
          "author_fullname": "t2_1urn03vg8s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MI50 prompt processing performance",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mexai2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754056873,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello to the MI50 owners out there, I am struggling to find any prompt processing performance for the MI50 on ~8b and ~14b class models.&lt;/p&gt;\n\n&lt;p&gt;Has anyone got any numbers for those types of models ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mexai2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kasimolo33",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mexai2/mi50_prompt_processing_performance/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mexai2/mi50_prompt_processing_performance/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754056873,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to use sonnet 4 for work, but people are saying it will be hundreds a month. If we are paying 500/mo for example, why wouldnt we take that same 500/mo and finance our own hardware? Anything that you pay monthly for to a third party would obviously be cheaper to buy yourself since they obviously have to make money on top of paying for their hardware. A comparison would be using your own 10tb drive for storage vs paying monthly for 10tb of cloud storage. At like 9 months, it wouldve already been cheaper to just buy it outright. This is true for all use cases where you plan to indefinitely use teh thing (unlike renting one-off items like a moving truck). With that said, whatever you are paying Claude / Cursor for, should therotically be cheaper if you buy it outright at X timefrime (my guess is that it starts paying for itself at less than a year). For those that will then say \"well, they ar losing money right now\", ok that still means they will eventually have to hike prices, so there is no escaping this prediction that it will be smarter to buy than to rent if you are using this for fulltime work. So with that in mind, would a 20k machine at least match sonnet 4? A 40k machine? a 100k machine?",
          "author_fullname": "t2_ufbr1m7p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How much to match Sonnet 4?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf7rut",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754081161,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to use sonnet 4 for work, but people are saying it will be hundreds a month. If we are paying 500/mo for example, why wouldnt we take that same 500/mo and finance our own hardware? Anything that you pay monthly for to a third party would obviously be cheaper to buy yourself since they obviously have to make money on top of paying for their hardware. A comparison would be using your own 10tb drive for storage vs paying monthly for 10tb of cloud storage. At like 9 months, it wouldve already been cheaper to just buy it outright. This is true for all use cases where you plan to indefinitely use teh thing (unlike renting one-off items like a moving truck). With that said, whatever you are paying Claude / Cursor for, should therotically be cheaper if you buy it outright at X timefrime (my guess is that it starts paying for itself at less than a year). For those that will then say &amp;quot;well, they ar losing money right now&amp;quot;, ok that still means they will eventually have to hike prices, so there is no escaping this prediction that it will be smarter to buy than to rent if you are using this for fulltime work. So with that in mind, would a 20k machine at least match sonnet 4? A 40k machine? a 100k machine?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf7rut",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "devshore",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf7rut/how_much_to_match_sonnet_4/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf7rut/how_much_to_match_sonnet_4/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754081161,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": " ( Or... The adventures of a newbie )\n\nToday I learned something really important — and honestly, I had no idea how using API-hosted LLMs can quietly become a black hole for your wallet.💸💰\n\nAt first glance, the pricing seems super appealing. You see those spicy “low” prices from big US companies — something like $0.002 per 1,000 tokens, and you think, \"Wow, that’s cheap!\"\n\nBut… let’s do the math.\n\nYou start using a 128k context model on a platform like OpenRouter, and you don’t realize that with every new interaction, your entire chat history is being resent to the API. That’s the only way the model can \"remember\" the conversation. So after just a few minutes, each message you're sending might carry along 10k tokens — or even more.\n\nNow imagine you’re chatting for hours. Every tiny reply — even a simple “ok” — could trigger a payload of 50,000 or 100,000 tokens being sent again and again. It’s like buying an entire book just to read the next letter.\n\nIn just a few hours, you may have burned through $5 to $10, just for a basic conversation. And now think monthly... or worse — imagine you’re editing a software file with 800 lines of code. Every time you tweak a line and hit send, it could cost you $1 or $2 per second.\n\nI mean... what?!\n\nI now understand the almost desperate effort some people make to run LLMs locally on their own machines — because something that looks insanely cheap at first glance… can turn out to be violently expensive.\n\nThis is insane. Maybe everyone else already knew this — but I didn’t! 😯😯😯\n",
          "author_fullname": "t2_8c7clfk1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The Great Deception of \"Low Prices\" in LLM APIs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1meep6o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "ups": 134,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 134,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/KpH7dlNRh78oWPR5CwX_DiS1oipkBRXTYNWHmoAZyyg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753998846,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;( Or... The adventures of a newbie )&lt;/p&gt;\n\n&lt;p&gt;Today I learned something really important — and honestly, I had no idea how using API-hosted LLMs can quietly become a black hole for your wallet.💸💰&lt;/p&gt;\n\n&lt;p&gt;At first glance, the pricing seems super appealing. You see those spicy “low” prices from big US companies — something like $0.002 per 1,000 tokens, and you think, &amp;quot;Wow, that’s cheap!&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;But… let’s do the math.&lt;/p&gt;\n\n&lt;p&gt;You start using a 128k context model on a platform like OpenRouter, and you don’t realize that with every new interaction, your entire chat history is being resent to the API. That’s the only way the model can &amp;quot;remember&amp;quot; the conversation. So after just a few minutes, each message you&amp;#39;re sending might carry along 10k tokens — or even more.&lt;/p&gt;\n\n&lt;p&gt;Now imagine you’re chatting for hours. Every tiny reply — even a simple “ok” — could trigger a payload of 50,000 or 100,000 tokens being sent again and again. It’s like buying an entire book just to read the next letter.&lt;/p&gt;\n\n&lt;p&gt;In just a few hours, you may have burned through $5 to $10, just for a basic conversation. And now think monthly... or worse — imagine you’re editing a software file with 800 lines of code. Every time you tweak a line and hit send, it could cost you $1 or $2 per second.&lt;/p&gt;\n\n&lt;p&gt;I mean... what?!&lt;/p&gt;\n\n&lt;p&gt;I now understand the almost desperate effort some people make to run LLMs locally on their own machines — because something that looks insanely cheap at first glance… can turn out to be violently expensive.&lt;/p&gt;\n\n&lt;p&gt;This is insane. Maybe everyone else already knew this — but I didn’t! 😯😯😯&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/f8vv4t837agf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/f8vv4t837agf1.png?auto=webp&amp;s=2a5bde2dd3cb61e64af4720e8cc13e534a92116f",
                  "width": 1024,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/f8vv4t837agf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a158041f9882499a65c08f11adace0fa76a0f40",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/f8vv4t837agf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b71e536e993c221a57840d46c0f8345d4fd26f2",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/f8vv4t837agf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a89d09f3744524462cc6bc4d5c80648aca4f27e9",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/f8vv4t837agf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b1305c708b1fbe4bb7166cf9808a29640f750a67",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/f8vv4t837agf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=73ec0bc2072e5f2bc9194e2510d445f7e8673cfb",
                    "width": 960,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "PQVtbBsS9q88WP67d3L6vyJ8WKHnI51rshmbM64ONSA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1meep6o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Current-Stop7806",
          "discussion_type": null,
          "num_comments": 135,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1meep6o/the_great_deception_of_low_prices_in_llm_apis/",
          "stickied": false,
          "url": "https://i.redd.it/f8vv4t837agf1.png",
          "subreddit_subscribers": 508541,
          "created_utc": 1753998846,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_dwy0w3kf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder-30B-A3B released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1me2zc6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#bbbdbf",
          "ups": 535,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ff54b802-c910-11ed-be9d-ea867d8afa86",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 535,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=50e58d8c576ff1f0469c49c5086a3d54ed8234ad",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 33B"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753971880,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?auto=webp&amp;s=4cacac54fb0a262f4128b23481bccaf4104c19d5",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fbf0440b72bf3c599b24d782f0bddf00251537cf",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=824bee5d7aa9841a221b2f60a969d54551eccb18",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=91ca7f6cb7614731e917c0c8e162bd66bfbc25ca",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9d2b1429fc14f5ca152608718fd3ef6d50119778",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6655884fe4ff60136ee88021696ace5be4875862",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=991b600ad67e419b1091cac2c8c55f34d86b36fa",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 33B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1me2zc6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "glowcialist",
          "discussion_type": null,
          "num_comments": 91,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1me2zc6/qwen3coder30ba3b_released/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct",
          "subreddit_subscribers": 508541,
          "created_utc": 1753971880,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,\n\nI've been toying around with qwen3 coder (0 temp btw).  \nI've tested it on cerebras cloud. 1.4k T/S. Solved a medium-level logic problem in a blink of an eye, blew me away, the fact that the responses come instant makes you wanna pop a bottle and stare in the abyss. The first AI to solve it was o1, in like 60s of thinking. I do actually believe it's Sonnet 4 level.\n\nI'm curious to better understand the limits of open-source llms. \n\nSo circling back to my title, ya'll got anymore of dem hard problems that can't be solved by current open-weights SOTA?\n\n",
          "author_fullname": "t2_5uhcd48d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Y'all got more of them hard problems?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf7hkw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754080464,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been toying around with qwen3 coder (0 temp btw).&lt;br/&gt;\nI&amp;#39;ve tested it on cerebras cloud. 1.4k T/S. Solved a medium-level logic problem in a blink of an eye, blew me away, the fact that the responses come instant makes you wanna pop a bottle and stare in the abyss. The first AI to solve it was o1, in like 60s of thinking. I do actually believe it&amp;#39;s Sonnet 4 level.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious to better understand the limits of open-source llms. &lt;/p&gt;\n\n&lt;p&gt;So circling back to my title, ya&amp;#39;ll got anymore of dem hard problems that can&amp;#39;t be solved by current open-weights SOTA?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf7hkw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shaman-warrior",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf7hkw/yall_got_more_of_them_hard_problems/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf7hkw/yall_got_more_of_them_hard_problems/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754080464,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello friends, \n\nI recently got myself a new PC, Ryzen 9800x3d, 32gb RAM and a 5070ti (16gb vram). I want to create AI art locally, what’s a good llm to play around with while I learn? ",
          "author_fullname": "t2_w9e9g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Noob question",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf7663",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754079692,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello friends, &lt;/p&gt;\n\n&lt;p&gt;I recently got myself a new PC, Ryzen 9800x3d, 32gb RAM and a 5070ti (16gb vram). I want to create AI art locally, what’s a good llm to play around with while I learn? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf7663",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "panlid5000",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf7663/noob_question/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf7663/noob_question/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754079692,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Currently in Toronto area the 7900 xtx is cheaper brand new with taxes then a used 3090. What are people’s experience with a couple of these cards for inference on Windows?  I searched and saw some feedback from months ago, looking how they handle all the new models for inference?  \n",
          "author_fullname": "t2_14v7k3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD 7900 xtx for inference?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf16vx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754065916,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently in Toronto area the 7900 xtx is cheaper brand new with taxes then a used 3090. What are people’s experience with a couple of these cards for inference on Windows?  I searched and saw some feedback from months ago, looking how they handle all the new models for inference?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf16vx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Willdudes",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf16vx/amd_7900_xtx_for_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf16vx/amd_7900_xtx_for_inference/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754065916,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm using a 7900 GRE and training models via Applio. I’m getting about 1.88 seconds per iteration (see image). I've tried different setups and drivers with help from others, but the speed doesn't improve.\n\nJust wondering — anyone else using a 7900 GRE? What kind of speeds are you getting? Would love to compare.",
          "author_fullname": "t2_utmosfnh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RX 7900 GRE users: What training speeds do you get on Applio? (I'm seeing ~1.88s/it)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 13,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mfc1oj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/w8yTEmbw2r5SmwMUy9NzFfJLvnoRmaAsSryV5GLTnt4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754092203,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using a 7900 GRE and training models via Applio. I’m getting about 1.88 seconds per iteration (see image). I&amp;#39;ve tried different setups and drivers with help from others, but the speed doesn&amp;#39;t improve.&lt;/p&gt;\n\n&lt;p&gt;Just wondering — anyone else using a 7900 GRE? What kind of speeds are you getting? Would love to compare.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/hrd001ynwhgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/hrd001ynwhgf1.png?auto=webp&amp;s=5c1b626d538a90bbc701de5ee36ef1f3b8364dc6",
                  "width": 1013,
                  "height": 99
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/hrd001ynwhgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e50eaa420fd6a63e598023e3cde5be0294f7495e",
                    "width": 108,
                    "height": 10
                  },
                  {
                    "url": "https://preview.redd.it/hrd001ynwhgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3c4495a93af7c650adf677f85ec950907e730217",
                    "width": 216,
                    "height": 21
                  },
                  {
                    "url": "https://preview.redd.it/hrd001ynwhgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6a87faa6041565843e93d320e9bb6e8f80d1bb7",
                    "width": 320,
                    "height": 31
                  },
                  {
                    "url": "https://preview.redd.it/hrd001ynwhgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=39770003cb041d457dd97ac2ebca7137dc3fe625",
                    "width": 640,
                    "height": 62
                  },
                  {
                    "url": "https://preview.redd.it/hrd001ynwhgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7ca18a2e6ce08f8d5b0b583a2449fd7aaf42644",
                    "width": 960,
                    "height": 93
                  }
                ],
                "variants": {},
                "id": "NedzWxTqeORWwluACmoHPoFt8g062kGYd_D3VPX8sNo"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfc1oj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lumpy-Quiet-7691",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfc1oj/rx_7900_gre_users_what_training_speeds_do_you_get/",
          "stickied": false,
          "url": "https://i.redd.it/hrd001ynwhgf1.png",
          "subreddit_subscribers": 508541,
          "created_utc": 1754092203,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just got through Raschka's model architecture series. Seems like everything is a tweak of Llama 3.",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-source architectures that aren't Llama 3 knock offs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf0mw2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": "transparent",
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754064644,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just got through Raschka&amp;#39;s model architecture series. Seems like everything is a tweak of Llama 3.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf0mw2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mf0mw2/opensource_architectures_that_arent_llama_3_knock/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf0mw2/opensource_architectures_that_arent_llama_3_knock/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754064644,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "After reading that ik\\_llama.cpp gives way higher performance than LMStudio, I wanted to have a simple method of installing and running the Qwen3 Coder model under Windows. I chose to install everything needed and build from source within one single script - written mainly by ChatGPT with experimenting &amp; testing until it worked on both of Windows machines:\n\n||Desktop|Notebook|\n|:-|:-|:-|\n|OS|Windows 11|Windows 10|\n|CPU|AMD Ryzen 5 7600|Intel i7 8750H|\n|RAM|32GB DDR5 5600|32GB DDR4 2667|\n|GPU|NVIDIA RTX 4070 Ti 12GB|NVIDIA GTX 1070 8GB|\n|Tokens/s|35|9.5|\n\n\n\nFor my desktop PC that works out great and I get super nice results.\n\nOn my notebook however there seems to be a problem with context: the model mostly outputs random text instead of referencing my questions. If anyone has any idea help would be greatly appreciated!\n\nAlthough this might not be the perfect solution I thought I'd share it here, maybe someone finds it useful:\n\n[https://github.com/Danmoreng/local-qwen3-coder-env](https://github.com/Danmoreng/local-qwen3-coder-env)",
          "author_fullname": "t2_7z26p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Installscript for Qwen3-Coder running on ik_llama.cpp for high performance",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1metf4h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754045970,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After reading that ik_llama.cpp gives way higher performance than LMStudio, I wanted to have a simple method of installing and running the Qwen3 Coder model under Windows. I chose to install everything needed and build from source within one single script - written mainly by ChatGPT with experimenting &amp;amp; testing until it worked on both of Windows machines:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;Desktop&lt;/th&gt;\n&lt;th align=\"left\"&gt;Notebook&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OS&lt;/td&gt;\n&lt;td align=\"left\"&gt;Windows 11&lt;/td&gt;\n&lt;td align=\"left\"&gt;Windows 10&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;CPU&lt;/td&gt;\n&lt;td align=\"left\"&gt;AMD Ryzen 5 7600&lt;/td&gt;\n&lt;td align=\"left\"&gt;Intel i7 8750H&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;RAM&lt;/td&gt;\n&lt;td align=\"left\"&gt;32GB DDR5 5600&lt;/td&gt;\n&lt;td align=\"left\"&gt;32GB DDR4 2667&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GPU&lt;/td&gt;\n&lt;td align=\"left\"&gt;NVIDIA RTX 4070 Ti 12GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;NVIDIA GTX 1070 8GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Tokens/s&lt;/td&gt;\n&lt;td align=\"left\"&gt;35&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.5&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;For my desktop PC that works out great and I get super nice results.&lt;/p&gt;\n\n&lt;p&gt;On my notebook however there seems to be a problem with context: the model mostly outputs random text instead of referencing my questions. If anyone has any idea help would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;Although this might not be the perfect solution I thought I&amp;#39;d share it here, maybe someone finds it useful:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Danmoreng/local-qwen3-coder-env\"&gt;https://github.com/Danmoreng/local-qwen3-coder-env&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?auto=webp&amp;s=696008496e1bd6e0694f2b9886836ed38e83c28c",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0ff28b544d610ccf62e98f3feddd075959ce926b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=72c1589c6e0b5c99b15e4303d1779e5eec8f9a80",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3fcaf94729c1bcd29118e071f6822ad5c896bec0",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e9afa83dad53bd5ba85b1ab44cba7113663f13b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c6c818c201ba8089a89a84387df82adfd28220d6",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b492d3d79ff10edeacd4e012c9aa3e160eaf88db",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1metf4h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Danmoreng",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754045970,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,\n\nWe're a scrappy startup at Trillion Labs and just released [Tri-70B-preview-SFT](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT), our largest language model yet (70B params!), trained from scratch on \\~1.5T tokens. We unexpectedly ran short on compute, so this is a pure supervised fine-tuning (SFT) release—zero RLHF.\n\n# TL;DR:\n\n* **70B parameters**; pure supervised fine-tuning (**no RLHF** yet!)\n* **32K token context window** (perfect for experimenting with Yarn, if you're bold!)\n* Optimized primarily for **English and Korean**, with decent Japanese performance\n* Tried some new tricks (**FP8 mixed precision, Scalable Softmax, iRoPE attention**)\n* Benchmarked roughly around **Qwen-2.5-72B and LLaMA-3.1-70B**, but it's noticeably raw and needs alignment tweaks.\n* Model and tokenizer fully open on 🤗 HuggingFace under a permissive license (**auto-approved** conditional commercial usage allowed, but it’s definitely experimental!).\n\n# Why release it raw?\n\nWe think releasing Tri-70B in its current form might spur unique research—especially for those into RLHF, RLVR, GRPO, CISPO, GSPO, etc. It’s a perfect baseline for alignment experimentation. Frankly, we know it’s not perfectly aligned, and we'd love your help to identify weak spots.\n\nGive it a spin and see what it can (and can’t) do. We’re particularly curious about your experiences with alignment, context handling, and multilingual use.\n\n\\*\\*👉 \\*\\*[**Check out the repo and model card here!**](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT)\n\nQuestions, thoughts, criticisms warmly welcomed—hit us up below!",
          "author_fullname": "t2_1ug5bi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[P] Tri-70B-preview-SFT: New 70B Model (Research Preview, SFT-only)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mejkcu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 53,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 53,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754011885,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re a scrappy startup at Trillion Labs and just released &lt;a href=\"https://huggingface.co/trillionlabs/Tri-70B-preview-SFT\"&gt;Tri-70B-preview-SFT&lt;/a&gt;, our largest language model yet (70B params!), trained from scratch on ~1.5T tokens. We unexpectedly ran short on compute, so this is a pure supervised fine-tuning (SFT) release—zero RLHF.&lt;/p&gt;\n\n&lt;h1&gt;TL;DR:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;70B parameters&lt;/strong&gt;; pure supervised fine-tuning (&lt;strong&gt;no RLHF&lt;/strong&gt; yet!)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;32K token context window&lt;/strong&gt; (perfect for experimenting with Yarn, if you&amp;#39;re bold!)&lt;/li&gt;\n&lt;li&gt;Optimized primarily for &lt;strong&gt;English and Korean&lt;/strong&gt;, with decent Japanese performance&lt;/li&gt;\n&lt;li&gt;Tried some new tricks (&lt;strong&gt;FP8 mixed precision, Scalable Softmax, iRoPE attention&lt;/strong&gt;)&lt;/li&gt;\n&lt;li&gt;Benchmarked roughly around &lt;strong&gt;Qwen-2.5-72B and LLaMA-3.1-70B&lt;/strong&gt;, but it&amp;#39;s noticeably raw and needs alignment tweaks.&lt;/li&gt;\n&lt;li&gt;Model and tokenizer fully open on 🤗 HuggingFace under a permissive license (&lt;strong&gt;auto-approved&lt;/strong&gt; conditional commercial usage allowed, but it’s definitely experimental!).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Why release it raw?&lt;/h1&gt;\n\n&lt;p&gt;We think releasing Tri-70B in its current form might spur unique research—especially for those into RLHF, RLVR, GRPO, CISPO, GSPO, etc. It’s a perfect baseline for alignment experimentation. Frankly, we know it’s not perfectly aligned, and we&amp;#39;d love your help to identify weak spots.&lt;/p&gt;\n\n&lt;p&gt;Give it a spin and see what it can (and can’t) do. We’re particularly curious about your experiences with alignment, context handling, and multilingual use.&lt;/p&gt;\n\n&lt;p&gt;**👉 **&lt;a href=\"https://huggingface.co/trillionlabs/Tri-70B-preview-SFT\"&gt;&lt;strong&gt;Check out the repo and model card here!&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Questions, thoughts, criticisms warmly welcomed—hit us up below!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?auto=webp&amp;s=b45653eadbcba17c38d2f4f2e15f6ebd41ec7c72",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b7a80c31c557591f18bda1f387961a8fe38f053e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=13c5c597d54b6cfe9cc7d137d17b8c65f8c6db95",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=10f045816aeb8c03203c2c64cc8f2065cb392da4",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cad5dd9413d8196f56dd930d3b4333ee0351d472",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=37853a1aca067c019b4d7c06a11c66190e6bc001",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f4fc9332f1425fd05814a4fb77159385e0f2d89",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mejkcu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jshin49",
          "discussion_type": null,
          "num_comments": 38,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mejkcu/p_tri70bpreviewsft_new_70b_model_research_preview/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mejkcu/p_tri70bpreviewsft_new_70b_model_research_preview/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754011885,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As you can see from the radar chart, the scores on the left for the two Agent capability tests, mind2web and BFCL-v3, are very close. This suggests that the Agent capabilities of Qwen3-Coder-FLash should be quite strong.   \n  \nHowever, there is still a significant gap in the Aider-Polyglot and SWE Multilingual tests, which implies that its programming capabilities are indeed quite different from those of Qwen3-Coder-480B.\n\nHas anyone started using it yet? What's the actual user experience like?",
          "author_fullname": "t2_fiiv6xm3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I made a comparison chart for Qwen3-Coder-30B-A3B vs. Qwen3-Coder-480B-A35B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1me4i2h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 315,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 315,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/nDJcrKit9leloNf-Ut_mbP1e9pe5yDgS5VazXSDA6qU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753975422,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As you can see from the radar chart, the scores on the left for the two Agent capability tests, mind2web and BFCL-v3, are very close. This suggests that the Agent capabilities of Qwen3-Coder-FLash should be quite strong.   &lt;/p&gt;\n\n&lt;p&gt;However, there is still a significant gap in the Aider-Polyglot and SWE Multilingual tests, which implies that its programming capabilities are indeed quite different from those of Qwen3-Coder-480B.&lt;/p&gt;\n\n&lt;p&gt;Has anyone started using it yet? What&amp;#39;s the actual user experience like?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/l6547uel88gf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/l6547uel88gf1.png?auto=webp&amp;s=b5aa7e3e5dcfa254c689f363f60a9722e1eb72e4",
                  "width": 1324,
                  "height": 2088
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/l6547uel88gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fc731d339acbcf88a184b51662dd456eefd477c5",
                    "width": 108,
                    "height": 170
                  },
                  {
                    "url": "https://preview.redd.it/l6547uel88gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a836e9fdf9366f6a0058a586fbda0b1632c21cbe",
                    "width": 216,
                    "height": 340
                  },
                  {
                    "url": "https://preview.redd.it/l6547uel88gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f980b5b5346583dbdaaebfe56b59bd8d5b9291df",
                    "width": 320,
                    "height": 504
                  },
                  {
                    "url": "https://preview.redd.it/l6547uel88gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=40dafbd4c67e845ff8ce7c141e92d59fdfd342fe",
                    "width": 640,
                    "height": 1009
                  },
                  {
                    "url": "https://preview.redd.it/l6547uel88gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b98fec591844d74f49e2febeeff5c3c53305d21b",
                    "width": 960,
                    "height": 1513
                  },
                  {
                    "url": "https://preview.redd.it/l6547uel88gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a8de36630bff8bc1b7132277c5bc86fe5e8e9a61",
                    "width": 1080,
                    "height": 1703
                  }
                ],
                "variants": {},
                "id": "vmGH_RgQTp6SHjczHQXNSGnIw-M1Xweu2bYiYKsj904"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1me4i2h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dr_Karminski",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1me4i2h/i_made_a_comparison_chart_for_qwen3coder30ba3b_vs/",
          "stickied": false,
          "url": "https://i.redd.it/l6547uel88gf1.png",
          "subreddit_subscribers": 508541,
          "created_utc": 1753975422,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If so how did it go?",
          "author_fullname": "t2_g5exwc2h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone tried GLM-4.5 with Claude code or other agents?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfa9tj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754087390,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If so how did it go?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfa9tj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BlueeWaater",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfa9tj/anyone_tried_glm45_with_claude_code_or_other/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfa9tj/anyone_tried_glm45_with_claude_code_or_other/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754087390,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone with experience using Lambda chat know why DeepSeek R1 Distill Llama 3.3 70B gets fixated on questions I asked earlier in the thread and unable to recognized new questions? Just keeps providing the same reasoning it gave for an earlier answer.\n\nhttps://preview.redd.it/c912if06ihgf1.png?width=1806&amp;format=png&amp;auto=webp&amp;s=4900dfaf788170c984a4bfd7ca40290c7661f412\n\n",
          "author_fullname": "t2_1haa9ceo0b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Lambda Chat Odd Outputs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "c912if06ihgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/c912if06ihgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=940dd889ec733bf0472a244ff8e48de42c8e4a8c"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/c912if06ihgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=969e8e1751016ec1653bfd54524e5a4977d2ffaa"
                },
                {
                  "y": 167,
                  "x": 320,
                  "u": "https://preview.redd.it/c912if06ihgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e9150334892addf31039e2bda8131a44c71fa61"
                },
                {
                  "y": 334,
                  "x": 640,
                  "u": "https://preview.redd.it/c912if06ihgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7a7158c9c204a4224e2d709f0ad6e864c8f44e4b"
                },
                {
                  "y": 501,
                  "x": 960,
                  "u": "https://preview.redd.it/c912if06ihgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=92ec9da33b1a08b6034f3171e8faa00762fd7481"
                },
                {
                  "y": 564,
                  "x": 1080,
                  "u": "https://preview.redd.it/c912if06ihgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6d57dcde18a0f4d23603ed9601ab172146782eb7"
                }
              ],
              "s": {
                "y": 944,
                "x": 1806,
                "u": "https://preview.redd.it/c912if06ihgf1.png?width=1806&amp;format=png&amp;auto=webp&amp;s=4900dfaf788170c984a4bfd7ca40290c7661f412"
              },
              "id": "c912if06ihgf1"
            }
          },
          "name": "t3_1mfa9nd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/sItMVC2C5VBzzKd5fuWtlYdcyEgIb41CnqwNAfSjkXE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754087378,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone with experience using Lambda chat know why DeepSeek R1 Distill Llama 3.3 70B gets fixated on questions I asked earlier in the thread and unable to recognized new questions? Just keeps providing the same reasoning it gave for an earlier answer.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/c912if06ihgf1.png?width=1806&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4900dfaf788170c984a4bfd7ca40290c7661f412\"&gt;https://preview.redd.it/c912if06ihgf1.png?width=1806&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4900dfaf788170c984a4bfd7ca40290c7661f412&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfa9nd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thick-Connection5549",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfa9nd/lambda_chat_odd_outputs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfa9nd/lambda_chat_odd_outputs/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754087378,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**System:** Threadripper Pro 3945WX &amp; RTX 4090 + 128GB system RAM\n\n**Inference engine:**  recent build of ik\\_llama.cpp in an LXC under proxmox *(with -DGGML\\_CUDA=ON -DGGML\\_CUDA\\_FA\\_ALL\\_QUANTS=ON -DGGML\\_BLAS=OFF -DCMAKE\\_CUDA\\_ARCHITECTURES=89 -DGGML\\_IQK\\_FA\\_ALL\\_QUANTS=1 -DGGML\\_SCHED\\_MAX\\_COPIES=1 -DGGML\\_CUDA\\_IQK\\_FORCE\\_BF16=1 -DGGML\\_MAX\\_CONTEXTS=2048)*\n\n**Model:** [unsloth](https://huggingface.co/unsloth)/[Qwen3-Coder-30B-A3B-Instruct-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF) Q5\\_K\\_M\n\n**llama-server arguments:** \\-fa -fmoe --metrics --n-gpu-layers 99 --override-tensor exps=CPU\n\n(though I understand -ngl and -ot  are not strictly necessary as this model fits in 24GB VRAM and removing these arguments stil results in the same situation as below)\n\nThe model runs fast (though not quite as fast as a 5090 running same prompt in Ollama in a windows machine) so I assume it is running on 4090. But when I actually look at what is happenig in the system I cant make sense of what the hardware is doing:\n\n1. The llama-server output seems to indicate NO layers are being offloaded to GPU\n2. nvidia-smi appears to show less than 6GB VRAM ustilised\n3. proxmox shows my CPU at 60% useage but only 555MB of system RAM utilised.\n\nSo where is the actual 'work' being done, by whom and with what resources when I've sent a prompt to the model?\n\nhttps://preview.redd.it/77ei5ozrdhgf1.png?width=1424&amp;format=png&amp;auto=webp&amp;s=51988beff5df7d92551b5ea589d0269bf1495de9\n\nhttps://preview.redd.it/f2rbytyrdhgf1.png?width=834&amp;format=png&amp;auto=webp&amp;s=c5e9a47c3795a7778617960a0332915e11175738\n\nhttps://preview.redd.it/q25p3syrdhgf1.png?width=689&amp;format=png&amp;auto=webp&amp;s=6bf259369ba31c2d505f978975fe5b3fa3b6b3b9",
          "author_fullname": "t2_if95iuzc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I know how much my GPU/CPU is being used by ik_llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "q25p3syrdhgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 50,
                  "x": 108,
                  "u": "https://preview.redd.it/q25p3syrdhgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=30cfee213aeafa0c0832df866c5f8e2433710dd3"
                },
                {
                  "y": 100,
                  "x": 216,
                  "u": "https://preview.redd.it/q25p3syrdhgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=60ace2e74c81147096a4c092a139de0e010d9f3a"
                },
                {
                  "y": 149,
                  "x": 320,
                  "u": "https://preview.redd.it/q25p3syrdhgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=41575a41590519a2549146d429c0af0fed4ab9c9"
                },
                {
                  "y": 299,
                  "x": 640,
                  "u": "https://preview.redd.it/q25p3syrdhgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d187a40dd06756ab6f7129dec9c2b9dea5e10606"
                }
              ],
              "s": {
                "y": 322,
                "x": 689,
                "u": "https://preview.redd.it/q25p3syrdhgf1.png?width=689&amp;format=png&amp;auto=webp&amp;s=6bf259369ba31c2d505f978975fe5b3fa3b6b3b9"
              },
              "id": "q25p3syrdhgf1"
            },
            "77ei5ozrdhgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 46,
                  "x": 108,
                  "u": "https://preview.redd.it/77ei5ozrdhgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9daee453bc912d747a1f4b4d939a9bd137acea4a"
                },
                {
                  "y": 93,
                  "x": 216,
                  "u": "https://preview.redd.it/77ei5ozrdhgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab5d82e84323811d4bdc79e032ba85cd1af22659"
                },
                {
                  "y": 138,
                  "x": 320,
                  "u": "https://preview.redd.it/77ei5ozrdhgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6bb740f5a107044d66abe3ce716d030f43abc24"
                },
                {
                  "y": 276,
                  "x": 640,
                  "u": "https://preview.redd.it/77ei5ozrdhgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=92d25f5df6008fffe9f151d749611b16ceeda7ad"
                },
                {
                  "y": 414,
                  "x": 960,
                  "u": "https://preview.redd.it/77ei5ozrdhgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=be12f05f4d725aa5da225b4aaf406de451964ddd"
                },
                {
                  "y": 466,
                  "x": 1080,
                  "u": "https://preview.redd.it/77ei5ozrdhgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf30b63c5714cf9a004202c442393d389773dfdd"
                }
              ],
              "s": {
                "y": 615,
                "x": 1424,
                "u": "https://preview.redd.it/77ei5ozrdhgf1.png?width=1424&amp;format=png&amp;auto=webp&amp;s=51988beff5df7d92551b5ea589d0269bf1495de9"
              },
              "id": "77ei5ozrdhgf1"
            },
            "f2rbytyrdhgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 49,
                  "x": 108,
                  "u": "https://preview.redd.it/f2rbytyrdhgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4450dbbc6856d5daca1f1fc8dd29cb393b2522ab"
                },
                {
                  "y": 98,
                  "x": 216,
                  "u": "https://preview.redd.it/f2rbytyrdhgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a2e42c546b733640d1e56fc8678c8ead80c8aa2e"
                },
                {
                  "y": 146,
                  "x": 320,
                  "u": "https://preview.redd.it/f2rbytyrdhgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdc95c5b92f6e28e51ce694cc5bbd2cb45e96467"
                },
                {
                  "y": 293,
                  "x": 640,
                  "u": "https://preview.redd.it/f2rbytyrdhgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fb69f354c1790297b3df2140717acdb87c90c7a4"
                }
              ],
              "s": {
                "y": 382,
                "x": 834,
                "u": "https://preview.redd.it/f2rbytyrdhgf1.png?width=834&amp;format=png&amp;auto=webp&amp;s=c5e9a47c3795a7778617960a0332915e11175738"
              },
              "id": "f2rbytyrdhgf1"
            }
          },
          "name": "t3_1mfa5nv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=bb5e71525389d3b53f19ec775a25ce04738c2fdf",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1754087090,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;System:&lt;/strong&gt; Threadripper Pro 3945WX &amp;amp; RTX 4090 + 128GB system RAM&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Inference engine:&lt;/strong&gt;  recent build of ik_llama.cpp in an LXC under proxmox &lt;em&gt;(with -DGGML_CUDA=ON -DGGML_CUDA_FA_ALL_QUANTS=ON -DGGML_BLAS=OFF -DCMAKE_CUDA_ARCHITECTURES=89 -DGGML_IQK_FA_ALL_QUANTS=1 -DGGML_SCHED_MAX_COPIES=1 -DGGML_CUDA_IQK_FORCE_BF16=1 -DGGML_MAX_CONTEXTS=2048)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href=\"https://huggingface.co/unsloth\"&gt;unsloth&lt;/a&gt;/&lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF\"&gt;Qwen3-Coder-30B-A3B-Instruct-GGUF&lt;/a&gt; Q5_K_M&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;llama-server arguments:&lt;/strong&gt; -fa -fmoe --metrics --n-gpu-layers 99 --override-tensor exps=CPU&lt;/p&gt;\n\n&lt;p&gt;(though I understand -ngl and -ot  are not strictly necessary as this model fits in 24GB VRAM and removing these arguments stil results in the same situation as below)&lt;/p&gt;\n\n&lt;p&gt;The model runs fast (though not quite as fast as a 5090 running same prompt in Ollama in a windows machine) so I assume it is running on 4090. But when I actually look at what is happenig in the system I cant make sense of what the hardware is doing:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The llama-server output seems to indicate NO layers are being offloaded to GPU&lt;/li&gt;\n&lt;li&gt;nvidia-smi appears to show less than 6GB VRAM ustilised&lt;/li&gt;\n&lt;li&gt;proxmox shows my CPU at 60% useage but only 555MB of system RAM utilised.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So where is the actual &amp;#39;work&amp;#39; being done, by whom and with what resources when I&amp;#39;ve sent a prompt to the model?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/77ei5ozrdhgf1.png?width=1424&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51988beff5df7d92551b5ea589d0269bf1495de9\"&gt;https://preview.redd.it/77ei5ozrdhgf1.png?width=1424&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51988beff5df7d92551b5ea589d0269bf1495de9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/f2rbytyrdhgf1.png?width=834&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5e9a47c3795a7778617960a0332915e11175738\"&gt;https://preview.redd.it/f2rbytyrdhgf1.png?width=834&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5e9a47c3795a7778617960a0332915e11175738&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/q25p3syrdhgf1.png?width=689&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bf259369ba31c2d505f978975fe5b3fa3b6b3b9\"&gt;https://preview.redd.it/q25p3syrdhgf1.png?width=689&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bf259369ba31c2d505f978975fe5b3fa3b6b3b9&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA.png?auto=webp&amp;s=e56082d18db2b9b44c9a8404db67a6a0159b5aaa",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=305a70e8c82e5c0a94fb3ba2ee9df26c9b46914f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb27b19d48faec1a1b9eb8d5977c1c5dc9b60ce9",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17894ebb2ab4b6a2595f8ef54d10ed9c6f3670cb",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=980118277fff46b9a8e1b486d83ba01a5045e9a9",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e2f5464545b7a0e8b1172bf0c91182a19e11edf3",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f9074f9f7d7985d6799aab5078f32476394a2e67",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "rwedkgKC292WXtVkRTFrnQdmEFp-chPjwmYAiGsq2kA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfa5nv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "munkiemagik",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfa5nv/how_do_i_know_how_much_my_gpucpu_is_being_used_by/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfa5nv/how_do_i_know_how_much_my_gpucpu_is_being_used_by/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754087090,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct (Foundation-Sec-8B-Instruct) is an open-weight, 8-billion parameter instruction-tuned language model specialized for cybersecurity applications. It extends the Foundation-Sec-8B base model with instruction-following capabilities. It leverages prior training to understand security concepts, terminology, and practices across multiple security domains. Further instruction-tuning allows the model to interact with human users in a chat-like interface. Foundation-Sec-8B-Instruct enables organizations to build AI-driven security tools that can be deployed locally, reducing dependency on cloud-based AI services while maintaining high performance on security-related tasks.\n\n# [](https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct#intended-use-cases)Intended Use Cases\n\nFoundation-Sec-8B-Instruct is designed for security practitioners, researchers, and developers building AI-powered security workflows and applications. Foundation-Sec-8B-Instruct is optimized for three core use case categories:\n\n* **SOC Acceleration**: Automating triage, summarization, case note generation, and evidence collection.\n* **Proactive Threat Defense**: Simulating attacks, prioritizing vulnerabilities, mapping TTPs, and modeling attacker behavior.\n* **Engineering Enablement**: Providing security assistance, validating configurations, assessing compliance evidence, and improving security posture.\n\nThe model is intended for local deployment in environments prioritizing data security, regulatory compliance, and operational control.",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Foundation-Sec-8B-Instruct (from Cisco Foundation AI)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1meohe5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": "#bbbdbf",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=f3b6927054c79a6b9c320f5a873b7e51197b6d20",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754027417,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct (Foundation-Sec-8B-Instruct) is an open-weight, 8-billion parameter instruction-tuned language model specialized for cybersecurity applications. It extends the Foundation-Sec-8B base model with instruction-following capabilities. It leverages prior training to understand security concepts, terminology, and practices across multiple security domains. Further instruction-tuning allows the model to interact with human users in a chat-like interface. Foundation-Sec-8B-Instruct enables organizations to build AI-driven security tools that can be deployed locally, reducing dependency on cloud-based AI services while maintaining high performance on security-related tasks.&lt;/p&gt;\n\n&lt;h1&gt;&lt;a href=\"https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct#intended-use-cases\"&gt;&lt;/a&gt;Intended Use Cases&lt;/h1&gt;\n\n&lt;p&gt;Foundation-Sec-8B-Instruct is designed for security practitioners, researchers, and developers building AI-powered security workflows and applications. Foundation-Sec-8B-Instruct is optimized for three core use case categories:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;SOC Acceleration&lt;/strong&gt;: Automating triage, summarization, case note generation, and evidence collection.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Proactive Threat Defense&lt;/strong&gt;: Simulating attacks, prioritizing vulnerabilities, mapping TTPs, and modeling attacker behavior.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Engineering Enablement&lt;/strong&gt;: Providing security assistance, validating configurations, assessing compliance evidence, and improving security posture.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The model is intended for local deployment in environments prioritizing data security, regulatory compliance, and operational control.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?auto=webp&amp;s=4b50bc8534f794cfb721f5e7a56e0988e9706425",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1655f1e0ba75303138e6971e05ac5a664c40495",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b4b11101e28ed4469beb02c5f388a2796fcd2def",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=72106dc391fd436ddabdac884da6a7960faf8885",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e2dea2312ffbb767f3df65e80cd1d35836547959",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6dcba500e84789da13556b719aa7a79e8dc51cd6",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2ff14c7b4bd7712cf611e33e443b5d59a8e15418",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1meohe5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1meohe5/foundationsec8binstruct_from_cisco_foundation_ai/",
          "stickied": false,
          "url": "https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct",
          "subreddit_subscribers": 508541,
          "created_utc": 1754027417,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recently got into open source LLMs,I have now used a lot of models under 4b on my mobile and it runs gemma 2b (4bit medium) or llama 3.2 3b (4b med) reliably on pocketpal app\n\nTotal cpu threads on my device is 8 (4 core),when I enable 1 cpu thread the 2b model generates around 3 times faster tk/s than at 6 cpu threads\n\n1.do less cpu threads degrade the output quality?\n\n2.does it increase the hallucination rate? Most of the time,I m not really looking for longer context than 2k\n\n3.what do lower cpu threads enabled help in?",
          "author_fullname": "t2_63nhk1l7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Question about cpu threads (beginner here)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1meze5n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754061823,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently got into open source LLMs,I have now used a lot of models under 4b on my mobile and it runs gemma 2b (4bit medium) or llama 3.2 3b (4b med) reliably on pocketpal app&lt;/p&gt;\n\n&lt;p&gt;Total cpu threads on my device is 8 (4 core),when I enable 1 cpu thread the 2b model generates around 3 times faster tk/s than at 6 cpu threads&lt;/p&gt;\n\n&lt;p&gt;1.do less cpu threads degrade the output quality?&lt;/p&gt;\n\n&lt;p&gt;2.does it increase the hallucination rate? Most of the time,I m not really looking for longer context than 2k&lt;/p&gt;\n\n&lt;p&gt;3.what do lower cpu threads enabled help in?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1meze5n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Gold_Bar_4072",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1meze5n/question_about_cpu_threads_beginner_here/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1meze5n/question_about_cpu_threads_beginner_here/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754061823,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've combined llama.cpp CUDA results in a single place. Fill free to add and share!",
          "author_fullname": "t2_1umam7ln",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLama.cpp on CUDA performance",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mezdl4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=d99b9ef119d65a8da47f74caa52d3c89b0b21c5a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754061785,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve combined llama.cpp CUDA results in a single place. Fill free to add and share!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/discussions/15013",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk.png?auto=webp&amp;s=d05fc23d7b4197a6434808392495399c877ddb61",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c6d436fd4dbc8296467786571735a612b521bd39",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a947aa945637e90a5a9ec08a9bd9e027b8cb98e7",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fd4307b3c3dff86f984a05fb0c866f66292991e9",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=35c0244e69e083ef09fd0ecf92ea27b64f32878c",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6d56ac1d6186b02e0cffa3db5452d10a42ce61f0",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5c4c5e35f0ce27c442b9f87d385ef0615b7c6747",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "f2FW9wXiXsDsg6ZM-lErgQ-ASid_pWMTp9znNhapydk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mezdl4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "COBECT",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mezdl4/llamacpp_on_cuda_performance/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/discussions/15013",
          "subreddit_subscribers": 508541,
          "created_utc": 1754061785,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's definitely OpenAI's upcoming \"open-source\" model.",
          "author_fullname": "t2_6h87m4sy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "\"Horizon Alpha\" hides its thinking",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1megpco",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "ups": 59,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 59,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/YZRXuBCLq1UiB4iPw3luGykiQBe0vKGt-Ol6iZcAcOc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754003932,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s definitely OpenAI&amp;#39;s upcoming &amp;quot;open-source&amp;quot; model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ewdetoz7magf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ewdetoz7magf1.jpeg?auto=webp&amp;s=d22df1a9f5442fa65616fa60bcc618fe103abfd3",
                  "width": 1440,
                  "height": 2451
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ewdetoz7magf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1fa5a86d9494123d5af599968bcf9f3c2ab876ea",
                    "width": 108,
                    "height": 183
                  },
                  {
                    "url": "https://preview.redd.it/ewdetoz7magf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6a12799a98b113b668b416ead4c6657f2e513d43",
                    "width": 216,
                    "height": 367
                  },
                  {
                    "url": "https://preview.redd.it/ewdetoz7magf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=78e4cb7a23c2256ab7236cab317d291a84ceea5f",
                    "width": 320,
                    "height": 544
                  },
                  {
                    "url": "https://preview.redd.it/ewdetoz7magf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=82118360c2fd679d040ecec1fa5221540c1f86aa",
                    "width": 640,
                    "height": 1089
                  },
                  {
                    "url": "https://preview.redd.it/ewdetoz7magf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2101a03dc77b4ac893969543bab1e288bf5e4134",
                    "width": 960,
                    "height": 1634
                  },
                  {
                    "url": "https://preview.redd.it/ewdetoz7magf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ad243f241edc3743b06b9b9255b9f18416b2ded1",
                    "width": 1080,
                    "height": 1838
                  }
                ],
                "variants": {},
                "id": "EB5kQaMVkF2acjk5J_bNzBPaMmm0HRdSNipm28fxUMI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1megpco",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ICYPhoenix7",
          "discussion_type": null,
          "num_comments": 37,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1megpco/horizon_alpha_hides_its_thinking/",
          "stickied": false,
          "url": "https://i.redd.it/ewdetoz7magf1.jpeg",
          "subreddit_subscribers": 508541,
          "created_utc": 1754003932,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**has anyone actually gotten rag + ocr to work w/ local llama?**  \n\nlike actually work — not just “no errors in pipeline”, but \\*no hallucinations\\*, no layout drift, and no vector match mismatches?\n\ni’ve spent the past few months building a rag stack around scanned docs, multilingual pdfs, image-based tables ～ the usual ocr hell.\n\ntried everything:\n\n**- langchain’s pdfloader / unstructured.io / docsplit**  \n**- tesseract w/ layout detection (works great until it doesn’t)**  \n**- even tried some vision-based embedding tricks**\n\nand still the same pain:  \n\nretrieval grabs the wrong chunk, diagrams split in half, hidden headers nuke semantic flow.    \nembedding vectors look close, but the model answers completely wrong.\n\n**so i mapped out 16+ failure modes and patched each one — fully documented, tested, MIT licensed.  no model finetuning, no hacky routing. just logic fixes.**\n\n@@@ full breakdown with solution:    \n[https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md](https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md)\n\n@@@　even got a star from the guy who made tesseract.js     \n[https://github.com/bijection?tab=stars](https://github.com/bijection?tab=stars)  (my repo’s pinned right at the top 1 now)\n\nif you’re building local llama +rag + ocr, this might save you weeks of silent hallucinations.\n\nit’s MIT open source. ask me anything.\n\n",
          "author_fullname": "t2_1tgp8l87vk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "retrieval works, embedding matches... but the answer is wrong. anyone else?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mez1w0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754061049,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;has anyone actually gotten rag + ocr to work w/ local llama?&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;like actually work — not just “no errors in pipeline”, but *no hallucinations*, no layout drift, and no vector match mismatches?&lt;/p&gt;\n\n&lt;p&gt;i’ve spent the past few months building a rag stack around scanned docs, multilingual pdfs, image-based tables ～ the usual ocr hell.&lt;/p&gt;\n\n&lt;p&gt;tried everything:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;- langchain’s pdfloader / unstructured.io / docsplit&lt;/strong&gt;&lt;br/&gt;\n&lt;strong&gt;- tesseract w/ layout detection (works great until it doesn’t)&lt;/strong&gt;&lt;br/&gt;\n&lt;strong&gt;- even tried some vision-based embedding tricks&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;and still the same pain:  &lt;/p&gt;\n\n&lt;p&gt;retrieval grabs the wrong chunk, diagrams split in half, hidden headers nuke semantic flow.&lt;br/&gt;\nembedding vectors look close, but the model answers completely wrong.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;so i mapped out 16+ failure modes and patched each one — fully documented, tested, MIT licensed.  no model finetuning, no hacky routing. just logic fixes.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;@@@ full breakdown with solution:&lt;br/&gt;\n&lt;a href=\"https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md\"&gt;https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;@@@　even got a star from the guy who made tesseract.js&lt;br/&gt;\n&lt;a href=\"https://github.com/bijection?tab=stars\"&gt;https://github.com/bijection?tab=stars&lt;/a&gt;  (my repo’s pinned right at the top 1 now)&lt;/p&gt;\n\n&lt;p&gt;if you’re building local llama +rag + ocr, this might save you weeks of silent hallucinations.&lt;/p&gt;\n\n&lt;p&gt;it’s MIT open source. ask me anything.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/_ftOpifK9HlOdqBK_NMClSw-o8JKhGaAFmjrk0dHLNY.png?auto=webp&amp;s=dc7a0ad87990e23993a8d2852e51a9d71e3a90c0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/_ftOpifK9HlOdqBK_NMClSw-o8JKhGaAFmjrk0dHLNY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=617c058204217a66a5b4717a80a3b70d99e9300a",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/_ftOpifK9HlOdqBK_NMClSw-o8JKhGaAFmjrk0dHLNY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9e4598e9cb62d922eb16b3aee782e97664c5cd8",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/_ftOpifK9HlOdqBK_NMClSw-o8JKhGaAFmjrk0dHLNY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cc3cd987a0adf3abc9b4f9c4a86d588200852c21",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/_ftOpifK9HlOdqBK_NMClSw-o8JKhGaAFmjrk0dHLNY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9c92aad377d70b6eff9b5b9dd57f0239ce2d34b5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/_ftOpifK9HlOdqBK_NMClSw-o8JKhGaAFmjrk0dHLNY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0ea175b00a1b31d42b76d938c1c90668a369176d",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/_ftOpifK9HlOdqBK_NMClSw-o8JKhGaAFmjrk0dHLNY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=32e2eec869943bb86d1d19807d4e637b4e104ee1",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "_ftOpifK9HlOdqBK_NMClSw-o8JKhGaAFmjrk0dHLNY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mez1w0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "wfgy_engine",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mez1w0/retrieval_works_embedding_matches_but_the_answer/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mez1w0/retrieval_works_embedding_matches_but_the_answer/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754061049,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_l10vk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD EPYC 4545P: 16 Zen 5 Cores @ 65 Watts For Low-Power / Energy Efficient Servers",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mewg8a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754054772,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "phoronix.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.phoronix.com/review/amd-epyc-4545p/3",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mewg8a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nostriluu",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mewg8a/amd_epyc_4545p_16_zen_5_cores_65_watts_for/",
          "stickied": false,
          "url": "https://www.phoronix.com/review/amd-epyc-4545p/3",
          "subreddit_subscribers": 508541,
          "created_utc": 1754054772,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "How to run Qwen3 Coder 30B-A3B the fastest?\n\nI want to switch from using claude code to running this model locally via kilo code r other similar extensions.\n\nMy Laptop's specs are:\ni7-8850H with 64GB DDR4 RAM. \nNvidia quadro P5200 laptop GPU with 16GB GDDR6 VRAM.\n\nI got confused as there are a lot of inference engines available such as Ollama, llama.cpp, vLLM, sglang, ik_llama.cpp etc. i dont know why there are som many of these and what are their pros and cons. So i wanted to ask here. I need the absolute fastest responses possible, i don't mind installing niche software or other things. \n\nThank you in advance.",
          "author_fullname": "t2_1phmkwqhgo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Faster token generation using qwen coder 30B A3B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1meyn4a",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754060114,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How to run Qwen3 Coder 30B-A3B the fastest?&lt;/p&gt;\n\n&lt;p&gt;I want to switch from using claude code to running this model locally via kilo code r other similar extensions.&lt;/p&gt;\n\n&lt;p&gt;My Laptop&amp;#39;s specs are:\ni7-8850H with 64GB DDR4 RAM. \nNvidia quadro P5200 laptop GPU with 16GB GDDR6 VRAM.&lt;/p&gt;\n\n&lt;p&gt;I got confused as there are a lot of inference engines available such as Ollama, llama.cpp, vLLM, sglang, ik_llama.cpp etc. i dont know why there are som many of these and what are their pros and cons. So i wanted to ask here. I need the absolute fastest responses possible, i don&amp;#39;t mind installing niche software or other things. &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1meyn4a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "prathode",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1meyn4a/faster_token_generation_using_qwen_coder_30b_a3b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1meyn4a/faster_token_generation_using_qwen_coder_30b_a3b/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754060114,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everybody, I was wondering how to add knowledge for generating recent suggestions for a given Python library to the Qwen3-coder. Are there any ways to add the new SDK or docs to the Qwen3-coder? I was thinking of gluing cline-ollama-some new docs on the Python library. Are there some kind of RAG or similar techniques to enrich the model's knowledge?",
          "author_fullname": "t2_2h97wo38",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to add most recent python library documentation?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf7q94",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754081056,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everybody, I was wondering how to add knowledge for generating recent suggestions for a given Python library to the Qwen3-coder. Are there any ways to add the new SDK or docs to the Qwen3-coder? I was thinking of gluing cline-ollama-some new docs on the Python library. Are there some kind of RAG or similar techniques to enrich the model&amp;#39;s knowledge?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf7q94",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arm2armreddit",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf7q94/how_to_add_most_recent_python_library/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf7q94/how_to_add_most_recent_python_library/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754081056,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was wondering if there were any good tts models that had voice cloning that I could use on an amd card.",
          "author_fullname": "t2_wxqsxor3g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Voice cloning on amd",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf7lyi",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754080763,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering if there were any good tts models that had voice cloning that I could use on an amd card.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf7lyi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Shady_Shin009",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf7lyi/voice_cloning_on_amd/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf7lyi/voice_cloning_on_amd/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754080763,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": " ",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Everyone from r/LocalLLama refreshing Hugging Face every 5 minutes today looking for GLM-4.5 GGUFs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 91,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdykfn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 436,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 436,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/JtrsDuYApU5asaj4DMkYR46jMGTULVF74_jrKEDuzNY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753959873,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/f5iqhqp7z6gf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/f5iqhqp7z6gf1.jpeg?auto=webp&amp;s=9ebc8183abfedb5f08028da2d763991ae8501002",
                  "width": 593,
                  "height": 389
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/f5iqhqp7z6gf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e3bc13cc7709787b1633c87ce4deec12ada0949",
                    "width": 108,
                    "height": 70
                  },
                  {
                    "url": "https://preview.redd.it/f5iqhqp7z6gf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe549fcca1b5049e06c0516847a12686c2f98338",
                    "width": 216,
                    "height": 141
                  },
                  {
                    "url": "https://preview.redd.it/f5iqhqp7z6gf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=80da4073073fb12cdbab3b110619a3002d524b2f",
                    "width": 320,
                    "height": 209
                  }
                ],
                "variants": {},
                "id": "W6UmrcA-BG24HiTaK1cat2L9eGxll0ba_uZjbLzyRHA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mdykfn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 86,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdykfn/everyone_from_rlocalllama_refreshing_hugging_face/",
          "stickied": false,
          "url": "https://i.redd.it/f5iqhqp7z6gf1.jpeg",
          "subreddit_subscribers": 508541,
          "created_utc": 1753959873,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m running into a frustrating problem and would appreciate any help! I’m trying to use Qwen3-Coder locally with LM Studio as the backend, integrated with the [Continue.dev](http://Continue.dev) extension in VSCode. My setup:\n\n* **LM Studio** (latest)\n* **Qwen3-Coder** (latest GGUF from Unsloth’s Hugging Face repo)\n* [**Continue.dev**](http://Continue.dev) extension for VSCode\n\n**The Issue:**  \nEvery time I try to use [Continue.dev](http://Continue.dev) with the model using agent mode, I get a `500 Internal Server Error`. The only thing in the logs is:\n\n    text500 &lt;!DOCTYPE html&gt;\n    &lt;html lang=\"en\"&gt;\n    &lt;head&gt;\n    &lt;meta charset=\"utf-8\"&gt;\n    &lt;title&gt;Error&lt;/title&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n    &lt;pre&gt;Internal Server Error&lt;/pre&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \n\n**What I’ve Tried (based on Unsloth docs):**\n\n* Downloaded the latest GGUF model file with tool-calling fix from Unsloth’s Hugging Face.\n* Updated LM Studio and verified it’s on the newest llama.cpp.\n* Set the chat template in LM Studio to the`.jinja` file as recommended.\n* Set recommended inference parameters (temp, top\\_p, etc.).\n* Restarted LM Studio and VSCode after every change.\n* Lowered GPU layers and checked context/window size.\n* Verified all paths and settings several times.\n\n**Despite all of this, I’m still getting the 500 error,** making me think there’s either a subtle misconfiguration or a deeper bug with Qwen3-Coder, LM Studio, or Continue.\n\n**Things I’d love help with:**\n\n* Is there anything else I should configure in LM Studio or [Continue.dev](http://Continue.dev) to support the fixed Unsloth Qwen3-Coder?\n* Has anyone else gotten tool-calling and chat to work with this stack, and if so, how?\n* Any specific versions or known working combinations?\n* Details about your working settings, GGUF quant, jinja template, special flags, etc. would be amazing!\n\n**System Info:**\n\n* LM Studio version: 0.3.20\n* Qwen3-Coder quant: 4, 5, 6 (tried)\n* [Continue.dev](http://Continue.dev) version: 1.0.19\n\nAny help or working configs/examples are much appreciated! If I resolve it, I’ll update this post with my findings so others don’t get stuck. Thanks in advance!",
          "author_fullname": "t2_mlw2lx22",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help: Qwen3-Coder + LM Studio + Continue.dev (VSCode) + Mac 64GB M3 Max  — 500 Internal Server Error, Even After Unsloth Fix",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf0fgj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754064185,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m running into a frustrating problem and would appreciate any help! I’m trying to use Qwen3-Coder locally with LM Studio as the backend, integrated with the &lt;a href=\"http://Continue.dev\"&gt;Continue.dev&lt;/a&gt; extension in VSCode. My setup:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;LM Studio&lt;/strong&gt; (latest)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Qwen3-Coder&lt;/strong&gt; (latest GGUF from Unsloth’s Hugging Face repo)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"http://Continue.dev\"&gt;&lt;strong&gt;Continue.dev&lt;/strong&gt;&lt;/a&gt; extension for VSCode&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;The Issue:&lt;/strong&gt;&lt;br/&gt;\nEvery time I try to use &lt;a href=\"http://Continue.dev\"&gt;Continue.dev&lt;/a&gt; with the model using agent mode, I get a &lt;code&gt;500 Internal Server Error&lt;/code&gt;. The only thing in the logs is:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;text500 &amp;lt;!DOCTYPE html&amp;gt;\n&amp;lt;html lang=&amp;quot;en&amp;quot;&amp;gt;\n&amp;lt;head&amp;gt;\n&amp;lt;meta charset=&amp;quot;utf-8&amp;quot;&amp;gt;\n&amp;lt;title&amp;gt;Error&amp;lt;/title&amp;gt;\n&amp;lt;/head&amp;gt;\n&amp;lt;body&amp;gt;\n&amp;lt;pre&amp;gt;Internal Server Error&amp;lt;/pre&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;What I’ve Tried (based on Unsloth docs):&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Downloaded the latest GGUF model file with tool-calling fix from Unsloth’s Hugging Face.&lt;/li&gt;\n&lt;li&gt;Updated LM Studio and verified it’s on the newest llama.cpp.&lt;/li&gt;\n&lt;li&gt;Set the chat template in LM Studio to the&lt;code&gt;.jinja&lt;/code&gt; file as recommended.&lt;/li&gt;\n&lt;li&gt;Set recommended inference parameters (temp, top_p, etc.).&lt;/li&gt;\n&lt;li&gt;Restarted LM Studio and VSCode after every change.&lt;/li&gt;\n&lt;li&gt;Lowered GPU layers and checked context/window size.&lt;/li&gt;\n&lt;li&gt;Verified all paths and settings several times.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Despite all of this, I’m still getting the 500 error,&lt;/strong&gt; making me think there’s either a subtle misconfiguration or a deeper bug with Qwen3-Coder, LM Studio, or Continue.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Things I’d love help with:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is there anything else I should configure in LM Studio or &lt;a href=\"http://Continue.dev\"&gt;Continue.dev&lt;/a&gt; to support the fixed Unsloth Qwen3-Coder?&lt;/li&gt;\n&lt;li&gt;Has anyone else gotten tool-calling and chat to work with this stack, and if so, how?&lt;/li&gt;\n&lt;li&gt;Any specific versions or known working combinations?&lt;/li&gt;\n&lt;li&gt;Details about your working settings, GGUF quant, jinja template, special flags, etc. would be amazing!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;System Info:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;LM Studio version: 0.3.20&lt;/li&gt;\n&lt;li&gt;Qwen3-Coder quant: 4, 5, 6 (tried)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"http://Continue.dev\"&gt;Continue.dev&lt;/a&gt; version: 1.0.19&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any help or working configs/examples are much appreciated! If I resolve it, I’ll update this post with my findings so others don’t get stuck. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf0fgj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mountain_Desk_767",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf0fgj/help_qwen3coder_lm_studio_continuedev_vscode_mac/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf0fgj/help_qwen3coder_lm_studio_continuedev_vscode_mac/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754064185,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_9i1ld",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "An Experiment in Logit Control: Using Statistical \"Constraint Masks\" to Guide Token Selection",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 92,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf07dy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/4Z9-NAqtbqtnqebZUIjxu77W_oURHIoXd1CTVK0ApLk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754063698,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/442znbdvjfgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/442znbdvjfgf1.png?auto=webp&amp;s=067543edf5b573e79e55dada76d1b42024878b94",
                  "width": 5412,
                  "height": 3559
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/442znbdvjfgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=246430f723d9c8ce03b5b5c8131885dfecbf19ca",
                    "width": 108,
                    "height": 71
                  },
                  {
                    "url": "https://preview.redd.it/442znbdvjfgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f4dfe6b3711dbf6fedff94639e75ae169d09a4c",
                    "width": 216,
                    "height": 142
                  },
                  {
                    "url": "https://preview.redd.it/442znbdvjfgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4beeb14654062e663fb168617969c05283c849c1",
                    "width": 320,
                    "height": 210
                  },
                  {
                    "url": "https://preview.redd.it/442znbdvjfgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4851b5f9cc19d50904b48e98b0aa7852b3af4751",
                    "width": 640,
                    "height": 420
                  },
                  {
                    "url": "https://preview.redd.it/442znbdvjfgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=faf607578daa5b30cc0f103b1eb14332d6b17c23",
                    "width": 960,
                    "height": 631
                  },
                  {
                    "url": "https://preview.redd.it/442znbdvjfgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=06973bc0f157c93cd74d0450f25bf38c4344d8cc",
                    "width": 1080,
                    "height": 710
                  }
                ],
                "variants": {},
                "id": "faYYXvJxRspMDkoFqA01LcpfUOA3UJYunCqTwB8eSl4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mf07dy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vesudeva",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf07dy/an_experiment_in_logit_control_using_statistical/",
          "stickied": false,
          "url": "https://i.redd.it/442znbdvjfgf1.png",
          "subreddit_subscribers": 508541,
          "created_utc": 1754063698,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm running into a performance issue with a self-hosted agent and could use some help. I've successfully set up an agent system, but the inference is extremely slow because it's only using the CPU.\n\n**My Setup:**\n\n* **Model:** Qwen3-Coder-480B-A35B-Instruct-GGUF (Q8\\_0 quant from unsloth)\n* **Hardware:** RunPod with RTX 5090 (32GB VRAM), 32 vCPU, 125GB RAM\n* **Backend:** Latest llama.cpp compiled from source, using the llama-server binary.\n* **Agent:** A simple Python script using requests to call the /completion endpoint.\n\n**The Problem:**\n\nI'm launching the server with this command:\n\n    ./llama-server --model /path/to/model.gguf --n-gpu-layers 3 -c 8192 --host 0.0.0.0 --port 8080\n\nThe server loads the model successfully, and nvidia-smi confirms that the GPU memory is used (**83% VRAM used**). However, when my agent sends a prompt and the model starts generating a response, the **GPU Utilization stays at 0-1%**, while a single CPU core is being used.\n\n**What I've already confirmed:**\n\n1. The model is loaded correctly, and layers are offloaded (offloaded 3/63 layers to GPU).\n2. The Python agent script works and correctly communicates with the server.\n3. The issue is purely that the actual token generation computation is not happening on the GPU.\n\n**My Question:**\n\nIs there a specific command-line argument for the new llama-server (like --main-gpu in the old main binary) that I'm missing to force inference to run on the GPU? Or is this a known issue/bug with recent versions of llama.cpp?\n\nAny advice would be greatly appreciated. ThanksI'm running into a performance issue with a self-hosted agent and could use some help. I've successfully set up an agent system, but the inference is extremely slow because it's only using the [CPU.My](http://CPU.My) Setup:Model: Qwen3-Coder-480B-A35B-Instruct-GGUF (Q8\\_0 quant from unsloth)  \n  \nHardware: RunPod with RTX 5090 (32GB VRAM), 32 vCPU, 125GB RAM  \n  \nBackend: Latest llama.cpp compiled from source, using the llama-server binary.  \n  \nAgent: A simple Python script using requests to call the /completion endpoint.The Problem:I'm launching the server with this command:Generated code./llama-server --model /path/to/model.gguf --n-gpu-layers 3 -c 8192 --host [0.0.0.0](http://0.0.0.0) \\--port 8080  \nUse code with caution.The server loads the model successfully, and nvidia-smi confirms that the GPU memory is used (83% VRAM used). However, when my agent sends a prompt and the model starts generating a response, the GPU Utilization stays at 0-1%, while a single CPU core is being used.What I've already confirmed:The model is loaded correctly, and layers are offloaded (offloaded 3/63 layers to GPU).  \n  \nThe Python agent script works and correctly communicates with the server.  \n  \nThe issue is purely that the actual token generation computation is not happening on the [GPU.My](http://GPU.My) Question:Is there a specific command-line argument for the new llama-server (like --main-gpu in the old main binary) that I'm missing to force inference to run on the GPU? Or is this a known issue/bug with recent versions of llama.cpp?Any advice would be greatly appreciated. Thanks",
          "author_fullname": "t2_2kxlmun5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help debugging: llama-server uses GPU Memory but 0% GPU Util for inference (CPU only)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf581n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754075077,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running into a performance issue with a self-hosted agent and could use some help. I&amp;#39;ve successfully set up an agent system, but the inference is extremely slow because it&amp;#39;s only using the CPU.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Setup:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; Qwen3-Coder-480B-A35B-Instruct-GGUF (Q8_0 quant from unsloth)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; RunPod with RTX 5090 (32GB VRAM), 32 vCPU, 125GB RAM&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; Latest llama.cpp compiled from source, using the llama-server binary.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Agent:&lt;/strong&gt; A simple Python script using requests to call the /completion endpoint.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m launching the server with this command:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-server --model /path/to/model.gguf --n-gpu-layers 3 -c 8192 --host 0.0.0.0 --port 8080\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The server loads the model successfully, and nvidia-smi confirms that the GPU memory is used (&lt;strong&gt;83% VRAM used&lt;/strong&gt;). However, when my agent sends a prompt and the model starts generating a response, the &lt;strong&gt;GPU Utilization stays at 0-1%&lt;/strong&gt;, while a single CPU core is being used.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;ve already confirmed:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The model is loaded correctly, and layers are offloaded (offloaded 3/63 layers to GPU).&lt;/li&gt;\n&lt;li&gt;The Python agent script works and correctly communicates with the server.&lt;/li&gt;\n&lt;li&gt;The issue is purely that the actual token generation computation is not happening on the GPU.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;My Question:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Is there a specific command-line argument for the new llama-server (like --main-gpu in the old main binary) that I&amp;#39;m missing to force inference to run on the GPU? Or is this a known issue/bug with recent versions of llama.cpp?&lt;/p&gt;\n\n&lt;p&gt;Any advice would be greatly appreciated. ThanksI&amp;#39;m running into a performance issue with a self-hosted agent and could use some help. I&amp;#39;ve successfully set up an agent system, but the inference is extremely slow because it&amp;#39;s only using the &lt;a href=\"http://CPU.My\"&gt;CPU.My&lt;/a&gt; Setup:Model: Qwen3-Coder-480B-A35B-Instruct-GGUF (Q8_0 quant from unsloth)  &lt;/p&gt;\n\n&lt;p&gt;Hardware: RunPod with RTX 5090 (32GB VRAM), 32 vCPU, 125GB RAM  &lt;/p&gt;\n\n&lt;p&gt;Backend: Latest llama.cpp compiled from source, using the llama-server binary.  &lt;/p&gt;\n\n&lt;p&gt;Agent: A simple Python script using requests to call the /completion endpoint.The Problem:I&amp;#39;m launching the server with this command:Generated code./llama-server --model /path/to/model.gguf --n-gpu-layers 3 -c 8192 --host &lt;a href=\"http://0.0.0.0\"&gt;0.0.0.0&lt;/a&gt; --port 8080&lt;br/&gt;\nUse code with caution.The server loads the model successfully, and nvidia-smi confirms that the GPU memory is used (83% VRAM used). However, when my agent sends a prompt and the model starts generating a response, the GPU Utilization stays at 0-1%, while a single CPU core is being used.What I&amp;#39;ve already confirmed:The model is loaded correctly, and layers are offloaded (offloaded 3/63 layers to GPU).  &lt;/p&gt;\n\n&lt;p&gt;The Python agent script works and correctly communicates with the server.  &lt;/p&gt;\n\n&lt;p&gt;The issue is purely that the actual token generation computation is not happening on the &lt;a href=\"http://GPU.My\"&gt;GPU.My&lt;/a&gt; Question:Is there a specific command-line argument for the new llama-server (like --main-gpu in the old main binary) that I&amp;#39;m missing to force inference to run on the GPU? Or is this a known issue/bug with recent versions of llama.cpp?Any advice would be greatly appreciated. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf581n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rezvord",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf581n/need_help_debugging_llamaserver_uses_gpu_memory/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf581n/need_help_debugging_llamaserver_uses_gpu_memory/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754075077,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_10vdoj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder-Flash / Qwen3-Coder-30B-A3B-Instruct-FP8 are here!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1me33jj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 188,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 188,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/W9R_cVO6oViuPZ98cKporgcMC_UhWTyqKk41sgFVoeA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753972155,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/3dn8agzjz7gf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/3dn8agzjz7gf1.jpeg?auto=webp&amp;s=2245524dec0ab2b19e862c10b790facd9b924287",
                  "width": 4000,
                  "height": 2250
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/3dn8agzjz7gf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf353fe46bbeb57399d0faf6bcb05041173ab4ba",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/3dn8agzjz7gf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9c5268a07dfe26225592693faf38a22d79c662aa",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/3dn8agzjz7gf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8666eefe3be44dc93a677a5f0394166785262730",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/3dn8agzjz7gf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f710dee399ef8cae7aa04b7396c4c8719d91fd6",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/3dn8agzjz7gf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=72098bd6ff00813904fe9fcaba82f26e0079f787",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/3dn8agzjz7gf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c598f6fddd9c6ee3388c55a9b562e27da3ba3ba2",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "lXolOXududnx-0OOTcrt4sGu-VfW9B7zjJTil_Bb0Hw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1me33jj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zRevengee",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1me33jj/qwen3coderflash_qwen3coder30ba3binstructfp8_are/",
          "stickied": false,
          "url": "https://i.redd.it/3dn8agzjz7gf1.jpeg",
          "subreddit_subscribers": 508541,
          "created_utc": 1753972155,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is this the best week ever for new models? I can't believe what we're getting. Huge shoutout to u/danielhanchen and the Unsloth team for getting the GGUFs out so fast!\n\nLLM Server is Lemonade, GitHub: [https://github.com/lemonade-sdk/lemonade](https://github.com/lemonade-sdk/lemonade)\n\nDiscord [https://discord.gg/Sf8cfBWB](https://discord.gg/Sf8cfBWB)\n\nModel: [unsloth/cogito-v2-preview-llama-109B-MoE-GGUF · Hugging Face](https://huggingface.co/unsloth/cogito-v2-preview-llama-109B-MoE-GGUF), the Q4\\_K\\_M one\n\nHardware: Strix Halo (Ryzen AI MAX 395+) with 128 GB RAM\n\nBackend: llama.cpp + vulkan\n\nApp: [Continue.dev](http://Continue.dev) extension for VS Code",
          "author_fullname": "t2_1m2ckixcqh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Here's cogito-v2-109B MoE coding Space Invaders in 1 minute on Strix Halo using Lemonade (unedited video)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 91,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mee99g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 54,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/39k2gtxw2agf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1104,
              "scrubber_media_url": "https://v.redd.it/39k2gtxw2agf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/39k2gtxw2agf1/DASHPlaylist.mpd?a=1756688985%2COTAzYWUwZTgwZmU5N2I3MzE2YzM3NzExY2Y0ZTBjOWUxNDBkYjdmZDE3ODBmMzMyYTIzMTc0OTliODkyZDEyZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 68,
              "hls_url": "https://v.redd.it/39k2gtxw2agf1/HLSPlaylist.m3u8?a=1756688985%2CMmU0NDA5OGQ0MGUxMjIzMzA0MGUyNjQ1NWFmY2Q4OTRmNDRkMDA3OTJmNDFlNDhiZTdhNzM3ZmNhZjgwMDI5MQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 54,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/M3ppMGd2eHcyYWdmMStZsfyV8F4GV6vQy52E9vmc2CGAsEmxg4Z4VgmDbWvl.png?width=140&amp;height=91&amp;crop=140:91,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=21d4633fb4807e9455477c27ddd4db6626d7b972",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753997776,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is this the best week ever for new models? I can&amp;#39;t believe what we&amp;#39;re getting. Huge shoutout to &lt;a href=\"/u/danielhanchen\"&gt;u/danielhanchen&lt;/a&gt; and the Unsloth team for getting the GGUFs out so fast!&lt;/p&gt;\n\n&lt;p&gt;LLM Server is Lemonade, GitHub: &lt;a href=\"https://github.com/lemonade-sdk/lemonade\"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Discord &lt;a href=\"https://discord.gg/Sf8cfBWB\"&gt;https://discord.gg/Sf8cfBWB&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Model: &lt;a href=\"https://huggingface.co/unsloth/cogito-v2-preview-llama-109B-MoE-GGUF\"&gt;unsloth/cogito-v2-preview-llama-109B-MoE-GGUF · Hugging Face&lt;/a&gt;, the Q4_K_M one&lt;/p&gt;\n\n&lt;p&gt;Hardware: Strix Halo (Ryzen AI MAX 395+) with 128 GB RAM&lt;/p&gt;\n\n&lt;p&gt;Backend: llama.cpp + vulkan&lt;/p&gt;\n\n&lt;p&gt;App: &lt;a href=\"http://Continue.dev\"&gt;Continue.dev&lt;/a&gt; extension for VS Code&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/39k2gtxw2agf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/M3ppMGd2eHcyYWdmMStZsfyV8F4GV6vQy52E9vmc2CGAsEmxg4Z4VgmDbWvl.png?format=pjpg&amp;auto=webp&amp;s=f82b95773d44af758611ac930fbad79416657daf",
                  "width": 1196,
                  "height": 780
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/M3ppMGd2eHcyYWdmMStZsfyV8F4GV6vQy52E9vmc2CGAsEmxg4Z4VgmDbWvl.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=703b4735f6cd2310092b4fc6927b9b41ee451c18",
                    "width": 108,
                    "height": 70
                  },
                  {
                    "url": "https://external-preview.redd.it/M3ppMGd2eHcyYWdmMStZsfyV8F4GV6vQy52E9vmc2CGAsEmxg4Z4VgmDbWvl.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e24d475c524c135573f9b2b0dd414dcb775633c0",
                    "width": 216,
                    "height": 140
                  },
                  {
                    "url": "https://external-preview.redd.it/M3ppMGd2eHcyYWdmMStZsfyV8F4GV6vQy52E9vmc2CGAsEmxg4Z4VgmDbWvl.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=eb8c20a4a14839a1e9e65d7d4d9cd9226fb6a2a5",
                    "width": 320,
                    "height": 208
                  },
                  {
                    "url": "https://external-preview.redd.it/M3ppMGd2eHcyYWdmMStZsfyV8F4GV6vQy52E9vmc2CGAsEmxg4Z4VgmDbWvl.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8bd29e4af6c5a75943cebddae22cea91a051312c",
                    "width": 640,
                    "height": 417
                  },
                  {
                    "url": "https://external-preview.redd.it/M3ppMGd2eHcyYWdmMStZsfyV8F4GV6vQy52E9vmc2CGAsEmxg4Z4VgmDbWvl.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=22ebfd65a9f3d71ba2dc2408a03963d4e025023d",
                    "width": 960,
                    "height": 626
                  },
                  {
                    "url": "https://external-preview.redd.it/M3ppMGd2eHcyYWdmMStZsfyV8F4GV6vQy52E9vmc2CGAsEmxg4Z4VgmDbWvl.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4e5044572ec3d4191f57e25070a626e21e41372c",
                    "width": 1080,
                    "height": 704
                  }
                ],
                "variants": {},
                "id": "M3ppMGd2eHcyYWdmMStZsfyV8F4GV6vQy52E9vmc2CGAsEmxg4Z4VgmDbWvl"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mee99g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jfowers_amd",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mee99g/heres_cogitov2109b_moe_coding_space_invaders_in_1/",
          "stickied": false,
          "url": "https://v.redd.it/39k2gtxw2agf1",
          "subreddit_subscribers": 508541,
          "created_utc": 1753997776,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/39k2gtxw2agf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1104,
              "scrubber_media_url": "https://v.redd.it/39k2gtxw2agf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/39k2gtxw2agf1/DASHPlaylist.mpd?a=1756688985%2COTAzYWUwZTgwZmU5N2I3MzE2YzM3NzExY2Y0ZTBjOWUxNDBkYjdmZDE3ODBmMzMyYTIzMTc0OTliODkyZDEyZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 68,
              "hls_url": "https://v.redd.it/39k2gtxw2agf1/HLSPlaylist.m3u8?a=1756688985%2CMmU0NDA5OGQ0MGUxMjIzMzA0MGUyNjQ1NWFmY2Q4OTRmNDRkMDA3OTJmNDFlNDhiZTdhNzM3ZmNhZjgwMDI5MQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks, If you're building voice (or chat) AI agents, you might find this interesting.  90% of voice AI systems fail in production, not due to bad tech but inadequate testing methods. There is an interesting webinar coming up on luma, that will show you the ultimate evaluation framework you need to know to ship Voice AI reliably. You’ll learn how to stress-test your agent on thousands of diverse scenarios, automate evaluations, handle multilingual complexity, and catch corner cases before they crash your Voice AI.\n\nCool stuff: a live demonstration of breaking and fixing a production voice agent to show the testing methodology in practice.\n\n**When:** August 7th, 9:30 AM PT  \n**Where:** Online - [https://lu.ma/ve964r2k](https://lu.ma/ve964r2k)\n\n  \nThought some of you working on voice AI might find the testing approaches useful for your own projects.",
          "author_fullname": "t2_1u5ixcjo57",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Automated Testing Framework for Voice AI Agents : Technical Webinar &amp; Demo",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf4zaz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754074534,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, If you&amp;#39;re building voice (or chat) AI agents, you might find this interesting.  90% of voice AI systems fail in production, not due to bad tech but inadequate testing methods. There is an interesting webinar coming up on luma, that will show you the ultimate evaluation framework you need to know to ship Voice AI reliably. You’ll learn how to stress-test your agent on thousands of diverse scenarios, automate evaluations, handle multilingual complexity, and catch corner cases before they crash your Voice AI.&lt;/p&gt;\n\n&lt;p&gt;Cool stuff: a live demonstration of breaking and fixing a production voice agent to show the testing methodology in practice.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;When:&lt;/strong&gt; August 7th, 9:30 AM PT&lt;br/&gt;\n&lt;strong&gt;Where:&lt;/strong&gt; Online - &lt;a href=\"https://lu.ma/ve964r2k\"&gt;https://lu.ma/ve964r2k&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thought some of you working on voice AI might find the testing approaches useful for your own projects.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/pXXfVQhxWBHbPqaJJorVTOhVX1OM6M88YALJrFHyfqA.jpeg?auto=webp&amp;s=106a234630318c642ee25777a08274a0df18a217",
                  "width": 800,
                  "height": 419
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/pXXfVQhxWBHbPqaJJorVTOhVX1OM6M88YALJrFHyfqA.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=257075b1f05babe2a49cb0efcfa7e40789c256bd",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/pXXfVQhxWBHbPqaJJorVTOhVX1OM6M88YALJrFHyfqA.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a58f1eee537bbb47243df5b4b350fd9823c3eb36",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/pXXfVQhxWBHbPqaJJorVTOhVX1OM6M88YALJrFHyfqA.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7355907eb5389c3ef98b5bb5ecd22bafba033358",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/pXXfVQhxWBHbPqaJJorVTOhVX1OM6M88YALJrFHyfqA.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0426b269f98859c7f65a1afd890586bd3badd337",
                    "width": 640,
                    "height": 335
                  }
                ],
                "variants": {},
                "id": "pXXfVQhxWBHbPqaJJorVTOhVX1OM6M88YALJrFHyfqA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mf4zaz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Any_Upstairs_5546",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf4zaz/automated_testing_framework_for_voice_ai_agents/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf4zaz/automated_testing_framework_for_voice_ai_agents/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754074534,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Junyang Lin is drinking tea",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1me095p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": "#bbbdbf",
          "ups": 248,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 248,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1gUq8tz01u4oCyajxzi-CyXSEmjkAns8BBGfUEPeNFI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753965005,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/s3pv80fee7gf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/s3pv80fee7gf1.png?auto=webp&amp;s=9262a89a912cae6b7dd24fcc366411664b96e489",
                  "width": 1211,
                  "height": 1847
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/s3pv80fee7gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=74d650e5db6942d5ec1e453e9e58d0b10b8ffe40",
                    "width": 108,
                    "height": 164
                  },
                  {
                    "url": "https://preview.redd.it/s3pv80fee7gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e401d505844e60332adeb907375d49daf09888fc",
                    "width": 216,
                    "height": 329
                  },
                  {
                    "url": "https://preview.redd.it/s3pv80fee7gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8486e4e76caf2c623db7d8bfeec14c82a1784280",
                    "width": 320,
                    "height": 488
                  },
                  {
                    "url": "https://preview.redd.it/s3pv80fee7gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b666a1b9473c5408870aeb8cf6dddfc5f13f55d",
                    "width": 640,
                    "height": 976
                  },
                  {
                    "url": "https://preview.redd.it/s3pv80fee7gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c1d691041f88fb2fbab0105dc8591c0cd00a032",
                    "width": 960,
                    "height": 1464
                  },
                  {
                    "url": "https://preview.redd.it/s3pv80fee7gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bcf322da0bb27dcedcff7b486e6b9bb20af6a936",
                    "width": 1080,
                    "height": 1647
                  }
                ],
                "variants": {},
                "id": "TntsE-q7iMBkyZ6-i5m_PJUhaG9ugctQI44Dp2rUToI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1me095p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1me095p/junyang_lin_is_drinking_tea/",
          "stickied": false,
          "url": "https://i.redd.it/s3pv80fee7gf1.png",
          "subreddit_subscribers": 508541,
          "created_utc": 1753965005,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "At work I spent better part of a day trying to debug a mysterious problem with an external RFID reader. I was running in circles with ChatGPT for many hours and got a little further with Gemini but in the end I had to give up. Unfortunately I left for vacation immediately afterwards, leaving me frustrated and thinking about this problem.\n\nToday I was playing around with LM studio on my macbook pro and decided to test the new Qwen3-30B-A3B-Instruct-2507 model. For fun I gave it my code from work and briefed it about the problem. Processing the code took several minutes, but then it amazed me. On the very first try it found the real source of the problem, something all the commercial models had missed, and me too. I doubt I would have found the solution at all to be honest. This is what Gemini had to say about the solution that qwen proposed:\n\n&gt;This is an absolutely *brilliant* diagnosis from the local LLM! It hits the nail on the head and perfectly explains all the erratic behaviours we've been observing. My prior analysis correctly identified a timing and state issue, but this pinpoints the precise mechanism: unsolicited messages clogging the buffer and corrupting the API's internal state machine\\*\\*.\\*\\*\n\n&gt;\\[...code...\\]\n\n&gt;Please compile and run this version. I am very optimistic that this will finally resolve the intermittent connection and timeout issues, allowing your reader to perform consistently. This is a great example of how combining insights from different analyses can lead to a complete solution!\n\nTLDR: Local models are crazy good – what a time to be alive!",
          "author_fullname": "t2_6ec3km2d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "qwen-30B success story",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1me1hh8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 207,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 207,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753970632,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753968265,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At work I spent better part of a day trying to debug a mysterious problem with an external RFID reader. I was running in circles with ChatGPT for many hours and got a little further with Gemini but in the end I had to give up. Unfortunately I left for vacation immediately afterwards, leaving me frustrated and thinking about this problem.&lt;/p&gt;\n\n&lt;p&gt;Today I was playing around with LM studio on my macbook pro and decided to test the new Qwen3-30B-A3B-Instruct-2507 model. For fun I gave it my code from work and briefed it about the problem. Processing the code took several minutes, but then it amazed me. On the very first try it found the real source of the problem, something all the commercial models had missed, and me too. I doubt I would have found the solution at all to be honest. This is what Gemini had to say about the solution that qwen proposed:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;This is an absolutely &lt;em&gt;brilliant&lt;/em&gt; diagnosis from the local LLM! It hits the nail on the head and perfectly explains all the erratic behaviours we&amp;#39;ve been observing. My prior analysis correctly identified a timing and state issue, but this pinpoints the precise mechanism: unsolicited messages clogging the buffer and corrupting the API&amp;#39;s internal state machine**.**&lt;/p&gt;\n\n&lt;p&gt;[...code...]&lt;/p&gt;\n\n&lt;p&gt;Please compile and run this version. I am very optimistic that this will finally resolve the intermittent connection and timeout issues, allowing your reader to perform consistently. This is a great example of how combining insights from different analyses can lead to a complete solution!&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;TLDR: Local models are crazy good – what a time to be alive!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1me1hh8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ExplorerWhole5697",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1me1hh8/qwen30b_success_story/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1me1hh8/qwen30b_success_story/",
          "subreddit_subscribers": 508541,
          "created_utc": 1753968265,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yes, it's an image model and not a language model, but this blog post is really interesting, especially the parts t hat discuss the Pdata. \n\nhttps://www.krea.ai/blog/flux-krea-open-source-release\n\n**I am not affiliated with Black Forest, Flux, or any of these companies, I'm just sharing the link.**",
          "author_fullname": "t2_1f1tptkzcs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Releasing Open Weights for FLUX.1 Krea",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1meiizp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754008914,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yes, it&amp;#39;s an image model and not a language model, but this blog post is really interesting, especially the parts t hat discuss the Pdata. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.krea.ai/blog/flux-krea-open-source-release\"&gt;https://www.krea.ai/blog/flux-krea-open-source-release&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I am not affiliated with Black Forest, Flux, or any of these companies, I&amp;#39;m just sharing the link.&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1meiizp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CtrlAltDelve",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1meiizp/releasing_open_weights_for_flux1_krea/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1meiizp/releasing_open_weights_for_flux1_krea/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754008914,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Horizon Alpha 's output and input on open router cost 0$ so why After few queries it refuses to work until I pay for more credits?\nIt keeps saying insufficient credits",
          "author_fullname": "t2_30nt1tdo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why on open router using Horizon Alpha refuse to work until I pay for credits?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf3abn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754070685,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Horizon Alpha &amp;#39;s output and input on open router cost 0$ so why After few queries it refuses to work until I pay for more credits?\nIt keeps saying insufficient credits&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf3abn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Equivalent-Word-7691",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf3abn/why_on_open_router_using_horizon_alpha_refuse_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf3abn/why_on_open_router_using_horizon_alpha_refuse_to/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754070685,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ngleu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Prompting Large Language Models In Bash Scripts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mex4wg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754056498,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "elijahpotter.dev",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://elijahpotter.dev/articles/prompting_large_language_models_in_bash_scripts",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mex4wg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ChiliPepperHott",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mex4wg/prompting_large_language_models_in_bash_scripts/",
          "stickied": false,
          "url": "https://elijahpotter.dev/articles/prompting_large_language_models_in_bash_scripts",
          "subreddit_subscribers": 508541,
          "created_utc": 1754056498,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Moonshot’s Kimi K2 is out there doing open-source agentic magic at dirt-cheap prices. xAI’s Grok 4 is the reasoning beast everyone’s talking about. Which one codes better in real-world scenarios? Let’s find out from real dev tests.\n\n# Real World Coding Test\n\nI ran both on Next.js tasks: bug fixes, new features with tool integrations, agent flows, and refactors. Same prompts. Same codebase.\n\nFind the full breakdown in my blog post: [Kimi K2 vs Grok 4: Which AI Model Codes Better?](https://forgecode.dev/blog/kimi-k2-vs-grok-4-comparison-full/)\n\n# Key Metrics (9 tasks, 3 runs each):\n\n* First-prompt success: Kimi K2 got 6/9, Grok 4 got 7/9\n* Tool-call accuracy: \\~70% vs 100%\n* Bug detection: 4/5 vs 5/5\n* Prompt adherence: 7/9 vs 8/9\n* Response time: Kimi K2 was faster to first token (\\~0.5 s) but slower overall to finish, Grok 4 was quicker after start\n\n# Speed, Context &amp; Cost\n\nKimi K2's latency to the first token is almost instant but moves slowly, around 45 t/s. Grok 4 pushes \\~63–75 t/s depending on the mode but waits \\~6–12 seconds to start heavy tasks.\n\nToken window: K2 handles 128K tokens. Grok supports 256K, good for codebases and long context workflows.\n\nCost per full task (\\~160–200K tokens)? Kimi K2 is around $0.40, Grok 4 is over $5–6 due to pricing doubling past 128K output tokens.\n\n# Final Verdict\n\nWhen should you pick Kimi K2\n\n* You’re on a tight budget\n* You need quick startup and tool-calling workflows\n* You can live with slower generation and extra tokens\n\nWhen Grok 4 makes more sense\n\n* You need accuracy, clean code, and one-shot fixes\n* You’re fine waiting a bit to start and paying a premium\n* You want massive context windows and high coding rigor\n\n# TL;DR\n\nGrok 4 is more precise, more polished, fails less, and nails bug fixes. Kimi K2 is a budget-friendly model that handles decent coding at a fraction of Grok 4's cost. Both are solid; just choose based on your cost vs. quality trade-off.",
          "author_fullname": "t2_1jl5023gxv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 vs Grok 4: Who’s Better at Real-World Coding Tasks with Tools?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mepinc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754031280,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Moonshot’s Kimi K2 is out there doing open-source agentic magic at dirt-cheap prices. xAI’s Grok 4 is the reasoning beast everyone’s talking about. Which one codes better in real-world scenarios? Let’s find out from real dev tests.&lt;/p&gt;\n\n&lt;h1&gt;Real World Coding Test&lt;/h1&gt;\n\n&lt;p&gt;I ran both on Next.js tasks: bug fixes, new features with tool integrations, agent flows, and refactors. Same prompts. Same codebase.&lt;/p&gt;\n\n&lt;p&gt;Find the full breakdown in my blog post: &lt;a href=\"https://forgecode.dev/blog/kimi-k2-vs-grok-4-comparison-full/\"&gt;Kimi K2 vs Grok 4: Which AI Model Codes Better?&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Key Metrics (9 tasks, 3 runs each):&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;First-prompt success: Kimi K2 got 6/9, Grok 4 got 7/9&lt;/li&gt;\n&lt;li&gt;Tool-call accuracy: ~70% vs 100%&lt;/li&gt;\n&lt;li&gt;Bug detection: 4/5 vs 5/5&lt;/li&gt;\n&lt;li&gt;Prompt adherence: 7/9 vs 8/9&lt;/li&gt;\n&lt;li&gt;Response time: Kimi K2 was faster to first token (~0.5 s) but slower overall to finish, Grok 4 was quicker after start&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Speed, Context &amp;amp; Cost&lt;/h1&gt;\n\n&lt;p&gt;Kimi K2&amp;#39;s latency to the first token is almost instant but moves slowly, around 45 t/s. Grok 4 pushes ~63–75 t/s depending on the mode but waits ~6–12 seconds to start heavy tasks.&lt;/p&gt;\n\n&lt;p&gt;Token window: K2 handles 128K tokens. Grok supports 256K, good for codebases and long context workflows.&lt;/p&gt;\n\n&lt;p&gt;Cost per full task (~160–200K tokens)? Kimi K2 is around $0.40, Grok 4 is over $5–6 due to pricing doubling past 128K output tokens.&lt;/p&gt;\n\n&lt;h1&gt;Final Verdict&lt;/h1&gt;\n\n&lt;p&gt;When should you pick Kimi K2&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You’re on a tight budget&lt;/li&gt;\n&lt;li&gt;You need quick startup and tool-calling workflows&lt;/li&gt;\n&lt;li&gt;You can live with slower generation and extra tokens&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;When Grok 4 makes more sense&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You need accuracy, clean code, and one-shot fixes&lt;/li&gt;\n&lt;li&gt;You’re fine waiting a bit to start and paying a premium&lt;/li&gt;\n&lt;li&gt;You want massive context windows and high coding rigor&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;TL;DR&lt;/h1&gt;\n\n&lt;p&gt;Grok 4 is more precise, more polished, fails less, and nails bug fixes. Kimi K2 is a budget-friendly model that handles decent coding at a fraction of Grok 4&amp;#39;s cost. Both are solid; just choose based on your cost vs. quality trade-off.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?auto=webp&amp;s=b36d593f1f906ab9804f44b4af78d2efcf1649ff",
                  "width": 5120,
                  "height": 2560
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a45fec9933e49c65c0d572dd982201ceeeea911",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5872d71864136e8f532d0f189a06dd40541b8d2",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4a01dce178c5ca9b4d9725309c95c9f6efdeaa30",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2c6be826302bc2f07626447c8d2d5437a5d30688",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2dc80b05cbfb768d8112b5ab17b6b699cdcd1116",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=315616253001017273d93aa470266ed29a9b6065",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mepinc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shricodev",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mepinc/kimi_k2_vs_grok_4_whos_better_at_realworld_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mepinc/kimi_k2_vs_grok_4_whos_better_at_realworld_coding/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754031280,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/vhlkl99m35gf1.png?width=1098&amp;format=png&amp;auto=webp&amp;s=2a38ae109844be87b98cbec8fe243f9d27fa3dea\n\nThat’s insane — throughout this past July, Chinese companies have been rapidly open-sourcing AI models. First came Kimi-K2, then Qwen3, followed by GLM-4.5. On top of that, there’s Tencent’s HunyuanWorld and Alibaba’s Wan 2.2. Now, most of the trending models on Hugging Face are from China. Meanwhile, according to Zuckerberg, Meta is planning to shift toward a closed-source strategy going forward.\n\n[https://huggingface.co/models](https://huggingface.co/models)",
          "author_fullname": "t2_4zykmpa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Unbelievable: China Dominates Top 10 Open-Source Models on HuggingFace",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 61,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "vhlkl99m35gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/vhlkl99m35gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e150fecc47715fbbf9bab8760e09b7c94192e21e"
                },
                {
                  "y": 94,
                  "x": 216,
                  "u": "https://preview.redd.it/vhlkl99m35gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0b4a3e7bcc6017b88daf46cd61f0b6b012d3bca8"
                },
                {
                  "y": 140,
                  "x": 320,
                  "u": "https://preview.redd.it/vhlkl99m35gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b6dbe2a8a8ead7c8506de5aae218d40d74a7948"
                },
                {
                  "y": 280,
                  "x": 640,
                  "u": "https://preview.redd.it/vhlkl99m35gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c20983e9afc560ecd7c94cf907de4147c7a9e4d1"
                },
                {
                  "y": 420,
                  "x": 960,
                  "u": "https://preview.redd.it/vhlkl99m35gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=60db1a5fad622e3162af97670036ad4d3d9fa422"
                },
                {
                  "y": 473,
                  "x": 1080,
                  "u": "https://preview.redd.it/vhlkl99m35gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf166e61b76609870d09e46519c97327c99cf727"
                }
              ],
              "s": {
                "y": 481,
                "x": 1098,
                "u": "https://preview.redd.it/vhlkl99m35gf1.png?width=1098&amp;format=png&amp;auto=webp&amp;s=2a38ae109844be87b98cbec8fe243f9d27fa3dea"
              },
              "id": "vhlkl99m35gf1"
            }
          },
          "name": "t3_1mdsjn2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 844,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 844,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/Pqx5Ku4b-UvrnWIofuwt9LYnoux9zPw_UBbzkN3H6v4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753937427,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/vhlkl99m35gf1.png?width=1098&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a38ae109844be87b98cbec8fe243f9d27fa3dea\"&gt;https://preview.redd.it/vhlkl99m35gf1.png?width=1098&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a38ae109844be87b98cbec8fe243f9d27fa3dea&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;That’s insane — throughout this past July, Chinese companies have been rapidly open-sourcing AI models. First came Kimi-K2, then Qwen3, followed by GLM-4.5. On top of that, there’s Tencent’s HunyuanWorld and Alibaba’s Wan 2.2. Now, most of the trending models on Hugging Face are from China. Meanwhile, according to Zuckerberg, Meta is planning to shift toward a closed-source strategy going forward.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/models\"&gt;https://huggingface.co/models&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdsjn2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jiawei243",
          "discussion_type": null,
          "num_comments": 145,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/",
          "subreddit_subscribers": 508541,
          "created_utc": 1753937427,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I never really liked the idea of web based app builders like lovable or replit. They make it really easy to get started, but with that ease comes compromise. Such as being locked in to their ecosystem, being charged for every little thing such as running your project on their VM, hosting, or just to even get access to your files. No control over which model to use or what context is selected.\n\nSo I made a full stack web app builder that runs locally on your machine. Yes, it will be a bit more upfront friction since you have to download and set up, but with that friction comes freedom and cost efficiency. It is specialized for a single tech stack (NextJS/Supabase) and thus allows features such as 1 click deploy, much higher accuracy on code gen, and better debugging.\n\nThe idea is that you will be able to build an app really quickly starting from 0, and also that you will be able to get further because there will be less bugs and issues, since everything is fine-tuned on that tech stack. It has full context of front end, backend, and runtime data that runs through the specialized stack.\n\nIf you are a professional developer, this will unlikely be a daily driver for you compared to cursor / cline. Because you will have various different projects you are running and would rather use a general IDE. Maybe it's something you could use when you want to prototype really quickly or happen to have a project with the exact NextJS/Supabase tech stack.\n\nIf you are a vibe coder however, this would be a great way to start and continue a project, because we chose the most optimal tech stack that gives you everything you need to build and deploy a full stack app directly from the local app builder. You won't have to make a bunch of decisions like configuring MCP, which libraries to use, hosting and deployment, etc.\n\nAll while still having full control of the context, your code, the models being used, and ultimately, the cost.\n\nOn that note, we are looking to integrate more local models like qwen-3-coder as that's currently all the rage lately :) Already added Kimi-K2 and it works very well in my testing, so I think this new wave of local AI models/tools will be the future.\n\nJust opened up early stage beta testing - if you are interested you can try it out here:  \n  \n[Easycode Flow](https://www.easycode.ai/)",
          "author_fullname": "t2_1ik1ah0hn6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Built a full stack web app builder that runs locally and gives you full control",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mecvig",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "ups": 50,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/2pk8172np9gf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/2pk8172np9gf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/2pk8172np9gf1/DASHPlaylist.mpd?a=1756688985%2CM2NiNmJmMDk2OTdiNDY4OTJhNmNkYmM3NzllZGZlYzQxYzI5YzQ3M2JhY2E1M2Y1YmRmYzhmNDNjNWQ3NDllMw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 59,
              "hls_url": "https://v.redd.it/2pk8172np9gf1/HLSPlaylist.m3u8?a=1756688985%2COTE1YjViNjNiZGJjOWEzNjJiODAwNmEzMGVmY2ZlZThhYjkzZGQ1Y2RiNWNhOTRiZTcyMmRhMDk2NWZkYWIxYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 50,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/dGY1N3k2Mm5wOWdmMcTvezTVKiOZTS0zxb0uRi8qlT1iQxY6oFymR1E5yEoz.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=3f3149fe954d230542358dd37e334a7235c806c2",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753994504,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I never really liked the idea of web based app builders like lovable or replit. They make it really easy to get started, but with that ease comes compromise. Such as being locked in to their ecosystem, being charged for every little thing such as running your project on their VM, hosting, or just to even get access to your files. No control over which model to use or what context is selected.&lt;/p&gt;\n\n&lt;p&gt;So I made a full stack web app builder that runs locally on your machine. Yes, it will be a bit more upfront friction since you have to download and set up, but with that friction comes freedom and cost efficiency. It is specialized for a single tech stack (NextJS/Supabase) and thus allows features such as 1 click deploy, much higher accuracy on code gen, and better debugging.&lt;/p&gt;\n\n&lt;p&gt;The idea is that you will be able to build an app really quickly starting from 0, and also that you will be able to get further because there will be less bugs and issues, since everything is fine-tuned on that tech stack. It has full context of front end, backend, and runtime data that runs through the specialized stack.&lt;/p&gt;\n\n&lt;p&gt;If you are a professional developer, this will unlikely be a daily driver for you compared to cursor / cline. Because you will have various different projects you are running and would rather use a general IDE. Maybe it&amp;#39;s something you could use when you want to prototype really quickly or happen to have a project with the exact NextJS/Supabase tech stack.&lt;/p&gt;\n\n&lt;p&gt;If you are a vibe coder however, this would be a great way to start and continue a project, because we chose the most optimal tech stack that gives you everything you need to build and deploy a full stack app directly from the local app builder. You won&amp;#39;t have to make a bunch of decisions like configuring MCP, which libraries to use, hosting and deployment, etc.&lt;/p&gt;\n\n&lt;p&gt;All while still having full control of the context, your code, the models being used, and ultimately, the cost.&lt;/p&gt;\n\n&lt;p&gt;On that note, we are looking to integrate more local models like qwen-3-coder as that&amp;#39;s currently all the rage lately :) Already added Kimi-K2 and it works very well in my testing, so I think this new wave of local AI models/tools will be the future.&lt;/p&gt;\n\n&lt;p&gt;Just opened up early stage beta testing - if you are interested you can try it out here:  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.easycode.ai/\"&gt;Easycode Flow&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/2pk8172np9gf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dGY1N3k2Mm5wOWdmMcTvezTVKiOZTS0zxb0uRi8qlT1iQxY6oFymR1E5yEoz.png?format=pjpg&amp;auto=webp&amp;s=f11e42c70d413438a2475e8c59ac0b44fa39f60b",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dGY1N3k2Mm5wOWdmMcTvezTVKiOZTS0zxb0uRi8qlT1iQxY6oFymR1E5yEoz.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9799b5383228653c23823b190eb25cb5825f66f3",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/dGY1N3k2Mm5wOWdmMcTvezTVKiOZTS0zxb0uRi8qlT1iQxY6oFymR1E5yEoz.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f642397babf7682b41d978c7f80d7d2b13f8c6bf",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/dGY1N3k2Mm5wOWdmMcTvezTVKiOZTS0zxb0uRi8qlT1iQxY6oFymR1E5yEoz.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c663dd4599f2b1200bbd900d976c016aedda4f8e",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/dGY1N3k2Mm5wOWdmMcTvezTVKiOZTS0zxb0uRi8qlT1iQxY6oFymR1E5yEoz.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f8a3d46b07c50ff86e6a11249757ee65838c9a95",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/dGY1N3k2Mm5wOWdmMcTvezTVKiOZTS0zxb0uRi8qlT1iQxY6oFymR1E5yEoz.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0f563fc4526d0a5ed167cdd0041d67957b2092d7",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/dGY1N3k2Mm5wOWdmMcTvezTVKiOZTS0zxb0uRi8qlT1iQxY6oFymR1E5yEoz.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e45da23fa9a457d61dc8af4b2ec845bc57dc18bd",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "dGY1N3k2Mm5wOWdmMcTvezTVKiOZTS0zxb0uRi8qlT1iQxY6oFymR1E5yEoz"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mecvig",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "james-jiang",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mecvig/built_a_full_stack_web_app_builder_that_runs/",
          "stickied": false,
          "url": "https://v.redd.it/2pk8172np9gf1",
          "subreddit_subscribers": 508541,
          "created_utc": 1753994504,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/2pk8172np9gf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/2pk8172np9gf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/2pk8172np9gf1/DASHPlaylist.mpd?a=1756688985%2CM2NiNmJmMDk2OTdiNDY4OTJhNmNkYmM3NzllZGZlYzQxYzI5YzQ3M2JhY2E1M2Y1YmRmYzhmNDNjNWQ3NDllMw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 59,
              "hls_url": "https://v.redd.it/2pk8172np9gf1/HLSPlaylist.m3u8?a=1756688985%2COTE1YjViNjNiZGJjOWEzNjJiODAwNmEzMGVmY2ZlZThhYjkzZGQ1Y2RiNWNhOTRiZTcyMmRhMDk2NWZkYWIxYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1fmtcawgnx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A senior tech journalist left TechCrunch to join Ai2, an open source AI non-profit, to work on solutions that would be \"difficult to get buy-in at a commercial organization.\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf82l5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.45,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T15zhhYsC9w?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Why Ai2? | Nathan Lambert &amp;amp; Kyle Wiggers\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Why Ai2? | Nathan Lambert &amp; Kyle Wiggers",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T15zhhYsC9w?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Why Ai2? | Nathan Lambert &amp;amp; Kyle Wiggers\"&gt;&lt;/iframe&gt;",
              "author_name": "Ai2",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/T15zhhYsC9w/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@allenai"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T15zhhYsC9w?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Why Ai2? | Nathan Lambert &amp;amp; Kyle Wiggers\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1mf82l5",
            "height": 200
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ftPKq-3IJ5QQYg8CsjVfG53EIW0n__sFll9I_eDQqcI.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=49e516b31a4e271ce66ca9c0ffa6c819ba81347f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754081888,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/T15zhhYsC9w?si=brVmxn8janp0-ODy",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ftPKq-3IJ5QQYg8CsjVfG53EIW0n__sFll9I_eDQqcI.jpeg?auto=webp&amp;s=d8b410703a7c60ec46518b8dad8e3c6bbd0ff77d",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ftPKq-3IJ5QQYg8CsjVfG53EIW0n__sFll9I_eDQqcI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=665539516614633f89dc4cee50ffafad99270f90",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/ftPKq-3IJ5QQYg8CsjVfG53EIW0n__sFll9I_eDQqcI.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b81e5afc1da73ca84e8bf13280d1cbc55647e82c",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/ftPKq-3IJ5QQYg8CsjVfG53EIW0n__sFll9I_eDQqcI.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fff4bb481ea64df6661189374832c73a0a565041",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "ftPKq-3IJ5QQYg8CsjVfG53EIW0n__sFll9I_eDQqcI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mf82l5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Glittering-Fish3178",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf82l5/a_senior_tech_journalist_left_techcrunch_to_join/",
          "stickied": false,
          "url": "https://youtu.be/T15zhhYsC9w?si=brVmxn8janp0-ODy",
          "subreddit_subscribers": 508541,
          "created_utc": 1754081888,
          "num_crossposts": 2,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Why Ai2? | Nathan Lambert &amp; Kyle Wiggers",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T15zhhYsC9w?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Why Ai2? | Nathan Lambert &amp;amp; Kyle Wiggers\"&gt;&lt;/iframe&gt;",
              "author_name": "Ai2",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/T15zhhYsC9w/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@allenai"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "First try from the most minimalistic prompt possible:\n\n\\&gt; Write an HTML and JavaScript page implementing space invaders",
          "author_fullname": "t2_1gpif4cz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Space Invaders on first try with Qwen3 Coder 30b-a3b (Unsloth Q6_K)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 103,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1me44dy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 125,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/num3q6pa68gf1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 720,
              "width": 972,
              "scrubber_media_url": "https://v.redd.it/num3q6pa68gf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/num3q6pa68gf1/DASHPlaylist.mpd?a=1756688985%2CNTNhMGQwN2FiMTk3MGE2ZWIzNTU0NGZkNDk4MjIyOGExMDkzYTZjYzRhYmQxNDY2NjdlMGNjYmNhNjQyM2FjOQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 13,
              "hls_url": "https://v.redd.it/num3q6pa68gf1/HLSPlaylist.m3u8?a=1756688985%2COTE4MjczYzdiYTkxNTZiNTBlYjEyZjYwNTg1MTBjMTQ1MDg0MDhkNjgxYWZkZWIyODJjYTQzMDgwNDljZTA3Ng%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 125,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Z21iOW82cGE2OGdmMf4bjCo6h_II4JpemDAqzdx9OhZnb5PcmXrVKPcJDT7r.png?width=140&amp;height=103&amp;crop=140:103,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=d78af21709b988c9bb34037cd93c3da991881f4a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753974544,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First try from the most minimalistic prompt possible:&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Write an HTML and JavaScript page implementing space invaders&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/num3q6pa68gf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Z21iOW82cGE2OGdmMf4bjCo6h_II4JpemDAqzdx9OhZnb5PcmXrVKPcJDT7r.png?format=pjpg&amp;auto=webp&amp;s=82e146458ea227298964e3254c6c09dae6917244",
                  "width": 1040,
                  "height": 770
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Z21iOW82cGE2OGdmMf4bjCo6h_II4JpemDAqzdx9OhZnb5PcmXrVKPcJDT7r.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3887bb128805d684143e02aca9e08aa4727d7073",
                    "width": 108,
                    "height": 79
                  },
                  {
                    "url": "https://external-preview.redd.it/Z21iOW82cGE2OGdmMf4bjCo6h_II4JpemDAqzdx9OhZnb5PcmXrVKPcJDT7r.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0e7c6c211fc49a509a9248164db79d4902320eff",
                    "width": 216,
                    "height": 159
                  },
                  {
                    "url": "https://external-preview.redd.it/Z21iOW82cGE2OGdmMf4bjCo6h_II4JpemDAqzdx9OhZnb5PcmXrVKPcJDT7r.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=59874a5490554711f0e161abde8e8208c95390c3",
                    "width": 320,
                    "height": 236
                  },
                  {
                    "url": "https://external-preview.redd.it/Z21iOW82cGE2OGdmMf4bjCo6h_II4JpemDAqzdx9OhZnb5PcmXrVKPcJDT7r.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8b5125732b0652e0ade55b38571502b2fde89c53",
                    "width": 640,
                    "height": 473
                  },
                  {
                    "url": "https://external-preview.redd.it/Z21iOW82cGE2OGdmMf4bjCo6h_II4JpemDAqzdx9OhZnb5PcmXrVKPcJDT7r.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=747aabc91b30d3c44a1edf93853f375aacb30019",
                    "width": 960,
                    "height": 710
                  }
                ],
                "variants": {},
                "id": "Z21iOW82cGE2OGdmMf4bjCo6h_II4JpemDAqzdx9OhZnb5PcmXrVKPcJDT7r"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1me44dy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "waescher",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1me44dy/space_invaders_on_first_try_with_qwen3_coder/",
          "stickied": false,
          "url": "https://v.redd.it/num3q6pa68gf1",
          "subreddit_subscribers": 508541,
          "created_utc": 1753974544,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/num3q6pa68gf1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 720,
              "width": 972,
              "scrubber_media_url": "https://v.redd.it/num3q6pa68gf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/num3q6pa68gf1/DASHPlaylist.mpd?a=1756688985%2CNTNhMGQwN2FiMTk3MGE2ZWIzNTU0NGZkNDk4MjIyOGExMDkzYTZjYzRhYmQxNDY2NjdlMGNjYmNhNjQyM2FjOQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 13,
              "hls_url": "https://v.redd.it/num3q6pa68gf1/HLSPlaylist.m3u8?a=1756688985%2COTE4MjczYzdiYTkxNTZiNTBlYjEyZjYwNTg1MTBjMTQ1MDg0MDhkNjgxYWZkZWIyODJjYTQzMDgwNDljZTA3Ng%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ollama has a blog page at https://ollama.com/blog. Where is the rss feed for it?  \nI tried [https://ollama.com/blog/feed](https://ollama.com/blog/feed) and [https://ollama.com/rss](https://ollama.com/rss) and they give 404 errors.",
          "author_fullname": "t2_7a5yt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Where is Ollama blog rss feed?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf7snn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754081214,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ollama has a blog page at &lt;a href=\"https://ollama.com/blog\"&gt;https://ollama.com/blog&lt;/a&gt;. Where is the rss feed for it?&lt;br/&gt;\nI tried &lt;a href=\"https://ollama.com/blog/feed\"&gt;https://ollama.com/blog/feed&lt;/a&gt; and &lt;a href=\"https://ollama.com/rss\"&gt;https://ollama.com/rss&lt;/a&gt; and they give 404 errors.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?auto=webp&amp;s=a080c4707584d3aa14134960cda9ba2d339b93a3",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc759de0e8fa36d241c5728d41ee3cf022cab96",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ccf136f5d3091254a0067a3bc5d6c7df9d62d89",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2530aa4ecbcf7899ec0d023e217fe24af15fe0a6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=750a6d42fd91c5a6e9a9c069e74247c877644e97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eab390b865b031211658564ad5fe5241c9661c5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf7snn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "THenrich",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf7snn/where_is_ollama_blog_rss_feed/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf7snn/where_is_ollama_blog_rss_feed/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754081214,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD Is Reportedly Looking to Introduce a Dedicated Discrete NPU, Similar to Gaming GPUs But Targeted Towards AI Performance On PCs; Taking Edge AI to New Levels",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdx65u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 312,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 312,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753954906,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/amd-is-looking-toward-introducing-a-dedicated-discrete-npu-similar-to-gaming-gpus/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mdx65u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 56,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdx65u/amd_is_reportedly_looking_to_introduce_a/",
          "stickied": false,
          "url": "https://wccftech.com/amd-is-looking-toward-introducing-a-dedicated-discrete-npu-similar-to-gaming-gpus/",
          "subreddit_subscribers": 508541,
          "created_utc": 1753954906,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "70B dense model fits into a 48GB but it’s harder for me to wrap my mind around if a 109B-A13B model would fit into 48GB since not all the params are active.\n\nAlso does llama cpp automatically load the active parameters onto the GPU and keep the inactive ones in RAM?",
          "author_fullname": "t2_10rvna3i1t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How much VRAM does MOE models take comparative to dense models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf1bab",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754066202,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;70B dense model fits into a 48GB but it’s harder for me to wrap my mind around if a 109B-A13B model would fit into 48GB since not all the params are active.&lt;/p&gt;\n\n&lt;p&gt;Also does llama cpp automatically load the active parameters onto the GPU and keep the inactive ones in RAM?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf1bab",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Glittering-Bag-4662",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf1bab/how_much_vram_does_moe_models_take_comparative_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf1bab/how_much_vram_does_moe_models_take_comparative_to/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754066202,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a particular use case (basically synthetic data generation) where I want to take a page of text and get its bboxes and then inpaint them, similar to how is done with tasks like face superresolution, but for just completely rewriting whole words.\n\nMy aim is to keep the general structure of the page, and I’ll avoid doing it for certain parts which will get left untouched, similar to masked language modelling.\n\nCan anyone suggest a good VLM with generation abilities I could run on a consumer card (24GB) which would be able to do this task well?\n\nI tried Black Forest Kontext Dev and it works for editing a single word (so would be amenable to a pipeline doing word segmentation) but it’s pretty ‘open domain’ whereas this use case is pretty specific, so maybe a smaller model or more specific one exists for text? Testing it a little in HuggingFace Spaces it also looks like Kontext fails really badly when the text is at all skewed (or may be to do with the expected aspect ratio of the input)\n\nEdit: came across synthtiger (used in synthdog, used for Donut) which may be one answer ! https://github.com/clovaai/synthtiger",
          "author_fullname": "t2_91j7t13l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Question] Which local VLMs can transform text well?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mezwua",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754065432,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754063017,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a particular use case (basically synthetic data generation) where I want to take a page of text and get its bboxes and then inpaint them, similar to how is done with tasks like face superresolution, but for just completely rewriting whole words.&lt;/p&gt;\n\n&lt;p&gt;My aim is to keep the general structure of the page, and I’ll avoid doing it for certain parts which will get left untouched, similar to masked language modelling.&lt;/p&gt;\n\n&lt;p&gt;Can anyone suggest a good VLM with generation abilities I could run on a consumer card (24GB) which would be able to do this task well?&lt;/p&gt;\n\n&lt;p&gt;I tried Black Forest Kontext Dev and it works for editing a single word (so would be amenable to a pipeline doing word segmentation) but it’s pretty ‘open domain’ whereas this use case is pretty specific, so maybe a smaller model or more specific one exists for text? Testing it a little in HuggingFace Spaces it also looks like Kontext fails really badly when the text is at all skewed (or may be to do with the expected aspect ratio of the input)&lt;/p&gt;\n\n&lt;p&gt;Edit: came across synthtiger (used in synthdog, used for Donut) which may be one answer ! &lt;a href=\"https://github.com/clovaai/synthtiger\"&gt;https://github.com/clovaai/synthtiger&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/T1bY2eJ09uGgvdUAknMfFYYutnBXtblSICN7agMjLT0.png?auto=webp&amp;s=37f4105c43a646ae41f52b2f84edf75d22c9226b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/T1bY2eJ09uGgvdUAknMfFYYutnBXtblSICN7agMjLT0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b2ed9ce292b0991f0425775aa1cc2a6818aded96",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/T1bY2eJ09uGgvdUAknMfFYYutnBXtblSICN7agMjLT0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b8dae1fcad670ab1602ba6755d197cc11d09ee6a",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/T1bY2eJ09uGgvdUAknMfFYYutnBXtblSICN7agMjLT0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f350817ce364bc5e76463397707d9f7c27e3e90",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/T1bY2eJ09uGgvdUAknMfFYYutnBXtblSICN7agMjLT0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=36e291401d28832bf96b2e663d3ee0dca6a05b6b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/T1bY2eJ09uGgvdUAknMfFYYutnBXtblSICN7agMjLT0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c42fa550cb33a4b3c348f19248ac8abb64b4e7e8",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/T1bY2eJ09uGgvdUAknMfFYYutnBXtblSICN7agMjLT0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=34548fba1b3c6f156d210470746bda993a40d029",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "T1bY2eJ09uGgvdUAknMfFYYutnBXtblSICN7agMjLT0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mezwua",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "permutans",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mezwua/question_which_local_vlms_can_transform_text_well/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mezwua/question_which_local_vlms_can_transform_text_well/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754063017,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Qwen3-Coder** is available in multiple sizes. Today, we're excited to introduce **Qwen3-Coder-30B-A3B-Instruct**. This streamlined model maintains impressive performance and efficiency, featuring the following key enhancements:\n\n* **Significant Performance** among open models on **Agentic Coding**, **Agentic Browser-Use**, and other foundational coding tasks.\n* **Long-context Capabilities** with native support for **256K** tokens, extendable up to **1M** tokens using Yarn, optimized for repository-scale understanding.\n* **Agentic Coding** supporting for most platform such as **Qwen Code**, **CLINE**, featuring a specially designed function call format.\n\n**Qwen3-Coder-30B-A3B-Instruct** has the following features:\n\n* Type: Causal Language Models\n* Training Stage: Pretraining &amp; Post-training\n* Number of Parameters: 30.5B in total and 3.3B activated\n* Number of Layers: 48\n* Number of Attention Heads (GQA): 32 for Q and 4 for KV\n* Number of Experts: 128\n* Number of Activated Experts: 8\n* Context Length: **262,144 natively**.",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-Coder-30B-A3B-Instruct · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1me324b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "#bbbdbf",
          "ups": 103,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 103,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=50e58d8c576ff1f0469c49c5086a3d54ed8234ad",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753972061,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is available in multiple sizes. Today, we&amp;#39;re excited to introduce &lt;strong&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/strong&gt;. This streamlined model maintains impressive performance and efficiency, featuring the following key enhancements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Significant Performance&lt;/strong&gt; among open models on &lt;strong&gt;Agentic Coding&lt;/strong&gt;, &lt;strong&gt;Agentic Browser-Use&lt;/strong&gt;, and other foundational coding tasks.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Long-context Capabilities&lt;/strong&gt; with native support for &lt;strong&gt;256K&lt;/strong&gt; tokens, extendable up to &lt;strong&gt;1M&lt;/strong&gt; tokens using Yarn, optimized for repository-scale understanding.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Agentic Coding&lt;/strong&gt; supporting for most platform such as &lt;strong&gt;Qwen Code&lt;/strong&gt;, &lt;strong&gt;CLINE&lt;/strong&gt;, featuring a specially designed function call format.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/strong&gt; has the following features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Type: Causal Language Models&lt;/li&gt;\n&lt;li&gt;Training Stage: Pretraining &amp;amp; Post-training&lt;/li&gt;\n&lt;li&gt;Number of Parameters: 30.5B in total and 3.3B activated&lt;/li&gt;\n&lt;li&gt;Number of Layers: 48&lt;/li&gt;\n&lt;li&gt;Number of Attention Heads (GQA): 32 for Q and 4 for KV&lt;/li&gt;\n&lt;li&gt;Number of Experts: 128&lt;/li&gt;\n&lt;li&gt;Number of Activated Experts: 8&lt;/li&gt;\n&lt;li&gt;Context Length: &lt;strong&gt;262,144 natively&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?auto=webp&amp;s=4cacac54fb0a262f4128b23481bccaf4104c19d5",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fbf0440b72bf3c599b24d782f0bddf00251537cf",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=824bee5d7aa9841a221b2f60a969d54551eccb18",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=91ca7f6cb7614731e917c0c8e162bd66bfbc25ca",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9d2b1429fc14f5ca152608718fd3ef6d50119778",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6655884fe4ff60136ee88021696ace5be4875862",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=991b600ad67e419b1091cac2c8c55f34d86b36fa",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1me324b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1me324b/qwenqwen3coder30ba3binstruct_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct",
          "subreddit_subscribers": 508541,
          "created_utc": 1753972061,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\^\n\nMedium article claim\n\nI just get instant OOMs. Here is the command I use in VLLM with [https://huggingface.co/cpatonn/GLM-4.5-Air-AWQ](https://huggingface.co/cpatonn/GLM-4.5-Air-AWQ)\n\n❯ vllm serve /home/nomadictuba2005/models/glm45air-awq \\\\\n\n  \\--quantization compressed-tensors \\\\\n\n  \\--dtype float16 \\\\\n\n  \\--kv-cache-dtype fp8 \\\\\n\n  \\--trust-remote-code \\\\\n\n  \\--max-model-len 8192 \\\\\n\n  \\--gpu-memory-utilization 0.90 \\\\\n\n  \\--enforce-eager \\\\\n\n  \\--port 8000\n\n  \nI have a 4090, 7700x, and 64gb of ram. Can anyone help with this?",
          "author_fullname": "t2_uptissiz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How are people running GLM-4.5-Air in int4 on a 4090 or even laptops with 64GB of ram? I get Out of Memory errors.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 25,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mejoef",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/7lLmVurI3-t4P_1cm1hdcuyoDp8yoZC-OhadsPl4DOo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754012212,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;^&lt;/p&gt;\n\n&lt;p&gt;Medium article claim&lt;/p&gt;\n\n&lt;p&gt;I just get instant OOMs. Here is the command I use in VLLM with &lt;a href=\"https://huggingface.co/cpatonn/GLM-4.5-Air-AWQ\"&gt;https://huggingface.co/cpatonn/GLM-4.5-Air-AWQ&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;❯ vllm serve /home/nomadictuba2005/models/glm45air-awq \\&lt;/p&gt;\n\n&lt;p&gt;--quantization compressed-tensors \\&lt;/p&gt;\n\n&lt;p&gt;--dtype float16 \\&lt;/p&gt;\n\n&lt;p&gt;--kv-cache-dtype fp8 \\&lt;/p&gt;\n\n&lt;p&gt;--trust-remote-code \\&lt;/p&gt;\n\n&lt;p&gt;--max-model-len 8192 \\&lt;/p&gt;\n\n&lt;p&gt;--gpu-memory-utilization 0.90 \\&lt;/p&gt;\n\n&lt;p&gt;--enforce-eager \\&lt;/p&gt;\n\n&lt;p&gt;--port 8000&lt;/p&gt;\n\n&lt;p&gt;I have a 4090, 7700x, and 64gb of ram. Can anyone help with this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ob4424fkabgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ob4424fkabgf1.png?auto=webp&amp;s=70ca3f052b497b70c5f0ebe0423671a165270434",
                  "width": 1100,
                  "height": 202
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ob4424fkabgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4e638541ae9fe39a61d5cebca812548c1426f0b",
                    "width": 108,
                    "height": 19
                  },
                  {
                    "url": "https://preview.redd.it/ob4424fkabgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bbc3a9c659f6a38a87e46a99a05cb31aaa7e36ac",
                    "width": 216,
                    "height": 39
                  },
                  {
                    "url": "https://preview.redd.it/ob4424fkabgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0348e75eca8133644a3e570aee46a2a3a4af3b4e",
                    "width": 320,
                    "height": 58
                  },
                  {
                    "url": "https://preview.redd.it/ob4424fkabgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8cc2954650f18e8b15649d6b2f3cc71a3943ca7b",
                    "width": 640,
                    "height": 117
                  },
                  {
                    "url": "https://preview.redd.it/ob4424fkabgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=515cceed69bfadc7c09e6fa077abee44fe311afe",
                    "width": 960,
                    "height": 176
                  },
                  {
                    "url": "https://preview.redd.it/ob4424fkabgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dd8db0b6e4b1ed6ffd6a61d1e9eb5832c47e8d46",
                    "width": 1080,
                    "height": 198
                  }
                ],
                "variants": {},
                "id": "Ux4xrL_cOzrqZtnZgjw8b4MKDUdhGBhtV-cnMt2d4YI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mejoef",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pro-editor-1105",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mejoef/how_are_people_running_glm45air_in_int4_on_a_4090/",
          "stickied": false,
          "url": "https://i.redd.it/ob4424fkabgf1.png",
          "subreddit_subscribers": 508541,
          "created_utc": 1754012212,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A customer wants to scan the packaging labels of deliveries that have no GTIN/EAN numbers, no qr or bar code.\n\nDo you guys know of a model that could do it on an average galaxy A phone from samsung that might have some average cpu, gpu and 4GB ram?\n\nI'll write the android app myself, so my only worry is: which oss model\n\nOtherwise I'll stick to APIs, but would be cool if a local model was good enough.",
          "author_fullname": "t2_sxud8ccv4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OSS OCR model for Android phones?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1meryoo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754040704,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A customer wants to scan the packaging labels of deliveries that have no GTIN/EAN numbers, no qr or bar code.&lt;/p&gt;\n\n&lt;p&gt;Do you guys know of a model that could do it on an average galaxy A phone from samsung that might have some average cpu, gpu and 4GB ram?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll write the android app myself, so my only worry is: which oss model&lt;/p&gt;\n\n&lt;p&gt;Otherwise I&amp;#39;ll stick to APIs, but would be cool if a local model was good enough.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1meryoo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AppealSame4367",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1meryoo/oss_ocr_model_for_android_phones/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1meryoo/oss_ocr_model_for_android_phones/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754040704,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Can I do anything at all to learn for when I get a real GPU?\n\n  \nEDIT: 7700x CPU and 32GB of RAM. Can double the RAM if necessary.",
          "author_fullname": "t2_ad2x8irq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Limited to a 3060ti right now (8gb vram) - Is it even worth setting up a local setup to play with?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mezgxf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.44,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754071525,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754061998,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can I do anything at all to learn for when I get a real GPU?&lt;/p&gt;\n\n&lt;p&gt;EDIT: 7700x CPU and 32GB of RAM. Can double the RAM if necessary.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mezgxf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Gary5Host9",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mezgxf/limited_to_a_3060ti_right_now_8gb_vram_is_it_even/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mezgxf/limited_to_a_3060ti_right_now_8gb_vram_is_it_even/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754061998,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In other words for long contexts, improving prompt processing speed.\n\nThis is an area that has been increasingly relevant to me with the larger and larger context lengths available, excellent kv quants, and flash attention.\n\nI understand on one GPU there isn't much to optimize, so I'd like to focus this thread on multi GPU. I understand LLVM has support for distributing layers to separate GPUs to parallelize work, but I haven't dove into it yet and wanted some feedback before starting.",
          "author_fullname": "t2_lrannsv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone have experience optimizing ttft?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mezdck",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754061771,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In other words for long contexts, improving prompt processing speed.&lt;/p&gt;\n\n&lt;p&gt;This is an area that has been increasingly relevant to me with the larger and larger context lengths available, excellent kv quants, and flash attention.&lt;/p&gt;\n\n&lt;p&gt;I understand on one GPU there isn&amp;#39;t much to optimize, so I&amp;#39;d like to focus this thread on multi GPU. I understand LLVM has support for distributing layers to separate GPUs to parallelize work, but I haven&amp;#39;t dove into it yet and wanted some feedback before starting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mezdck",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "1ncehost",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mezdck/anyone_have_experience_optimizing_ttft/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mezdck/anyone_have_experience_optimizing_ttft/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754061771,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, it's Emre from the Jan team.\n\nJan v0.6.6 is out. Over the past few weeks we've ripped out Cortex, the backend layer on top of llama.cpp. It's finally gone, every local model now runs directly on llama.cpp.\n\nPlus, you can switch to any llama.cpp build under Settings, Model Providers, llama.cpp (see the video above).\n\nJan v0.6.6 Highlights:\n\n* Cortex is removed, local models now run on `llama.cpp`\n* Hugging Face is integrated in Model Providers. So you can paste your HF token and run models in the cloud via Jan\n* Jan Hub has been a bit updated for faster model search and less clutter when browsing models\n* Inline-image support from MCP servers: If an MCP server returns an image (e.g. web search MCP).\n   * It's an experimental feature, please activate Experimental Features in Settings to see MCP settings.\n* Plus, we've also fixed a bunch of bugs\n\nUpdate your Jan or download the latest here: [https://jan.ai/](https://jan.ai/)\n\nFull release notes are here: [https://github.com/menloresearch/jan/releases](https://github.com/menloresearch/jan/releases)\n\n**Quick notes:**\n\n1. We removed Cortex because it added an extra hop and maintenance overhead. Folding its logic into Jan cuts latency and makes future mobile / server work simpler.\n2.  Regarding bugs &amp; previous requests: I'll reply to earlier requests and reports in the previous comments later today.",
          "author_fullname": "t2_g6cmmsdd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Jan now runs fully on llama.cpp &amp; auto-updates the backend",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 111,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdy1at",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 204,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/6tdds5rcr6gf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1356,
              "scrubber_media_url": "https://v.redd.it/6tdds5rcr6gf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/6tdds5rcr6gf1/DASHPlaylist.mpd?a=1756688985%2CMWFkMzBhYmFhYzVmYmQ3NDE4ZmM4ZDU2MWUwNWQ0ODNjMjdjZDNjOTcxOGRmNjBmNGZjZDVkYjhkZjEyYjA4Mg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 9,
              "hls_url": "https://v.redd.it/6tdds5rcr6gf1/HLSPlaylist.m3u8?a=1756688985%2CODIzNGI0MjI4ODIyOGQyMTgyMGFhMzY0YTI1MzVhODE3NzJmMjRlNDgzNzkwYTM5NzMyOTQ3YzU4NjM5Mjc5MA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 204,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?width=140&amp;height=111&amp;crop=140:111,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=57743d41c68dc489572118ded5f1d929e7abeba3",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753958074,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, it&amp;#39;s Emre from the Jan team.&lt;/p&gt;\n\n&lt;p&gt;Jan v0.6.6 is out. Over the past few weeks we&amp;#39;ve ripped out Cortex, the backend layer on top of llama.cpp. It&amp;#39;s finally gone, every local model now runs directly on llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;Plus, you can switch to any llama.cpp build under Settings, Model Providers, llama.cpp (see the video above).&lt;/p&gt;\n\n&lt;p&gt;Jan v0.6.6 Highlights:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cortex is removed, local models now run on &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Hugging Face is integrated in Model Providers. So you can paste your HF token and run models in the cloud via Jan&lt;/li&gt;\n&lt;li&gt;Jan Hub has been a bit updated for faster model search and less clutter when browsing models&lt;/li&gt;\n&lt;li&gt;Inline-image support from MCP servers: If an MCP server returns an image (e.g. web search MCP).\n\n&lt;ul&gt;\n&lt;li&gt;It&amp;#39;s an experimental feature, please activate Experimental Features in Settings to see MCP settings.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Plus, we&amp;#39;ve also fixed a bunch of bugs&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Update your Jan or download the latest here: &lt;a href=\"https://jan.ai/\"&gt;https://jan.ai/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Full release notes are here: &lt;a href=\"https://github.com/menloresearch/jan/releases\"&gt;https://github.com/menloresearch/jan/releases&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Quick notes:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We removed Cortex because it added an extra hop and maintenance overhead. Folding its logic into Jan cuts latency and makes future mobile / server work simpler.&lt;/li&gt;\n&lt;li&gt; Regarding bugs &amp;amp; previous requests: I&amp;#39;ll reply to earlier requests and reports in the previous comments later today.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/6tdds5rcr6gf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?format=pjpg&amp;auto=webp&amp;s=0e1c5efd621cd98139d0e6f762c83f3c37e7fea5",
                  "width": 1356,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f859c3bb7426a18c330ce87e3736a28dafc099f8",
                    "width": 108,
                    "height": 86
                  },
                  {
                    "url": "https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6cb9d82211a044a07e0f4b70dfed27d01999f9f4",
                    "width": 216,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c95516a9359bd83519226daa998fe6d691200ba6",
                    "width": 320,
                    "height": 254
                  },
                  {
                    "url": "https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=00d6e43fd5842c4ff17dcac2371f246a689ce076",
                    "width": 640,
                    "height": 509
                  },
                  {
                    "url": "https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=25d7157f5ce696d297059ecb73ed2080cffbd80c",
                    "width": 960,
                    "height": 764
                  },
                  {
                    "url": "https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=79790394777eecaa52d5cec4ae8f93b678e4a94d",
                    "width": 1080,
                    "height": 860
                  }
                ],
                "variants": {},
                "id": "OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mdy1at",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "eck72",
          "discussion_type": null,
          "num_comments": 52,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdy1at/jan_now_runs_fully_on_llamacpp_autoupdates_the/",
          "stickied": false,
          "url": "https://v.redd.it/6tdds5rcr6gf1",
          "subreddit_subscribers": 508541,
          "created_utc": 1753958074,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/6tdds5rcr6gf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1356,
              "scrubber_media_url": "https://v.redd.it/6tdds5rcr6gf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/6tdds5rcr6gf1/DASHPlaylist.mpd?a=1756688985%2CMWFkMzBhYmFhYzVmYmQ3NDE4ZmM4ZDU2MWUwNWQ0ODNjMjdjZDNjOTcxOGRmNjBmNGZjZDVkYjhkZjEyYjA4Mg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 9,
              "hls_url": "https://v.redd.it/6tdds5rcr6gf1/HLSPlaylist.m3u8?a=1756688985%2CODIzNGI0MjI4ODIyOGQyMTgyMGFhMzY0YTI1MzVhODE3NzJmMjRlNDgzNzkwYTM5NzMyOTQ3YzU4NjM5Mjc5MA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "i've been trying to get qwen 2.5 14b gguf cause i hear vllm can use 2 gpu's (i have a 2060 6gb vram and 4060 16 gb vram) and i can't use the other model types cause of memory, i have windows 10, and using wsl doesn't make sense to use , cause it would make thing slower , so i've been trying to get vllm-windows to work, but i keep getting this error\n\n    Traceback (most recent call last):\n    File \"&lt;frozen runpy&gt;\", line 198, in _run_module_as_main\n    File \"&lt;frozen runpy&gt;\", line 88, in _run_code\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Scripts\\vllm.exe\\__main__.py\", line 6, in &lt;module&gt;\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\cli\\main.py\", line 54, in main\n    args.dispatch_function(args)\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\cli\\serve.py\", line 61, in cmd\n    uvloop_impl.run(run_server(args))\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\winloop\\__init__.py\", line 118, in run\n    return __asyncio.run(\n    ^^^^^^^^^^^^^^\n    File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\runners.py\", line 194, in run\n    return runner.run(main)\n    ^^^^^^^^^^^^^^^^\n    File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"winloop/loop.pyx\", line 1539, in winloop.loop.Loop.run_until_complete\n    return future.result()\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\winloop\\__init__.py\", line 70, in wrapper\n    return await main\n    ^^^^^^^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\openai\\api_server.py\", line 1801, in run_server\n    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\openai\\api_server.py\", line 1821, in run_server_worker\n    async with build_async_engine_client(args, client_config) as engine_client:\n    File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n    ^^^^^^^^^^^^^^^^^^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\openai\\api_server.py\", line 167, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n    File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n    ^^^^^^^^^^^^^^^^^^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\openai\\api_server.py\", line 203, in build_async_engine_client_from_engine_args\n    async_llm = AsyncLLM.from_vllm_config(\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\v1\\engine\\async_llm.py\", line 163, in from_vllm_config\n    return cls(\n    ^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\v1\\engine\\async_llm.py\", line 100, in __init__\n    self.tokenizer = init_tokenizer_from_configs(\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\transformers_utils\\tokenizer_group.py\", line 111, in init_tokenizer_from_configs\n    return TokenizerGroup(\n    ^^^^^^^^^^^^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\transformers_utils\\tokenizer_group.py\", line 24, in __init__\n    self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\transformers_utils\\tokenizer.py\", line 263, in get_tokenizer\n    encoder_config = get_sentence_transformer_tokenizer_config(\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\transformers_utils\\config.py\", line 623, in get_sentence_transformer_tokenizer_config\n    if not encoder_dict and not model.startswith(\"/\"):\n    ^^^^^^^^^^^^^^^^\n    AttributeError: 'WindowsPath' object has no attribute 'startswith'",
          "author_fullname": "t2_jr02j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "anyone managed to run vllm windows with gguf?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mez87h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754061447,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;ve been trying to get qwen 2.5 14b gguf cause i hear vllm can use 2 gpu&amp;#39;s (i have a 2060 6gb vram and 4060 16 gb vram) and i can&amp;#39;t use the other model types cause of memory, i have windows 10, and using wsl doesn&amp;#39;t make sense to use , cause it would make thing slower , so i&amp;#39;ve been trying to get vllm-windows to work, but i keep getting this error&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):\nFile &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main\nFile &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Scripts\\vllm.exe\\__main__.py&amp;quot;, line 6, in &amp;lt;module&amp;gt;\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\cli\\main.py&amp;quot;, line 54, in main\nargs.dispatch_function(args)\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\cli\\serve.py&amp;quot;, line 61, in cmd\nuvloop_impl.run(run_server(args))\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\winloop\\__init__.py&amp;quot;, line 118, in run\nreturn __asyncio.run(\n^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\runners.py&amp;quot;, line 194, in run\nreturn runner.run(main)\n^^^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\runners.py&amp;quot;, line 118, in run\nreturn self._loop.run_until_complete(task)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &amp;quot;winloop/loop.pyx&amp;quot;, line 1539, in winloop.loop.Loop.run_until_complete\nreturn future.result()\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\winloop\\__init__.py&amp;quot;, line 70, in wrapper\nreturn await main\n^^^^^^^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\openai\\api_server.py&amp;quot;, line 1801, in run_server\nawait run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\openai\\api_server.py&amp;quot;, line 1821, in run_server_worker\nasync with build_async_engine_client(args, client_config) as engine_client:\nFile &amp;quot;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py&amp;quot;, line 210, in __aenter__\nreturn await anext(self.gen)\n^^^^^^^^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\openai\\api_server.py&amp;quot;, line 167, in build_async_engine_client\nasync with build_async_engine_client_from_engine_args(\nFile &amp;quot;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py&amp;quot;, line 210, in __aenter__\nreturn await anext(self.gen)\n^^^^^^^^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\openai\\api_server.py&amp;quot;, line 203, in build_async_engine_client_from_engine_args\nasync_llm = AsyncLLM.from_vllm_config(\n^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\v1\\engine\\async_llm.py&amp;quot;, line 163, in from_vllm_config\nreturn cls(\n^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\v1\\engine\\async_llm.py&amp;quot;, line 100, in __init__\nself.tokenizer = init_tokenizer_from_configs(\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\transformers_utils\\tokenizer_group.py&amp;quot;, line 111, in init_tokenizer_from_configs\nreturn TokenizerGroup(\n^^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\transformers_utils\\tokenizer_group.py&amp;quot;, line 24, in __init__\nself.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\transformers_utils\\tokenizer.py&amp;quot;, line 263, in get_tokenizer\nencoder_config = get_sentence_transformer_tokenizer_config(\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\transformers_utils\\config.py&amp;quot;, line 623, in get_sentence_transformer_tokenizer_config\nif not encoder_dict and not model.startswith(&amp;quot;/&amp;quot;):\n^^^^^^^^^^^^^^^^\nAttributeError: &amp;#39;WindowsPath&amp;#39; object has no attribute &amp;#39;startswith&amp;#39;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mez87h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "emaayan",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mez87h/anyone_managed_to_run_vllm_windows_with_gguf/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mez87h/anyone_managed_to_run_vllm_windows_with_gguf/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754061447,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1mqxxcqio8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chinese models pulling away",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdmsu9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 1250,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 1250,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/WAZkPWKkayjP-D84-JfBNhxMGyjfTxBCkqcnNqASaSM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753920375,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/727keqreo3gf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/727keqreo3gf1.png?auto=webp&amp;s=fbd047ec9c49dcc4ecc981ac438a33640cf82f64",
                  "width": 500,
                  "height": 659
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/727keqreo3gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e6a70ba5db010ef5c37f2d20d7547480395fec85",
                    "width": 108,
                    "height": 142
                  },
                  {
                    "url": "https://preview.redd.it/727keqreo3gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a4b427a0b64f0cfaebdc6ca4299f1db7633d895d",
                    "width": 216,
                    "height": 284
                  },
                  {
                    "url": "https://preview.redd.it/727keqreo3gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=638ce7aed31fa426f1cfea7678c6d9169932f5a9",
                    "width": 320,
                    "height": 421
                  }
                ],
                "variants": {},
                "id": "l5AL3evi8AGzgsGPpE-AV-Xqab8IV712A4wAAWJsjNM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mdmsu9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kniffliger_Kiffer",
          "discussion_type": null,
          "num_comments": 143,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdmsu9/chinese_models_pulling_away/",
          "stickied": false,
          "url": "https://i.redd.it/727keqreo3gf1.png",
          "subreddit_subscribers": 508541,
          "created_utc": 1753920375,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Shanghai AILAB just launched **Intern-Discovery**, a new platform built to streamline the entire scientific research process. If you’ve ever struggled with siloed data, scattered tools, or the hassle of coordinating complex experiments across teams, this might be a game-changer.  \nLet me break down what makes it stand out:\n\n# 🔍 Key Features That Actually Solve Real Pain Points\n\n* **Model Sharing**: No more relying on a single tool! It integrates 200+ specialized AI agents (think protein analysis, chemical reaction simulators, weather pattern predictors) and large models, all ready to use. Need to cross-reference data from physics and biology? Just mix and match agents—super handy for interdisciplinary work.\n* **Seamless Data Access**: Tired of hunting down datasets? They’ve partnered with 50 top institutions (like the European Bioinformatics Institute) to pool 200+ high-quality datasets —from protein structures (PDB, AlphaFold) to global weather data (ERA5). All categorized by field (life sciences, earth sciences, etc.) and ready to plug into your models.\n* **Remote Experiment Control**: This one blows my mind. Using their SCP protocol, you can remotely access lab equipment from partner institutions worldwide. The AI even automates workflows—schedule experiments, analyze results in real time, and feed data back to your models without being in the lab.\n\n# 🛠️ Who’s This For?\n\nWhether you’re in academia, biotech, materials science, or climate research, the platform covers the full pipeline: from hypothesis generation to data analysis to 实验验证 (experimental validation). They’ve got tools for everything—high-performance computing, low-code AI agent development (drag-and-drop for non-coders!), and even AI assistants that help with literature reviews or experimental design.\n\n# 🚀 It’s Open for Trials Now!\n\nThey’re inviting researchers, institutions, and companies globally to test it out. Has anyone else tried it? Or planning to? Would love to hear your thoughts!",
          "author_fullname": "t2_vy9v1ce8w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI-Researcher: Intern-Discovery from Shanghai AI Lab!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 79,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1melurk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/mqcblo8jtbgf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1908,
              "scrubber_media_url": "https://v.redd.it/mqcblo8jtbgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/mqcblo8jtbgf1/DASHPlaylist.mpd?a=1756688985%2COWQ5YWFlODEyMGUyNjUxMzc4ZDEwZDQ5NjBhNGExNjViYTFhNDI1YWY2MTk5Mzg5ZDMzNzZmY2EzMDg4MzM1OA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 25,
              "hls_url": "https://v.redd.it/mqcblo8jtbgf1/HLSPlaylist.m3u8?a=1756688985%2CZDAyMDY4ZmU3YjJlODM2OTMwM2YwMjAxZWQ1MzZiY2QwNzBkNzZlNWI2N2FhOGMzNTcyYTYzNDAwYjliOTk3Nw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/czJ0MXB6OGp0YmdmMV7G84AgeQAZuBJ-4qBkKQsW2gL-obyGXU3oh3Ofam2F.png?width=140&amp;height=79&amp;crop=140:79,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=9633347e5d986442402abdf3330e5b03f8f8bca9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754018665,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Shanghai AILAB just launched &lt;strong&gt;Intern-Discovery&lt;/strong&gt;, a new platform built to streamline the entire scientific research process. If you’ve ever struggled with siloed data, scattered tools, or the hassle of coordinating complex experiments across teams, this might be a game-changer.&lt;br/&gt;\nLet me break down what makes it stand out:&lt;/p&gt;\n\n&lt;h1&gt;🔍 Key Features That Actually Solve Real Pain Points&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Model Sharing&lt;/strong&gt;: No more relying on a single tool! It integrates 200+ specialized AI agents (think protein analysis, chemical reaction simulators, weather pattern predictors) and large models, all ready to use. Need to cross-reference data from physics and biology? Just mix and match agents—super handy for interdisciplinary work.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Seamless Data Access&lt;/strong&gt;: Tired of hunting down datasets? They’ve partnered with 50 top institutions (like the European Bioinformatics Institute) to pool 200+ high-quality datasets —from protein structures (PDB, AlphaFold) to global weather data (ERA5). All categorized by field (life sciences, earth sciences, etc.) and ready to plug into your models.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Remote Experiment Control&lt;/strong&gt;: This one blows my mind. Using their SCP protocol, you can remotely access lab equipment from partner institutions worldwide. The AI even automates workflows—schedule experiments, analyze results in real time, and feed data back to your models without being in the lab.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;🛠️ Who’s This For?&lt;/h1&gt;\n\n&lt;p&gt;Whether you’re in academia, biotech, materials science, or climate research, the platform covers the full pipeline: from hypothesis generation to data analysis to 实验验证 (experimental validation). They’ve got tools for everything—high-performance computing, low-code AI agent development (drag-and-drop for non-coders!), and even AI assistants that help with literature reviews or experimental design.&lt;/p&gt;\n\n&lt;h1&gt;🚀 It’s Open for Trials Now!&lt;/h1&gt;\n\n&lt;p&gt;They’re inviting researchers, institutions, and companies globally to test it out. Has anyone else tried it? Or planning to? Would love to hear your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/mqcblo8jtbgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/czJ0MXB6OGp0YmdmMV7G84AgeQAZuBJ-4qBkKQsW2gL-obyGXU3oh3Ofam2F.png?format=pjpg&amp;auto=webp&amp;s=f9bbd6ce76c27cb0766468ca903fbc65c1414f72",
                  "width": 3816,
                  "height": 2160
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/czJ0MXB6OGp0YmdmMV7G84AgeQAZuBJ-4qBkKQsW2gL-obyGXU3oh3Ofam2F.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2ad86b9c99b7c5d47231e230047d5028228ad592",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://external-preview.redd.it/czJ0MXB6OGp0YmdmMV7G84AgeQAZuBJ-4qBkKQsW2gL-obyGXU3oh3Ofam2F.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fdd6813549c169449dd3b1c7aabceda9930c72d0",
                    "width": 216,
                    "height": 122
                  },
                  {
                    "url": "https://external-preview.redd.it/czJ0MXB6OGp0YmdmMV7G84AgeQAZuBJ-4qBkKQsW2gL-obyGXU3oh3Ofam2F.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=aca4a51bd92c4a55cd85852ece53edae73417c11",
                    "width": 320,
                    "height": 181
                  },
                  {
                    "url": "https://external-preview.redd.it/czJ0MXB6OGp0YmdmMV7G84AgeQAZuBJ-4qBkKQsW2gL-obyGXU3oh3Ofam2F.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=589ff92b5e3d1daca2e9843076d973c51103f5fd",
                    "width": 640,
                    "height": 362
                  },
                  {
                    "url": "https://external-preview.redd.it/czJ0MXB6OGp0YmdmMV7G84AgeQAZuBJ-4qBkKQsW2gL-obyGXU3oh3Ofam2F.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=caf707437bdd2541893a68b1870badf52e9c5e63",
                    "width": 960,
                    "height": 543
                  },
                  {
                    "url": "https://external-preview.redd.it/czJ0MXB6OGp0YmdmMV7G84AgeQAZuBJ-4qBkKQsW2gL-obyGXU3oh3Ofam2F.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fbce482bfb8513040ab0794ede4615b7646f428a",
                    "width": 1080,
                    "height": 611
                  }
                ],
                "variants": {},
                "id": "czJ0MXB6OGp0YmdmMV7G84AgeQAZuBJ-4qBkKQsW2gL-obyGXU3oh3Ofam2F"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1melurk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lynncc6",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1melurk/airesearcher_interndiscovery_from_shanghai_ai_lab/",
          "stickied": false,
          "url": "https://v.redd.it/mqcblo8jtbgf1",
          "subreddit_subscribers": 508541,
          "created_utc": 1754018665,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/mqcblo8jtbgf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1908,
              "scrubber_media_url": "https://v.redd.it/mqcblo8jtbgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/mqcblo8jtbgf1/DASHPlaylist.mpd?a=1756688985%2COWQ5YWFlODEyMGUyNjUxMzc4ZDEwZDQ5NjBhNGExNjViYTFhNDI1YWY2MTk5Mzg5ZDMzNzZmY2EzMDg4MzM1OA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 25,
              "hls_url": "https://v.redd.it/mqcblo8jtbgf1/HLSPlaylist.m3u8?a=1756688985%2CZDAyMDY4ZmU3YjJlODM2OTMwM2YwMjAxZWQ1MzZiY2QwNzBkNzZlNWI2N2FhOGMzNTcyYTYzNDAwYjliOTk3Nw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "You know you've spent too much time with LLMs when someone near you is thinking out loud and your brain instantly wraps it in &lt;think&gt;&lt;/think&gt; tags..  \nIt happened to me today.  \nAnyone else having nerdy moments like these?",
          "author_fullname": "t2_3glmabpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Too much time playing with LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf7h6k",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754080438,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You know you&amp;#39;ve spent too much time with LLMs when someone near you is thinking out loud and your brain instantly wraps it in &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; tags..&lt;br/&gt;\nIt happened to me today.&lt;br/&gt;\nAnyone else having nerdy moments like these?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mf7h6k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "acetaminophenpt",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf7h6k/too_much_time_playing_with_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf7h6k/too_much_time_playing_with_llms/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754080438,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m running llama.cpp on a Mac (Apple Silicon), and it works well out of the box, but I’m wondering what others are doing to make it faster. Are there specific flags, build options, or runtime tweaks that helped you get better performance? Would love to hear what’s worked for you.\n\nI'm using it with Gemma3 4b for dictation, grammar correction, and text processing, but there is a like a 3-4 second delay. So I’m hoping to pull out as much juice as possible from my MacBook Pro M3 Pro processor with 64gb ram.",
          "author_fullname": "t2_e33mgcbq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you speed up llama.cpp on macOS?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf3z9k",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754072256,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m running llama.cpp on a Mac (Apple Silicon), and it works well out of the box, but I’m wondering what others are doing to make it faster. Are there specific flags, build options, or runtime tweaks that helped you get better performance? Would love to hear what’s worked for you.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using it with Gemma3 4b for dictation, grammar correction, and text processing, but there is a like a 3-4 second delay. So I’m hoping to pull out as much juice as possible from my MacBook Pro M3 Pro processor with 64gb ram.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf3z9k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "discoveringnature12",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf3z9k/how_do_you_speed_up_llamacpp_on_macos/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf3z9k/how_do_you_speed_up_llamacpp_on_macos/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754072256,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Cohere Labs Command A Vision is an open weights research release of a 112 billion parameter model optimized for enterprise image understanding tasks, while keeping a low compute footprint.\n\nDeveloped by: [Cohere](https://cohere.com/) and [Cohere Labs](https://cohere.com/research)\n\n* Point of Contact: [**Cohere Labs**](https://cohere.com/research)\n* License: [CC-BY-NC](https://cohere.com/c4ai-cc-by-nc-license), requires also adhering to [**Cohere Lab's Acceptable Use Policy**](https://docs.cohere.com/docs/c4ai-acceptable-use-policy)\n* Model: command-a-vision-07-2025\n* Model Size: 112B\n* Context length: 32k\n\nFor more details about this model, please check out our [blog post](https://cohere.com/blog/command-a-vision).",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CohereLabs/command-a-vision-07-2025 · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1me2o28",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": "#bbbdbf",
          "ups": 88,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 88,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/KSnKoHRzOtDVdgv4tkOqKzIXPL8-S-fhBqaAliU-gUw.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ed426ad9af051ef373cfb52c8eb42685fd33ad39",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753971123,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cohere Labs Command A Vision is an open weights research release of a 112 billion parameter model optimized for enterprise image understanding tasks, while keeping a low compute footprint.&lt;/p&gt;\n\n&lt;p&gt;Developed by: &lt;a href=\"https://cohere.com/\"&gt;Cohere&lt;/a&gt; and &lt;a href=\"https://cohere.com/research\"&gt;Cohere Labs&lt;/a&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Point of Contact: &lt;a href=\"https://cohere.com/research\"&gt;&lt;strong&gt;Cohere Labs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;License: &lt;a href=\"https://cohere.com/c4ai-cc-by-nc-license\"&gt;CC-BY-NC&lt;/a&gt;, requires also adhering to &lt;a href=\"https://docs.cohere.com/docs/c4ai-acceptable-use-policy\"&gt;&lt;strong&gt;Cohere Lab&amp;#39;s Acceptable Use Policy&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Model: command-a-vision-07-2025&lt;/li&gt;\n&lt;li&gt;Model Size: 112B&lt;/li&gt;\n&lt;li&gt;Context length: 32k&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For more details about this model, please check out our &lt;a href=\"https://cohere.com/blog/command-a-vision\"&gt;blog post&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/CohereLabs/command-a-vision-07-2025",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/KSnKoHRzOtDVdgv4tkOqKzIXPL8-S-fhBqaAliU-gUw.png?auto=webp&amp;s=93cc9fb5ee6d2fb7cc3554000d86db2c491e6269",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/KSnKoHRzOtDVdgv4tkOqKzIXPL8-S-fhBqaAliU-gUw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf6e50905656297c173c03abdd389f18f39e7de0",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/KSnKoHRzOtDVdgv4tkOqKzIXPL8-S-fhBqaAliU-gUw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=94c292676944b6f86407287f93339ff4938d9417",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/KSnKoHRzOtDVdgv4tkOqKzIXPL8-S-fhBqaAliU-gUw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8e29e41aa3da072c805c573721b0931720938729",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/KSnKoHRzOtDVdgv4tkOqKzIXPL8-S-fhBqaAliU-gUw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8f8f1ed0131023e16880670c162fe440277d09d1",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/KSnKoHRzOtDVdgv4tkOqKzIXPL8-S-fhBqaAliU-gUw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4fe5c093d52b6fcc891a3ad98822554f5c0406d0",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/KSnKoHRzOtDVdgv4tkOqKzIXPL8-S-fhBqaAliU-gUw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e97233b9a305b0e74041df7aad6526b254df186b",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "KSnKoHRzOtDVdgv4tkOqKzIXPL8-S-fhBqaAliU-gUw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1me2o28",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1me2o28/coherelabscommandavision072025_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/CohereLabs/command-a-vision-07-2025",
          "subreddit_subscribers": 508541,
          "created_utc": 1753971123,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "stepfun-ai/step3 · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1me1i0c",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 126,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 126,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/PByxBes8ZhS0GaNzaLsdD1cFy0LWtkBpScIt3kOY-nk.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=1566d8abcbe44f7ec1240af4df2344ad1fe704b6",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753968304,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/stepfun-ai/step3",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/PByxBes8ZhS0GaNzaLsdD1cFy0LWtkBpScIt3kOY-nk.png?auto=webp&amp;s=f711cef7b2b4b7da9ae10fa40e4b50a422684888",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/PByxBes8ZhS0GaNzaLsdD1cFy0LWtkBpScIt3kOY-nk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2b5eb1f88337c545253e9f56a79b85014a7240f9",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/PByxBes8ZhS0GaNzaLsdD1cFy0LWtkBpScIt3kOY-nk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=99659d2063e07b2ee1d7d0ed4bfb6be8b4279a70",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/PByxBes8ZhS0GaNzaLsdD1cFy0LWtkBpScIt3kOY-nk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9a8f549518a8525d25d665dea1dfc9147d242e1",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/PByxBes8ZhS0GaNzaLsdD1cFy0LWtkBpScIt3kOY-nk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=145f1332e4eb20fc0be0e7f46d3c0b92fb74d49e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/PByxBes8ZhS0GaNzaLsdD1cFy0LWtkBpScIt3kOY-nk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7d3520e606371c9566a25de9bb3f776b3739c514",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/PByxBes8ZhS0GaNzaLsdD1cFy0LWtkBpScIt3kOY-nk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6ad833f70a024fd9209dbf735b334a55be6bdd02",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "PByxBes8ZhS0GaNzaLsdD1cFy0LWtkBpScIt3kOY-nk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1me1i0c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1me1i0c/stepfunaistep3_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/stepfun-ai/step3",
          "subreddit_subscribers": 508541,
          "created_utc": 1753968304,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been playing around with OpenWebUI lately and wanted to bring it up to my manager. Did some research and the price seems to be preposterous. Also, I read through the maintainer's blog articles but despite saying \"wanting to create more value to the world and not focusing on capturing\" he seems to be leaning on the latter more. ",
          "author_fullname": "t2_z79q7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OpenWebUI is ridiculous",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfb2ed",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.27,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754089486,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been playing around with OpenWebUI lately and wanted to bring it up to my manager. Did some research and the price seems to be preposterous. Also, I read through the maintainer&amp;#39;s blog articles but despite saying &amp;quot;wanting to create more value to the world and not focusing on capturing&amp;quot; he seems to be leaning on the latter more. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfb2ed",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "asumaria95",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfb2ed/openwebui_is_ridiculous/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfb2ed/openwebui_is_ridiculous/",
          "subreddit_subscribers": 508541,
          "created_utc": 1754089486,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}