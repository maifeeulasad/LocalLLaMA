{
  "kind": "Listing",
  "data": {
    "after": "t3_1mi0wkg",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Model introduction:**\n\nKitten ML has released open source code and weights of their new TTS model's preview.\n\nGithub: [https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)\n\nHuggingface: [https://huggingface.co/KittenML/kitten-tts-nano-0.1](https://huggingface.co/KittenML/kitten-tts-nano-0.1)\n\nThe model is less than 25 MB, around 15M parameters. The full release next week will include another open source \\~80M parameter model with these same 8 voices, that can also run on CPU.\n\n**Key features and Advantages**\n\n1. Eight Different Expressive voices - 4 female and 4 male voices. For a tiny model, the expressivity sounds pretty impressive. This release will support TTS in English and multilingual support expected in future releases.\n2. Super-small in size: The two text to speech models will be \\~15M and \\~80M parameters .\n3. Can literally run anywhere lol : Forget “No gpu required.” - this thing can even run on raspberry pi’s and phones. Great news for gpu-poor folks like me.\n4. Open source (hell yeah!): the model can used for free.",
          "author_fullname": "t2_1pt65ozmj9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kitten TTS : SOTA Super-tiny TTS Model (Less than 25 MB)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhyzp7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 990,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/vdfv5uihi4hf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/vdfv5uihi4hf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/vdfv5uihi4hf1/DASHPlaylist.mpd?a=1756989093%2CNjdkNWExNjEyOTBhMGFjNWQ0M2Q2OGQ3NzI4YWViNTYyYTM2YWI0NTc2ZjRmYmNlMDViOGU4ZDlmYjZjNjk4MQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 60,
              "hls_url": "https://v.redd.it/vdfv5uihi4hf1/HLSPlaylist.m3u8?a=1756989093%2CZmI2ZTM2MmZkNzlkNTUyYTZhMzFiZTk5NTFkMDE0MWQxZDRlNmMwZGUyOGU4MGU4M2VhZGFjZTJjYjkzYTUzYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 990,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ajlvMGd2aWhpNGhmMUpar5lWZvhVHx9_BWGYhGbOyuld4cLO275_Q90LHrwX.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=c2d4b66e6eedc90dae86190c3db65dcdb9665f77",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754365946,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Model introduction:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Kitten ML has released open source code and weights of their new TTS model&amp;#39;s preview.&lt;/p&gt;\n\n&lt;p&gt;Github: &lt;a href=\"https://github.com/KittenML/KittenTTS\"&gt;https://github.com/KittenML/KittenTTS&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Huggingface: &lt;a href=\"https://huggingface.co/KittenML/kitten-tts-nano-0.1\"&gt;https://huggingface.co/KittenML/kitten-tts-nano-0.1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The model is less than 25 MB, around 15M parameters. The full release next week will include another open source ~80M parameter model with these same 8 voices, that can also run on CPU.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Key features and Advantages&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Eight Different Expressive voices - 4 female and 4 male voices. For a tiny model, the expressivity sounds pretty impressive. This release will support TTS in English and multilingual support expected in future releases.&lt;/li&gt;\n&lt;li&gt;Super-small in size: The two text to speech models will be ~15M and ~80M parameters .&lt;/li&gt;\n&lt;li&gt;Can literally run anywhere lol : Forget “No gpu required.” - this thing can even run on raspberry pi’s and phones. Great news for gpu-poor folks like me.&lt;/li&gt;\n&lt;li&gt;Open source (hell yeah!): the model can used for free.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/vdfv5uihi4hf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ajlvMGd2aWhpNGhmMUpar5lWZvhVHx9_BWGYhGbOyuld4cLO275_Q90LHrwX.png?format=pjpg&amp;auto=webp&amp;s=eb23cad2f64b7a97b1d5f287cd5dfdb1774d0ac7",
                  "width": 1280,
                  "height": 720
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ajlvMGd2aWhpNGhmMUpar5lWZvhVHx9_BWGYhGbOyuld4cLO275_Q90LHrwX.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=eea9ab012c1208b48649b975de77487ec70631f5",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/ajlvMGd2aWhpNGhmMUpar5lWZvhVHx9_BWGYhGbOyuld4cLO275_Q90LHrwX.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=80ba4da39b27ab851eedbd4f4e4399a54dc47e5f",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/ajlvMGd2aWhpNGhmMUpar5lWZvhVHx9_BWGYhGbOyuld4cLO275_Q90LHrwX.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=caa2545fe834b93ed6f7d6ca5b1a20e3cfd9fee5",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/ajlvMGd2aWhpNGhmMUpar5lWZvhVHx9_BWGYhGbOyuld4cLO275_Q90LHrwX.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e0e2ea54209d3aeb2ad9a9dc1331348bb968943e",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/ajlvMGd2aWhpNGhmMUpar5lWZvhVHx9_BWGYhGbOyuld4cLO275_Q90LHrwX.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=567df74a7b4d360b4d510261007cbc5cd624aa11",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/ajlvMGd2aWhpNGhmMUpar5lWZvhVHx9_BWGYhGbOyuld4cLO275_Q90LHrwX.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e68f85952878d4d122a6868224241c41c938f213",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "ajlvMGd2aWhpNGhmMUpar5lWZvhVHx9_BWGYhGbOyuld4cLO275_Q90LHrwX"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mhyzp7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ElectricalBar7464",
          "discussion_type": null,
          "num_comments": 161,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/",
          "stickied": false,
          "url": "https://v.redd.it/vdfv5uihi4hf1",
          "subreddit_subscribers": 510540,
          "created_utc": 1754365946,
          "num_crossposts": 4,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/vdfv5uihi4hf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/vdfv5uihi4hf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/vdfv5uihi4hf1/DASHPlaylist.mpd?a=1756989093%2CNjdkNWExNjEyOTBhMGFjNWQ0M2Q2OGQ3NzI4YWViNTYyYTM2YWI0NTc2ZjRmYmNlMDViOGU4ZDlmYjZjNjk4MQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 60,
              "hls_url": "https://v.redd.it/vdfv5uihi4hf1/HLSPlaylist.m3u8?a=1756989093%2CZmI2ZTM2MmZkNzlkNTUyYTZhMzFiZTk5NTFkMDE0MWQxZDRlNmMwZGUyOGU4MGU4M2VhZGFjZTJjYjkzYTUzYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "From Dario Amodei's recent interview on Big Technology Podcast discussing open source AI models. Thoughts on this reasoning?\n\nSource: [https://x.com/jikkujose/status/1952588432280051930](https://x.com/jikkujose/status/1952588432280051930)",
          "author_fullname": "t2_e3eey",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anthropic's CEO dismisses open source as 'red herring' - but his reasoning seems to miss the point entirely!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi0co2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 257,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 257,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ZsVbxNTuclgi1RAjYzfr9marbPXEqwhG_yALyYEoUjE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754370368,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From Dario Amodei&amp;#39;s recent interview on Big Technology Podcast discussing open source AI models. Thoughts on this reasoning?&lt;/p&gt;\n\n&lt;p&gt;Source: &lt;a href=\"https://x.com/jikkujose/status/1952588432280051930\"&gt;https://x.com/jikkujose/status/1952588432280051930&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/9z1vbpnsu4hf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/9z1vbpnsu4hf1.png?auto=webp&amp;s=c7c080e3549f84532cb8cfcf4cd844516bed009d",
                  "width": 1194,
                  "height": 1406
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/9z1vbpnsu4hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e84a5f122eeace4cb08d0cfe862ccf07e8d62424",
                    "width": 108,
                    "height": 127
                  },
                  {
                    "url": "https://preview.redd.it/9z1vbpnsu4hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=36c1f2eefb0a92d642e081357bb1b6ebf8353ae4",
                    "width": 216,
                    "height": 254
                  },
                  {
                    "url": "https://preview.redd.it/9z1vbpnsu4hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6579c6496efbc6c05047aa3136a41c21cc605330",
                    "width": 320,
                    "height": 376
                  },
                  {
                    "url": "https://preview.redd.it/9z1vbpnsu4hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bafb282a6194808b25822f60262b2b9d1dd1570e",
                    "width": 640,
                    "height": 753
                  },
                  {
                    "url": "https://preview.redd.it/9z1vbpnsu4hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6ffaf48a699cf143d7ffe7b445eaeaa32d9564ea",
                    "width": 960,
                    "height": 1130
                  },
                  {
                    "url": "https://preview.redd.it/9z1vbpnsu4hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cc5a46a4a169743718bd2e909785c3fa17e7636e",
                    "width": 1080,
                    "height": 1271
                  }
                ],
                "variants": {},
                "id": "-Tz5PxFOPTpW8orq21Z8a7ZHC3qLR7g41hltqrpfXd4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi0co2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MrJiks",
          "discussion_type": null,
          "num_comments": 157,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi0co2/anthropics_ceo_dismisses_open_source_as_red/",
          "stickied": false,
          "url": "https://i.redd.it/9z1vbpnsu4hf1.png",
          "subreddit_subscribers": 510540,
          "created_utc": 1754370368,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1gs77fbujj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "generated using Qwen",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "647acy43y4hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/647acy43y4hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=50d79fbbcf5b1ed46388f6cba2a9b4a134a4df7f"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/647acy43y4hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb5e4a0cd3ffa80ba8f4bab954d7747ecbbf64ee"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/647acy43y4hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=69b3ea04c298ed4c1a7e8ac29866d15e4b01317b"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/647acy43y4hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=341e9a0279eb1ad95d148a743df4f885a415ac79"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/647acy43y4hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=70ca7d7d70e54737f37ce3c25acf84deaab2bf6b"
                },
                {
                  "y": 1080,
                  "x": 1080,
                  "u": "https://preview.redd.it/647acy43y4hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a7a7ad58275e068c7a09a5f68209ef2cbd0e4335"
                }
              ],
              "s": {
                "y": 1328,
                "x": 1328,
                "u": "https://preview.redd.it/647acy43y4hf1.png?width=1328&amp;format=png&amp;auto=webp&amp;s=6bd393b98c6bf30bb7e984a978ea2b6d19314524"
              },
              "id": "647acy43y4hf1"
            },
            "rqxr5je4y4hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/rqxr5je4y4hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=65ba8b876f2f97a066aa74ec11636442bc792a2b"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/rqxr5je4y4hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0b493af3b404ec8890963cb013b59546bf095bbb"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/rqxr5je4y4hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c79e1f15646b9dcce6b0bf69e0cfea6d53926e3e"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/rqxr5je4y4hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2729d1e8670ef7df67dbaf21cc7634b7d86c73e9"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/rqxr5je4y4hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c2d381db9d6609998b3af6ab87c2c407ebea868c"
                },
                {
                  "y": 1080,
                  "x": 1080,
                  "u": "https://preview.redd.it/rqxr5je4y4hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=055600447463061a019ed940183374d887ba15ea"
                }
              ],
              "s": {
                "y": 1328,
                "x": 1328,
                "u": "https://preview.redd.it/rqxr5je4y4hf1.png?width=1328&amp;format=png&amp;auto=webp&amp;s=08b53b09b0ad31bedc392191d3457013433dd43e"
              },
              "id": "rqxr5je4y4hf1"
            }
          },
          "name": "t3_1mi0luy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 132,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "647acy43y4hf1",
                "id": 721553303
              },
              {
                "media_id": "rqxr5je4y4hf1",
                "id": 721553304
              }
            ]
          },
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 132,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/xCXpPKDOF28RU5xW4gyR6Ubwr1ZXuMCJDS0Yp62MVtI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754371223,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mi0luy",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mi0luy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Vision--SuperAI",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi0luy/generated_using_qwen/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mi0luy",
          "subreddit_subscribers": 510540,
          "created_utc": 1754371223,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_tq216",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DFLoat11 Quantization for Qwen-Image Drops – Run It on 17GB VRAM with CPU Offloading!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 116,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi1fdc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "transparent",
          "ups": 112,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 112,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/7y-aD7wDrG384m4HbYr0gBp_ij1y4yDiTmFAIJLMK-A.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754374161,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/sv779zmy65hf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/sv779zmy65hf1.png?auto=webp&amp;s=8bc54d408e7d2277f2f2342a472a86f3cadcbc6c",
                  "width": 1128,
                  "height": 942
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/sv779zmy65hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4ebbb232e4b60bf6fe7db6c4eb2170131c369e86",
                    "width": 108,
                    "height": 90
                  },
                  {
                    "url": "https://preview.redd.it/sv779zmy65hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ccb4bff2ace3ff980986c2ba9316cb64ca197e86",
                    "width": 216,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/sv779zmy65hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=39aeca05b78502d0094e4a2106d2132226f4b482",
                    "width": 320,
                    "height": 267
                  },
                  {
                    "url": "https://preview.redd.it/sv779zmy65hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=935b56fc5932d42d92b2f9647262c1937a3e7446",
                    "width": 640,
                    "height": 534
                  },
                  {
                    "url": "https://preview.redd.it/sv779zmy65hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7e57e3ff1ce713f874b79174c768c327bcb05fe",
                    "width": 960,
                    "height": 801
                  },
                  {
                    "url": "https://preview.redd.it/sv779zmy65hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e18bdfb639d25a40353ab97356ade8069d7c966a",
                    "width": 1080,
                    "height": 901
                  }
                ],
                "variants": {},
                "id": "nRW6eaNh4BL0f8JAlDnfeATP4eKlt1Q4D61NMahPmfk"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mi1fdc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "XMasterrrr",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mi1fdc/dfloat11_quantization_for_qwenimage_drops_run_it/",
          "stickied": false,
          "url": "https://i.redd.it/sv779zmy65hf1.png",
          "subreddit_subscribers": 510540,
          "created_utc": 1754374161,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_cyrs5dhp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The Chess Arena pairings for today's Kaggle exhibition are out, commentary by grandmasters like Hikaru Nakamura!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mi6bkf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 39,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 39,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/wtdfQBqvC1yJAFuDIo4Muy057jBft-7s56tIgQjmEF8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754392413,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/h2p8ceo4p6hf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/h2p8ceo4p6hf1.png?auto=webp&amp;s=a39820f095788160b3b31cbace46505f96bbf0be",
                  "width": 1036,
                  "height": 1113
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/h2p8ceo4p6hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5370674f4434a1fbf10228182bb629df91219be4",
                    "width": 108,
                    "height": 116
                  },
                  {
                    "url": "https://preview.redd.it/h2p8ceo4p6hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e806c860164d4fcaf9f6adcd41f5f3732c11870b",
                    "width": 216,
                    "height": 232
                  },
                  {
                    "url": "https://preview.redd.it/h2p8ceo4p6hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=70c0c0cdaee278db69e444a514358c9953cd5e3d",
                    "width": 320,
                    "height": 343
                  },
                  {
                    "url": "https://preview.redd.it/h2p8ceo4p6hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=04ddb7dfeb9a7a66e33196efe1d60a4acb157f2a",
                    "width": 640,
                    "height": 687
                  },
                  {
                    "url": "https://preview.redd.it/h2p8ceo4p6hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6bcadf7c76af7f7b94385f80d0cf952f53d06085",
                    "width": 960,
                    "height": 1031
                  }
                ],
                "variants": {},
                "id": "RzgXjPAFYUSOZOqNUcfBYvg9vYMClqcT0AL1f13R7KM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mi6bkf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Final_Wheel_7486",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi6bkf/the_chess_arena_pairings_for_todays_kaggle/",
          "stickied": false,
          "url": "https://i.redd.it/h2p8ceo4p6hf1.png",
          "subreddit_subscribers": 510540,
          "created_utc": 1754392413,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "No more need for super-complex regular expression in the -ot option! Just do `--cpu-moe` or `--n-cpu-moe #` and reduce the number until the model no longer fits on the GPU.",
          "author_fullname": "t2_5b972ieo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New llama.cpp options make MoE offloading trivial: `--n-cpu-moe`",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mi7bem",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=a6683566ea3a2ab3f10144b97e4072ab0639db68",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754395457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;No more need for super-complex regular expression in the -ot option! Just do &lt;code&gt;--cpu-moe&lt;/code&gt; or &lt;code&gt;--n-cpu-moe #&lt;/code&gt; and reduce the number until the model no longer fits on the GPU.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/15077",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?auto=webp&amp;s=6d449514411bd16575e7077a57c169d46f49d2e5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad9e8961b664b6710eb58c6fa604f119639c53e1",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=734713090c897be69c4245b0a525571c45526c1e",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=03bee70399e29da7291f5a8bd625456274a76ec6",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc3c355c6e5a0c2867654d9ea9c2e7c8ed9c618b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a4e77bcd6622924abcac1fa3acd0c374064d0fda",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc470b6cb695b2d267be6deae5c3dd1b77f91432",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mi7bem",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pristine-Woodpecker",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/15077",
          "subreddit_subscribers": 510540,
          "created_utc": 1754395457,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I made a quick web demo of the new [Kitten TTS](https://www.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/). Loads the model up using transformers.js in the browser, running fully locally client-side: https://clowerweb.github.io/kitten-tts-web-demo/\n\nRepo: https://github.com/clowerweb/kitten-tts-web-demo\n\nOnly uses CPU for now, but I'm going to add WebGPU support for it later today, plus maybe a Whisper implementation also in transformers.js for a nice little local STS pipeline, if anyone is interested in something like that.\n\nI also have a little open-source chat interface in progress that I might plop the STS pipeline into here: https://github.com/clowerweb/Simple-AI (built with Nuxt 3 &amp; Tailwind 4) -- supports chat tabs &amp; history, markdown, code highlighting, and LaTeX, and also lets you run Qwen3 4B via transformers.js or add your own custom API endpoints, with settings for temperature, top_p, top_k, etc. Only supports OpenAI-compatible endpoints currently. You can add custom API providers (including your own llama.cpp servers and whatnot), custom models with their own settings, custom system prompts, etc. If you're interested in seeing an STS pipeline added to that though with Kitten &amp; Whisper, lemme know what the interest levels are for something like that. I'll probably toss this project into Electron when it's ready and make it into a desktop app for Mac, Windows, and Linux as well.",
          "author_fullname": "t2_1iuzpxw7eg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kitten TTS Web Demo",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi45h1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 42,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 42,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754385492,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754384662,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made a quick web demo of the new &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/\"&gt;Kitten TTS&lt;/a&gt;. Loads the model up using transformers.js in the browser, running fully locally client-side: &lt;a href=\"https://clowerweb.github.io/kitten-tts-web-demo/\"&gt;https://clowerweb.github.io/kitten-tts-web-demo/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Repo: &lt;a href=\"https://github.com/clowerweb/kitten-tts-web-demo\"&gt;https://github.com/clowerweb/kitten-tts-web-demo&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Only uses CPU for now, but I&amp;#39;m going to add WebGPU support for it later today, plus maybe a Whisper implementation also in transformers.js for a nice little local STS pipeline, if anyone is interested in something like that.&lt;/p&gt;\n\n&lt;p&gt;I also have a little open-source chat interface in progress that I might plop the STS pipeline into here: &lt;a href=\"https://github.com/clowerweb/Simple-AI\"&gt;https://github.com/clowerweb/Simple-AI&lt;/a&gt; (built with Nuxt 3 &amp;amp; Tailwind 4) -- supports chat tabs &amp;amp; history, markdown, code highlighting, and LaTeX, and also lets you run Qwen3 4B via transformers.js or add your own custom API endpoints, with settings for temperature, top_p, top_k, etc. Only supports OpenAI-compatible endpoints currently. You can add custom API providers (including your own llama.cpp servers and whatnot), custom models with their own settings, custom system prompts, etc. If you&amp;#39;re interested in seeing an STS pipeline added to that though with Kitten &amp;amp; Whisper, lemme know what the interest levels are for something like that. I&amp;#39;ll probably toss this project into Electron when it&amp;#39;s ready and make it into a desktop app for Mac, Windows, and Linux as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mi45h1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CommunityTough1",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi45h1/kitten_tts_web_demo/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi45h1/kitten_tts_web_demo/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754384662,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "and it's better than Flux Kontext Pro (according to their benchmarks). That's insane. Really looking forward to it. ",
          "author_fullname": "t2_ghr4m7l1n",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "QWEN-IMAGE is released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhhdig",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 915,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 915,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=a720ed50938a0f7d099fa5095eeaa524819f6b87",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754323135,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;and it&amp;#39;s better than Flux Kontext Pro (according to their benchmarks). That&amp;#39;s insane. Really looking forward to it. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen-Image",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?auto=webp&amp;s=7578d1430d51fe3898437256626f5bd7f9c643b5",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=55a19a341313ab08b43f3737ad0171a6dc27a3a6",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=561673e4e6ac3694e8d08fb8f3b50de1d4d7bafc",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5972624b3a9fe2057d6c44275f111b27e3e66505",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a65aaca919e956009709dd069f70c0c907403912",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5b308acfa4755323a0b8731404fd7599940db7ca",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1fcc7db6ec2f7f9365fb1224503b8e2a33a798c7",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mhhdig",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheIncredibleHem",
          "discussion_type": null,
          "num_comments": 222,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhhdig/qwenimage_is_released/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen-Image",
          "subreddit_subscribers": 510540,
          "created_utc": 1754323135,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/Alibaba_Qwen/status/1952398250121756992\n\nIt's better than Flux Kontext, gpt-image level",
          "author_fullname": "t2_58t8ty6v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen-Image is out",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhiqqn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 744,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/4077mfg081hf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/4077mfg081hf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/4077mfg081hf1/DASHPlaylist.mpd?a=1756989093%2CNjk5MWE4MGMyNjZlNGQwY2Q5NTlhZDEwZGVkZWZiYTNjZGEwZTc4N2MwZmI0MjRmMjBjNzgzNDFkNjY5OTMwMg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 116,
              "hls_url": "https://v.redd.it/4077mfg081hf1/HLSPlaylist.m3u8?a=1756989093%2CMTczYmEyZDI4NTQzOTU2MTE1MjYyMDFhYmZmNDJlYjA5ZTI3NjJiM2NiZmYwNWMxYmEwZjE1ZmY1OTViZjk4MQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 744,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=55f564e25aed19bc0608b4b7b3a6e94a2cb852a6",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754326154,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/Alibaba_Qwen/status/1952398250121756992\"&gt;https://x.com/Alibaba_Qwen/status/1952398250121756992&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s better than Flux Kontext, gpt-image level&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/4077mfg081hf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?format=pjpg&amp;auto=webp&amp;s=f2e05f42033e081c6d088d575d93180ae723d268",
                  "width": 1080,
                  "height": 607
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e13de73d5d88711e89158aadf32e2b1b9d0f8cbe",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c028dc1b30127eeeea0df5fa246f97fe6e74c36e",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=19d2e5447a0dd5f36d7e648492262c90d292c666",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=aa9773d48043db6c700f05cbcf3ad034dcb15761",
                    "width": 640,
                    "height": 359
                  },
                  {
                    "url": "https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8d02cdd020b1b671aea766bcf6625b2e345dc93a",
                    "width": 960,
                    "height": 539
                  },
                  {
                    "url": "https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=30e6c8ef3172bb477014b575290651e650238236",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mhiqqn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BoJackHorseMan53",
          "discussion_type": null,
          "num_comments": 89,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhiqqn/qwenimage_is_out/",
          "stickied": false,
          "url": "https://v.redd.it/4077mfg081hf1",
          "subreddit_subscribers": 510540,
          "created_utc": 1754326154,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/4077mfg081hf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/4077mfg081hf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/4077mfg081hf1/DASHPlaylist.mpd?a=1756989093%2CNjk5MWE4MGMyNjZlNGQwY2Q5NTlhZDEwZGVkZWZiYTNjZGEwZTc4N2MwZmI0MjRmMjBjNzgzNDFkNjY5OTMwMg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 116,
              "hls_url": "https://v.redd.it/4077mfg081hf1/HLSPlaylist.m3u8?a=1756989093%2CMTczYmEyZDI4NTQzOTU2MTE1MjYyMDFhYmZmNDJlYjA5ZTI3NjJiM2NiZmYwNWMxYmEwZjE1ZmY1OTViZjk4MQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_4ois9219",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Sam Altman watching Qwen drop model after model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 79,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhgu6t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 879,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 879,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/2ksLlK4qMpHN0wJRX622eitQNrDUtW9RxXFJtAx_L2U.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754321919,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/g7t8cmgrv0hf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/g7t8cmgrv0hf1.jpeg?auto=webp&amp;s=77f4646870c66b9ec3797f5deb047528f80e4e1f",
                  "width": 1170,
                  "height": 662
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/g7t8cmgrv0hf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=44351caaa0ea4099b9344000e708dfe04a848bdc",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://preview.redd.it/g7t8cmgrv0hf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a20f236ad7e68ab4eef6370b9ec71122c5dedbb",
                    "width": 216,
                    "height": 122
                  },
                  {
                    "url": "https://preview.redd.it/g7t8cmgrv0hf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9652e2e8eda7d2e28de7a1b107b0a8adc8c5dbf",
                    "width": 320,
                    "height": 181
                  },
                  {
                    "url": "https://preview.redd.it/g7t8cmgrv0hf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=460e01fe2091b6bc2347e41a918d258022a40353",
                    "width": 640,
                    "height": 362
                  },
                  {
                    "url": "https://preview.redd.it/g7t8cmgrv0hf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=83bf7b9bcffae519199437557b6b6d459d2e6727",
                    "width": 960,
                    "height": 543
                  },
                  {
                    "url": "https://preview.redd.it/g7t8cmgrv0hf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28f72356208bc50b38ca8b396360ae4b3b217050",
                    "width": 1080,
                    "height": 611
                  }
                ],
                "variants": {},
                "id": "_zkJ38t23EZXuiZZ8jVwSAIkImC1wga8Qejk17AVZuM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mhgu6t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheRealSerdra",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhgu6t/sam_altman_watching_qwen_drop_model_after_model/",
          "stickied": false,
          "url": "https://i.redd.it/g7t8cmgrv0hf1.jpeg",
          "subreddit_subscribers": 510540,
          "created_utc": 1754321919,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "At last after wait of few hours, ComfyUI now has support for Qwen-Image. Its from their [git repo](https://github.com/comfyanonymous/ComfyUI/pull/9179).\n\n",
          "author_fullname": "t2_8c6ji8bg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen-image now supported in ComfyUI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi1vov",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 52,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 52,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754375836,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At last after wait of few hours, ComfyUI now has support for Qwen-Image. Its from their &lt;a href=\"https://github.com/comfyanonymous/ComfyUI/pull/9179\"&gt;git repo&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/__JsZtcXYjOJBdf-yj1H7jt4OhvbL9fq0fnYqZ8VZTE.png?auto=webp&amp;s=1e73c9fa15d499e85baa1ccaa3b5ba87b0e04fe5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/__JsZtcXYjOJBdf-yj1H7jt4OhvbL9fq0fnYqZ8VZTE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b771b90ef6b66d110ba5fc17e7800c3c4c7b89d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/__JsZtcXYjOJBdf-yj1H7jt4OhvbL9fq0fnYqZ8VZTE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e43a5fe6c4eb44a08b3ce8ed13cbf102828d8355",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/__JsZtcXYjOJBdf-yj1H7jt4OhvbL9fq0fnYqZ8VZTE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a6521371979cec16447ad0ac7c905dfaf2d76fcf",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/__JsZtcXYjOJBdf-yj1H7jt4OhvbL9fq0fnYqZ8VZTE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7705231e7aa8e7da20a031dd6a89fc14492851ef",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/__JsZtcXYjOJBdf-yj1H7jt4OhvbL9fq0fnYqZ8VZTE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2820dea92769256b1acf61413c88efdadcefea54",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/__JsZtcXYjOJBdf-yj1H7jt4OhvbL9fq0fnYqZ8VZTE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b5825b4cfd1f6cbeadd2425846f101264bc33631",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "__JsZtcXYjOJBdf-yj1H7jt4OhvbL9fq0fnYqZ8VZTE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mi1vov",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lopsided_Dot_4557",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi1vov/qwenimage_now_supported_in_comfyui/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi1vov/qwenimage_now_supported_in_comfyui/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754375836,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🚀 Meet Qwen-Image — a 20B MMDiT model for next-gen text-to-image generation. Especially strong at creating stunning graphic posters with native text. Now open-source.\n\n🔍 Key Highlights:\n\n🔹 SOTA text rendering — rivals GPT-4o in English, best-in-class for Chinese\n\n🔹 In-pixel text generation — no overlays, fully integrated\n\n🔹 Bilingual support, diverse fonts, complex layouts\n\n🎨 Also excels at general image generation — from photorealistic to anime, impressionist to minimalist. A true creative powerhouse.\n\n\n",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🚀 Meet Qwen-Image",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 83,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhhctd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 667,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 667,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/MksEcgMoK6Jmwoikg8X4-cDPzaFA1Qb7KAVt6Vkaa8o.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754323091,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🚀 Meet Qwen-Image — a 20B MMDiT model for next-gen text-to-image generation. Especially strong at creating stunning graphic posters with native text. Now open-source.&lt;/p&gt;\n\n&lt;p&gt;🔍 Key Highlights:&lt;/p&gt;\n\n&lt;p&gt;🔹 SOTA text rendering — rivals GPT-4o in English, best-in-class for Chinese&lt;/p&gt;\n\n&lt;p&gt;🔹 In-pixel text generation — no overlays, fully integrated&lt;/p&gt;\n\n&lt;p&gt;🔹 Bilingual support, diverse fonts, complex layouts&lt;/p&gt;\n\n&lt;p&gt;🎨 Also excels at general image generation — from photorealistic to anime, impressionist to minimalist. A true creative powerhouse.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/7a463it8z0hf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/7a463it8z0hf1.jpeg?auto=webp&amp;s=3d345126dacc633d373b82475dde325d90c4c76f",
                  "width": 3665,
                  "height": 2181
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/7a463it8z0hf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fa3d443b56ce6f98c27acfab23a04897cc07af2c",
                    "width": 108,
                    "height": 64
                  },
                  {
                    "url": "https://preview.redd.it/7a463it8z0hf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=de833da505be2db0cdc52261408e16c27ab8f2db",
                    "width": 216,
                    "height": 128
                  },
                  {
                    "url": "https://preview.redd.it/7a463it8z0hf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=099ee55093c2e0a097645bf6c077a0a7f1218cfa",
                    "width": 320,
                    "height": 190
                  },
                  {
                    "url": "https://preview.redd.it/7a463it8z0hf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2a9bc46837ad4e1dac08bb6879d131c650ac6476",
                    "width": 640,
                    "height": 380
                  },
                  {
                    "url": "https://preview.redd.it/7a463it8z0hf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1bcf4277e31c7930cf3baf2cb6e844086778af3",
                    "width": 960,
                    "height": 571
                  },
                  {
                    "url": "https://preview.redd.it/7a463it8z0hf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f2c3dc19a952ac3c12699f79444b9c742d1caf7",
                    "width": 1080,
                    "height": 642
                  }
                ],
                "variants": {},
                "id": "SSvyPfajItjWlZ5XZvl958Hd8gBJ9Y2EIdDLuhYSzlg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mhhctd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 82,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhhctd/meet_qwenimage/",
          "stickied": false,
          "url": "https://i.redd.it/7a463it8z0hf1.jpeg",
          "subreddit_subscribers": 510540,
          "created_utc": 1754323091,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Fast and local TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB (based on the language). Can train on new voices.\n\nGithub Link: [https://github.com/OHF-Voice/piper1-gpl](https://github.com/OHF-Voice/piper1-gpl)\n\n",
          "author_fullname": "t2_etmr2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fast and local open source TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB. Can train on new voices.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mi6brm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/4f9mf37ap6hf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/4f9mf37ap6hf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/4f9mf37ap6hf1/DASHPlaylist.mpd?a=1756989093%2CNjNlNDZmM2ZkYTBhYzc0ZmU4ZWU3YWYwMjNjYmQ5ZTNiZWQ3MGQwYjZmZDI2ZmZmYzY1YWNkZmIyMGYzNWFmZg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 12,
              "hls_url": "https://v.redd.it/4f9mf37ap6hf1/HLSPlaylist.m3u8?a=1756989093%2CZDk0YTRkZTQ2ZWIzZDA3N2Y0MWFiZGM3NDY0MmYwMmVjNGMxNjFlZmNmNzQzMjExYWIyMDFlMWY4MzFhYWE2Ng%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Y3B1eTg4N2FwNmhmMcdJr-o9P4ZouvxhN_0BwF4rV8WaxRXMUy0jmPSG-wmF.png?width=140&amp;height=93&amp;crop=140:93,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=a81623d356a287a11530c6c829db8c681a127bf8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754392432,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Fast and local TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB (based on the language). Can train on new voices.&lt;/p&gt;\n\n&lt;p&gt;Github Link: &lt;a href=\"https://github.com/OHF-Voice/piper1-gpl\"&gt;https://github.com/OHF-Voice/piper1-gpl&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/4f9mf37ap6hf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Y3B1eTg4N2FwNmhmMcdJr-o9P4ZouvxhN_0BwF4rV8WaxRXMUy0jmPSG-wmF.png?format=pjpg&amp;auto=webp&amp;s=43772286681a15656ff49d01d87ec5d5a16fd720",
                  "width": 1536,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Y3B1eTg4N2FwNmhmMcdJr-o9P4ZouvxhN_0BwF4rV8WaxRXMUy0jmPSG-wmF.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ab39c76d32b254190a2a0b0839a0d803368dbfce",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://external-preview.redd.it/Y3B1eTg4N2FwNmhmMcdJr-o9P4ZouvxhN_0BwF4rV8WaxRXMUy0jmPSG-wmF.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=765968ceacc9f87ac2b392b62acee843e94ea5dc",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://external-preview.redd.it/Y3B1eTg4N2FwNmhmMcdJr-o9P4ZouvxhN_0BwF4rV8WaxRXMUy0jmPSG-wmF.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0337c6239ce125b6d649727cb6773afaa5ca76fc",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://external-preview.redd.it/Y3B1eTg4N2FwNmhmMcdJr-o9P4ZouvxhN_0BwF4rV8WaxRXMUy0jmPSG-wmF.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fa64aa582bd6cb8b8de59825713125ad9f2f032c",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://external-preview.redd.it/Y3B1eTg4N2FwNmhmMcdJr-o9P4ZouvxhN_0BwF4rV8WaxRXMUy0jmPSG-wmF.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d0844ecbad3d8ba4f40fc1da7f2b367b276a3c05",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/Y3B1eTg4N2FwNmhmMcdJr-o9P4ZouvxhN_0BwF4rV8WaxRXMUy0jmPSG-wmF.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=72b22f9c4663c1d05d6608beb5eccf580c165a0b",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "Y3B1eTg4N2FwNmhmMcdJr-o9P4ZouvxhN_0BwF4rV8WaxRXMUy0jmPSG-wmF"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mi6brm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "phone_radio_tv",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi6brm/fast_and_local_open_source_tts_engine_20/",
          "stickied": false,
          "url": "https://v.redd.it/4f9mf37ap6hf1",
          "subreddit_subscribers": 510540,
          "created_utc": 1754392432,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/4f9mf37ap6hf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/4f9mf37ap6hf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/4f9mf37ap6hf1/DASHPlaylist.mpd?a=1756989093%2CNjNlNDZmM2ZkYTBhYzc0ZmU4ZWU3YWYwMjNjYmQ5ZTNiZWQ3MGQwYjZmZDI2ZmZmYzY1YWNkZmIyMGYzNWFmZg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 12,
              "hls_url": "https://v.redd.it/4f9mf37ap6hf1/HLSPlaylist.m3u8?a=1756989093%2CZDk0YTRkZTQ2ZWIzZDA3N2Y0MWFiZGM3NDY0MmYwMmVjNGMxNjFlZmNmNzQzMjExYWIyMDFlMWY4MzFhYWE2Ng%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "FINALLY",
          "author_fullname": "t2_uptissiz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 GGUFs are coming",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mht910",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 158,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 158,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=e4e7a0c9fa14f2888f5f07598ff2ff6958188b94",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754349968,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;FINALLY&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/mradermacher/GLM-4.5-Air-GGUF",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?auto=webp&amp;s=4f29cdd9d86d464f3541c6b772fcf35782c87ded",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=13c5f6e652403be83b71873e1bbc87da605d3006",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=534e020aa7468d38197ed4c44174cf3a57584f8b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a9cae5191f82a2f0f0af1bc09cc8d9f635c7ec6",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cbaa78bb76999536a7337e9b0c9e2f578691b200",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d454810ac3eaf0025e5d8ae947c98b915310812f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=33f3ae9e2091370aa36282d51b361680fb28f807",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mht910",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pro-editor-1105",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mht910/glm_45_ggufs_are_coming/",
          "stickied": false,
          "url": "https://huggingface.co/mradermacher/GLM-4.5-Air-GGUF",
          "subreddit_subscribers": 510540,
          "created_utc": 1754349968,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "r/LocalLLaMA right now",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhe1rl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bbbdbf",
          "ups": 753,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 753,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/PPKozUiF3SYW1-d8ApDHQ6cvdG7JpTuJ10ijiFT_rnE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754315546,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/f0xr7mshc0hf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/f0xr7mshc0hf1.png?auto=webp&amp;s=406d23785d295484462e63db9c1321b110db9e37",
                  "width": 968,
                  "height": 1280
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/f0xr7mshc0hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e4fb1ed4f97bf1ef6b619b68d73f264e3545abe",
                    "width": 108,
                    "height": 142
                  },
                  {
                    "url": "https://preview.redd.it/f0xr7mshc0hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=36a72274c84378acb2518b49234bdf66ec250ea3",
                    "width": 216,
                    "height": 285
                  },
                  {
                    "url": "https://preview.redd.it/f0xr7mshc0hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e3f3479dd1b60048227ed41014880f4578f9763e",
                    "width": 320,
                    "height": 423
                  },
                  {
                    "url": "https://preview.redd.it/f0xr7mshc0hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a0d701cbdf33b1ec6ea47dd4b4874202dbea647e",
                    "width": 640,
                    "height": 846
                  },
                  {
                    "url": "https://preview.redd.it/f0xr7mshc0hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9cadb3c3eb6f9eefb04092b385b82e9f357ad09e",
                    "width": 960,
                    "height": 1269
                  }
                ],
                "variants": {},
                "id": "3GbmLMftfHxRNuEdGZh7axZUKeKfa45TSKaX6bMQoro"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mhe1rl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 81,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mhe1rl/rlocalllama_right_now/",
          "stickied": false,
          "url": "https://i.redd.it/f0xr7mshc0hf1.png",
          "subreddit_subscribers": 510540,
          "created_utc": 1754315546,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "support for GLM 4.5 family of models has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhlkyx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": "#bbbdbf",
          "ups": 291,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 291,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=b40f582039621f51c445ac0fcfdb827b930d7f2c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754332231,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14939",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?auto=webp&amp;s=083930ea54b88a7f6eaadda136c2185460baf66e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=78369c4a613d24a26f628c7b0d0788fbd02727b4",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7d7893c234acd63db0445e0010c29d3054bf72a",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=be66666d37f3f19b8f252dc5f32ba0b7be39e97c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e04ffa9cbd0a435f87d74eaf876a5853c1e06023",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=658e3d76ab28bc884f80a72e29bf1040fe464132",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=91b76178fba350b1d785fbd980fb39979366d649",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mhlkyx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 72,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mhlkyx/support_for_glm_45_family_of_models_has_been/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14939",
          "subreddit_subscribers": 510540,
          "created_utc": 1754332231,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_aedi2k9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Qwen Models Today!!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 54,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhbpmo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 739,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 739,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/TROn1uPQcH0PujeybIidpkc9G7nZ0H_qibt1MPjtmMI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754309520,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/qemmgysvuzgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/qemmgysvuzgf1.png?auto=webp&amp;s=42888269e8a8be81d80e8a6d5692747211e04c55",
                  "width": 1220,
                  "height": 476
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3f9e5dff4613eb055af874621d1a213848bf522f",
                    "width": 108,
                    "height": 42
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=06f6672b49f5b95cc45a9b23e3598d09d05496d7",
                    "width": 216,
                    "height": 84
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=28a355bc0adcfc06eaf4b216b3ef61b9d652f5eb",
                    "width": 320,
                    "height": 124
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e45a424e82bdde4384e3d7ba1be6631b2a25639",
                    "width": 640,
                    "height": 249
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fd0a3cc19923cbf380af7306ff3f4d5335556fb8",
                    "width": 960,
                    "height": 374
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f267cdf32b56403571256c354e628f69fadd7b15",
                    "width": 1080,
                    "height": 421
                  }
                ],
                "variants": {},
                "id": "34KTkl_1uxrvHPhAnaWXTjSZ6bmw11ut0GxXsPRfDZY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mhbpmo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "R46H4V",
          "discussion_type": null,
          "num_comments": 104,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhbpmo/new_qwen_models_today/",
          "stickied": false,
          "url": "https://i.redd.it/qemmgysvuzgf1.png",
          "subreddit_subscribers": 510540,
          "created_utc": 1754309520,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://x.com/OfficialLoganK/status/1952430214375493808](https://x.com/OfficialLoganK/status/1952430214375493808)",
          "author_fullname": "t2_1uzhw00y56",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gemini 3 is coming?..",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 24,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhl5yo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 200,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 200,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/EDf-MI0GF8x5ZjyG0YABbe4z3xjcAqC0nMWmMq6CEE0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754331340,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/OfficialLoganK/status/1952430214375493808\"&gt;https://x.com/OfficialLoganK/status/1952430214375493808&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/59joqndkn1hf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/59joqndkn1hf1.png?auto=webp&amp;s=728c888953031ef3f33df909853396c378ea05ee",
                  "width": 671,
                  "height": 116
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/59joqndkn1hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=318f53c18e18eee36e48194ccf41cec21ffc52db",
                    "width": 108,
                    "height": 18
                  },
                  {
                    "url": "https://preview.redd.it/59joqndkn1hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3754309bb4fb6ba5a78d3ebb019551b59ec630e8",
                    "width": 216,
                    "height": 37
                  },
                  {
                    "url": "https://preview.redd.it/59joqndkn1hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7e6fd5243afbed6f2958a397457b412f0bc4e31",
                    "width": 320,
                    "height": 55
                  },
                  {
                    "url": "https://preview.redd.it/59joqndkn1hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1e89c768daaac9653e4f2ad00c6a0ee5f6412107",
                    "width": 640,
                    "height": 110
                  }
                ],
                "variants": {},
                "id": "uhYd1Q9dnVcQrpLekV8fk4gEIwtVrTg3elgEcFfUKqM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhl5yo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SlerpE",
          "discussion_type": null,
          "num_comments": 72,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhl5yo/gemini_3_is_coming/",
          "stickied": false,
          "url": "https://i.redd.it/59joqndkn1hf1.png",
          "subreddit_subscribers": 510540,
          "created_utc": 1754331340,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**TLDR:** I built this **open source** and **local** app that lets your local models **watch your screen** and do stuff! It is now **suuuper easy to install** and use, to make local AI **accessible to** **everybody**!\n\nHey r/LocalLLaMA! I'm back with some Observer updates c: first of all **Thank You** so much for all of your support and feedback, i've been working hard to take this project to this current state. I added the app installation which is a significant QOL improvement for ease of use for first time users!! The docker-compose option is still supported and viable for people wanting a more specific and custom install.\n\nThe new app tools are a **game-changer**!! You can now have direct system-level pop ups or notifications that come up right **up to your face** hahaha. And sorry to everyone who tried out SMS and WhatsApp and were frustrated because you weren't getting notifications, Meta started blocking my account thinking i was just spamming messages to you guys.\n\nBut the pushover and discord notifications work perfectly well!\n\nIf you have any feedback please reach out through the discord, i'm really open to suggestions.\n\nThis is the projects [Github](https://github.com/Roy3838/Observer) (completely open source)  \nAnd the discord: [https://discord.gg/wnBb7ZQDUC](https://discord.gg/wnBb7ZQDUC)\n\nIf you have any questions i'll be hanging out here for a while!",
          "author_fullname": "t2_p443m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to use your Local Models to watch your screen. Open Source and Completely Free!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhrx3m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 87,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g3pod2zlw2hf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g3pod2zlw2hf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g3pod2zlw2hf1/DASHPlaylist.mpd?a=1756989093%2CNTM1YzBhMzFhNWYxMzE2ZTZkMTEyMzhmZjgxM2UwNTk2MGM5ZGVjZThmOTdjMTkyODQ1NTM3YzcxZGMxMGU0ZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 248,
              "hls_url": "https://v.redd.it/g3pod2zlw2hf1/HLSPlaylist.m3u8?a=1756989093%2CMWZhZmE2YzQ5ZGQxYjQwOWY1NDI3NWIzOTIyYzkzODhlYzYwNTY0ZWI4MmFlNTJjMTMzODAyOTZmNjk1YmRlYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 87,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=6b85a1bb34e9e7db698603fcebc73b8fd360d283",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754346605,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; I built this &lt;strong&gt;open source&lt;/strong&gt; and &lt;strong&gt;local&lt;/strong&gt; app that lets your local models &lt;strong&gt;watch your screen&lt;/strong&gt; and do stuff! It is now &lt;strong&gt;suuuper easy to install&lt;/strong&gt; and use, to make local AI &lt;strong&gt;accessible to&lt;/strong&gt; &lt;strong&gt;everybody&lt;/strong&gt;!&lt;/p&gt;\n\n&lt;p&gt;Hey r/LocalLLaMA! I&amp;#39;m back with some Observer updates c: first of all &lt;strong&gt;Thank You&lt;/strong&gt; so much for all of your support and feedback, i&amp;#39;ve been working hard to take this project to this current state. I added the app installation which is a significant QOL improvement for ease of use for first time users!! The docker-compose option is still supported and viable for people wanting a more specific and custom install.&lt;/p&gt;\n\n&lt;p&gt;The new app tools are a &lt;strong&gt;game-changer&lt;/strong&gt;!! You can now have direct system-level pop ups or notifications that come up right &lt;strong&gt;up to your face&lt;/strong&gt; hahaha. And sorry to everyone who tried out SMS and WhatsApp and were frustrated because you weren&amp;#39;t getting notifications, Meta started blocking my account thinking i was just spamming messages to you guys.&lt;/p&gt;\n\n&lt;p&gt;But the pushover and discord notifications work perfectly well!&lt;/p&gt;\n\n&lt;p&gt;If you have any feedback please reach out through the discord, i&amp;#39;m really open to suggestions.&lt;/p&gt;\n\n&lt;p&gt;This is the projects &lt;a href=\"https://github.com/Roy3838/Observer\"&gt;Github&lt;/a&gt; (completely open source)&lt;br/&gt;\nAnd the discord: &lt;a href=\"https://discord.gg/wnBb7ZQDUC\"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you have any questions i&amp;#39;ll be hanging out here for a while!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/g3pod2zlw2hf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?format=pjpg&amp;auto=webp&amp;s=c244884240671677f86749f32fc9389093c74e36",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0cafd483e776962ac85274492a6320c14f4f96e7",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a8ecd52b64c1d70725e4ee9f8d68e4818913af28",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=289118bfb31cf4da7f18b52f49d4b5323299468e",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bf3944d90b3c752805d02808f6d8a1f7bb9aa87a",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=57af74c0dbffe776156ed3cfe8ff487f087da0d4",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=18a084198c43fec7cfd85211109df2e00512f2ff",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mhrx3m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Roy3838",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhrx3m/how_to_use_your_local_models_to_watch_your_screen/",
          "stickied": false,
          "url": "https://v.redd.it/g3pod2zlw2hf1",
          "subreddit_subscribers": 510540,
          "created_utc": 1754346605,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g3pod2zlw2hf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g3pod2zlw2hf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g3pod2zlw2hf1/DASHPlaylist.mpd?a=1756989093%2CNTM1YzBhMzFhNWYxMzE2ZTZkMTEyMzhmZjgxM2UwNTk2MGM5ZGVjZThmOTdjMTkyODQ1NTM3YzcxZGMxMGU0ZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 248,
              "hls_url": "https://v.redd.it/g3pod2zlw2hf1/HLSPlaylist.m3u8?a=1756989093%2CMWZhZmE2YzQ5ZGQxYjQwOWY1NDI3NWIzOTIyYzkzODhlYzYwNTY0ZWI4MmFlNTJjMTMzODAyOTZmNjk1YmRlYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/3n3tfhqbj0hf1.png?width=529&amp;format=png&amp;auto=webp&amp;s=5234584d7973049a12fc3c428b50a1d35e48858f\n\nhttps://preview.redd.it/uxg8kr5ej0hf1.png?width=1664&amp;format=png&amp;auto=webp&amp;s=f06cc61180cfced07aa66367d23e552e605c0f75\n\nQwen image is ready to drop:https://github.com/huggingface/diffusers/pull/12055",
          "author_fullname": "t2_u398xzta",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen image 20B is coming!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "uxg8kr5ej0hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/uxg8kr5ej0hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2fcfc8d951cc2bccbf0ade68de7f27e62dfeeab9"
                },
                {
                  "y": 120,
                  "x": 216,
                  "u": "https://preview.redd.it/uxg8kr5ej0hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=27e928f28592fb88701fa631cfd74d00fd77612e"
                },
                {
                  "y": 178,
                  "x": 320,
                  "u": "https://preview.redd.it/uxg8kr5ej0hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f2c331eeefd2b5dfb25080342a3f21dca5499a5b"
                },
                {
                  "y": 356,
                  "x": 640,
                  "u": "https://preview.redd.it/uxg8kr5ej0hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=62b9a2502c11596faff798266ea7cf2bc24914fa"
                },
                {
                  "y": 535,
                  "x": 960,
                  "u": "https://preview.redd.it/uxg8kr5ej0hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0a39f911d55049fa1c419c36f6714ca64a12d529"
                },
                {
                  "y": 602,
                  "x": 1080,
                  "u": "https://preview.redd.it/uxg8kr5ej0hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7a6538e66dc7da30f1dcc6d6ad85f1e81705d3cf"
                }
              ],
              "s": {
                "y": 928,
                "x": 1664,
                "u": "https://preview.redd.it/uxg8kr5ej0hf1.png?width=1664&amp;format=png&amp;auto=webp&amp;s=f06cc61180cfced07aa66367d23e552e605c0f75"
              },
              "id": "uxg8kr5ej0hf1"
            },
            "3n3tfhqbj0hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 143,
                  "x": 108,
                  "u": "https://preview.redd.it/3n3tfhqbj0hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=146b14f2f3fc2707a993a9a97f3a60bb2723d2b6"
                },
                {
                  "y": 286,
                  "x": 216,
                  "u": "https://preview.redd.it/3n3tfhqbj0hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2ca82d65ca0223fb27a32b5d739c376bf79ef56f"
                },
                {
                  "y": 424,
                  "x": 320,
                  "u": "https://preview.redd.it/3n3tfhqbj0hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec51a5877ed2a2ed6c5670705517c391266e83d3"
                }
              ],
              "s": {
                "y": 702,
                "x": 529,
                "u": "https://preview.redd.it/3n3tfhqbj0hf1.png?width=529&amp;format=png&amp;auto=webp&amp;s=5234584d7973049a12fc3c428b50a1d35e48858f"
              },
              "id": "3n3tfhqbj0hf1"
            }
          },
          "name": "t3_1mhf0kl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 341,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 341,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=562eb305e5946c82a293b905b50f03961ef833dd",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1754317809,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/3n3tfhqbj0hf1.png?width=529&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5234584d7973049a12fc3c428b50a1d35e48858f\"&gt;https://preview.redd.it/3n3tfhqbj0hf1.png?width=529&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5234584d7973049a12fc3c428b50a1d35e48858f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/uxg8kr5ej0hf1.png?width=1664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f06cc61180cfced07aa66367d23e552e605c0f75\"&gt;https://preview.redd.it/uxg8kr5ej0hf1.png?width=1664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f06cc61180cfced07aa66367d23e552e605c0f75&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Qwen image is ready to drop:&lt;a href=\"https://github.com/huggingface/diffusers/pull/12055\"&gt;https://github.com/huggingface/diffusers/pull/12055&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?auto=webp&amp;s=c2f942543ac8d9ccfd9ae80646c3b7a133f8ba08",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5be1c7fe4a021337cd223c3f63b471eaee167539",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=190517067fa2ac3d0abfa3102bd8d0e9195a2419",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a2787be181adcc2160884957e19b14738dfdf391",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b2d120bc77d25aceff54c01b358c0fa229b74ef",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cd70e8035ced6265cd47615c4892322de1172577",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3a4c65249b77ca0b2996bdcb0af318f2a83e8f33",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mhf0kl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sunshinecheung",
          "discussion_type": null,
          "num_comments": 60,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhf0kl/qwen_image_20b_is_coming/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhf0kl/qwen_image_20b_is_coming/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754317809,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_on5es7pe3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I see people rushing to GLM Air GGUF's on this repo - what does this warning usually mean? I haven't seen a model flagged since we passed around pickled weights",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 96,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhx1kc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": "#bbbdbf",
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/32GFsn0Iqd2CVFX14X9oXpT7geOPGMgkybOdRgrvBAg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754360260,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8xcfsxcl14hf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8xcfsxcl14hf1.jpeg?auto=webp&amp;s=ce0c5063188d717b13e9f0b2680dde049ae39e3c",
                  "width": 1080,
                  "height": 741
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8xcfsxcl14hf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1843680fc5241a1208ee966b3390bd7243129de3",
                    "width": 108,
                    "height": 74
                  },
                  {
                    "url": "https://preview.redd.it/8xcfsxcl14hf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4ffed5da7d76ec048d08bb3a2616a2c362ba1247",
                    "width": 216,
                    "height": 148
                  },
                  {
                    "url": "https://preview.redd.it/8xcfsxcl14hf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1541e3268525b48fa40a1f77de08b042230b65d2",
                    "width": 320,
                    "height": 219
                  },
                  {
                    "url": "https://preview.redd.it/8xcfsxcl14hf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=50f98e0a3e5a4233ccef0bc4ee09bdf72af5c26e",
                    "width": 640,
                    "height": 439
                  },
                  {
                    "url": "https://preview.redd.it/8xcfsxcl14hf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3ed07c128ef8b528554ae4859a24b8fd701c56ea",
                    "width": 960,
                    "height": 658
                  },
                  {
                    "url": "https://preview.redd.it/8xcfsxcl14hf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1022daa9a19f845ca3f838c7857f8031921563c4",
                    "width": 1080,
                    "height": 741
                  }
                ],
                "variants": {},
                "id": "3ogH6u2X3kPt7XOhcag4Q9HT4mu_HmECgiIZ_fRs0oI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhx1kc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ForsookComparison",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mhx1kc/i_see_people_rushing_to_glm_air_ggufs_on_this/",
          "stickied": false,
          "url": "https://i.redd.it/8xcfsxcl14hf1.jpeg",
          "subreddit_subscribers": 510540,
          "created_utc": 1754360260,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I find that GLM4.5 can successfully understand and translate the slang in Chinese. Take an example in Seed-X-Challenge benchmark: the source text is \"离谱她妈给离谱开门 ​ 离谱到家了\", and this sentence needs to be translated in a way that captures its extremely absurd, rather than being translated literally.\n\nThe translation result of GPT-4o is \"Absurdity's mom opens the door for absurdity—it's utterly absurd.\"\n\nWhile the translation result of GLM4.5 is \"Ridiculous to the extreme - it's reached peak ridiculousness.\"\n\nIt seems that GLM4.5 has a better understanding of Chinese slang and produces better translations. Has anyone tried GLM4.5’s translation capabilities?",
          "author_fullname": "t2_1c9ruz7y97",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The translation capability of GLM4.5 for Chinese slang.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi3igq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754382137,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I find that GLM4.5 can successfully understand and translate the slang in Chinese. Take an example in Seed-X-Challenge benchmark: the source text is &amp;quot;离谱她妈给离谱开门 ​ 离谱到家了&amp;quot;, and this sentence needs to be translated in a way that captures its extremely absurd, rather than being translated literally.&lt;/p&gt;\n\n&lt;p&gt;The translation result of GPT-4o is &amp;quot;Absurdity&amp;#39;s mom opens the door for absurdity—it&amp;#39;s utterly absurd.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;While the translation result of GLM4.5 is &amp;quot;Ridiculous to the extreme - it&amp;#39;s reached peak ridiculousness.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;It seems that GLM4.5 has a better understanding of Chinese slang and produces better translations. Has anyone tried GLM4.5’s translation capabilities?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mi3igq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OddUnderstanding1633",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi3igq/the_translation_capability_of_glm45_for_chinese/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi3igq/the_translation_capability_of_glm45_for_chinese/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754382137,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Thought I'd share some knowledge after a week with an Mi50 32gb bought from Ebay. Was originally supposed to be a response but hyper-focus took over and this is more suited as a post.\n\nIt arrived new-looking. Anti-static bag, not a spec of dust and plastic peel still on the AMD Instinct branded shroud. Mine came with an extra radial fan which can be mounted on the back and connected to a 12v header. Some tape was necessary to direct the air into the heat-sink. I was sceptical about the capability of this small radial fan but it seem to keep the GPU edge under 80C under heavy use, though I have not stress tested it.\n\n**Weirdness**\n\nOne weird thing is how it is listed in lspci:\n\n&gt;0a:00.0 VGA compatible controller \\[0300\\]: Advanced Micro Devices, Inc. \\[AMD/ATI\\] Vega 20 \\[Radeon Pro Vega II/Radeon Pro Vega II Duo\\] \\[1002:66a3\\]\n\n&gt;Subsystem: Apple Inc. Vega 20 \\[Radeon Pro Vega II/Radeon Pro Vega II Duo\\] \\[106b:0201\\]\n\nWhich suggests it is not an Mi50 at all? Or some weird Chinese shifting of components. Note the Apple subsystem. In rocm-smi it does boost over 1700mhz and pull near 300w, which is consistent with Mi50 specs. However, Mi50 seem to be a cut down Radeon Pro Vega II. So maybe it is a Radeon Pro Vega II put on a Mi50 board and flashed with Mi50 BIOS? Could it be flashed back to a Radeon Pro Vega II. I have no idea, even less why that would make any sense. Maybe I'm just overthinking it.\n\nAnother curious thing is that the card lacks a fan or even fan header but reports fan speed in rocm-smi.\n\n**Working configuration**\n\nI got it to work on the following configuration\n\n&gt;GPU: AMD Instinct MI50 (32 GB, gfx906)\n\n&gt;Proxmox: 8.4.6\n\n&gt;Kernel: 6.8.12-4-pve (downgraded from 6.8.12-13-pve, though I am unsure if this mattered)\n\n&gt;OS in the Proxmox host: Debian 12 (Bookworm) + Ubuntu 24.04 (\"Noble\") repositories for ROCm\n\n&gt;ROCm-version: 6.4.2\n\n&gt;Driver: amdgpu-dkms installed after headers\n\nMy method was as stupid as it sounds. But it worked after hours if trial and error. Right now I am just happy it works.\n\n[https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html)\n\nRun the commands for ROCm Ubuntu 24.04, then AMDGPU driver commands for Ubuntu 24.04, and then the commands for ROCm Ubuntu 24.04 again. There's probably some way simpler way and maybe something else I did contributed. But right now I am happy it works without installing a 5.15 Ubuntu kernel and I can still use Proxmox.\n\n**Pass-through not working, LXC working fine**\n\nOnce it register in rocm-smi it was easy to use the OpenWebUI LXC community script to make an LXC container. Then I manually installed Ollama inside of it. I did not get it to work pass-through and I have not seen any example where this works. AMD also lists it as not compatible with pass-through. Use it bare metal. Make sure to give the LXC the resources /dev/kfd, /dev/dri/card0, and /dev/dri/renderD128 with the right GID.\n\n**Power draw**\n\nIdle power draw is 25w according to rocm-smi, which seems accurate compared to measure usage from the wall and UPS. During benchmarking it reached 220-260w and 68c.\n\n**Performance**\n\nThe card is in a server with a Ryzen 5 3600 and 64gb of ram, where the LXC container is limited to 8 cores and 8gb of ram. This seem to be overkill as basically all computation is done in the GPU and usage is under 20% of the 8 logical cores/4gb. The Mi50 boosts all the way to 1730mhz/&gt;95% usage and remains there.\n\nllm\\_benchmark:\n\n&gt;mistral:7b Median run average of eval rate:  63.754  tokens/s\n\n&gt;llama3.1:8b Median run average of eval rate: 56.772  tokens/s\n\n&gt;gemma2:9b Median run average of eval rate: 43.736  tokens/s\n\n&gt;llava:7b Median run average of eval rate: 74.874  tokens/s\n\nIt had a dip in performance on the 2nd run of 5 prompts and for some reason couldn't finish deepseek-r1:8b. Not sure why as I have been able to do deepseek-r1:32b just fine in OpenWebUI.\n\n**VRAM**\n\nVRAM is absolutely fantastic of course and the main reason to consider the Mi50 in my opinion. If not for the VRAM you may as well get an RTX 3060 12gb or similar from Nvidia to save you from some AMD driver headaches. 30b models doesn't seem to be any issues at all with vram to spare.\n\n**Conclusion**\n\nThe Mi50 right now gives you big GPU capability for a cheap price. In my opinion it is mainly for you who want the 32gb. I see less point in the 16gb, but it is even cheaper I suppose. Be aware though that AMD considers the Mi50 unsupported and depending on your use-case you may encounter a poor experience getting the drivers to work properly. Not to mention I don't think it works at all in Windows. It is not a card for someone who just want things to work, but it is cheap 32gb of HBM.",
          "author_fullname": "t2_5wpcuu4n",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mi50 32gb (Working config, weirdness and performance)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mi5s6w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754390636,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thought I&amp;#39;d share some knowledge after a week with an Mi50 32gb bought from Ebay. Was originally supposed to be a response but hyper-focus took over and this is more suited as a post.&lt;/p&gt;\n\n&lt;p&gt;It arrived new-looking. Anti-static bag, not a spec of dust and plastic peel still on the AMD Instinct branded shroud. Mine came with an extra radial fan which can be mounted on the back and connected to a 12v header. Some tape was necessary to direct the air into the heat-sink. I was sceptical about the capability of this small radial fan but it seem to keep the GPU edge under 80C under heavy use, though I have not stress tested it.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Weirdness&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;One weird thing is how it is listed in lspci:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;0a:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Vega 20 [Radeon Pro Vega II/Radeon Pro Vega II Duo] [1002:66a3]&lt;/p&gt;\n\n&lt;p&gt;Subsystem: Apple Inc. Vega 20 [Radeon Pro Vega II/Radeon Pro Vega II Duo] [106b:0201]&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Which suggests it is not an Mi50 at all? Or some weird Chinese shifting of components. Note the Apple subsystem. In rocm-smi it does boost over 1700mhz and pull near 300w, which is consistent with Mi50 specs. However, Mi50 seem to be a cut down Radeon Pro Vega II. So maybe it is a Radeon Pro Vega II put on a Mi50 board and flashed with Mi50 BIOS? Could it be flashed back to a Radeon Pro Vega II. I have no idea, even less why that would make any sense. Maybe I&amp;#39;m just overthinking it.&lt;/p&gt;\n\n&lt;p&gt;Another curious thing is that the card lacks a fan or even fan header but reports fan speed in rocm-smi.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Working configuration&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I got it to work on the following configuration&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;GPU: AMD Instinct MI50 (32 GB, gfx906)&lt;/p&gt;\n\n&lt;p&gt;Proxmox: 8.4.6&lt;/p&gt;\n\n&lt;p&gt;Kernel: 6.8.12-4-pve (downgraded from 6.8.12-13-pve, though I am unsure if this mattered)&lt;/p&gt;\n\n&lt;p&gt;OS in the Proxmox host: Debian 12 (Bookworm) + Ubuntu 24.04 (&amp;quot;Noble&amp;quot;) repositories for ROCm&lt;/p&gt;\n\n&lt;p&gt;ROCm-version: 6.4.2&lt;/p&gt;\n\n&lt;p&gt;Driver: amdgpu-dkms installed after headers&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;My method was as stupid as it sounds. But it worked after hours if trial and error. Right now I am just happy it works.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html\"&gt;https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Run the commands for ROCm Ubuntu 24.04, then AMDGPU driver commands for Ubuntu 24.04, and then the commands for ROCm Ubuntu 24.04 again. There&amp;#39;s probably some way simpler way and maybe something else I did contributed. But right now I am happy it works without installing a 5.15 Ubuntu kernel and I can still use Proxmox.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Pass-through not working, LXC working fine&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Once it register in rocm-smi it was easy to use the OpenWebUI LXC community script to make an LXC container. Then I manually installed Ollama inside of it. I did not get it to work pass-through and I have not seen any example where this works. AMD also lists it as not compatible with pass-through. Use it bare metal. Make sure to give the LXC the resources /dev/kfd, /dev/dri/card0, and /dev/dri/renderD128 with the right GID.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Power draw&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Idle power draw is 25w according to rocm-smi, which seems accurate compared to measure usage from the wall and UPS. During benchmarking it reached 220-260w and 68c.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Performance&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The card is in a server with a Ryzen 5 3600 and 64gb of ram, where the LXC container is limited to 8 cores and 8gb of ram. This seem to be overkill as basically all computation is done in the GPU and usage is under 20% of the 8 logical cores/4gb. The Mi50 boosts all the way to 1730mhz/&amp;gt;95% usage and remains there.&lt;/p&gt;\n\n&lt;p&gt;llm_benchmark:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;mistral:7b Median run average of eval rate:  63.754  tokens/s&lt;/p&gt;\n\n&lt;p&gt;llama3.1:8b Median run average of eval rate: 56.772  tokens/s&lt;/p&gt;\n\n&lt;p&gt;gemma2:9b Median run average of eval rate: 43.736  tokens/s&lt;/p&gt;\n\n&lt;p&gt;llava:7b Median run average of eval rate: 74.874  tokens/s&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It had a dip in performance on the 2nd run of 5 prompts and for some reason couldn&amp;#39;t finish deepseek-r1:8b. Not sure why as I have been able to do deepseek-r1:32b just fine in OpenWebUI.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;VRAM&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;VRAM is absolutely fantastic of course and the main reason to consider the Mi50 in my opinion. If not for the VRAM you may as well get an RTX 3060 12gb or similar from Nvidia to save you from some AMD driver headaches. 30b models doesn&amp;#39;t seem to be any issues at all with vram to spare.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The Mi50 right now gives you big GPU capability for a cheap price. In my opinion it is mainly for you who want the 32gb. I see less point in the 16gb, but it is even cheaper I suppose. Be aware though that AMD considers the Mi50 unsupported and depending on your use-case you may encounter a poor experience getting the drivers to work properly. Not to mention I don&amp;#39;t think it works at all in Windows. It is not a card for someone who just want things to work, but it is cheap 32gb of HBM.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mi5s6w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Danternas",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi5s6w/mi50_32gb_working_config_weirdness_and_performance/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi5s6w/mi50_32gb_working_config_weirdness_and_performance/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754390636,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_qqgbes3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Huawei released weights of Pangu Ultra,a 718B model.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhctvk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 322,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 70,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 322,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/b4vAhRXu0QISFGzKNI7MMEeFrdQG1UWQqC8GhQPUCNU.png?width=70&amp;height=70&amp;crop=70:70,smart&amp;auto=webp&amp;s=42d2a16045706201098a626b24ce7402818223f6",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754312524,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "ai.gitcode.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://ai.gitcode.com/ascend-tribe/openpangu-ultra-moe-718b-model/blob/main/README_EN.md",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/b4vAhRXu0QISFGzKNI7MMEeFrdQG1UWQqC8GhQPUCNU.png?auto=webp&amp;s=da6a5e01cd36a70882b4a98dbc5b14b02b19a809",
                  "width": 96,
                  "height": 96
                },
                "resolutions": [],
                "variants": {},
                "id": "b4vAhRXu0QISFGzKNI7MMEeFrdQG1UWQqC8GhQPUCNU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mhctvk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Overflow_al",
          "discussion_type": null,
          "num_comments": 60,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhctvk/huawei_released_weights_of_pangu_ultraa_718b_model/",
          "stickied": false,
          "url": "https://ai.gitcode.com/ascend-tribe/openpangu-ultra-moe-718b-model/blob/main/README_EN.md",
          "subreddit_subscribers": 510540,
          "created_utc": 1754312524,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🚀 Meet Qwen-Image — a 20B MMDiT model for next-gen text-to-image generation. Especially strong at creating stunning graphic posters with native text. Now open-source.\n\n🔍 Key Highlights:\n\n🔹 SOTA text rendering — rivals GPT-4o in English, best-in-class for Chinese\n\n🔹 In-pixel text generation — no overlays, fully integrated\n\n🔹 Bilingual support, diverse fonts, complex layouts\n\n🎨 Also excels at general image generation — from photorealistic to anime, impressionist to minimalist. A true creative powerhouse.\n\nBlog: https://qwenlm.github.io/blog/qwen-image/[Blog](https://qwenlm.github.io/blog/qwen-image/)\n\nHugging Face: [huggingface.co/Qwen/Qwen-Image](http://huggingface.co/Qwen/Qwen-Image)",
          "author_fullname": "t2_e9mfhlg7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen-Image — a 20B MMDiT model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhhhpi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 148,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 148,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754323671,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754323369,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🚀 Meet Qwen-Image — a 20B MMDiT model for next-gen text-to-image generation. Especially strong at creating stunning graphic posters with native text. Now open-source.&lt;/p&gt;\n\n&lt;p&gt;🔍 Key Highlights:&lt;/p&gt;\n\n&lt;p&gt;🔹 SOTA text rendering — rivals GPT-4o in English, best-in-class for Chinese&lt;/p&gt;\n\n&lt;p&gt;🔹 In-pixel text generation — no overlays, fully integrated&lt;/p&gt;\n\n&lt;p&gt;🔹 Bilingual support, diverse fonts, complex layouts&lt;/p&gt;\n\n&lt;p&gt;🎨 Also excels at general image generation — from photorealistic to anime, impressionist to minimalist. A true creative powerhouse.&lt;/p&gt;\n\n&lt;p&gt;Blog: &lt;a href=\"https://qwenlm.github.io/blog/qwen-image/%5BBlog%5D(https://qwenlm.github.io/blog/qwen-image/)\"&gt;https://qwenlm.github.io/blog/qwen-image/[Blog](https://qwenlm.github.io/blog/qwen-image/)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hugging Face: &lt;a href=\"http://huggingface.co/Qwen/Qwen-Image\"&gt;huggingface.co/Qwen/Qwen-Image&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mhhhpi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xhehab_",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhhhpi/qwenimage_a_20b_mmdit_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhhhpi/qwenimage_a_20b_mmdit_model/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754323369,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here is the original blog post: [https://blog.google/technology/ai/kaggle-game-arena/](https://blog.google/technology/ai/kaggle-game-arena/) \n\nAbout the benchmark, I personally prefer game as a head-to-head benchmark to LMArena. At least if they do benchmaxxing, we might have models that's more intelligent comparing to the more glazing effect of LMArena. \n\n  \nAbout the exhibition stream, it's funny to see they let Deepseek R1 play against o4-mini and Grok 4 play against gemini flash. Kimi-K2 vs O3 would be fun though. \n\nhttps://preview.redd.it/83xmndz6q1hf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=376f9bc5e41eb6f87a3f4c9cfd3e4be8aadf8814\n\n",
          "author_fullname": "t2_6mjqz0at",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Google introduces a new Benchmark: Game Arena and they're streaming your favorite open weight models playing chess against close source models.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "83xmndz6q1hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/83xmndz6q1hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bbb73ebf70ed7f8fe4edb4b29ff277e291b77a6d"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/83xmndz6q1hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d34e0673562d914366016c116db33e499f3ce9aa"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/83xmndz6q1hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bead8eabd9003c8382152c86297522e0510c667d"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/83xmndz6q1hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd802cf4c986633ec0b89a53831c9d72912f09d6"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/83xmndz6q1hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=094f632a5252c9911b447458f7f11504d60b9251"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/83xmndz6q1hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ef3ab4f4ba1906268baacf02f2b46029ee03f84b"
                }
              ],
              "s": {
                "y": 720,
                "x": 1280,
                "u": "https://preview.redd.it/83xmndz6q1hf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=376f9bc5e41eb6f87a3f4c9cfd3e4be8aadf8814"
              },
              "id": "83xmndz6q1hf1"
            }
          },
          "name": "t3_1mhlo6g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 94,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 94,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=8eae0b8d98140f6575481359160da04e2933f01e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1754332426,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is the original blog post: &lt;a href=\"https://blog.google/technology/ai/kaggle-game-arena/\"&gt;https://blog.google/technology/ai/kaggle-game-arena/&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;About the benchmark, I personally prefer game as a head-to-head benchmark to LMArena. At least if they do benchmaxxing, we might have models that&amp;#39;s more intelligent comparing to the more glazing effect of LMArena. &lt;/p&gt;\n\n&lt;p&gt;About the exhibition stream, it&amp;#39;s funny to see they let Deepseek R1 play against o4-mini and Grok 4 play against gemini flash. Kimi-K2 vs O3 would be fun though. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/83xmndz6q1hf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=376f9bc5e41eb6f87a3f4c9cfd3e4be8aadf8814\"&gt;https://preview.redd.it/83xmndz6q1hf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=376f9bc5e41eb6f87a3f4c9cfd3e4be8aadf8814&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?auto=webp&amp;s=bb83dc4652169f42acbd54e2777ee5ed8aa93432",
                  "width": 1300,
                  "height": 731
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e6c4c1e50bf3d40b8b76be77b34dbecd15f1ff79",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=653d78e943b4fecfa3184638cf2aa4fc2cc2ecb0",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e08d2c25b02ab639e27806ae3a4d28a7be6bd144",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc604dc195cda31f472811e38b2354a3cb7b4e27",
                    "width": 640,
                    "height": 359
                  },
                  {
                    "url": "https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0d9db50afa75d54d2a2dfc8f4debd87a0c7879e7",
                    "width": 960,
                    "height": 539
                  },
                  {
                    "url": "https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=900c76acd835190ef74ca8257cf1c3213a79312d",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhlo6g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mtmttuan",
          "discussion_type": null,
          "num_comments": 45,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhlo6g/google_introduces_a_new_benchmark_game_arena_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhlo6g/google_introduces_a_new_benchmark_game_arena_and/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754332426,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This model is insane!   I have been testing the ongoing llama.cpp PR and this morning has been amazing!    GLM can spit out LOOOOOOOOOOOOOOOOOONG tokens!     The original was a beast, and the new one is even better.   I gave it 2500 lines of python code, told it to refactor it, it do so without dropping anything!   Then I told it to translate it to ruby and it did so completely.   The model is very coherent across long contexts, the quality so far is great.   The model is fast!  Full loaded on 3090's, It starts out at 45tk/sec and this is with llama.cpp.\n\nI have only driven it for about an hour and this is the smaller model air, not the big one!  I'm very convinced that this will replace deepseek-r1/chimera/v3/ernie-300b/kimi-k2 for me.\n\nIs this better than sonnet/opus/gemini/openai?   For me yup!  I don't use closed models, so I really can't tell, but this so far is looking like the best damn model locally.  I have only thrown code generation at it, so I can't tell how it would perform in creative writing, role play, other sorts of generation etc.   I haven't played at all with tool calling, instruction following, etc, but based on how well it's responding, I think it's going to be great.  The only short coming I see is the 128k context window.\n\nIt's fast too, 50k+ token, 16.44 tk/sec\n\nslot      release: id  0 | task 42155 | stop processing: n\\_past = 51785, truncated = 0\n\nslot print\\_timing: id  0 | task 42155 |\n\nprompt eval time =     421.72 ms /    35 tokens (   12.05 ms per token,    82.99 tokens per second)\n\neval time =  983525.01 ms / 16169 tokens (   60.83 ms per token,    16.44 tokens per second)\n\nEdit:  \nq4 quants down to 67.85gb  \nI decide to run q4, offload only shared experts to 1 3090 GPU and the rest to system ram (ddr4 2400mhz quad channel on dual x99 platform).  The entire shared experts for 47 layers takes about 4gb of vram, that means you can put all of the shared expert on your 8gb GPU.  I decide to not load any other tensor but just these and see how it performs.  It start out at 10tk/sec.   I'm going to run q3\\_k\\_l on a 3060 and P40 and put up the results later.",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Get ready for GLM-4-5 local gguf woot woot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhg8rt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 165,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 165,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754326570,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754320611,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This model is insane!   I have been testing the ongoing llama.cpp PR and this morning has been amazing!    GLM can spit out LOOOOOOOOOOOOOOOOOONG tokens!     The original was a beast, and the new one is even better.   I gave it 2500 lines of python code, told it to refactor it, it do so without dropping anything!   Then I told it to translate it to ruby and it did so completely.   The model is very coherent across long contexts, the quality so far is great.   The model is fast!  Full loaded on 3090&amp;#39;s, It starts out at 45tk/sec and this is with llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;I have only driven it for about an hour and this is the smaller model air, not the big one!  I&amp;#39;m very convinced that this will replace deepseek-r1/chimera/v3/ernie-300b/kimi-k2 for me.&lt;/p&gt;\n\n&lt;p&gt;Is this better than sonnet/opus/gemini/openai?   For me yup!  I don&amp;#39;t use closed models, so I really can&amp;#39;t tell, but this so far is looking like the best damn model locally.  I have only thrown code generation at it, so I can&amp;#39;t tell how it would perform in creative writing, role play, other sorts of generation etc.   I haven&amp;#39;t played at all with tool calling, instruction following, etc, but based on how well it&amp;#39;s responding, I think it&amp;#39;s going to be great.  The only short coming I see is the 128k context window.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s fast too, 50k+ token, 16.44 tk/sec&lt;/p&gt;\n\n&lt;p&gt;slot      release: id  0 | task 42155 | stop processing: n_past = 51785, truncated = 0&lt;/p&gt;\n\n&lt;p&gt;slot print_timing: id  0 | task 42155 |&lt;/p&gt;\n\n&lt;p&gt;prompt eval time =     421.72 ms /    35 tokens (   12.05 ms per token,    82.99 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;eval time =  983525.01 ms / 16169 tokens (   60.83 ms per token,    16.44 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;br/&gt;\nq4 quants down to 67.85gb&lt;br/&gt;\nI decide to run q4, offload only shared experts to 1 3090 GPU and the rest to system ram (ddr4 2400mhz quad channel on dual x99 platform).  The entire shared experts for 47 layers takes about 4gb of vram, that means you can put all of the shared expert on your 8gb GPU.  I decide to not load any other tensor but just these and see how it performs.  It start out at 10tk/sec.   I&amp;#39;m going to run q3_k_l on a 3060 and P40 and put up the results later.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mhg8rt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 88,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mhg8rt/get_ready_for_glm45_local_gguf_woot_woot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhg8rt/get_ready_for_glm45_local_gguf_woot_woot/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754320611,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just tested the new **Qwen-Image** model from Alibaba using 🤗 Diffusers with bfloat16 + dual-GPU memory config (4090 + 3060). Prompted it to generate a **cyberpunk night market scene**—complete with neon signs, rainy pavement, futuristic street food vendors, and a monorail in the background.\n\nRan at `1472x832`, 32 steps, `true_cfg_scale=3.0`. No LoRA, no refiner—just straight from the base checkpoint.\n\nFull prompt and code below. Let me know what you think of the result or if you’ve got prompt ideas to push it further.\n\n\\`\\`\\`\n\nfrom diffusers import DiffusionPipeline\n\nimport torch, gc\n\npipe = DiffusionPipeline.from\\_pretrained(\n\n\"Qwen/Qwen-Image\",\n\ntorch\\_dtype=torch.bfloat16,\n\ndevice\\_map=\"balanced\",\n\nmax\\_memory={0: \"23GiB\", 1: \"11GiB\"},\n\n)\n\npipe.enable\\_attention\\_slicing()\n\npipe.enable\\_vae\\_tiling()\n\nprompt = (\n\n\"A bustling cyberpunk night market street scene. Neon signs in Chinese hang above steaming food stalls. \"\n\n\"A robotic vendor is grilling skewers while a crowd of futuristic characters—some wearing glowing visors, \"\n\n\"some holding umbrellas under a light drizzle—gathers around. Bright reflections on the wet pavement. \"\n\n\"In the distance, a monorail passes by above the alley. Ultra HD, 4K, cinematic composition.\"\n\n)\n\nnegative\\_prompt = (\n\n\"low quality, blurry, distorted, bad anatomy, text artifacts, poor lighting\"\n\n)\n\nimg = pipe(\n\nprompt=prompt,\n\nnegative\\_prompt=negative\\_prompt,\n\nwidth=1472, height=832,\n\nnum\\_inference\\_steps=32,\n\ntrue\\_cfg\\_scale=3.0,\n\ngenerator=torch.Generator(\"cuda\").manual\\_seed(8899)\n\n).images\\[0\\]\n\nimg.save(\"qwen\\_cyberpunk\\_market.png\")\n\ndel pipe; gc.collect(); torch.cuda.empty\\_cache()\n\n\\`\\`\\`\n\nhttps://preview.redd.it/0djfy60ms3hf1.png?width=1472&amp;format=png&amp;auto=webp&amp;s=77be5a785c6eb51c997d0faa3226371547b7fcdc\n\nthanks to [motorcycle\\_frenzy889](https://www.reddit.com/user/motorcycle_frenzy889/) , 60 steps can craft correct text.",
          "author_fullname": "t2_1tp8zldw5g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Quick Qwen Image Gen with 4090+3060",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 79,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "0djfy60ms3hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 61,
                  "x": 108,
                  "u": "https://preview.redd.it/0djfy60ms3hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ed73af255e180eefcaf4f4416e8f3d5ff52e51b"
                },
                {
                  "y": 122,
                  "x": 216,
                  "u": "https://preview.redd.it/0djfy60ms3hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8158189ea23a41651c117e3ec2ac454432f9aaf3"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/0djfy60ms3hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=803dbdcde5db1bb7de9d0c05f7252be2b6341562"
                },
                {
                  "y": 361,
                  "x": 640,
                  "u": "https://preview.redd.it/0djfy60ms3hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c1c2e36c90d21683b43c3a4bc55d9399c47a57d6"
                },
                {
                  "y": 542,
                  "x": 960,
                  "u": "https://preview.redd.it/0djfy60ms3hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bc610c14f101da632f68af20f0f44ae5d6ac30fe"
                },
                {
                  "y": 610,
                  "x": 1080,
                  "u": "https://preview.redd.it/0djfy60ms3hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=56095aadb8528ed17cdfeb89c4477e90ef5a136a"
                }
              ],
              "s": {
                "y": 832,
                "x": 1472,
                "u": "https://preview.redd.it/0djfy60ms3hf1.png?width=1472&amp;format=png&amp;auto=webp&amp;s=77be5a785c6eb51c997d0faa3226371547b7fcdc"
              },
              "id": "0djfy60ms3hf1"
            }
          },
          "name": "t3_1mhpm02",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 51,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 51,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/rc6cs4fuadas1yBd9N2cMHV6iAM0qvhoekwcLUHsAfU.jpg",
          "edited": 1754357261,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754341198,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just tested the new &lt;strong&gt;Qwen-Image&lt;/strong&gt; model from Alibaba using 🤗 Diffusers with bfloat16 + dual-GPU memory config (4090 + 3060). Prompted it to generate a &lt;strong&gt;cyberpunk night market scene&lt;/strong&gt;—complete with neon signs, rainy pavement, futuristic street food vendors, and a monorail in the background.&lt;/p&gt;\n\n&lt;p&gt;Ran at &lt;code&gt;1472x832&lt;/code&gt;, 32 steps, &lt;code&gt;true_cfg_scale=3.0&lt;/code&gt;. No LoRA, no refiner—just straight from the base checkpoint.&lt;/p&gt;\n\n&lt;p&gt;Full prompt and code below. Let me know what you think of the result or if you’ve got prompt ideas to push it further.&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;from diffusers import DiffusionPipeline&lt;/p&gt;\n\n&lt;p&gt;import torch, gc&lt;/p&gt;\n\n&lt;p&gt;pipe = DiffusionPipeline.from_pretrained(&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Qwen/Qwen-Image&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;torch_dtype=torch.bfloat16,&lt;/p&gt;\n\n&lt;p&gt;device_map=&amp;quot;balanced&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;max_memory={0: &amp;quot;23GiB&amp;quot;, 1: &amp;quot;11GiB&amp;quot;},&lt;/p&gt;\n\n&lt;p&gt;)&lt;/p&gt;\n\n&lt;p&gt;pipe.enable_attention_slicing()&lt;/p&gt;\n\n&lt;p&gt;pipe.enable_vae_tiling()&lt;/p&gt;\n\n&lt;p&gt;prompt = (&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;A bustling cyberpunk night market street scene. Neon signs in Chinese hang above steaming food stalls. &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;A robotic vendor is grilling skewers while a crowd of futuristic characters—some wearing glowing visors, &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;some holding umbrellas under a light drizzle—gathers around. Bright reflections on the wet pavement. &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;In the distance, a monorail passes by above the alley. Ultra HD, 4K, cinematic composition.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;)&lt;/p&gt;\n\n&lt;p&gt;negative_prompt = (&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;low quality, blurry, distorted, bad anatomy, text artifacts, poor lighting&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;)&lt;/p&gt;\n\n&lt;p&gt;img = pipe(&lt;/p&gt;\n\n&lt;p&gt;prompt=prompt,&lt;/p&gt;\n\n&lt;p&gt;negative_prompt=negative_prompt,&lt;/p&gt;\n\n&lt;p&gt;width=1472, height=832,&lt;/p&gt;\n\n&lt;p&gt;num_inference_steps=32,&lt;/p&gt;\n\n&lt;p&gt;true_cfg_scale=3.0,&lt;/p&gt;\n\n&lt;p&gt;generator=torch.Generator(&amp;quot;cuda&amp;quot;).manual_seed(8899)&lt;/p&gt;\n\n&lt;p&gt;).images[0]&lt;/p&gt;\n\n&lt;p&gt;img.save(&amp;quot;qwen_cyberpunk_market.png&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;del pipe; gc.collect(); torch.cuda.empty_cache()&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/0djfy60ms3hf1.png?width=1472&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=77be5a785c6eb51c997d0faa3226371547b7fcdc\"&gt;https://preview.redd.it/0djfy60ms3hf1.png?width=1472&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=77be5a785c6eb51c997d0faa3226371547b7fcdc&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;thanks to &lt;a href=\"https://www.reddit.com/user/motorcycle_frenzy889/\"&gt;motorcycle_frenzy889&lt;/a&gt; , 60 steps can craft correct text.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhpm02",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fp4guru",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhpm02/quick_qwen_image_gen_with_40903060/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhpm02/quick_qwen_image_gen_with_40903060/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754341198,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_17g3lg5snf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Qwen model has vision",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhdnye",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 161,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 161,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/qwP5df-vG6P0fFg9M5s5Bb9e4eBggj8ue2wrQPXOkpY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754314625,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/vypcvak2a0hf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/vypcvak2a0hf1.jpeg?auto=webp&amp;s=7b3f188c42679012c6d075a4fcc5a071d61204d7",
                  "width": 1080,
                  "height": 2400
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/vypcvak2a0hf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=812d44c02d8ace8217583425393f7b465984241c",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/vypcvak2a0hf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fb25ae2781a0c7777e59497293a17f1774ff05dc",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://preview.redd.it/vypcvak2a0hf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=992e05e9602caaa9daf551b46544dbc837f44c4b",
                    "width": 320,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/vypcvak2a0hf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb260c7a9be4af5c73e35111d5c25d23acd339bd",
                    "width": 640,
                    "height": 1280
                  },
                  {
                    "url": "https://preview.redd.it/vypcvak2a0hf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5818ada9a4af9e0c0af26645f62509d35e647bd4",
                    "width": 960,
                    "height": 1920
                  },
                  {
                    "url": "https://preview.redd.it/vypcvak2a0hf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=35d69a7f561bc9c11b13d04b86bbec74533e0e24",
                    "width": 1080,
                    "height": 2160
                  }
                ],
                "variants": {},
                "id": "t3HB7rEFEpA_c84oL5rMK_DhP-86PeLTNUvQX50FdoU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mhdnye",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Relative_Rope4234",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhdnye/new_qwen_model_has_vision/",
          "stickied": false,
          "url": "https://i.redd.it/vypcvak2a0hf1.jpeg",
          "subreddit_subscribers": 510540,
          "created_utc": 1754314625,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "When it comes to AI Roleplay, people have had both good and bad experiences with DeepSeek R1 and DeepSeek V3. We wanted to examine how DeepSeek R1 vs. V3 perform in roleplay when they go head-to-head against each other under different scenarios.\n\nThis little deep-dive will help you figure out which model will give you the experience you are looking for without wasting your time, request limits/tokens, or money.\n\n## 5 Different Characters, Several Themes, And Complete Conversation Logs\nWe tested both the models with **5 different characters**. We explored each scenario up to a satisfactory depth. \n\n- Knight Araeth Ruene by Yoiiru (Themes: Medieval, Politics, Morality)\n- Harumi – Your Traitorous Daughter from Jgag2 (Themes: Drama, Angst, Battle)\n- Time Looping Friend Amara Schwartz by Sleep Deprived (Themes: Sci-fi, Psychological Drama)\n- You’re A Ghost! Irish by Calrston (Themes: Paranormal, Comedy)\n- Royal Mess, Astrid by KornyPony (Themes: Fantasy, Magic, Fluff)\n\nComplete conversation logs for both models with each character is available for you to read through and understand how the models perform.\n\n## In-Depth Observations, Character Creator’s Opinions, And Conclusions.\nWe provide our in-depth observation along with the character creator's opinion on how the models portrayed their creation. If you want a TLDR, each scenario has a condensed conclusion!\n\n## Read The Article\nYou can read the article here: [**DeepSeek R1 vs. V3 – Which Is Better For AI Roleplay?**](https://rpwithai.com/deepseek-r1-vs-v3-for-roleplay/)\n\n***\n\n# The Final Conclusion\nAcross our five head-to-head roleplay tests, neither model claims dominance. Each excels in its own area.\n\n**DeepSeek R1** won three scenarios (Knight Araeth, Time-Looping Friend Amara, You’re a Ghost! Irish) by staying focused on character traits, providing deeper hypotheticals, and maintaining emotionally rich, dialogue-driven exchanges. Its strength is in consistent meta-reasoning and faithful, restrained portrayal, even if it sometimes feels heavy or needs more user guidance to push the action forward.\n\n**DeepSeek V3** took the lead in two scenarios (Traitorous Daughter Harumi, Royal Mess Astrid) by adding expressive flourishes, dynamic actions, and cinematic details that made characters feel more alive. It performs well when you want vivid, action-oriented storytelling, although it can sometimes lead to chaos or cut emotional beats short.\n\nIf you crave in-depth conversation, logical consistency, and true-to-character dialogue, DeepSeek R1 is your go-to. If you prefer a more visual, emotionally expressive, and fast-paced narrative, DeepSeek V3 will serve you better. Both models bring unique strengths; your choice should match the roleplay style you want to create.\n\n***\n\nThank you for taking your time to check this out!",
          "author_fullname": "t2_1t233wma4s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DeepSeek R1 vs. V3 - Going Head-To-Head In AI Roleplay",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mi7pei",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/078KX2JOJbgzKwAGsWu6xPvajl1VjjFOwNmdxWSzZUI.png?width=140&amp;height=93&amp;crop=140:93,smart&amp;auto=webp&amp;s=1682f05680696141ba12945550185ca2b9868ffc",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754396572,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "rpwithai.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When it comes to AI Roleplay, people have had both good and bad experiences with DeepSeek R1 and DeepSeek V3. We wanted to examine how DeepSeek R1 vs. V3 perform in roleplay when they go head-to-head against each other under different scenarios.&lt;/p&gt;\n\n&lt;p&gt;This little deep-dive will help you figure out which model will give you the experience you are looking for without wasting your time, request limits/tokens, or money.&lt;/p&gt;\n\n&lt;h2&gt;5 Different Characters, Several Themes, And Complete Conversation Logs&lt;/h2&gt;\n\n&lt;p&gt;We tested both the models with &lt;strong&gt;5 different characters&lt;/strong&gt;. We explored each scenario up to a satisfactory depth. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Knight Araeth Ruene by Yoiiru (Themes: Medieval, Politics, Morality)&lt;/li&gt;\n&lt;li&gt;Harumi – Your Traitorous Daughter from Jgag2 (Themes: Drama, Angst, Battle)&lt;/li&gt;\n&lt;li&gt;Time Looping Friend Amara Schwartz by Sleep Deprived (Themes: Sci-fi, Psychological Drama)&lt;/li&gt;\n&lt;li&gt;You’re A Ghost! Irish by Calrston (Themes: Paranormal, Comedy)&lt;/li&gt;\n&lt;li&gt;Royal Mess, Astrid by KornyPony (Themes: Fantasy, Magic, Fluff)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Complete conversation logs for both models with each character is available for you to read through and understand how the models perform.&lt;/p&gt;\n\n&lt;h2&gt;In-Depth Observations, Character Creator’s Opinions, And Conclusions.&lt;/h2&gt;\n\n&lt;p&gt;We provide our in-depth observation along with the character creator&amp;#39;s opinion on how the models portrayed their creation. If you want a TLDR, each scenario has a condensed conclusion!&lt;/p&gt;\n\n&lt;h2&gt;Read The Article&lt;/h2&gt;\n\n&lt;p&gt;You can read the article here: &lt;a href=\"https://rpwithai.com/deepseek-r1-vs-v3-for-roleplay/\"&gt;&lt;strong&gt;DeepSeek R1 vs. V3 – Which Is Better For AI Roleplay?&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h1&gt;The Final Conclusion&lt;/h1&gt;\n\n&lt;p&gt;Across our five head-to-head roleplay tests, neither model claims dominance. Each excels in its own area.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;DeepSeek R1&lt;/strong&gt; won three scenarios (Knight Araeth, Time-Looping Friend Amara, You’re a Ghost! Irish) by staying focused on character traits, providing deeper hypotheticals, and maintaining emotionally rich, dialogue-driven exchanges. Its strength is in consistent meta-reasoning and faithful, restrained portrayal, even if it sometimes feels heavy or needs more user guidance to push the action forward.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;DeepSeek V3&lt;/strong&gt; took the lead in two scenarios (Traitorous Daughter Harumi, Royal Mess Astrid) by adding expressive flourishes, dynamic actions, and cinematic details that made characters feel more alive. It performs well when you want vivid, action-oriented storytelling, although it can sometimes lead to chaos or cut emotional beats short.&lt;/p&gt;\n\n&lt;p&gt;If you crave in-depth conversation, logical consistency, and true-to-character dialogue, DeepSeek R1 is your go-to. If you prefer a more visual, emotionally expressive, and fast-paced narrative, DeepSeek V3 will serve you better. Both models bring unique strengths; your choice should match the roleplay style you want to create.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Thank you for taking your time to check this out!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://rpwithai.com/deepseek-r1-vs-v3-for-roleplay/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/078KX2JOJbgzKwAGsWu6xPvajl1VjjFOwNmdxWSzZUI.png?auto=webp&amp;s=44869442831747595ef311f94dac922177e266ab",
                  "width": 1200,
                  "height": 800
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/078KX2JOJbgzKwAGsWu6xPvajl1VjjFOwNmdxWSzZUI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a8a9099063a7974764b020a9ed879f8caeef9a04",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://external-preview.redd.it/078KX2JOJbgzKwAGsWu6xPvajl1VjjFOwNmdxWSzZUI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9fe927508decd17fe598e31969fe4a453dcb5b0e",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://external-preview.redd.it/078KX2JOJbgzKwAGsWu6xPvajl1VjjFOwNmdxWSzZUI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d74a26535c1bdc026d3295300ed437925f121120",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://external-preview.redd.it/078KX2JOJbgzKwAGsWu6xPvajl1VjjFOwNmdxWSzZUI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=959c9afd9f6202323ed8c3a296d7f74c135ccc04",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://external-preview.redd.it/078KX2JOJbgzKwAGsWu6xPvajl1VjjFOwNmdxWSzZUI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c25bd7561688c56b95db4d57ae466a412033ebd",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/078KX2JOJbgzKwAGsWu6xPvajl1VjjFOwNmdxWSzZUI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bce8f779b7fba849ae2554d20681890634f2d4bb",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "078KX2JOJbgzKwAGsWu6xPvajl1VjjFOwNmdxWSzZUI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mi7pei",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RPWithAI",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi7pei/deepseek_r1_vs_v3_going_headtohead_in_ai_roleplay/",
          "stickied": false,
          "url": "https://rpwithai.com/deepseek-r1-vs-v3-for-roleplay/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754396572,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Prompts:\n\n    write a story in german\n    write a story in french\n    write a story in italian\n    write a story in japanese",
          "author_fullname": "t2_exk6m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Exaone 4.0-1.2B is creating pretty wild fake language stories when asking to write in any other language than English or Korean.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 110,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "l1e008ll75hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 88,
                  "x": 108,
                  "u": "https://preview.redd.it/l1e008ll75hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=019bb34f9825880e5d41973c8f84a36e43a8ae4b"
                },
                {
                  "y": 176,
                  "x": 216,
                  "u": "https://preview.redd.it/l1e008ll75hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=47b455e72038c7cede47a3e84431c3dfa8b36b55"
                },
                {
                  "y": 261,
                  "x": 320,
                  "u": "https://preview.redd.it/l1e008ll75hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6dda723aae3a46a7437a2902db9d5e808b074fc8"
                },
                {
                  "y": 523,
                  "x": 640,
                  "u": "https://preview.redd.it/l1e008ll75hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce7032f41c2755d6d34d4a2a38ab7c87fc5a86be"
                },
                {
                  "y": 785,
                  "x": 960,
                  "u": "https://preview.redd.it/l1e008ll75hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6982f3a50b9ffcade6cdfa910797881648d663b4"
                }
              ],
              "s": {
                "y": 790,
                "x": 966,
                "u": "https://preview.redd.it/l1e008ll75hf1.png?width=966&amp;format=png&amp;auto=webp&amp;s=85469b0da7e34a2fd723840a7731f7300a4d98dc"
              },
              "id": "l1e008ll75hf1"
            },
            "8g6bxj7e75hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 85,
                  "x": 108,
                  "u": "https://preview.redd.it/8g6bxj7e75hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a5db9fa8b1d9ac0a52a7dc72ca8b72c658baacfd"
                },
                {
                  "y": 170,
                  "x": 216,
                  "u": "https://preview.redd.it/8g6bxj7e75hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d75acada8bf84d74be1a45a4401100519b97927f"
                },
                {
                  "y": 252,
                  "x": 320,
                  "u": "https://preview.redd.it/8g6bxj7e75hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b6f1d6c9503a81fe87b49e525043ffb733b232aa"
                },
                {
                  "y": 505,
                  "x": 640,
                  "u": "https://preview.redd.it/8g6bxj7e75hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2c17d7b21b410fd91603ddb9121122ca4d787129"
                },
                {
                  "y": 757,
                  "x": 960,
                  "u": "https://preview.redd.it/8g6bxj7e75hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9374e512d536165134e7b38666182e372b35797f"
                }
              ],
              "s": {
                "y": 761,
                "x": 964,
                "u": "https://preview.redd.it/8g6bxj7e75hf1.png?width=964&amp;format=png&amp;auto=webp&amp;s=997f200a328828f4836611591f2bac9bc0555230"
              },
              "id": "8g6bxj7e75hf1"
            },
            "fsuxumuj75hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 88,
                  "x": 108,
                  "u": "https://preview.redd.it/fsuxumuj75hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2ca59eb282c4a8acf6588f7651f9b8615a59f6c8"
                },
                {
                  "y": 176,
                  "x": 216,
                  "u": "https://preview.redd.it/fsuxumuj75hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f2e569da92c5e6d30b619fde06bf99ba75a593d"
                },
                {
                  "y": 260,
                  "x": 320,
                  "u": "https://preview.redd.it/fsuxumuj75hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d4d3952410ea7d1ebe975c1fb5d68c2c7bbd3dc4"
                },
                {
                  "y": 521,
                  "x": 640,
                  "u": "https://preview.redd.it/fsuxumuj75hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=557ea8277c4e84ae80853dd6f1ad4c9a916b7403"
                }
              ],
              "s": {
                "y": 772,
                "x": 947,
                "u": "https://preview.redd.it/fsuxumuj75hf1.png?width=947&amp;format=png&amp;auto=webp&amp;s=aa357686d7b02756777ea7bec08a4bc2def61caa"
              },
              "id": "fsuxumuj75hf1"
            },
            "4gp4ikbn75hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 55,
                  "x": 108,
                  "u": "https://preview.redd.it/4gp4ikbn75hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fe61a577c1c896a6a252af528ddd6e6b9c9316be"
                },
                {
                  "y": 111,
                  "x": 216,
                  "u": "https://preview.redd.it/4gp4ikbn75hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb5e3e9a630cd85cbb1d7442b68f284a4b253c50"
                },
                {
                  "y": 164,
                  "x": 320,
                  "u": "https://preview.redd.it/4gp4ikbn75hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4cf7e108788c8698f7a884281d370f64f13bc6f6"
                },
                {
                  "y": 328,
                  "x": 640,
                  "u": "https://preview.redd.it/4gp4ikbn75hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=43cbb104f042bcb43fd049d3f4a4602d8a213874"
                },
                {
                  "y": 493,
                  "x": 960,
                  "u": "https://preview.redd.it/4gp4ikbn75hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d26e3d4c797af0b33786fa54a2ae6849a9f4d88"
                }
              ],
              "s": {
                "y": 499,
                "x": 971,
                "u": "https://preview.redd.it/4gp4ikbn75hf1.png?width=971&amp;format=png&amp;auto=webp&amp;s=d9a1a09c14de21d52d17d6944e6dd00f67ca9d70"
              },
              "id": "4gp4ikbn75hf1"
            }
          },
          "name": "t3_1mi1hl9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 6,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "8g6bxj7e75hf1",
                "id": 721573829
              },
              {
                "media_id": "fsuxumuj75hf1",
                "id": 721573830
              },
              {
                "media_id": "l1e008ll75hf1",
                "id": 721573831
              },
              {
                "media_id": "4gp4ikbn75hf1",
                "id": 721573832
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/AKZiogJCbPhc1lG34obCAfh6yFGj6ZUwwFk3GzRaKbg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754374392,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Prompts:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;write a story in german\nwrite a story in french\nwrite a story in italian\nwrite a story in japanese\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mi1hl9",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mi1hl9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cpldcpu",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi1hl9/exaone_4012b_is_creating_pretty_wild_fake/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mi1hl9",
          "subreddit_subscribers": 510540,
          "created_utc": 1754374392,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nLooking to transcribe Japanese vocals in a track - wondering what the best LLM is to transcribe it to English?\n\ntrack is this:\nhttps://www.youtube.com/watch?v=ZGWgRa95xv8\n\nI also have the audio file. \n",
          "author_fullname": "t2_4nahh865",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are the best LLMs to transcribe Japanese audio to English?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mi6lek",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754393277,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking to transcribe Japanese vocals in a track - wondering what the best LLM is to transcribe it to English?&lt;/p&gt;\n\n&lt;p&gt;track is this:\n&lt;a href=\"https://www.youtube.com/watch?v=ZGWgRa95xv8\"&gt;https://www.youtube.com/watch?v=ZGWgRa95xv8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I also have the audio file. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/581jyGh8lbM909lg3YhO1-KQ70H0Vae1AS2u99zGgGo.jpeg?auto=webp&amp;s=ee770487fb27e03823d57945970260bd5ecbb1ee",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/581jyGh8lbM909lg3YhO1-KQ70H0Vae1AS2u99zGgGo.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3612915e6d1f169e9418850f016fa99b3a9e98af",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/581jyGh8lbM909lg3YhO1-KQ70H0Vae1AS2u99zGgGo.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=05c4f361495ebc4e27dff253d4d8a8a3329d4d82",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/581jyGh8lbM909lg3YhO1-KQ70H0Vae1AS2u99zGgGo.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9bf9363660d7265ede22f20b516ca975c45cd707",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "581jyGh8lbM909lg3YhO1-KQ70H0Vae1AS2u99zGgGo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi6lek",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nomiimon",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi6lek/what_are_the_best_llms_to_transcribe_japanese/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi6lek/what_are_the_best_llms_to_transcribe_japanese/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754393277,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen-Image · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhh6se",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 83,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 83,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=a720ed50938a0f7d099fa5095eeaa524819f6b87",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754322707,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen-Image",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?auto=webp&amp;s=7578d1430d51fe3898437256626f5bd7f9c643b5",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=55a19a341313ab08b43f3737ad0171a6dc27a3a6",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=561673e4e6ac3694e8d08fb8f3b50de1d4d7bafc",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5972624b3a9fe2057d6c44275f111b27e3e66505",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a65aaca919e956009709dd069f70c0c907403912",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5b308acfa4755323a0b8731404fd7599940db7ca",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1fcc7db6ec2f7f9365fb1224503b8e2a33a798c7",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mhh6se",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhh6se/qwenqwenimage_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen-Image",
          "subreddit_subscribers": 510540,
          "created_utc": 1754322707,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "With all the new models coming out recently, I've been more and more curious about this. It seems like a few months ago we were all running Gemma 3, now everybody seems to be running Qwen 3, but with recent model releases, which is your go-to daily-driver and why, and if you have secondary model(s), what do you use them for?\n\nI've got a 7900 XTX 24GB, so all of my models are &lt;32B. But here are mine;\n\n* Mistral Small 3.2: A \"better\" version of Gemma 3, in a way. I really liked Gemma 3, but it hallucinated far too much on basic facts. Mistral doesn't on the other hand, it hallucinates far less ime. I'm mainly using it for general knowledge and image analysis and consistently does a better job at both than Gemma for me. Feels a bit cold or sterile compared to Gemma 3 though.\n\n* Qwen 3 30B-A3B-Thinking-2507: The \"Gemini 2.5\" at home model. I've compared it pretty extensively to 2.5 Flash Reasoning, and 2.5 Pro, and it's able to consistently beat Flash and more often than not come close to or match 2.5 Pro. I'm mainly using this model for complex queries, problem solving, and writing. It's a damn good writing model imo, but that's not a *major* use-case for me.\n\n* Qwen 3-Coder 30B-A3B-Instruct-2507: This model acts a lot like a mix of Gemini, Claude, and an openAI model to me in my eyes. It's a really, really capable coder. I'm a software engineer and it's a nice companion in that regard. A lot of people say it's like most like Claude, and from what I've seen from Claude outputs, I tend to agree. although I've never used Claude, admittedly.\n\nSo there we have it, those are the models I use and the use-case for each. I do occasionally use OpenRouter to serve GLM 4.5-Air and Kimi K2, but that's mostly just out of curiosity. So what's everybody else here running?",
          "author_fullname": "t2_oqajf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's your 'primary' model and why? Do you run a secondary model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhp2e5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754339961,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With all the new models coming out recently, I&amp;#39;ve been more and more curious about this. It seems like a few months ago we were all running Gemma 3, now everybody seems to be running Qwen 3, but with recent model releases, which is your go-to daily-driver and why, and if you have secondary model(s), what do you use them for?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a 7900 XTX 24GB, so all of my models are &amp;lt;32B. But here are mine;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Mistral Small 3.2: A &amp;quot;better&amp;quot; version of Gemma 3, in a way. I really liked Gemma 3, but it hallucinated far too much on basic facts. Mistral doesn&amp;#39;t on the other hand, it hallucinates far less ime. I&amp;#39;m mainly using it for general knowledge and image analysis and consistently does a better job at both than Gemma for me. Feels a bit cold or sterile compared to Gemma 3 though.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Qwen 3 30B-A3B-Thinking-2507: The &amp;quot;Gemini 2.5&amp;quot; at home model. I&amp;#39;ve compared it pretty extensively to 2.5 Flash Reasoning, and 2.5 Pro, and it&amp;#39;s able to consistently beat Flash and more often than not come close to or match 2.5 Pro. I&amp;#39;m mainly using this model for complex queries, problem solving, and writing. It&amp;#39;s a damn good writing model imo, but that&amp;#39;s not a &lt;em&gt;major&lt;/em&gt; use-case for me.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Qwen 3-Coder 30B-A3B-Instruct-2507: This model acts a lot like a mix of Gemini, Claude, and an openAI model to me in my eyes. It&amp;#39;s a really, really capable coder. I&amp;#39;m a software engineer and it&amp;#39;s a nice companion in that regard. A lot of people say it&amp;#39;s like most like Claude, and from what I&amp;#39;ve seen from Claude outputs, I tend to agree. although I&amp;#39;ve never used Claude, admittedly.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So there we have it, those are the models I use and the use-case for each. I do occasionally use OpenRouter to serve GLM 4.5-Air and Kimi K2, but that&amp;#39;s mostly just out of curiosity. So what&amp;#39;s everybody else here running?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhp2e5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ayylmaonade",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754339961,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What kind of Qwen 2508 do you want tonight? ;)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 28,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhbvig",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": "#bbbdbf",
          "ups": 131,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 131,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/xW5rjzTOCVnresjRs9FDgm4fGUjo6_kF-kfsc1LJrK8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754309979,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/3f5by1b8wzgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/3f5by1b8wzgf1.png?auto=webp&amp;s=8943dfd99bf22f0cc0c02f506e7d3fd14a2f8352",
                  "width": 1216,
                  "height": 246
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c79933ab77e3d643053f37ab6382908ac1eb9af",
                    "width": 108,
                    "height": 21
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=768aae7713836c3c2a3aea894b45a4f967d9fa2f",
                    "width": 216,
                    "height": 43
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b20bde67926e3e8040aaf57a6389e737abd4fc69",
                    "width": 320,
                    "height": 64
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f4a0d87b3973237c269d2cef31fbc18fbe655b1",
                    "width": 640,
                    "height": 129
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2eec3557cccdfd47534f0ee7386653e12cd322ea",
                    "width": 960,
                    "height": 194
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=db751d3024ba27f2cf55f1c5324f9b111e96ef8c",
                    "width": 1080,
                    "height": 218
                  }
                ],
                "variants": {},
                "id": "COyPpURqpsrEIUBXoEpCUbhYmsp15c4NUFjXeHFSIu8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mhbvig",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 66,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mhbvig/what_kind_of_qwen_2508_do_you_want_tonight/",
          "stickied": false,
          "url": "https://i.redd.it/3f5by1b8wzgf1.png",
          "subreddit_subscribers": 510540,
          "created_utc": 1754309979,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,  \nI'm hoping someone can help me.  \nCurrently, I'm creating an agentic workflow.  \nMy agent has a tool called `interact_with_customer`.  \nWith this tool, the agent should be able to communicate with the customer.  \nThat means the method should send a message to the frontend and also wait until a response is received.  \nThis sounds simple, but it's turning out to be a real struggle, especially with the WebSocket connection and related issues.  \nIs there anyone who can give me some advice?  \nThanks!",
          "author_fullname": "t2_59schni7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI Agent Human Feedback within Tool Use",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi5er2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754389306,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;br/&gt;\nI&amp;#39;m hoping someone can help me.&lt;br/&gt;\nCurrently, I&amp;#39;m creating an agentic workflow.&lt;br/&gt;\nMy agent has a tool called &lt;code&gt;interact_with_customer&lt;/code&gt;.&lt;br/&gt;\nWith this tool, the agent should be able to communicate with the customer.&lt;br/&gt;\nThat means the method should send a message to the frontend and also wait until a response is received.&lt;br/&gt;\nThis sounds simple, but it&amp;#39;s turning out to be a real struggle, especially with the WebSocket connection and related issues.&lt;br/&gt;\nIs there anyone who can give me some advice?&lt;br/&gt;\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi5er2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Zealousideal_Sir_328",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi5er2/ai_agent_human_feedback_within_tool_use/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi5er2/ai_agent_human_feedback_within_tool_use/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754389306,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was having a very tough time in getting OCR of Medical Prescriptions. Medical prescriptions have so many different formats. Conversion to a JSON directly causes issues. So to preserve the structure and the semantic meaning I thought to convert it to ASCII.\n\n[https://limewire.com/d/JGqOt#o7boivJrZv](https://limewire.com/d/JGqOt#o7boivJrZv)\n\nThis is what I got as an Output from Gemini 2.5Pro thinking. Now the structure is somewhat preserved but the table runs all the way down. Also in some parts the position is wrong.  \n  \n**Now my Question is how to convert this using an open source VLM ? Which VLM to use ? How to fine tune ? I want it to use ASCII characters and if there are no tables then don't make them** \n\n*TLDR - See link . Want to OCR Medical Prescription and convert to ASCII for structure preservation . But structure must be very similar to Original*",
          "author_fullname": "t2_7m88zu40",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OCR Recognition and ASCII Generation of Medical Prescription",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi3b19",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754381335,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was having a very tough time in getting OCR of Medical Prescriptions. Medical prescriptions have so many different formats. Conversion to a JSON directly causes issues. So to preserve the structure and the semantic meaning I thought to convert it to ASCII.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://limewire.com/d/JGqOt#o7boivJrZv\"&gt;https://limewire.com/d/JGqOt#o7boivJrZv&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is what I got as an Output from Gemini 2.5Pro thinking. Now the structure is somewhat preserved but the table runs all the way down. Also in some parts the position is wrong.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Now my Question is how to convert this using an open source VLM ? Which VLM to use ? How to fine tune ? I want it to use ASCII characters and if there are no tables then don&amp;#39;t make them&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;TLDR - See link . Want to OCR Medical Prescription and convert to ASCII for structure preservation . But structure must be very similar to Original&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi3b19",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rukelele_Dixit21",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi3b19/ocr_recognition_and_ascii_generation_of_medical/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi3b19/ocr_recognition_and_ascii_generation_of_medical/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754381335,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have an old laptop i5 1145g7 11gen 2x8gb ddr4 ram iris xe igpu 8bg shared vram. I recently came across [intel article to run llms utilizing igpu in 11,12,13 gen](https://www.intel.com/content/www/us/en/developer/articles/technical/run-llms-on-gpus-using-llama-cpp.html). I have been trying to run [this](https://huggingface.co/mradermacher/UIGEN-X-4B-0729-GGUF) model which i have used a lot on ollama but it takes really long. Saw posts here telling to use llama.cpp so i decided to give it a shot. i downloaded sycl zip from llama.cpp github and i can see the igpu working but dont see any improvement in performance it takes similar or maybe more time than ollama to generate output. one issue i noticed is that on default context size 4096 whenever it reached the limit, It would just repeat the last token in loop whereas in ollama, the same default context size did cause loop but never repeated the same token infact it would give a coherent code which works fantastically and then would proceed to answer again in loop and not stopping. \n\nAs im new to all this i used gemini deepthink and came up with the following but it doesnt work at all. Any help would be greatly appreciated and also if anyone has managed to successfully increased token/s using sycl backend please let me know if it was worth it or not thanks.\n\nWhat gemini deepthink recommended:\n\nllama-cli.exe -m \"E:\\\\llama sycl\\\\models\\\\unigenx4bq4s.gguf\" -p \"Create a breath taking saas page with modern features, glassmorphism design, cyberpunk aesthetic, modern Css animations/transitions and make responsive, functional buttons\" --ctx-size 8192 -ngl 99 -fa -t 8 --mlock --temp 0.7 --top-k 20 --top-p 0.8 --min-p 0.0 --presence-penalty 1.5 --repeat-penalty 1.05 --repeat-last-n 256 --cache-type-k q4\\_0 --cache-type-v q4\\_0 --no-mmap",
          "author_fullname": "t2_1jiqvabt65",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is llama.cpp sycl backend really worth it?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi1tns",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754375620,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an old laptop i5 1145g7 11gen 2x8gb ddr4 ram iris xe igpu 8bg shared vram. I recently came across &lt;a href=\"https://www.intel.com/content/www/us/en/developer/articles/technical/run-llms-on-gpus-using-llama-cpp.html\"&gt;intel article to run llms utilizing igpu in 11,12,13 gen&lt;/a&gt;. I have been trying to run &lt;a href=\"https://huggingface.co/mradermacher/UIGEN-X-4B-0729-GGUF\"&gt;this&lt;/a&gt; model which i have used a lot on ollama but it takes really long. Saw posts here telling to use llama.cpp so i decided to give it a shot. i downloaded sycl zip from llama.cpp github and i can see the igpu working but dont see any improvement in performance it takes similar or maybe more time than ollama to generate output. one issue i noticed is that on default context size 4096 whenever it reached the limit, It would just repeat the last token in loop whereas in ollama, the same default context size did cause loop but never repeated the same token infact it would give a coherent code which works fantastically and then would proceed to answer again in loop and not stopping. &lt;/p&gt;\n\n&lt;p&gt;As im new to all this i used gemini deepthink and came up with the following but it doesnt work at all. Any help would be greatly appreciated and also if anyone has managed to successfully increased token/s using sycl backend please let me know if it was worth it or not thanks.&lt;/p&gt;\n\n&lt;p&gt;What gemini deepthink recommended:&lt;/p&gt;\n\n&lt;p&gt;llama-cli.exe -m &amp;quot;E:\\llama sycl\\models\\unigenx4bq4s.gguf&amp;quot; -p &amp;quot;Create a breath taking saas page with modern features, glassmorphism design, cyberpunk aesthetic, modern Css animations/transitions and make responsive, functional buttons&amp;quot; --ctx-size 8192 -ngl 99 -fa -t 8 --mlock --temp 0.7 --top-k 20 --top-p 0.8 --min-p 0.0 --presence-penalty 1.5 --repeat-penalty 1.05 --repeat-last-n 256 --cache-type-k q4_0 --cache-type-v q4_0 --no-mmap&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dxXECHarBSKz1hyLrBwlOcCTI-CwnnfIQLajz9bmY9Y.png?auto=webp&amp;s=7ff6c745939c452914047ff3d2df441274bd34cb",
                  "width": 1920,
                  "height": 1920
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dxXECHarBSKz1hyLrBwlOcCTI-CwnnfIQLajz9bmY9Y.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=89ff655453a7c1ebf52a06d95507c865ac6aab18",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/dxXECHarBSKz1hyLrBwlOcCTI-CwnnfIQLajz9bmY9Y.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0fb3beddf7df72d642bad515b481f4c1d0c95547",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/dxXECHarBSKz1hyLrBwlOcCTI-CwnnfIQLajz9bmY9Y.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=469c0192771376534793365f556747a418d28ecd",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/dxXECHarBSKz1hyLrBwlOcCTI-CwnnfIQLajz9bmY9Y.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1b1e620beec81c523bfb35cc5443ec0a0159ff65",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/dxXECHarBSKz1hyLrBwlOcCTI-CwnnfIQLajz9bmY9Y.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8fa2193b23b44c7ed205780e62af28a37aba2405",
                    "width": 960,
                    "height": 960
                  },
                  {
                    "url": "https://external-preview.redd.it/dxXECHarBSKz1hyLrBwlOcCTI-CwnnfIQLajz9bmY9Y.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=68cb581f80f80d878acfee3903a1799931dbc330",
                    "width": 1080,
                    "height": 1080
                  }
                ],
                "variants": {},
                "id": "dxXECHarBSKz1hyLrBwlOcCTI-CwnnfIQLajz9bmY9Y"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi1tns",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sweet_Eggplant4659",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi1tns/is_llamacpp_sycl_backend_really_worth_it/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi1tns/is_llamacpp_sycl_backend_really_worth_it/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754375620,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Style control removed.\n\n|Rank (UB)|Model|Score|95% CI (±)|Votes|Company|License|\n|:-|:-|:-|:-|:-|:-|:-|\n|1|gemini-2.5-pro|1470|±5|26,019|Google|Closed|\n|2|grok-4-0709|1435|±6|13,058|xAI|Closed|\n|2|glm-4.5|1435|±9|4,112|[Z.ai](http://Z.ai)|MIT|\n|2|chatgpt-4o-latest-20250326|1430|±5|30,777|Closed AI|Closed|\n|2|o3-2025-04-16|1429|±5|32,033|Closed AI|Closed|\n|2|deepseek-r1-0528|1427|±6|18,284|DeepSeek|MIT|\n|2|qwen3-235b-a22b-instruct-2507|1427|±9|4,154|Alibaba|Apache 2.0|\n\n[https://x.com/lmarena\\_ai/status/1952402506497020330](https://x.com/lmarena_ai/status/1952402506497020330)\n\n[https://lmarena.ai/leaderboard/text](https://lmarena.ai/leaderboard/text)",
          "author_fullname": "t2_m40tjcn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM ranks #2 for chat according to lmarena",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhix7d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 45,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754326721,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754326551,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Style control removed.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Rank (UB)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Score&lt;/th&gt;\n&lt;th align=\"left\"&gt;95% CI (±)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Votes&lt;/th&gt;\n&lt;th align=\"left\"&gt;Company&lt;/th&gt;\n&lt;th align=\"left\"&gt;License&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;gemini-2.5-pro&lt;/td&gt;\n&lt;td align=\"left\"&gt;1470&lt;/td&gt;\n&lt;td align=\"left\"&gt;±5&lt;/td&gt;\n&lt;td align=\"left\"&gt;26,019&lt;/td&gt;\n&lt;td align=\"left\"&gt;Google&lt;/td&gt;\n&lt;td align=\"left\"&gt;Closed&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;grok-4-0709&lt;/td&gt;\n&lt;td align=\"left\"&gt;1435&lt;/td&gt;\n&lt;td align=\"left\"&gt;±6&lt;/td&gt;\n&lt;td align=\"left\"&gt;13,058&lt;/td&gt;\n&lt;td align=\"left\"&gt;xAI&lt;/td&gt;\n&lt;td align=\"left\"&gt;Closed&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;glm-4.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;1435&lt;/td&gt;\n&lt;td align=\"left\"&gt;±9&lt;/td&gt;\n&lt;td align=\"left\"&gt;4,112&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;MIT&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;chatgpt-4o-latest-20250326&lt;/td&gt;\n&lt;td align=\"left\"&gt;1430&lt;/td&gt;\n&lt;td align=\"left\"&gt;±5&lt;/td&gt;\n&lt;td align=\"left\"&gt;30,777&lt;/td&gt;\n&lt;td align=\"left\"&gt;Closed AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;Closed&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;o3-2025-04-16&lt;/td&gt;\n&lt;td align=\"left\"&gt;1429&lt;/td&gt;\n&lt;td align=\"left\"&gt;±5&lt;/td&gt;\n&lt;td align=\"left\"&gt;32,033&lt;/td&gt;\n&lt;td align=\"left\"&gt;Closed AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;Closed&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;deepseek-r1-0528&lt;/td&gt;\n&lt;td align=\"left\"&gt;1427&lt;/td&gt;\n&lt;td align=\"left\"&gt;±6&lt;/td&gt;\n&lt;td align=\"left\"&gt;18,284&lt;/td&gt;\n&lt;td align=\"left\"&gt;DeepSeek&lt;/td&gt;\n&lt;td align=\"left\"&gt;MIT&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;qwen3-235b-a22b-instruct-2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;1427&lt;/td&gt;\n&lt;td align=\"left\"&gt;±9&lt;/td&gt;\n&lt;td align=\"left\"&gt;4,154&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apache 2.0&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;a href=\"https://x.com/lmarena_ai/status/1952402506497020330\"&gt;https://x.com/lmarena_ai/status/1952402506497020330&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://lmarena.ai/leaderboard/text\"&gt;https://lmarena.ai/leaderboard/text&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhix7d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Terminator857",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhix7d/glm_ranks_2_for_chat_according_to_lmarena/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhix7d/glm_ranks_2_for_chat_according_to_lmarena/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754326551,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**I’ve worked really hard and launched a FREE resource with 30+ detailed tutorials for building comprehensive production-level AI agents, as part of my Gen AI educational initiative.**\n\nThe tutorials cover all the key components you need to create agents that are ready for real-world deployment. I plan to keep adding more tutorials over time and will make sure the content stays up to date.\n\nThe response so far has been incredible! (the repo got nearly 10,000 stars in one month from launch - all organic) This is part of my broader effort to create high-quality open source educational material. I already have over 130 code tutorials on GitHub with over 50,000 stars.\n\nI hope you find it useful. The tutorials are available here: [https://github.com/NirDiamant/agents-towards-production](https://github.com/NirDiamant/agents-towards-production)\n\n(most of the tutorials can be run locally, but some of them don't, so please enjoy those who are and don't hate me for those how aren't :D )\n\nThe content is organized into these categories:\n\n1. Orchestration\n2. Tool integration\n3. Observability\n4. Deployment\n5. Memory\n6. UI &amp; Frontend\n7. Agent Frameworks\n8. Model Customization\n9. Multi-agent Coordination\n10. Security\n11. Evaluation\n12. Tracing &amp; Debugging\n13. Web Scraping",
          "author_fullname": "t2_4x84zf5l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A free goldmine of tutorials for the components you need to create production-level agents\nExtensive open source resource with tutorials for creating robust AI agents",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhhy47",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 43,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 43,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754324380,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;I’ve worked really hard and launched a FREE resource with 30+ detailed tutorials for building comprehensive production-level AI agents, as part of my Gen AI educational initiative.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The tutorials cover all the key components you need to create agents that are ready for real-world deployment. I plan to keep adding more tutorials over time and will make sure the content stays up to date.&lt;/p&gt;\n\n&lt;p&gt;The response so far has been incredible! (the repo got nearly 10,000 stars in one month from launch - all organic) This is part of my broader effort to create high-quality open source educational material. I already have over 130 code tutorials on GitHub with over 50,000 stars.&lt;/p&gt;\n\n&lt;p&gt;I hope you find it useful. The tutorials are available here: &lt;a href=\"https://github.com/NirDiamant/agents-towards-production\"&gt;https://github.com/NirDiamant/agents-towards-production&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;(most of the tutorials can be run locally, but some of them don&amp;#39;t, so please enjoy those who are and don&amp;#39;t hate me for those how aren&amp;#39;t :D )&lt;/p&gt;\n\n&lt;p&gt;The content is organized into these categories:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Orchestration&lt;/li&gt;\n&lt;li&gt;Tool integration&lt;/li&gt;\n&lt;li&gt;Observability&lt;/li&gt;\n&lt;li&gt;Deployment&lt;/li&gt;\n&lt;li&gt;Memory&lt;/li&gt;\n&lt;li&gt;UI &amp;amp; Frontend&lt;/li&gt;\n&lt;li&gt;Agent Frameworks&lt;/li&gt;\n&lt;li&gt;Model Customization&lt;/li&gt;\n&lt;li&gt;Multi-agent Coordination&lt;/li&gt;\n&lt;li&gt;Security&lt;/li&gt;\n&lt;li&gt;Evaluation&lt;/li&gt;\n&lt;li&gt;Tracing &amp;amp; Debugging&lt;/li&gt;\n&lt;li&gt;Web Scraping&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/23K9xyykQzGugCXJyC20OMixBbwPZe-S5vv1or7jJHM.png?auto=webp&amp;s=d017e4b3980f41b097659de2d8b747fc4e9c4c69",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/23K9xyykQzGugCXJyC20OMixBbwPZe-S5vv1or7jJHM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45b07e7616995751a757eb80d771bad2ee406619",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/23K9xyykQzGugCXJyC20OMixBbwPZe-S5vv1or7jJHM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=562290004967cb312a5c268f53c8c35b73c5f15f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/23K9xyykQzGugCXJyC20OMixBbwPZe-S5vv1or7jJHM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b3c364fe34ee3f00d4c026b7767af05aee0562fa",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/23K9xyykQzGugCXJyC20OMixBbwPZe-S5vv1or7jJHM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bd6cf48cc2c23488347c25c8e9102da79a44ec5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/23K9xyykQzGugCXJyC20OMixBbwPZe-S5vv1or7jJHM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f66d0734736f318d392878aec339b4d288894401",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/23K9xyykQzGugCXJyC20OMixBbwPZe-S5vv1or7jJHM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d2ceace9004b5c00980685a03a3ff9c4aa2227ed",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "23K9xyykQzGugCXJyC20OMixBbwPZe-S5vv1or7jJHM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mhhy47",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nir777",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhhy47/a_free_goldmine_of_tutorials_for_the_components/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhhy47/a_free_goldmine_of_tutorials_for_the_components/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754324380,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Current status:\n\nhttps://github.com/ggml-org/llama.cpp/pull/14939#issuecomment-3150197036\n\nEveryone get ready to fire up your GPUs...",
          "author_fullname": "t2_1utnp17o3h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5 llama.cpp PR is nearing completion",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhb5el",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 105,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 105,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754307842,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Current status:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14939#issuecomment-3150197036\"&gt;https://github.com/ggml-org/llama.cpp/pull/14939#issuecomment-3150197036&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Everyone get ready to fire up your GPUs...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhb5el",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DistanceSolar1449",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhb5el/glm45_llamacpp_pr_is_nearing_completion/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhb5el/glm45_llamacpp_pr_is_nearing_completion/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754307842,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA! I'm struggling to get decent performance from the new GLM-4.5-Air model and could use some help finding the optimal config.\n\n**Hardware:**\n\n* RTX 4090 (24GB VRAM)\n* 64GB DDR4 RAM\n* Using latest llama.cpp build (6088 with clang 19.1.5)\n\n**Model:**\n\n* DevQuasar/zai-org.GLM-4.5-Air-GGUF (Q4\\_K\\_M, 6 shards, \\~67GB total)\n* 110B parameters, MoE with 128 experts (8 active)\n\n**Current working config:**\n\n    llama-server -hf DevQuasar/zai-org.GLM-4.5-Air-GGUF:zai-org.GLM-4.5-Air.Q4_K_M-00001-of-00006.gguf -fa -c 4096 -ngl 99 -ot \".ffn_.*_exps.=CPU\" --port 8081\n\n**Performance issues:**\n\n* Only getting **3.37 tokens/sec** generation\n* **1.53 tokens/sec** prompt processing\n* GPU utilization only **12%** (should be much higher!)\n* GPU memory usage: 7.7GB/24GB (tons of headroom)\n* System RAM: 62.9GB/64GB (98% full - this seems to be the bottleneck)\n\n**What I've tried:**\n\n* Without expert offloading (`-ot` flag) → OOMs trying to allocate 66GB on 24GB GPU\n* Higher `-ngl` values without expert offloading → System freezes/crashes after 30min loading\n* Different batch sizes → No improvement\n* For comparison, 12B dense models get 40-50 TPS on this setup",
          "author_fullname": "t2_uptissiz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help optimizing GLM-4.5-Air 110B (Q4_K_M) on RTX 4090 + 64GB RAM - Getting only 3.37 TPS",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhsyv9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754349244,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;! I&amp;#39;m struggling to get decent performance from the new GLM-4.5-Air model and could use some help finding the optimal config.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;RTX 4090 (24GB VRAM)&lt;/li&gt;\n&lt;li&gt;64GB DDR4 RAM&lt;/li&gt;\n&lt;li&gt;Using latest llama.cpp build (6088 with clang 19.1.5)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;DevQuasar/zai-org.GLM-4.5-Air-GGUF (Q4_K_M, 6 shards, ~67GB total)&lt;/li&gt;\n&lt;li&gt;110B parameters, MoE with 128 experts (8 active)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Current working config:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama-server -hf DevQuasar/zai-org.GLM-4.5-Air-GGUF:zai-org.GLM-4.5-Air.Q4_K_M-00001-of-00006.gguf -fa -c 4096 -ngl 99 -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; --port 8081\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Performance issues:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Only getting &lt;strong&gt;3.37 tokens/sec&lt;/strong&gt; generation&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;1.53 tokens/sec&lt;/strong&gt; prompt processing&lt;/li&gt;\n&lt;li&gt;GPU utilization only &lt;strong&gt;12%&lt;/strong&gt; (should be much higher!)&lt;/li&gt;\n&lt;li&gt;GPU memory usage: 7.7GB/24GB (tons of headroom)&lt;/li&gt;\n&lt;li&gt;System RAM: 62.9GB/64GB (98% full - this seems to be the bottleneck)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;ve tried:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Without expert offloading (&lt;code&gt;-ot&lt;/code&gt; flag) → OOMs trying to allocate 66GB on 24GB GPU&lt;/li&gt;\n&lt;li&gt;Higher &lt;code&gt;-ngl&lt;/code&gt; values without expert offloading → System freezes/crashes after 30min loading&lt;/li&gt;\n&lt;li&gt;Different batch sizes → No improvement&lt;/li&gt;\n&lt;li&gt;For comparison, 12B dense models get 40-50 TPS on this setup&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhsyv9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pro-editor-1105",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754349244,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm working on a robot that uses a server-based LLM for voice conversations, but I'm planning to add an on-device LLM as a fallback when there's no internet connection.\n\nHere are the current specs:\n\n* **CPU**: Cortex-A53 x 4 @ 1.8GHz\n* **RAM**: 8GB LPDDR4\n* **OS**: Android (AOSP-based)\n\nI've asked models like ChatGPT and Gemini, and got mixed answers. Some say it's possible to run a 4-bit quantized model on a Cortex-A53, while others say it's not feasible.\n\nAlso, when it comes to natural voice interaction, some say **5 tokens per second (TPS)** is enough, while others insist you need at least **30 TPS** for smooth conversations. I'm a bit confused.\n\nFor lightweight, auxiliary voice interactions, what TPS rate would be considered sufficient? And what kind of hardware specs would realistically support that?",
          "author_fullname": "t2_1v0amj15h7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Confused About TPS Needs for On-Device LLM: 5 vs 30 TPS for Voice?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi1wwr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754375967,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a robot that uses a server-based LLM for voice conversations, but I&amp;#39;m planning to add an on-device LLM as a fallback when there&amp;#39;s no internet connection.&lt;/p&gt;\n\n&lt;p&gt;Here are the current specs:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: Cortex-A53 x 4 @ 1.8GHz&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: 8GB LPDDR4&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt;: Android (AOSP-based)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ve asked models like ChatGPT and Gemini, and got mixed answers. Some say it&amp;#39;s possible to run a 4-bit quantized model on a Cortex-A53, while others say it&amp;#39;s not feasible.&lt;/p&gt;\n\n&lt;p&gt;Also, when it comes to natural voice interaction, some say &lt;strong&gt;5 tokens per second (TPS)&lt;/strong&gt; is enough, while others insist you need at least &lt;strong&gt;30 TPS&lt;/strong&gt; for smooth conversations. I&amp;#39;m a bit confused.&lt;/p&gt;\n\n&lt;p&gt;For lightweight, auxiliary voice interactions, what TPS rate would be considered sufficient? And what kind of hardware specs would realistically support that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi1wwr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Public_Paint8683",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi1wwr/confused_about_tps_needs_for_ondevice_llm_5_vs_30/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi1wwr/confused_about_tps_needs_for_ondevice_llm_5_vs_30/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754375967,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, i wanted to experiment around with quantisation and get to more about them indepth. Hence ill be using the Gemma 3 1B QAT unquantised model. Which one should i use, there is and Int4 version and a Q4\\_0 version. Which libraries or tools should i use to quantise. Just need guidance on it.",
          "author_fullname": "t2_aedi2k9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best quantisation method, libraries and tools?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mi7sg0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754396806,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, i wanted to experiment around with quantisation and get to more about them indepth. Hence ill be using the Gemma 3 1B QAT unquantised model. Which one should i use, there is and Int4 version and a Q4_0 version. Which libraries or tools should i use to quantise. Just need guidance on it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi7sg0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "R46H4V",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi7sg0/best_quantisation_method_libraries_and_tools/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi7sg0/best_quantisation_method_libraries_and_tools/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754396806,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I saw the weights were released (they were supposed to be [a couple days ago](https://www.reddit.com/r/LocalLLaMA/comments/1mghy1u/qihoo360lightif32b/), but [the upload failed](https://huggingface.co/qihoo360/Light-IF-32B/discussions/1#68901831d6b49f34311d92d3)), but nobody made GGUFs, so I used GGUF-my-repo (no imatrix) and gave it a shot. [https://huggingface.co/DeProgrammer/Light-IF-32B-Q4\\_K\\_M-GGUF](https://huggingface.co/DeProgrammer/Light-IF-32B-Q4_K_M-GGUF)\n\nIt's based on Qwen3-32B, and they don't seem to have lobotomized it, but I couldn't tell a difference from trying two \"write a bunch of code based on this spec\" prompts because most recent models follow instructions pretty well. (This is a fine-tune focused on instruction following, if you didn't catch that.) For the third attempt, I used a prompt from my first LLM experiment last March, trying to generate informational text with a lot of constraints in the form of JavaScript.\n\nI ran it with temperature = 0, and it got stuck in a loop on one out of three prompts. I also think its overthinking is about on par with QwQ. This is about 9500 tokens of thinking: [Prompt](https://pastebin.com/XAau1rxk), [response](https://pastebin.com/xW6CLQtP)\n\nHow I'd rate the results on a per-requirement basis (again, keeping in mind this is Q4\\_K\\_M and no imatrix):\n\n|Result (out of 10)|Requirement|Notes|\n|:-|:-|:-|\n|9|Valid JavaScript|It didn't escape an example it gave using `&lt;` despite the inclusion of HTML nodes in my example; i.e., it didn't **infer** that it should escape HTML.|\n|7|you should try to structure the information with many levels of hierarchy and more details at each level|It stuck to strictly 3 levels. My example doesn't show an accordion in an accordion, but the instructions did indicate that it's allowed.|\n|4|Top-level categories don't have to make statements or suggestions, but others must do so to the maximum feasible extent.|The accordion nodes don't make any sort of statement or suggestion; they're just labels. Additionally, one item node said \"failing to\"--not a statement or suggestion.|\n|10|Place any lengthy reasoning (if it requires more than five words), more esoteric knowledge (language-, domain-, library-, or algorithm-specific), and examples in POP nodes.||\n|10|The KnowledgeNode constructor's first parameter can be 0 for all your responses||\n|6|Dig deep for obscure knowledge and tips; skip generally-obvious information|By design, I believe LLMs aren't particularly good at obscurity.|\n|8|write terse statements and imperative sentences instead of using wordy \"professional college textbook author\" language|I don't think the text could be *much* shorter.|\n|8|but also describing conflicting views|It recommended markdown, wikis, note-taking apps, *and* a dedicated tool.|\n|4|(unstated) Subtrees should not contain similar numbers of nodes just for the sake of consistency|This is another thing I noticed LLMs in general seem to love--3-4 points in every section even though it's unrealistic for there to be such a perfect split in real-world categories. It produced 3, 3, 3, 4, 4, 4, but at least there were 6 accordion nodes.|",
          "author_fullname": "t2_w4j8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Light-IF-32B weights",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhxhi5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754361785,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754361519,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw the weights were released (they were supposed to be &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1mghy1u/qihoo360lightif32b/\"&gt;a couple days ago&lt;/a&gt;, but &lt;a href=\"https://huggingface.co/qihoo360/Light-IF-32B/discussions/1#68901831d6b49f34311d92d3\"&gt;the upload failed&lt;/a&gt;), but nobody made GGUFs, so I used GGUF-my-repo (no imatrix) and gave it a shot. &lt;a href=\"https://huggingface.co/DeProgrammer/Light-IF-32B-Q4_K_M-GGUF\"&gt;https://huggingface.co/DeProgrammer/Light-IF-32B-Q4_K_M-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s based on Qwen3-32B, and they don&amp;#39;t seem to have lobotomized it, but I couldn&amp;#39;t tell a difference from trying two &amp;quot;write a bunch of code based on this spec&amp;quot; prompts because most recent models follow instructions pretty well. (This is a fine-tune focused on instruction following, if you didn&amp;#39;t catch that.) For the third attempt, I used a prompt from my first LLM experiment last March, trying to generate informational text with a lot of constraints in the form of JavaScript.&lt;/p&gt;\n\n&lt;p&gt;I ran it with temperature = 0, and it got stuck in a loop on one out of three prompts. I also think its overthinking is about on par with QwQ. This is about 9500 tokens of thinking: &lt;a href=\"https://pastebin.com/XAau1rxk\"&gt;Prompt&lt;/a&gt;, &lt;a href=\"https://pastebin.com/xW6CLQtP\"&gt;response&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;How I&amp;#39;d rate the results on a per-requirement basis (again, keeping in mind this is Q4_K_M and no imatrix):&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Result (out of 10)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Requirement&lt;/th&gt;\n&lt;th align=\"left\"&gt;Notes&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;9&lt;/td&gt;\n&lt;td align=\"left\"&gt;Valid JavaScript&lt;/td&gt;\n&lt;td align=\"left\"&gt;It didn&amp;#39;t escape an example it gave using &lt;code&gt;&amp;lt;&lt;/code&gt; despite the inclusion of HTML nodes in my example; i.e., it didn&amp;#39;t &lt;strong&gt;infer&lt;/strong&gt; that it should escape HTML.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;you should try to structure the information with many levels of hierarchy and more details at each level&lt;/td&gt;\n&lt;td align=\"left\"&gt;It stuck to strictly 3 levels. My example doesn&amp;#39;t show an accordion in an accordion, but the instructions did indicate that it&amp;#39;s allowed.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;Top-level categories don&amp;#39;t have to make statements or suggestions, but others must do so to the maximum feasible extent.&lt;/td&gt;\n&lt;td align=\"left\"&gt;The accordion nodes don&amp;#39;t make any sort of statement or suggestion; they&amp;#39;re just labels. Additionally, one item node said &amp;quot;failing to&amp;quot;--not a statement or suggestion.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;10&lt;/td&gt;\n&lt;td align=\"left\"&gt;Place any lengthy reasoning (if it requires more than five words), more esoteric knowledge (language-, domain-, library-, or algorithm-specific), and examples in POP nodes.&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;10&lt;/td&gt;\n&lt;td align=\"left\"&gt;The KnowledgeNode constructor&amp;#39;s first parameter can be 0 for all your responses&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;6&lt;/td&gt;\n&lt;td align=\"left\"&gt;Dig deep for obscure knowledge and tips; skip generally-obvious information&lt;/td&gt;\n&lt;td align=\"left\"&gt;By design, I believe LLMs aren&amp;#39;t particularly good at obscurity.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;write terse statements and imperative sentences instead of using wordy &amp;quot;professional college textbook author&amp;quot; language&lt;/td&gt;\n&lt;td align=\"left\"&gt;I don&amp;#39;t think the text could be &lt;em&gt;much&lt;/em&gt; shorter.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;but also describing conflicting views&lt;/td&gt;\n&lt;td align=\"left\"&gt;It recommended markdown, wikis, note-taking apps, &lt;em&gt;and&lt;/em&gt; a dedicated tool.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;(unstated) Subtrees should not contain similar numbers of nodes just for the sake of consistency&lt;/td&gt;\n&lt;td align=\"left\"&gt;This is another thing I noticed LLMs in general seem to love--3-4 points in every section even though it&amp;#39;s unrealistic for there to be such a perfect split in real-world categories. It produced 3, 3, 3, 4, 4, 4, but at least there were 6 accordion nodes.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tI1sapCmqbZGukHzbMC9_a1hbFiuJ7D5e252D-gJnrc.png?auto=webp&amp;s=d1d445d7e1c7e87311bd6343052f8b3d97c89755",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tI1sapCmqbZGukHzbMC9_a1hbFiuJ7D5e252D-gJnrc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a54e357184df8fd6f26574e2c0b7b571a1e22ce",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/tI1sapCmqbZGukHzbMC9_a1hbFiuJ7D5e252D-gJnrc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ffe22b9b72abd7c913d9f976f6c446d16f1c478",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/tI1sapCmqbZGukHzbMC9_a1hbFiuJ7D5e252D-gJnrc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=64af7db451b58e1345e62b5b21fb831dcae9c1ea",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/tI1sapCmqbZGukHzbMC9_a1hbFiuJ7D5e252D-gJnrc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=80887f38291adb0acf529d86a8f4b04f1953edb6",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/tI1sapCmqbZGukHzbMC9_a1hbFiuJ7D5e252D-gJnrc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b8bd084a33806e89ad5f4d5bdfa76a5cac18c3db",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/tI1sapCmqbZGukHzbMC9_a1hbFiuJ7D5e252D-gJnrc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6a4ef870f8531e9522139c71b54b46b7308ec566",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "tI1sapCmqbZGukHzbMC9_a1hbFiuJ7D5e252D-gJnrc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhxhi5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DeProgrammer99",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhxhi5/lightif32b_weights/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhxhi5/lightif32b_weights/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754361519,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I am looking to buy a new machine for running AI locally, preferably under 1200USD.\n\nMy end goal is to use AI to assist me with everything in life including work but I would like it to run locally.\n\nI have traditionally been a Windows-Android kinda guy but I am leaning towards buying Apple Mac Studio and run on my Windows laptop. What do you think, it the AI ecosystem on Apple as mature as it is on Windows?\n\nWhich one should I choose?",
          "author_fullname": "t2_1rfhip3pge",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking to buy a new machine - Mac vs NVIDIA",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mi7lwi",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754396303,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am looking to buy a new machine for running AI locally, preferably under 1200USD.&lt;/p&gt;\n\n&lt;p&gt;My end goal is to use AI to assist me with everything in life including work but I would like it to run locally.&lt;/p&gt;\n\n&lt;p&gt;I have traditionally been a Windows-Android kinda guy but I am leaning towards buying Apple Mac Studio and run on my Windows laptop. What do you think, it the AI ecosystem on Apple as mature as it is on Windows?&lt;/p&gt;\n\n&lt;p&gt;Which one should I choose?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi7lwi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "KindlyAnything1996",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi7lwi/looking_to_buy_a_new_machine_mac_vs_nvidia/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi7lwi/looking_to_buy_a_new_machine_mac_vs_nvidia/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754396303,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\"Unlike traditional approaches that mainly compress existing models built for clouds, we architect SmallThinker from the ground up to thrive within these limitations.\" \n\nI like the idea of an simple model for on device usage. It seems DeepSeek was probably used for a lot of the training right?",
          "author_fullname": "t2_4m7q4va",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "SmallThinker trained on DeepSeek?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "7hg307qzw6hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 96,
                  "x": 108,
                  "u": "https://preview.redd.it/7hg307qzw6hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=91607f5bc06ec08c73f63c91219f0b7043c58616"
                },
                {
                  "y": 192,
                  "x": 216,
                  "u": "https://preview.redd.it/7hg307qzw6hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=42ea778c12728d510ac8b2094ab2efc973d8a2ef"
                },
                {
                  "y": 285,
                  "x": 320,
                  "u": "https://preview.redd.it/7hg307qzw6hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8bce16b5dfdf035259e227b793866c1619939c9f"
                },
                {
                  "y": 571,
                  "x": 640,
                  "u": "https://preview.redd.it/7hg307qzw6hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7a0dc03b0d5897a1b5e0b35de3f72ea44aa70dcc"
                },
                {
                  "y": 857,
                  "x": 960,
                  "u": "https://preview.redd.it/7hg307qzw6hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a7cd07545397d54a677964a878531224379ac624"
                }
              ],
              "s": {
                "y": 874,
                "x": 979,
                "u": "https://preview.redd.it/7hg307qzw6hf1.png?width=979&amp;format=png&amp;auto=webp&amp;s=5c5b978ed3d06e0c0d612c61e60aec5ac9f94f0f"
              },
              "id": "7hg307qzw6hf1"
            },
            "j4oypbgzw6hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 119,
                  "x": 108,
                  "u": "https://preview.redd.it/j4oypbgzw6hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc827a7b9845610835ddb8ecaf8b116e02b5bbb3"
                },
                {
                  "y": 238,
                  "x": 216,
                  "u": "https://preview.redd.it/j4oypbgzw6hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=230d26e5ebfccd556525fe47e8cea9be90e82eb3"
                },
                {
                  "y": 353,
                  "x": 320,
                  "u": "https://preview.redd.it/j4oypbgzw6hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c443576382014aedb8af7284fb189f4b58eaec5a"
                },
                {
                  "y": 707,
                  "x": 640,
                  "u": "https://preview.redd.it/j4oypbgzw6hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c8ccd8acf50c0f74620b01fc8979ea5c64007b97"
                },
                {
                  "y": 1061,
                  "x": 960,
                  "u": "https://preview.redd.it/j4oypbgzw6hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=39c484cd6c0af824417f425b8d2ceb0af8fd0742"
                },
                {
                  "y": 1194,
                  "x": 1080,
                  "u": "https://preview.redd.it/j4oypbgzw6hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=67278121a82c2938fd5eecde21117b8941b2bcc5"
                }
              ],
              "s": {
                "y": 1201,
                "x": 1086,
                "u": "https://preview.redd.it/j4oypbgzw6hf1.png?width=1086&amp;format=png&amp;auto=webp&amp;s=16b0542f6fafc3eab1cc81a27413bc42942f7af2"
              },
              "id": "j4oypbgzw6hf1"
            }
          },
          "name": "t3_1mi7bec",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "j4oypbgzw6hf1",
                "id": 721702951
              },
              {
                "media_id": "7hg307qzw6hf1",
                "id": 721702952
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/w5f3xQSzkC28pTOLrhTaisAWSHOqAGA9AZAkyuI2V-s.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754395457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;Unlike traditional approaches that mainly compress existing models built for clouds, we architect SmallThinker from the ground up to thrive within these limitations.&amp;quot; &lt;/p&gt;\n\n&lt;p&gt;I like the idea of an simple model for on device usage. It seems DeepSeek was probably used for a lot of the training right?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mi7bec",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mi7bec",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheWingsOfWar",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi7bec/smallthinker_trained_on_deepseek/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mi7bec",
          "subreddit_subscribers": 510540,
          "created_utc": 1754395457,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm dabbling with Qwen Coder and Roo, but it looks like the model was trained to do tool calls in XML instead of the more common JSON. Would Cline do better there? It doesn't seem to work as well with local models.",
          "author_fullname": "t2_4feaa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "VS Code plugins that can handle XML tool calling?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mi789a",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754395219,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m dabbling with Qwen Coder and Roo, but it looks like the model was trained to do tool calls in XML instead of the more common JSON. Would Cline do better there? It doesn&amp;#39;t seem to work as well with local models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi789a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sciencewarrior",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi789a/vs_code_plugins_that_can_handle_xml_tool_calling/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi789a/vs_code_plugins_that_can_handle_xml_tool_calling/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754395219,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I know about the rule of researching beforehand, but I didn't find any satisfactory answers. Right now, after setting up koboldcpp and sillytavern I use dolphin 2.6 mistral 7b which was recommended to me by deepseek, I installed everything with it's help and because of that I didn't at first had a thought about searching for other models, but when looking it up I noticed it's at least 2 years old.\n\nSo, TLDR: What is the best free model that I can run locally with my hardware constraints for good(-ish?) roleplay?",
          "author_fullname": "t2_emzorlxl8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the best current model for roleplay if I have 8gb vram 6600xt 16gb ram and 3600 ryzen?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi0sr3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754371884,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know about the rule of researching beforehand, but I didn&amp;#39;t find any satisfactory answers. Right now, after setting up koboldcpp and sillytavern I use dolphin 2.6 mistral 7b which was recommended to me by deepseek, I installed everything with it&amp;#39;s help and because of that I didn&amp;#39;t at first had a thought about searching for other models, but when looking it up I noticed it&amp;#39;s at least 2 years old.&lt;/p&gt;\n\n&lt;p&gt;So, TLDR: What is the best free model that I can run locally with my hardware constraints for good(-ish?) roleplay?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi0sr3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mental_Budget_5085",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi0sr3/what_is_the_best_current_model_for_roleplay_if_i/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi0sr3/what_is_the_best_current_model_for_roleplay_if_i/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754371884,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I'm sure you all have seen a million posts of yet another ollama vibe coded frontend, yet another I made xyz but free!\n\nBut I'm not looking for a fly by night tool. What alternative open source projects are actually really good, well maintained and active? I'm thinking about replacements for perplexity that replicate the search performance, it can't *just* have search. What fully local tool comes close to actually replicating that performance? Openwebui has search, but it isn't the same. Same for alternatives for notebookLM, or similar tools.\n\nP.S. I don't care about \"it's better because they have gpus\", assume an ideal hardware scenario and then provide the software tools. No technicalities, unless it's pertinent to the function of the software in comparison to others irrespective of the hardware.",
          "author_fullname": "t2_9yxfq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Actual replacements for perplexity, notebookLm",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mi6c0x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754392454,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m sure you all have seen a million posts of yet another ollama vibe coded frontend, yet another I made xyz but free!&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m not looking for a fly by night tool. What alternative open source projects are actually really good, well maintained and active? I&amp;#39;m thinking about replacements for perplexity that replicate the search performance, it can&amp;#39;t &lt;em&gt;just&lt;/em&gt; have search. What fully local tool comes close to actually replicating that performance? Openwebui has search, but it isn&amp;#39;t the same. Same for alternatives for notebookLM, or similar tools.&lt;/p&gt;\n\n&lt;p&gt;P.S. I don&amp;#39;t care about &amp;quot;it&amp;#39;s better because they have gpus&amp;quot;, assume an ideal hardware scenario and then provide the software tools. No technicalities, unless it&amp;#39;s pertinent to the function of the software in comparison to others irrespective of the hardware.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi6c0x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zelkovamoon",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi6c0x/actual_replacements_for_perplexity_notebooklm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi6c0x/actual_replacements_for_perplexity_notebooklm/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754392454,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I got frustrated trying to use GitHub Copilot in restricted environments, airgapped networks, legacy IDEs, places with no extensions allowed. So I built [**QuietPrompt**](https://github.com/viktorfaubl/QuietPrompt), a local-first Copilot-style helper. It comes with an installer to Windows too, [Installer](https://github.com/viktorfaubl/QuietPrompt/releases/tag/v1.0.1).\n\nIt reads your screen (via hotkey OCR), listens to your mic (Whisper), or grabs from clipboard, and pipes it to your own LLM, totally offline. Works with Qwen 30B but requires a heavy PC, 32GB RAM min and a 3060 Ti GPU min.\n\nOpen-source, free for personal use, Windows-only for now. Happy to hear what you'd improve or add.\n\n[Overlay](https://preview.redd.it/a90esne532hf1.png?width=2559&amp;format=png&amp;auto=webp&amp;s=c6d31471235423f1e83f353e55037317583fc851)\n\n[System Tray](https://preview.redd.it/71ydew6732hf1.png?width=311&amp;format=png&amp;auto=webp&amp;s=e3d8d2b6b12fcd6196f2d3067bcd90ab7cd17f46)\n\n",
          "author_fullname": "t2_p9c3a6hj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Built my own Copilot-style assistant that works offline, with screen/mic/text input",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "a90esne532hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 21,
                  "x": 108,
                  "u": "https://preview.redd.it/a90esne532hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e8e09852c31ff8a16d99e87089223139b887d9de"
                },
                {
                  "y": 42,
                  "x": 216,
                  "u": "https://preview.redd.it/a90esne532hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a98b61371abe809adc084f08f122a6f671efb6d3"
                },
                {
                  "y": 63,
                  "x": 320,
                  "u": "https://preview.redd.it/a90esne532hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=70d910a057e05136ff023ea4f6c9ccaef2caad1d"
                },
                {
                  "y": 127,
                  "x": 640,
                  "u": "https://preview.redd.it/a90esne532hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7974b21f949296cc70eae62258417250d1d92e96"
                },
                {
                  "y": 190,
                  "x": 960,
                  "u": "https://preview.redd.it/a90esne532hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=55ee0575d5a35cf66cdc12daa130d6305c4bfb4e"
                },
                {
                  "y": 214,
                  "x": 1080,
                  "u": "https://preview.redd.it/a90esne532hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=169f8f054401fb8bf3bc396dc82bb8c832fedbed"
                }
              ],
              "s": {
                "y": 508,
                "x": 2559,
                "u": "https://preview.redd.it/a90esne532hf1.png?width=2559&amp;format=png&amp;auto=webp&amp;s=c6d31471235423f1e83f353e55037317583fc851"
              },
              "id": "a90esne532hf1"
            },
            "71ydew6732hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 123,
                  "x": 108,
                  "u": "https://preview.redd.it/71ydew6732hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab83cbc84dcebef553b7cf60564c471f070581c2"
                },
                {
                  "y": 246,
                  "x": 216,
                  "u": "https://preview.redd.it/71ydew6732hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b433c06ca8095004a5eb0359efb72e0eb88db21"
                }
              ],
              "s": {
                "y": 355,
                "x": 311,
                "u": "https://preview.redd.it/71ydew6732hf1.png?width=311&amp;format=png&amp;auto=webp&amp;s=e3d8d2b6b12fcd6196f2d3067bcd90ab7cd17f46"
              },
              "id": "71ydew6732hf1"
            }
          },
          "name": "t3_1mhng5b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=40de8b1d66a8f64f3c7c5f40dd4ccd15dafc22d7",
          "edited": 1754336536,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1754336332,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got frustrated trying to use GitHub Copilot in restricted environments, airgapped networks, legacy IDEs, places with no extensions allowed. So I built &lt;a href=\"https://github.com/viktorfaubl/QuietPrompt\"&gt;&lt;strong&gt;QuietPrompt&lt;/strong&gt;&lt;/a&gt;, a local-first Copilot-style helper. It comes with an installer to Windows too, &lt;a href=\"https://github.com/viktorfaubl/QuietPrompt/releases/tag/v1.0.1\"&gt;Installer&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;It reads your screen (via hotkey OCR), listens to your mic (Whisper), or grabs from clipboard, and pipes it to your own LLM, totally offline. Works with Qwen 30B but requires a heavy PC, 32GB RAM min and a 3060 Ti GPU min.&lt;/p&gt;\n\n&lt;p&gt;Open-source, free for personal use, Windows-only for now. Happy to hear what you&amp;#39;d improve or add.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/a90esne532hf1.png?width=2559&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6d31471235423f1e83f353e55037317583fc851\"&gt;Overlay&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/71ydew6732hf1.png?width=311&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3d8d2b6b12fcd6196f2d3067bcd90ab7cd17f46\"&gt;System Tray&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI.png?auto=webp&amp;s=a0953ccf24cc8b19472a33e455d85b6348f05e44",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=08780895207f09503731d5f70b6b0303fc1c85e9",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a341a9eebbb9bde1fbfa64752012747f836c8bb2",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=63b707a929ac3550a2b3b5edfb431dfeef480405",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d39528d4df352bc10ef277ec0f70cebb15267de0",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e151bcde6e98cbb0cee11b9a53d6ebb050eab40",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=93c850ef421494500aa15c6e56055262d93bd768",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mhng5b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Natural-Ad6682",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhng5b/built_my_own_copilotstyle_assistant_that_works/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhng5b/built_my_own_copilotstyle_assistant_that_works/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754336332,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been debating the use of quantized KV cache with llama.cpp (no less than q8) for a long time, but I still can't tell if it's a good idea:\n\n* On one hand, the [original PR](https://github.com/ggml-org/llama.cpp/pull/7412) mentions that perplexity is more sensitive to model weight quants than to KV cache. Or in other words, quantizing kv cache is worth it if it frees memory for a slightly less quantized model (eg q4kL vs q4kM).\n* On the other hand, many commenters in this community seem to strongly suggest against it.\n* My own experience hasn't been conclusive one way or another. In theory, more quantization = less quality, but I don't have a concrete measure of the degradation introduced by quantized kv cache to rule out the idea entirely.\n\nSo I'm here to gauge everyone's experience with this, hopefully someone has a strong argument with or against that feature.\n\nThank you!",
          "author_fullname": "t2_nc2u4f7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the verdict on using quantized KV cache?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhlj69",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754335757,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754332125,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been debating the use of quantized KV cache with llama.cpp (no less than q8) for a long time, but I still can&amp;#39;t tell if it&amp;#39;s a good idea:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;On one hand, the &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/7412\"&gt;original PR&lt;/a&gt; mentions that perplexity is more sensitive to model weight quants than to KV cache. Or in other words, quantizing kv cache is worth it if it frees memory for a slightly less quantized model (eg q4kL vs q4kM).&lt;/li&gt;\n&lt;li&gt;On the other hand, many commenters in this community seem to strongly suggest against it.&lt;/li&gt;\n&lt;li&gt;My own experience hasn&amp;#39;t been conclusive one way or another. In theory, more quantization = less quality, but I don&amp;#39;t have a concrete measure of the degradation introduced by quantized kv cache to rule out the idea entirely.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So I&amp;#39;m here to gauge everyone&amp;#39;s experience with this, hopefully someone has a strong argument with or against that feature.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?auto=webp&amp;s=9747fc98599981e69ef3dbabd18ac955767b61a1",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=719eefafd72ad9dfefe61307fd88e8316866de92",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ae4f338ba2a27a6abf4d22dbfd20af9b475a6e6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8782f872b86a598db0a3a6e2778f7fb6d2731edb",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6c1e124ade737ee359be91a0766310ce074cadd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0453b80458e72b57f76e7907738a09419b3abbb2",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3a6f64546df415a5fb04416ad5ef8205b6fb933c",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhlj69",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ParaboloidalCrest",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754332125,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For a bit of context, I'm a software developer with 4 years of exp, in dotnet and I've worked with python as well. My goal is to hit the ground running by creating projects using LLMs, I feel like the way to learn is by doing the thing, but I'm a bit lost on probably getting started.\n\nFor the most part there seems to be a lot of snake oil content out there, the usual learn LLMs in 30 mins kinda of stuff, where all they \"teach\" you is to clone a git report and run ollama, **what I'm looking for is a hands on way to built actual projects with LLMs and then integrate newer tech like RAG, MCP etc etc.**\n\nI would really appreciate any books, videos lectures, series that you can recommend. I'm not looking for the academic side of this, honestly I don't know if it's worth spending all that time, learning how an LLMs is made when I can just start using it (please feel free to object to my ignorance here). I feel like this industry is moving at the speed of light with something new everyday.",
          "author_fullname": "t2_eevwuk4z0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How does someone with programming exp get started with LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi2ebo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754377801,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For a bit of context, I&amp;#39;m a software developer with 4 years of exp, in dotnet and I&amp;#39;ve worked with python as well. My goal is to hit the ground running by creating projects using LLMs, I feel like the way to learn is by doing the thing, but I&amp;#39;m a bit lost on probably getting started.&lt;/p&gt;\n\n&lt;p&gt;For the most part there seems to be a lot of snake oil content out there, the usual learn LLMs in 30 mins kinda of stuff, where all they &amp;quot;teach&amp;quot; you is to clone a git report and run ollama, &lt;strong&gt;what I&amp;#39;m looking for is a hands on way to built actual projects with LLMs and then integrate newer tech like RAG, MCP etc etc.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I would really appreciate any books, videos lectures, series that you can recommend. I&amp;#39;m not looking for the academic side of this, honestly I don&amp;#39;t know if it&amp;#39;s worth spending all that time, learning how an LLMs is made when I can just start using it (please feel free to object to my ignorance here). I feel like this industry is moving at the speed of light with something new everyday.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi2ebo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "parleG_OP",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi2ebo/how_does_someone_with_programming_exp_get_started/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi2ebo/how_does_someone_with_programming_exp_get_started/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754377801,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "NexNotes AI is an AI-powered tool that helps you streamline your study and learning process. With a suite of features including mind maps, study plans, flowcharts, summaries, and quizzes, NexNotes AI empowers you to grasp complex information quickly and effectively. Whether you're a student, professional, or lifelong learner, this versatile platform can transform the way you approach your studies and boost your knowledge retention. It also gives awards for solving questions. I need feedback from you about My tool overall\n[NexNotesAI ](https://nexnotes-ai.pages.dev)",
          "author_fullname": "t2_1utt8r8ixm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a one stop AI powered study solution",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhv99h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754355346,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;NexNotes AI is an AI-powered tool that helps you streamline your study and learning process. With a suite of features including mind maps, study plans, flowcharts, summaries, and quizzes, NexNotes AI empowers you to grasp complex information quickly and effectively. Whether you&amp;#39;re a student, professional, or lifelong learner, this versatile platform can transform the way you approach your studies and boost your knowledge retention. It also gives awards for solving questions. I need feedback from you about My tool overall\n&lt;a href=\"https://nexnotes-ai.pages.dev\"&gt;NexNotesAI &lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mhv99h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pls_Do_not_ban",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhv99h/i_built_a_one_stop_ai_powered_study_solution/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhv99h/i_built_a_one_stop_ai_powered_study_solution/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754355346,
          "num_crossposts": 9,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks!  \nI’ve been testing a few document parsers to extract formulas from PDFs (like scientific papers, math-heavy docs, etc). Tried **Docling**, but the results are not great so far. Especially struggling with keeping the formula structure intact.\n\nCurious if anyone here has found a good method or tool that actually works well for this?  \nWould love to hear what worked (or didn’t) for you.\n\nThanks in advance 🙌",
          "author_fullname": "t2_hpqb9e8q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone here figured out how to reliably extract formulas from PDFs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi24mj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754376798,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks!&lt;br/&gt;\nI’ve been testing a few document parsers to extract formulas from PDFs (like scientific papers, math-heavy docs, etc). Tried &lt;strong&gt;Docling&lt;/strong&gt;, but the results are not great so far. Especially struggling with keeping the formula structure intact.&lt;/p&gt;\n\n&lt;p&gt;Curious if anyone here has found a good method or tool that actually works well for this?&lt;br/&gt;\nWould love to hear what worked (or didn’t) for you.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance 🙌&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi24mj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "duke_x91",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi24mj/anyone_here_figured_out_how_to_reliably_extract/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi24mj/anyone_here_figured_out_how_to_reliably_extract/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754376798,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve just finished building a desk-side powerhouse and I want to run a community-driven series of inference latency benchmarks (in ms) on the latest high-performance open-source LLMs (&gt; 70 B parameters), including models like QWEN, GLM 4.5, and others you recommend. \n\nI’m also keen to try any RAM/CPU tricks for models that may not fit entirely in GPU VRAM—e.g. K-Transformers, paged-KV llama.cpp, host-offload, thread-affinity hacks, etc.\n\nIf your interested in a benchmark send me: \n\n- Model (&gt; 70 B): NUQUEN-xxx, GLM 4.5,etc\n- Quantization: Q4_K, Q5_K, BF16, FP16, 4-bit, etc.\n- Engine: vLLM, k-Transformers, llama.cpp (paged KV), Text Generation Inference (TGI), etc.\n- Context length: e.g. 32 K, 100 K tokens\n- Batch size: default 1 unless you specify otherwise\nAny Tricks: any settings or hacks to leverage host RAM, CPU offload, etc.\n\nI’ll report back with:\n- Generation latency (e.g. tg128)\n- Prompt-processing throughput (e.g. pp512)\n\nMy system specs:\n\nCPU: AMD EPYC 9255 (24 C / 48 T)\nRAM: 12 × 64 GB DDR5-6000 ≈ 760 GiB\nGPUs: 2 × RTX PRO 6000 Blackwell Max-Q (96 GB VRAM each)\nOS: Pop!_OS",
          "author_fullname": "t2_5l4zmzcw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Offering Benchmarks on my New RIG for Larger Models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhoaxs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754338256,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve just finished building a desk-side powerhouse and I want to run a community-driven series of inference latency benchmarks (in ms) on the latest high-performance open-source LLMs (&amp;gt; 70 B parameters), including models like QWEN, GLM 4.5, and others you recommend. &lt;/p&gt;\n\n&lt;p&gt;I’m also keen to try any RAM/CPU tricks for models that may not fit entirely in GPU VRAM—e.g. K-Transformers, paged-KV llama.cpp, host-offload, thread-affinity hacks, etc.&lt;/p&gt;\n\n&lt;p&gt;If your interested in a benchmark send me: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Model (&amp;gt; 70 B): NUQUEN-xxx, GLM 4.5,etc&lt;/li&gt;\n&lt;li&gt;Quantization: Q4_K, Q5_K, BF16, FP16, 4-bit, etc.&lt;/li&gt;\n&lt;li&gt;Engine: vLLM, k-Transformers, llama.cpp (paged KV), Text Generation Inference (TGI), etc.&lt;/li&gt;\n&lt;li&gt;Context length: e.g. 32 K, 100 K tokens&lt;/li&gt;\n&lt;li&gt;Batch size: default 1 unless you specify otherwise\nAny Tricks: any settings or hacks to leverage host RAM, CPU offload, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I’ll report back with:\n- Generation latency (e.g. tg128)\n- Prompt-processing throughput (e.g. pp512)&lt;/p&gt;\n\n&lt;p&gt;My system specs:&lt;/p&gt;\n\n&lt;p&gt;CPU: AMD EPYC 9255 (24 C / 48 T)\nRAM: 12 × 64 GB DDR5-6000 ≈ 760 GiB\nGPUs: 2 × RTX PRO 6000 Blackwell Max-Q (96 GB VRAM each)\nOS: Pop!_OS&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhoaxs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Infamous_Jaguar_2151",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754338256,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Tescent has released new models (llama.cpp support is already merged!)\n\n[https://huggingface.co/tencent/Hunyuan-7B-Instruct](https://huggingface.co/tencent/Hunyuan-7B-Instruct)\n\n[https://huggingface.co/tencent/Hunyuan-4B-Instruct](https://huggingface.co/tencent/Hunyuan-4B-Instruct)\n\n[https://huggingface.co/tencent/Hunyuan-1.8B-Instruct](https://huggingface.co/tencent/Hunyuan-1.8B-Instruct)\n\n[https://huggingface.co/tencent/Hunyuan-0.5B-Instruct](https://huggingface.co/tencent/Hunyuan-0.5B-Instruct)\n\n# Model Introduction\n\nHunyuan is Tencent's open-source efficient large language model series, designed for versatile deployment across diverse computational environments. From edge devices to high-concurrency production systems, these models deliver optimal performance with advanced quantization support and ultra-long context capabilities.\n\nWe have released a series of Hunyuan dense models, comprising both pre-trained and instruction-tuned variants, with parameter scales of 0.5B, 1.8B, 4B, and 7B. These models adopt training strategies similar to the Hunyuan-A13B, thereby inheriting its robust performance characteristics. This comprehensive model family enables flexible deployment optimization - from resource-constrained edge computing with smaller variants to high-throughput production environments with larger models, all while maintaining strong capabilities across diverse scenarios.\n\n# \n\n# Key Features and Advantages\n\n* **Hybrid Reasoning Support**: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.\n* **Ultra-Long Context Understanding**: Natively supports a 256K context window, maintaining stable performance on long-text tasks.\n* **Enhanced Agent Capabilities**: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3, τ-Bench and C3-Bench.\n* **Efficient Inference**: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.\n\nUPDATE\n\npretrain models\n\n[https://huggingface.co/tencent/Hunyuan-7B-Pretrain](https://huggingface.co/tencent/Hunyuan-7B-Pretrain)\n\n[https://huggingface.co/tencent/Hunyuan-4B-Pretrain](https://huggingface.co/tencent/Hunyuan-4B-Pretrain)\n\n[https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain](https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain)\n\n[https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain](https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain)\n\nGGUFs\n\n[https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF](https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF)\n\n[https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF](https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF)\n\n[https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF](https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF)\n\n[https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF](https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF)",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "new Hunyuan Instruct 7B/4B/1.8B/0.5B models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh3s7q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 260,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 260,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754292850,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754280980,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tescent has released new models (llama.cpp support is already merged!)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-7B-Instruct\"&gt;https://huggingface.co/tencent/Hunyuan-7B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-4B-Instruct\"&gt;https://huggingface.co/tencent/Hunyuan-4B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-1.8B-Instruct\"&gt;https://huggingface.co/tencent/Hunyuan-1.8B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-0.5B-Instruct\"&gt;https://huggingface.co/tencent/Hunyuan-0.5B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Model Introduction&lt;/h1&gt;\n\n&lt;p&gt;Hunyuan is Tencent&amp;#39;s open-source efficient large language model series, designed for versatile deployment across diverse computational environments. From edge devices to high-concurrency production systems, these models deliver optimal performance with advanced quantization support and ultra-long context capabilities.&lt;/p&gt;\n\n&lt;p&gt;We have released a series of Hunyuan dense models, comprising both pre-trained and instruction-tuned variants, with parameter scales of 0.5B, 1.8B, 4B, and 7B. These models adopt training strategies similar to the Hunyuan-A13B, thereby inheriting its robust performance characteristics. This comprehensive model family enables flexible deployment optimization - from resource-constrained edge computing with smaller variants to high-throughput production environments with larger models, all while maintaining strong capabilities across diverse scenarios.&lt;/p&gt;\n\n&lt;h1&gt;Key Features and Advantages&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Hybrid Reasoning Support&lt;/strong&gt;: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Ultra-Long Context Understanding&lt;/strong&gt;: Natively supports a 256K context window, maintaining stable performance on long-text tasks.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Enhanced Agent Capabilities&lt;/strong&gt;: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3, τ-Bench and C3-Bench.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Efficient Inference&lt;/strong&gt;: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;UPDATE&lt;/p&gt;\n\n&lt;p&gt;pretrain models&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-7B-Pretrain\"&gt;https://huggingface.co/tencent/Hunyuan-7B-Pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-4B-Pretrain\"&gt;https://huggingface.co/tencent/Hunyuan-4B-Pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain\"&gt;https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain\"&gt;https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GGUFs&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF\"&gt;https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF\"&gt;https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF\"&gt;https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF\"&gt;https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?auto=webp&amp;s=6f622eceb5c359f202e3b99375e5e58502d2ec49",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d7d208147546310820cea26a2856210455054de",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3afe2af4a5c98a2eeb74d6b8f4e87d8e7d67379",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b15fc318112698d6e74e7ae2a79fe2b70cfdb24b",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=12addf5b08bd142a26edf444c94debefdd48b9e6",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d1df2789194ea707b707209c04d7485c60a743d3",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=858c8b8c5cba69037df907c1924a66347161a674",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mh3s7q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 54,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mh3s7q/new_hunyuan_instruct_7b4b18b05b_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh3s7q/new_hunyuan_instruct_7b4b18b05b_models/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754280980,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hunyuan just released 4 new dense models. It’s a new architecture and supports hybrid reasoning, 256K context and agent capabilities with tool support! The benchmarks are great but will need to really test them in real world.\n\nLove to see more small models as I'm developing an iOS local chat called [Locally AI](https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692). Will look to add them but since it's new architecture it will need to be ported to Apple MLX.\n\nThe choice of size here is perfect:\n\n- 0.5B, 1.8B and 4B great for all iPhones models\n- 7B great for iPad with M chip",
          "author_fullname": "t2_1jgkfm9u25",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "New small models from Hunyuan (0.5B, 1.8B, 4B, 7B)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 98,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "kgm0t9q6gygf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 76,
                  "x": 108,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab02c39ede4b3675d5617c5a8ddb2fb3f55006f4"
                },
                {
                  "y": 152,
                  "x": 216,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f682f7d72c17329d449b47977069ca02c39fa1e6"
                },
                {
                  "y": 226,
                  "x": 320,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6612e1a30d154e85c954c2f4bf92f9692eb8a014"
                },
                {
                  "y": 452,
                  "x": 640,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e0ac6cc6275921def7a509a1781af2b65af8bde6"
                },
                {
                  "y": 678,
                  "x": 960,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a477236ce1c7be739f9e2ebe072ecc9b8fd2d81"
                },
                {
                  "y": 762,
                  "x": 1080,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7550d21b9e1e389fe9d001a54e3fd11a2930b7e2"
                }
              ],
              "s": {
                "y": 1017,
                "x": 1440,
                "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=a8519dca9785d55ee61e5339bdaeb27132f847db"
              },
              "id": "kgm0t9q6gygf1"
            },
            "gjb5n6r6gygf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 76,
                  "x": 108,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa3891d5431eebe6ada8791f8182fd2d48e67ee5"
                },
                {
                  "y": 152,
                  "x": 216,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8334b244623b60f52c4f15cb7063d53958162e33"
                },
                {
                  "y": 226,
                  "x": 320,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0d1ba8e6a77f342fc6a563af8de0367ed05e2cd0"
                },
                {
                  "y": 452,
                  "x": 640,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1d76dab340c1fa53b85a369ea6e8b95108c9bde7"
                },
                {
                  "y": 678,
                  "x": 960,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8154e0ac9db723bf05c41f09b4e569480e065627"
                },
                {
                  "y": 762,
                  "x": 1080,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2e967a101a45103a134b4fc9b01af49525bbf803"
                }
              ],
              "s": {
                "y": 1017,
                "x": 1440,
                "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=3be7da2fbfc3a5e6e7e4a66e1e0acf38c12b3156"
              },
              "id": "gjb5n6r6gygf1"
            }
          },
          "name": "t3_1mh6z16",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 144,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "kgm0t9q6gygf1",
                "id": 720841480
              },
              {
                "media_id": "gjb5n6r6gygf1",
                "id": 720841481
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 144,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/r2IMyxKAeZgSqc_fmrMhOP7SMYeTy8apLYYgezLqpRg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754292468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hunyuan just released 4 new dense models. It’s a new architecture and supports hybrid reasoning, 256K context and agent capabilities with tool support! The benchmarks are great but will need to really test them in real world.&lt;/p&gt;\n\n&lt;p&gt;Love to see more small models as I&amp;#39;m developing an iOS local chat called &lt;a href=\"https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692\"&gt;Locally AI&lt;/a&gt;. Will look to add them but since it&amp;#39;s new architecture it will need to be ported to Apple MLX.&lt;/p&gt;\n\n&lt;p&gt;The choice of size here is perfect:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;0.5B, 1.8B and 4B great for all iPhones models&lt;/li&gt;\n&lt;li&gt;7B great for iPad with M chip&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mh6z16",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mh6z16",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adrgrondin",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh6z16/new_small_models_from_hunyuan_05b_18b_4b_7b/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mh6z16",
          "subreddit_subscribers": 510540,
          "created_utc": 1754292468,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Bolt Graphics’ Zeus GPU Makes Bold Claim of Outperforming NVIDIA’s RTX 5090 by 10x in Rendering Workloads, That Too Using Laptop-Grade Memory",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhfbsi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NOVvMZCeb6MJYdVCo6dYfR7H6AIJZt8iOvly0YwXZZ0.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=e3db4f91381596fd7b067db702881b90371ab295",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754318547,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/bolt-graphics-zeus-gpu-makes-bold-claim-of-outperforming-rtx-5090-by-10x-in-rendering-workloads/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NOVvMZCeb6MJYdVCo6dYfR7H6AIJZt8iOvly0YwXZZ0.png?auto=webp&amp;s=5baaa86e6e026ac34488570fe1ff2f085cdbd5e1",
                  "width": 728,
                  "height": 409
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NOVvMZCeb6MJYdVCo6dYfR7H6AIJZt8iOvly0YwXZZ0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7bb50bf5bad3f11662fc8e79df150764f625417a",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/NOVvMZCeb6MJYdVCo6dYfR7H6AIJZt8iOvly0YwXZZ0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=409e4417ae6631361d5a51c8a9bc3820aea75d3c",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/NOVvMZCeb6MJYdVCo6dYfR7H6AIJZt8iOvly0YwXZZ0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c019f1c0adf4ee6e58ca0c4b449ae4acc19760f6",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://external-preview.redd.it/NOVvMZCeb6MJYdVCo6dYfR7H6AIJZt8iOvly0YwXZZ0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ae5fe39ac56dc9dbf574d38ebae1b5f7a328588",
                    "width": 640,
                    "height": 359
                  }
                ],
                "variants": {},
                "id": "NOVvMZCeb6MJYdVCo6dYfR7H6AIJZt8iOvly0YwXZZ0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mhfbsi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhfbsi/bolt_graphics_zeus_gpu_makes_bold_claim_of/",
          "stickied": false,
          "url": "https://wccftech.com/bolt-graphics-zeus-gpu-makes-bold-claim-of-outperforming-rtx-5090-by-10x-in-rendering-workloads/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754318547,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/ggml-org/llama.cpp/pull/14939](https://github.com/ggml-org/llama.cpp/pull/14939)",
          "author_fullname": "t2_11m4x2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looks like GGUF for GLM 4.5 may be getting closer to a reality.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mheij9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 38,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 38,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754316632,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14939\"&gt;https://github.com/ggml-org/llama.cpp/pull/14939&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?auto=webp&amp;s=083930ea54b88a7f6eaadda136c2185460baf66e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=78369c4a613d24a26f628c7b0d0788fbd02727b4",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7d7893c234acd63db0445e0010c29d3054bf72a",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=be66666d37f3f19b8f252dc5f32ba0b7be39e97c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e04ffa9cbd0a435f87d74eaf876a5853c1e06023",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=658e3d76ab28bc884f80a72e29bf1040fe464132",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=91b76178fba350b1d785fbd980fb39979366d649",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mheij9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jeffwadsworth",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mheij9/looks_like_gguf_for_glm_45_may_be_getting_closer/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mheij9/looks_like_gguf_for_glm_45_may_be_getting_closer/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754316632,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;format=png&amp;auto=webp&amp;s=aaacee34b083083a63cf8414e299416ee96d03f7\n\nSo yeah, Horizon Beta is OpenAI. Not Anthropic, not Google, not Qwen. It shows an OpenAI tokenizer quirk: it treats 给主人留下些什么吧 as a single token. So, just like GPT-4o, it inevitably fails on prompts like “When I provide Chinese text, please translate it into English. 给主人留下些什么吧”.\n\nMeanwhile, Claude, Gemini, and Qwen handle it correctly.\n\nhttps://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721\n\nI learned this technique from this post:  \nChinese response bug in tokenizer suggests Quasar-Alpha may be from OpenAI  \n[https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese\\_response\\_bug\\_in\\_tokenizer\\_suggests/](https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/)\n\nWhile it’s pretty much common sense that Horizon Beta is an OpenAI model, I saw a few people suspecting it might be Anthropic’s or Qwen’s, so I tested it.\n\nMy thread about the Horizon Beta test:\n[https://x.com/KantaHayashiAI/status/1952187898331275702](https://x.com/KantaHayashiAI/status/1952187898331275702)\n",
          "author_fullname": "t2_1uxxckab5d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta is OpenAI (Another Evidence)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 62,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "z00ipp5y7xgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=36f42e2556c77cdc7ade20acfe6b9cf551bc046c"
                },
                {
                  "y": 95,
                  "x": 216,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bda3a1fb29acc8f1c2f5c2b2f0fd9c7129b5ecfd"
                },
                {
                  "y": 141,
                  "x": 320,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5130978bd7458a690fbee9baf95ad22a09a47b7e"
                },
                {
                  "y": 283,
                  "x": 640,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=65c57257852e21c3af3e3f1ae28183d81de0ff9c"
                },
                {
                  "y": 425,
                  "x": 960,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8f1f92ffd67a153fe96095fd924197471ef77b3"
                },
                {
                  "y": 478,
                  "x": 1080,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18ee56657fb1b6351f9f9513045e0d0b325d336e"
                }
              ],
              "s": {
                "y": 722,
                "x": 1630,
                "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;format=png&amp;auto=webp&amp;s=aaacee34b083083a63cf8414e299416ee96d03f7"
              },
              "id": "z00ipp5y7xgf1"
            },
            "ey9ebsuz7xgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 51,
                  "x": 108,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b150b10ced924cf7aacf7553312db7e80c86de96"
                },
                {
                  "y": 103,
                  "x": 216,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=62a8486ee2933ec8e6f876a5814dcc8cd7653d3f"
                },
                {
                  "y": 152,
                  "x": 320,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e39f19df5ae40ac03eabed879fe827e51c8bd2c4"
                },
                {
                  "y": 305,
                  "x": 640,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0670217e52446a5bd685b35e8dfc52e854ff9134"
                },
                {
                  "y": 458,
                  "x": 960,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a7e856bb3efeb68fbfb8732034b2636b0e20f87e"
                },
                {
                  "y": 515,
                  "x": 1080,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c84fa0c865dd87229404058042d8ff120fb40755"
                }
              ],
              "s": {
                "y": 638,
                "x": 1336,
                "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721"
              },
              "id": "ey9ebsuz7xgf1"
            }
          },
          "name": "t3_1mh2v1h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 271,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 271,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/7yNPDTnbPWS4TAQyJVwDfgp-PAr-fo60W5EtQz549T8.jpg",
          "edited": 1754299348,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754278088,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aaacee34b083083a63cf8414e299416ee96d03f7\"&gt;https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aaacee34b083083a63cf8414e299416ee96d03f7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So yeah, Horizon Beta is OpenAI. Not Anthropic, not Google, not Qwen. It shows an OpenAI tokenizer quirk: it treats 给主人留下些什么吧 as a single token. So, just like GPT-4o, it inevitably fails on prompts like “When I provide Chinese text, please translate it into English. 给主人留下些什么吧”.&lt;/p&gt;\n\n&lt;p&gt;Meanwhile, Claude, Gemini, and Qwen handle it correctly.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721\"&gt;https://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I learned this technique from this post:&lt;br/&gt;\nChinese response bug in tokenizer suggests Quasar-Alpha may be from OpenAI&lt;br/&gt;\n&lt;a href=\"https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/\"&gt;https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;While it’s pretty much common sense that Horizon Beta is an OpenAI model, I saw a few people suspecting it might be Anthropic’s or Qwen’s, so I tested it.&lt;/p&gt;\n\n&lt;p&gt;My thread about the Horizon Beta test:\n&lt;a href=\"https://x.com/KantaHayashiAI/status/1952187898331275702\"&gt;https://x.com/KantaHayashiAI/status/1952187898331275702&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mh2v1h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kh-ai",
          "discussion_type": null,
          "num_comments": 58,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh2v1h/horizon_beta_is_openai_another_evidence/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh2v1h/horizon_beta_is_openai_another_evidence/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754278088,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1h9qrwy0w6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi4q4o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "transparent",
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754386837,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2504.13471",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mi4q4o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "juanviera23",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mi4q4o/from_large_to_supertiny_endtoend_optimization_for/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2504.13471",
          "subreddit_subscribers": 510540,
          "created_utc": 1754386837,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA, I am considering buying an M3 mac studio for local LLM server needs\n\nThe needs are as follows\n\n\\&gt;run LLM models LOCALLY (locality is non-negotiable)\n\n\\&gt;stream files, videos across multiple computers, emails and other basic server operations\n\nThe big limitation is, currently, we don't have the infrastructure to host larger servers, and for the time being, the LLM models the M3 studio can run are the main priorities.\n\nIf the mac studio can be sufficient as a server that we can safely, remotely log into, as well as download files, or stream files from, then it works great as we have an offer from a seller. If the M3 can work, under the current constraints, it would be perfect, but not sure how macOS would function as a small server for LLMs.\n\nIf not, we will focus on eliminating our current constraints and consider other options.\n\nThanks!",
          "author_fullname": "t2_s784can9o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Maxed out M3 Mac studio as an LLM server for local employees?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhqgv1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754343447,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754343152,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;, I am considering buying an M3 mac studio for local LLM server needs&lt;/p&gt;\n\n&lt;p&gt;The needs are as follows&lt;/p&gt;\n\n&lt;p&gt;&amp;gt;run LLM models LOCALLY (locality is non-negotiable)&lt;/p&gt;\n\n&lt;p&gt;&amp;gt;stream files, videos across multiple computers, emails and other basic server operations&lt;/p&gt;\n\n&lt;p&gt;The big limitation is, currently, we don&amp;#39;t have the infrastructure to host larger servers, and for the time being, the LLM models the M3 studio can run are the main priorities.&lt;/p&gt;\n\n&lt;p&gt;If the mac studio can be sufficient as a server that we can safely, remotely log into, as well as download files, or stream files from, then it works great as we have an offer from a seller. If the M3 can work, under the current constraints, it would be perfect, but not sure how macOS would function as a small server for LLMs.&lt;/p&gt;\n\n&lt;p&gt;If not, we will focus on eliminating our current constraints and consider other options.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhqgv1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Manderbillt2000",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754343152,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "After being frustrated with Ollama about their slow new model support thair configs etc, \n\nI built Mindforge, a tiny Python tool that runs Hugging Face and GGUF models locally with an OpenAI-compatible API. It’s like a minimal Ollama but in Python, using transformers and llama.cpp.\n\nHighlights:\n\n• Run HF (transformer) or GGUF (llama.cpp) models locally\n• OpenAI-compatible endpoints: /v1/models, /v1/completions, /v1/chat/completions, /v1/embeddings\n• Simple CLI (interactive REPL, pull/list/rm/create)\n• Modelfile support (FROM/TAGS/PARAMS/SYSTEM) for custom models\n\nExample GGUF: mindforge run unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q4_K_M\n\n[Repo](https://github.com/Exw27/mindforge)\n\nWould love feedback!\n\nNote. \nIt is still experimental, and some things might not work.",
          "author_fullname": "t2_2fnzmgph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mindforge — Ollama-like local LLM runner with HF + GGUF, OpenAI-compatible API",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhmgci",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754334135,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After being frustrated with Ollama about their slow new model support thair configs etc, &lt;/p&gt;\n\n&lt;p&gt;I built Mindforge, a tiny Python tool that runs Hugging Face and GGUF models locally with an OpenAI-compatible API. It’s like a minimal Ollama but in Python, using transformers and llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;Highlights:&lt;/p&gt;\n\n&lt;p&gt;• Run HF (transformer) or GGUF (llama.cpp) models locally\n• OpenAI-compatible endpoints: /v1/models, /v1/completions, /v1/chat/completions, /v1/embeddings\n• Simple CLI (interactive REPL, pull/list/rm/create)\n• Modelfile support (FROM/TAGS/PARAMS/SYSTEM) for custom models&lt;/p&gt;\n\n&lt;p&gt;Example GGUF: mindforge run unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q4_K_M&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Exw27/mindforge\"&gt;Repo&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love feedback!&lt;/p&gt;\n\n&lt;p&gt;Note. \nIt is still experimental, and some things might not work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/EZzTTg1hdOCFMDwS0zyBmN2ARykDd0eCrL3pSZxiib8.png?auto=webp&amp;s=c2468c4a9fbe9613a83b6f3af40878f060d08308",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/EZzTTg1hdOCFMDwS0zyBmN2ARykDd0eCrL3pSZxiib8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d06f68b2553cb89685c7d7b8d1c373c7cba8d93e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/EZzTTg1hdOCFMDwS0zyBmN2ARykDd0eCrL3pSZxiib8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b4fd1dd51ef98e008d443a8a43e313099da5d592",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/EZzTTg1hdOCFMDwS0zyBmN2ARykDd0eCrL3pSZxiib8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1383cd994f4f1aa261cd7434c48feb1b27a327b1",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/EZzTTg1hdOCFMDwS0zyBmN2ARykDd0eCrL3pSZxiib8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=75a5bc866f173516cb7ac9b2235c800f040e793d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/EZzTTg1hdOCFMDwS0zyBmN2ARykDd0eCrL3pSZxiib8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e5b14f24507b7186bab893742aa0e2de25a6259",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/EZzTTg1hdOCFMDwS0zyBmN2ARykDd0eCrL3pSZxiib8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7a65296d3503db19920c7585291152b38ed7424a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "EZzTTg1hdOCFMDwS0zyBmN2ARykDd0eCrL3pSZxiib8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mhmgci",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Exw00",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhmgci/mindforge_ollamalike_local_llm_runner_with_hf/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhmgci/mindforge_ollamalike_local_llm_runner_with_hf/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754334135,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just created this tool that might be useful to some. It's a multithreaded github scraper that can query the github api for different repository structures and label file contents in json format. It's meant for creating agentic ais which can query the files in the github repo to decide which file contents should be added to its context. Please let me know if there's anything I should improve to fit your use case!  \n[https://github.com/rchotacode/GitHubScraper](https://github.com/rchotacode/GitHubScraper)",
          "author_fullname": "t2_1v0bczt9ms",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A simple python module to scrape github api for agentic code generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhx9d2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754360877,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just created this tool that might be useful to some. It&amp;#39;s a multithreaded github scraper that can query the github api for different repository structures and label file contents in json format. It&amp;#39;s meant for creating agentic ais which can query the files in the github repo to decide which file contents should be added to its context. Please let me know if there&amp;#39;s anything I should improve to fit your use case!&lt;br/&gt;\n&lt;a href=\"https://github.com/rchotacode/GitHubScraper\"&gt;https://github.com/rchotacode/GitHubScraper&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/pQdqaglLDwop7G3Q27fprUkgVqo8SOyl1xOdSjlOJSg.png?auto=webp&amp;s=493d0c09e52cf041d569991d49fe1c3336e78739",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/pQdqaglLDwop7G3Q27fprUkgVqo8SOyl1xOdSjlOJSg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2db040b2f1f6176ffd5713a6d7238bf73bd1f885",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/pQdqaglLDwop7G3Q27fprUkgVqo8SOyl1xOdSjlOJSg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5991e3d5ca1c6ead2b89316dd88e79a8be13d85c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/pQdqaglLDwop7G3Q27fprUkgVqo8SOyl1xOdSjlOJSg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0a7544bcdd43c10f510e043a8aeb914cf66b833",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/pQdqaglLDwop7G3Q27fprUkgVqo8SOyl1xOdSjlOJSg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d147c36d7e1a9e2d51302d1adbb9bcd5a355e10e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/pQdqaglLDwop7G3Q27fprUkgVqo8SOyl1xOdSjlOJSg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d5d5c59d44f8f2fcec255a1e1154a749f160e969",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/pQdqaglLDwop7G3Q27fprUkgVqo8SOyl1xOdSjlOJSg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=127417ee840816efb94c2d4b33a35add1a64e285",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "pQdqaglLDwop7G3Q27fprUkgVqo8SOyl1xOdSjlOJSg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mhx9d2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Infinite_Mix_31",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhx9d2/a_simple_python_module_to_scrape_github_api_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhx9d2/a_simple_python_module_to_scrape_github_api_for/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754360877,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Since the release of GLM 4.5, I've seen many contributors working hard to support at llama.cpp.\n\nHowever, as far as I remember, serise of quant model were registered on MLX community almost on the zero day in GLM case.\n\n1. Can the safetensor of usual MOE model be easily converted to quant using MLX? Or did Apple provides additional support for releasing of GLM model?\n\n2. Is it possible to perform fine-tuning using QLoRA with an already quantized MLX? GGUF cannot be used for fine-tuning once it is generated as I know.\n\n3. The most important question: Is it possible to fine-tune the GLM-4.5 Air model on Mac using the MLX framework right now?",
          "author_fullname": "t2_1dhesoqqtu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can I fine-tune GLM-4.5 Air via MLX?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi3has",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754382278,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754382014,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since the release of GLM 4.5, I&amp;#39;ve seen many contributors working hard to support at llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;However, as far as I remember, serise of quant model were registered on MLX community almost on the zero day in GLM case.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Can the safetensor of usual MOE model be easily converted to quant using MLX? Or did Apple provides additional support for releasing of GLM model?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Is it possible to perform fine-tuning using QLoRA with an already quantized MLX? GGUF cannot be used for fine-tuning once it is generated as I know.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The most important question: Is it possible to fine-tune the GLM-4.5 Air model on Mac using the MLX framework right now?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi3has",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Desperate-Sir-5088",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi3has/can_i_finetune_glm45_air_via_mlx/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi3has/can_i_finetune_glm45_air_via_mlx/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754382014,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3cle71ya",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPU-enabled Llama3 inference in Java now runs Qwen3, Phi-3, Mistral and Llama3 models in FP16, Q8 and Q4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhhiw2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/7S3n210Mbo3m_5SVfjJqbryTPGhWTE95IrYGDnPhDiI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754323440,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/r887j3c401hf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/r887j3c401hf1.png?auto=webp&amp;s=c32b99267b50dc24bd615d3eb72cbb44eb463397",
                  "width": 640,
                  "height": 960
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/r887j3c401hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=edc47e70ed41dfecf98c112cda83ad38681ebd5c",
                    "width": 108,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/r887j3c401hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d246a2c0b810b5612aa6d0d385d12c4301a1662c",
                    "width": 216,
                    "height": 324
                  },
                  {
                    "url": "https://preview.redd.it/r887j3c401hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=73dd415bcaa0b793a7e3587a34d751df41412d86",
                    "width": 320,
                    "height": 480
                  },
                  {
                    "url": "https://preview.redd.it/r887j3c401hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1a526327b790e21780f771391119fea341975354",
                    "width": 640,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "T-dzijjLHxhok6UBJPX3mgRpxypAdwfyi4rcFToq4gE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mhhiw2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mikebmx1",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhhiw2/gpuenabled_llama3_inference_in_java_now_runs/",
          "stickied": false,
          "url": "https://i.redd.it/r887j3c401hf1.png",
          "subreddit_subscribers": 510540,
          "created_utc": 1754323440,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This might be somewhat unusual, but if the goal of model quantization is to reduce the model size, what about quantizing only the text encoder? I found that the Qwen Image model consists of text encoders(Qwen2.5 VL) and diffusion transformers. If the text encoder is more robust to quantization than the diffusion, wouldn't it make sense to quantize only the text encoder while keeping the image generation part intact?\n\n  \nDoes this idea make no sense at all, or is it theoretically possible?",
          "author_fullname": "t2_73xg2fw4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen Image quantization idea",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhtjqo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754350727,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This might be somewhat unusual, but if the goal of model quantization is to reduce the model size, what about quantizing only the text encoder? I found that the Qwen Image model consists of text encoders(Qwen2.5 VL) and diffusion transformers. If the text encoder is more robust to quantization than the diffusion, wouldn&amp;#39;t it make sense to quantize only the text encoder while keeping the image generation part intact?&lt;/p&gt;\n\n&lt;p&gt;Does this idea make no sense at all, or is it theoretically possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhtjqo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ExcuseAccomplished97",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhtjqo/qwen_image_quantization_idea/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhtjqo/qwen_image_quantization_idea/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754350727,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a running Ollama server on a remote linux machine. Is it possible to include in an Ollama REST API request a directive that allows the use of code interpreter? If yes, can someone point me to any documentation? I tried looking at https://ollama.qubitpi.org/api/ without success. I found instructions for tool use, but not for interpreter use.",
          "author_fullname": "t2_febsd7s8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama RESTful API and code interpreter",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mi5wk1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754391069,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a running Ollama server on a remote linux machine. Is it possible to include in an Ollama REST API request a directive that allows the use of code interpreter? If yes, can someone point me to any documentation? I tried looking at &lt;a href=\"https://ollama.qubitpi.org/api/\"&gt;https://ollama.qubitpi.org/api/&lt;/a&gt; without success. I found instructions for tool use, but not for interpreter use.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi5wk1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ConiglioPipo",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi5wk1/ollama_restful_api_and_code_interpreter/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi5wk1/ollama_restful_api_and_code_interpreter/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754391069,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\nhttps://youtu.be/YR0KYO1YxsM?si=PEZJci3xJXITSuHM&amp;utm_source=ZTQxO",
          "author_fullname": "t2_8c7clfk1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 3 - 7B has a rival - Hunyuan.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhchdb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/V3Dgg9PfGn8ZeCKm1IkrYHrrLdHHjEj3QDpsDaMHbYc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754311641,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://youtu.be/YR0KYO1YxsM?si=PEZJci3xJXITSuHM&amp;amp;utm_source=ZTQxO\"&gt;https://youtu.be/YR0KYO1YxsM?si=PEZJci3xJXITSuHM&amp;amp;utm_source=ZTQxO&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/sfrqq83710hf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/sfrqq83710hf1.jpeg?auto=webp&amp;s=c172e2b98a6859f2b372038cbc1c42fa64c28a98",
                  "width": 1080,
                  "height": 622
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/sfrqq83710hf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=480e2bd14f64df792e7eaa9981b21f6d929c3d90",
                    "width": 108,
                    "height": 62
                  },
                  {
                    "url": "https://preview.redd.it/sfrqq83710hf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f1b6e98127b512019566b074ca83014406f284c3",
                    "width": 216,
                    "height": 124
                  },
                  {
                    "url": "https://preview.redd.it/sfrqq83710hf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=795b60e1c577441ffc10cd1765de8635cdae6e1a",
                    "width": 320,
                    "height": 184
                  },
                  {
                    "url": "https://preview.redd.it/sfrqq83710hf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc1c4d80ba59b73371c7a600daa85753805381da",
                    "width": 640,
                    "height": 368
                  },
                  {
                    "url": "https://preview.redd.it/sfrqq83710hf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2f3cd8deea253229adc2a8f284762a9d98277e22",
                    "width": 960,
                    "height": 552
                  },
                  {
                    "url": "https://preview.redd.it/sfrqq83710hf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fffabaf71378a58e7172bf0288389f1a7c31cbe6",
                    "width": 1080,
                    "height": 622
                  }
                ],
                "variants": {},
                "id": "CeOPnrkdAVTWcw3RfH95g_byZ-S--H7GKlR46QmXQLU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mhchdb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Current-Stop7806",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhchdb/qwen_3_7b_has_a_rival_hunyuan/",
          "stickied": false,
          "url": "https://i.redd.it/sfrqq83710hf1.jpeg",
          "subreddit_subscribers": 510540,
          "created_utc": 1754311641,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It seems they're updating their meta-benchmark with some less saturated ones which is good.\n\nA bit strange to continue using MMLU pro as it's quite saturated.\n\nThis update will make comparisons across time invalid.\n\nGrok and o3 are now tied. It's not clear if they are done updating.",
          "author_fullname": "t2_syq52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Artificial analysis meta-benchmark update",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhrke2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=140&amp;height=77&amp;crop=140:77,smart&amp;auto=webp&amp;s=938c7c54c5afab9fd0496cba4e5d012b557db44d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754345749,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "artificialanalysis.ai",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems they&amp;#39;re updating their meta-benchmark with some less saturated ones which is good.&lt;/p&gt;\n\n&lt;p&gt;A bit strange to continue using MMLU pro as it&amp;#39;s quite saturated.&lt;/p&gt;\n\n&lt;p&gt;This update will make comparisons across time invalid.&lt;/p&gt;\n\n&lt;p&gt;Grok and o3 are now tied. It&amp;#39;s not clear if they are done updating.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://artificialanalysis.ai/models/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?auto=webp&amp;s=efc17c9f241b4403d22cbacfe5d71900ee1cf85a",
                  "width": 1260,
                  "height": 700
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=700f91dbca11e5a7030b915550ae877ef725a0d4",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b97954336b79c1390848d0e44fa056a85de68672",
                    "width": 216,
                    "height": 120
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65f53b80ab9674ee645013e3e8eeac4f953d657e",
                    "width": 320,
                    "height": 177
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc",
                    "width": 640,
                    "height": 355
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f4359d47b78f5c1aa35de8804dbe36a749fc11a",
                    "width": 960,
                    "height": 533
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62eb4b7216f41af6600fc4df79cfa67425c19442",
                    "width": 1080,
                    "height": 600
                  }
                ],
                "variants": {},
                "id": "RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mhrke2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nomorebuttsplz",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhrke2/artificial_analysis_metabenchmark_update/",
          "stickied": false,
          "url": "https://artificialanalysis.ai/models/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754345749,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I downloaded Qwen3-Coder-30B-A3B-Instruct this morning and it surprised me. The model wrote a working Snake game on the first try.\n\nHere's what I did:\n\n1. Converted the model to MLX format with one command: `mlx_lm.convert --hf-path Qwen/Qwen3-Coder-30B-A3B-Instruct --mlx-path ~/models/Qwen3-Coder-30B-A3B-Instruct.mlx --q-group-size 64` (EDIT: --q-group-size is not needed for full precision. Only if quantizing. But it seemed to have no ill effect.)\n2. Set up a symlink for LM Studio (you can also use mlx\\_lm.chat)\n3. Gave it a simple prompt: \"Write a snake game in python.\"\n4. Created a Python environment and ran the code: `python3 -m venv ./snake &amp;&amp; . ./snake/bin/activate &amp;&amp; pip install pygame &amp;&amp; python ./snake`\n\nThe results:\n\n* 56 tokens per second at full 16-bit precision\n* 0.17 seconds to first token\n* Total time to complete game: 24 seconds\n* The game worked perfectly on the first run\n\nThe code included some nice graphical touches like a grid overlay and a distinct snake head. Six months ago, this would have been tough for most models.\n\nYes, Snake game examples probably exist in the training data. But running a 60GB model at full precision on a laptop at this speed still feels remarkable. I ran this prompt multiple times and it never failed to produce working pygame code, though the features and graphics varied slightly.\n\nSetup: MacBook Pro M4 Max with 128GB RAM\n\n[Screenshot of Game Over screen with score from a single short prompt.](https://preview.redd.it/ecwmh5acn1hf1.png?width=1604&amp;format=png&amp;auto=webp&amp;s=80fc551198145487c877d02ee099304fe1c58a40)",
          "author_fullname": "t2_jh5lk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder-30B nailed Snake game in one shot on my MacBook",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 110,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ecwmh5acn1hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 84,
                  "x": 108,
                  "u": "https://preview.redd.it/ecwmh5acn1hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc6d897752248a91312ae58d9eed61b1d872df51"
                },
                {
                  "y": 169,
                  "x": 216,
                  "u": "https://preview.redd.it/ecwmh5acn1hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb14dd66ec113f158e4deb9ffb6864bc1eba14e8"
                },
                {
                  "y": 251,
                  "x": 320,
                  "u": "https://preview.redd.it/ecwmh5acn1hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d1ce125b795512f57e241c084d583e3a2456a16a"
                },
                {
                  "y": 503,
                  "x": 640,
                  "u": "https://preview.redd.it/ecwmh5acn1hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b17c385bd563df8e36085bdcbb0111960400a8c7"
                },
                {
                  "y": 755,
                  "x": 960,
                  "u": "https://preview.redd.it/ecwmh5acn1hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a042ec97cefc1f5eab73c88a06679ac30409ea89"
                },
                {
                  "y": 849,
                  "x": 1080,
                  "u": "https://preview.redd.it/ecwmh5acn1hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49db9fb40d886a2d4c69563f1a2c7bc760f3d7cd"
                }
              ],
              "s": {
                "y": 1262,
                "x": 1604,
                "u": "https://preview.redd.it/ecwmh5acn1hf1.png?width=1604&amp;format=png&amp;auto=webp&amp;s=80fc551198145487c877d02ee099304fe1c58a40"
              },
              "id": "ecwmh5acn1hf1"
            }
          },
          "name": "t3_1mhl49l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/R6uYGl8qMhNCvvGDq584gvz1OcqCyGM1AH7v0jQ2LQg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754331237,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I downloaded Qwen3-Coder-30B-A3B-Instruct this morning and it surprised me. The model wrote a working Snake game on the first try.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what I did:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Converted the model to MLX format with one command: &lt;code&gt;mlx_lm.convert --hf-path Qwen/Qwen3-Coder-30B-A3B-Instruct --mlx-path ~/models/Qwen3-Coder-30B-A3B-Instruct.mlx --q-group-size 64&lt;/code&gt; (EDIT: --q-group-size is not needed for full precision. Only if quantizing. But it seemed to have no ill effect.)&lt;/li&gt;\n&lt;li&gt;Set up a symlink for LM Studio (you can also use mlx_lm.chat)&lt;/li&gt;\n&lt;li&gt;Gave it a simple prompt: &amp;quot;Write a snake game in python.&amp;quot;&lt;/li&gt;\n&lt;li&gt;Created a Python environment and ran the code: &lt;code&gt;python3 -m venv ./snake &amp;amp;&amp;amp; . ./snake/bin/activate &amp;amp;&amp;amp; pip install pygame &amp;amp;&amp;amp; python ./snake&lt;/code&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The results:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;56 tokens per second at full 16-bit precision&lt;/li&gt;\n&lt;li&gt;0.17 seconds to first token&lt;/li&gt;\n&lt;li&gt;Total time to complete game: 24 seconds&lt;/li&gt;\n&lt;li&gt;The game worked perfectly on the first run&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The code included some nice graphical touches like a grid overlay and a distinct snake head. Six months ago, this would have been tough for most models.&lt;/p&gt;\n\n&lt;p&gt;Yes, Snake game examples probably exist in the training data. But running a 60GB model at full precision on a laptop at this speed still feels remarkable. I ran this prompt multiple times and it never failed to produce working pygame code, though the features and graphics varied slightly.&lt;/p&gt;\n\n&lt;p&gt;Setup: MacBook Pro M4 Max with 128GB RAM&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ecwmh5acn1hf1.png?width=1604&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80fc551198145487c877d02ee099304fe1c58a40\"&gt;Screenshot of Game Over screen with score from a single short prompt.&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhl49l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "txgsync",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhl49l/qwen3coder30b_nailed_snake_game_in_one_shot_on_my/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhl49l/qwen3coder30b_nailed_snake_game_in_one_shot_on_my/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754331237,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Reading [https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after\\_6\\_months\\_of\\_fiddling\\_with\\_local\\_ai\\_heres\\_my/](https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/) it occurred to me...\n\nThere should be a BitTorrent tracker on the internet which has torrents of the models on HF.\n\n\n\n\n\nCreating torrents &amp; initial seeding can be automated to a point of only needing a monitoring &amp; alerting setup plus an oncall rotation to investigate and resolve it whenever it (inevitably) goes down/has trouble...\n\n\n\nIt's what BitTorrent was made for. The most popular models would attract thousands of seeders, meaning they'd download super fast.\n\nAnyone interested to work on this?",
          "author_fullname": "t2_45gug1j9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "BItTorrent tracker that mirrors HuggingFace",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh4r0s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 103,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 103,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754284264,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Reading &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/&lt;/a&gt; it occurred to me...&lt;/p&gt;\n\n&lt;p&gt;There should be a BitTorrent tracker on the internet which has torrents of the models on HF.&lt;/p&gt;\n\n&lt;p&gt;Creating torrents &amp;amp; initial seeding can be automated to a point of only needing a monitoring &amp;amp; alerting setup plus an oncall rotation to investigate and resolve it whenever it (inevitably) goes down/has trouble...&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s what BitTorrent was made for. The most popular models would attract thousands of seeders, meaning they&amp;#39;d download super fast.&lt;/p&gt;\n\n&lt;p&gt;Anyone interested to work on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mh4r0s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "lurkystrike",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh4r0s/bittorrent_tracker_that_mirrors_huggingface/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh4r0s/bittorrent_tracker_that_mirrors_huggingface/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754284264,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nVector Space now runs Qwen3 0.6B with up to 100 token/second on Apple Neural Engine. \n\nThe Neural Engine is a new kind of hardware unlike GPU or CPU that requires extensive changes to model architecture to make the model run on it - but we could get a significant speed gain and 1/4 energy consumption. \n\n🎉 Try it now on TestFlight:  \nhttps://testflight.apple.com/join/HXyt2bjU\n\n\n⚠️ First-time model load takes ~2 minutes (one-time setup).  \nAfter that, it’s just 1–2 seconds.",
          "author_fullname": "t2_w5xu1ep7l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Run 0.6B LLM 100token/s locally on iPhone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhl06m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/PeBg1vkGuYw6HMk_eEaN_xBxBQFb8kxcuQQi-a12uPk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754330997,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Vector Space now runs Qwen3 0.6B with up to 100 token/second on Apple Neural Engine. &lt;/p&gt;\n\n&lt;p&gt;The Neural Engine is a new kind of hardware unlike GPU or CPU that requires extensive changes to model architecture to make the model run on it - but we could get a significant speed gain and 1/4 energy consumption. &lt;/p&gt;\n\n&lt;p&gt;🎉 Try it now on TestFlight:&lt;br/&gt;\n&lt;a href=\"https://testflight.apple.com/join/HXyt2bjU\"&gt;https://testflight.apple.com/join/HXyt2bjU&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;⚠️ First-time model load takes ~2 minutes (one-time setup).&lt;br/&gt;\nAfter that, it’s just 1–2 seconds.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/lls41nzqm1hf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/lls41nzqm1hf1.jpeg?auto=webp&amp;s=a9757bad7ed1660066f3a946468a210f0589895f",
                  "width": 1320,
                  "height": 1615
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/lls41nzqm1hf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bda34bbec8fdf3925252d345c21af2d53ce861e2",
                    "width": 108,
                    "height": 132
                  },
                  {
                    "url": "https://preview.redd.it/lls41nzqm1hf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=018fe4f3ce7c0ab9be2f8ee0ae281dd7bdf76719",
                    "width": 216,
                    "height": 264
                  },
                  {
                    "url": "https://preview.redd.it/lls41nzqm1hf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e0a6353d63539bf16245aaa38749c972b9798e12",
                    "width": 320,
                    "height": 391
                  },
                  {
                    "url": "https://preview.redd.it/lls41nzqm1hf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=26b6959bbf0837900160d9c038af01d00c3f46fb",
                    "width": 640,
                    "height": 783
                  },
                  {
                    "url": "https://preview.redd.it/lls41nzqm1hf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=56d243fc4618bd59acfd62bf3368d2cc47b206e1",
                    "width": 960,
                    "height": 1174
                  },
                  {
                    "url": "https://preview.redd.it/lls41nzqm1hf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b42845624fe8bec3999ce79cee1a524c2c871d10",
                    "width": 1080,
                    "height": 1321
                  }
                ],
                "variants": {},
                "id": "pzO2nUXm5T7ElM7k5qjpe9WHFUaEPttFlE2rHWKrYK0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mhl06m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Glad-Speaker3006",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhl06m/run_06b_llm_100tokens_locally_on_iphone/",
          "stickied": false,
          "url": "https://i.redd.it/lls41nzqm1hf1.jpeg",
          "subreddit_subscribers": 510540,
          "created_utc": 1754330997,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nI'm a frequent reader but too poor to actually invest. \nWith all new models and upcomming hardware release I think it is the time to start planning.\n\nMy use case is quite straight foward, just code agent and design doc (md/mermaid) generation. With the rising of AI tool I'm actually spending more and more time on doc generation.\n\nSo what do you guys think from your experience ? Does smaller model but much faster token/s better for your daily work ? Or will the GX10 (x2) beat everything else as openAI server once released",
          "author_fullname": "t2_g1o0v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What should I pick ? 5090 or Asus GX10 or Halo Strix MiniPC at similar prices",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi1bic",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754373767,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a frequent reader but too poor to actually invest. \nWith all new models and upcomming hardware release I think it is the time to start planning.&lt;/p&gt;\n\n&lt;p&gt;My use case is quite straight foward, just code agent and design doc (md/mermaid) generation. With the rising of AI tool I&amp;#39;m actually spending more and more time on doc generation.&lt;/p&gt;\n\n&lt;p&gt;So what do you guys think from your experience ? Does smaller model but much faster token/s better for your daily work ? Or will the GX10 (x2) beat everything else as openAI server once released&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mi1bic",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "quyetnd",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi1bic/what_should_i_pick_5090_or_asus_gx10_or_halo/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi1bic/what_should_i_pick_5090_or_asus_gx10_or_halo/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754373767,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I see in the release notes that the GLM model supports Multi-Token-Prediction, but am unsure how to actually make use of it. Im currently using the 4bit quant (MLX) on mac through LM Studio, and it supports MTP through speculative decoding with a draft model, but that is different to what GLM has right?\n\nI also see discussion that llama cpp doesnt support MTP yet, so am wondering if there is any way to make use of GLM's MTP at the moment when running locally on mac.\n\nEDIT: Am i being stupid... is LM Studio with MLX already doing this when it runs the model? I'm struggling to find confirmation of this though..",
          "author_fullname": "t2_vfpls5x4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MTP with GLM 4.5 Air on Mac possible?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi0y3u",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754373381,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754372416,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see in the release notes that the GLM model supports Multi-Token-Prediction, but am unsure how to actually make use of it. Im currently using the 4bit quant (MLX) on mac through LM Studio, and it supports MTP through speculative decoding with a draft model, but that is different to what GLM has right?&lt;/p&gt;\n\n&lt;p&gt;I also see discussion that llama cpp doesnt support MTP yet, so am wondering if there is any way to make use of GLM&amp;#39;s MTP at the moment when running locally on mac.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Am i being stupid... is LM Studio with MLX already doing this when it runs the model? I&amp;#39;m struggling to find confirmation of this though..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi0y3u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TangerineRough4628",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi0y3u/mtp_with_glm_45_air_on_mac_possible/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi0y3u/mtp_with_glm_45_air_on_mac_possible/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754372416,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all, I've been experimenting with various LLM apps and have an idea for a small open-source project to address a frustration I'm hitting repeatedly. But before I dive deep, I wanted to quickly check if it already exists (fingers crossed)!\n\n**My Pain Point:**  \nI'm tired of being stuck with linear conversations. When exploring complex problems, like debugging or research, I often want to:\n\n* Ask side-questions without polluting the main conversation\n* Explore multiple paths (e.g., testing two possible solutions simultaneously)\n\nRight now, these side explorations clutter my main context, inflate token usage/costs, and make responses less relevant.\n\n**My Idea (OS)**: Small self-hosted micro-service + API that lets you:\n\n1. Branch a conversation\n2. Toggle past messages (i.e. ability to pick and choose which message are included in the context to minimize tokens and boost relevance)\n3. Get an optimized JSON context output, which you then feed into your existing LLM connector or custom client (thinking it makes the most sense to avoid direct complexity of sending messages directly to Local LLM, OpenAI, Anthropic, etc.)\n\n**Does something like this already exist?**  \nDoes this bother anyone else, is it just me, or am I missing something obvious?\n\nThanks so much for any candid feedback!\n\n**TLDR: Sick of linear LLM chats causing wasted tokens and cluttered context. Considering making an open-source tool/service for branching conversations + explicit message toggling, returning optimized JSON contexts for easy integration. Does this exist? Good idea, bad idea?**",
          "author_fullname": "t2_7xaxpi27",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tool for chat branching &amp; selective-context control exist?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhlxe1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754334058,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754332985,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;ve been experimenting with various LLM apps and have an idea for a small open-source project to address a frustration I&amp;#39;m hitting repeatedly. But before I dive deep, I wanted to quickly check if it already exists (fingers crossed)!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Pain Point:&lt;/strong&gt;&lt;br/&gt;\nI&amp;#39;m tired of being stuck with linear conversations. When exploring complex problems, like debugging or research, I often want to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ask side-questions without polluting the main conversation&lt;/li&gt;\n&lt;li&gt;Explore multiple paths (e.g., testing two possible solutions simultaneously)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Right now, these side explorations clutter my main context, inflate token usage/costs, and make responses less relevant.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Idea (OS)&lt;/strong&gt;: Small self-hosted micro-service + API that lets you:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Branch a conversation&lt;/li&gt;\n&lt;li&gt;Toggle past messages (i.e. ability to pick and choose which message are included in the context to minimize tokens and boost relevance)&lt;/li&gt;\n&lt;li&gt;Get an optimized JSON context output, which you then feed into your existing LLM connector or custom client (thinking it makes the most sense to avoid direct complexity of sending messages directly to Local LLM, OpenAI, Anthropic, etc.)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Does something like this already exist?&lt;/strong&gt;&lt;br/&gt;\nDoes this bother anyone else, is it just me, or am I missing something obvious?&lt;/p&gt;\n\n&lt;p&gt;Thanks so much for any candid feedback!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR: Sick of linear LLM chats causing wasted tokens and cluttered context. Considering making an open-source tool/service for branching conversations + explicit message toggling, returning optimized JSON contexts for easy integration. Does this exist? Good idea, bad idea?&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhlxe1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IsWired",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhlxe1/tool_for_chat_branching_selectivecontext_control/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhlxe1/tool_for_chat_branching_selectivecontext_control/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754332985,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_9b9s4a7g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Profanity: QwenCode... but is Devstral in the background. And it works. Just slower than Coder-30b-a3b... but it works.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 81,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhhbrr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/8gUCH4MJD2WmMBZraicAK7p72XubGXeMH1jGLOBdkBk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754323027,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/wa8tbrp0z0hf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/wa8tbrp0z0hf1.png?auto=webp&amp;s=2bd18a2eb9714d944827ae11190b0ee083fa0930",
                  "width": 2600,
                  "height": 1518
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/wa8tbrp0z0hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec77a68e170a79bdacd792c9b8fc588ef1dc83b7",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://preview.redd.it/wa8tbrp0z0hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f20e2b3c25bf73bcd3ab1fd96f6c6f224e8ef826",
                    "width": 216,
                    "height": 126
                  },
                  {
                    "url": "https://preview.redd.it/wa8tbrp0z0hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9be5e63324a7bfc20b041ce16d7f07b9cdc92acb",
                    "width": 320,
                    "height": 186
                  },
                  {
                    "url": "https://preview.redd.it/wa8tbrp0z0hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d6c812e6f8b133e97cedd6562f5b6358b92a0bc",
                    "width": 640,
                    "height": 373
                  },
                  {
                    "url": "https://preview.redd.it/wa8tbrp0z0hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=48fa7636b7828cb4380ae2e49075840247b2e512",
                    "width": 960,
                    "height": 560
                  },
                  {
                    "url": "https://preview.redd.it/wa8tbrp0z0hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=08d9b6f9a38fc50d111ee53aff8dc4d9b61381b8",
                    "width": 1080,
                    "height": 630
                  }
                ],
                "variants": {},
                "id": "jiRDjFD0hzFt2Lxgzc9ZLtFEvRnbsiVy-1nmLKMO1W0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhhbrr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JLeonsarmiento",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhhbrr/profanity_qwencode_but_is_devstral_in_the/",
          "stickied": false,
          "url": "https://i.redd.it/wa8tbrp0z0hf1.png",
          "subreddit_subscribers": 510540,
          "created_utc": 1754323027,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been using Gemini 2.5 Pro Deep Research with infographics since release, but I tried GLM-4.5's slides the past few days... and wow, I actually might prefer it now.\n\nHere is example of same topic:\n\nGLM 4.5 AI Slides:  \n[https://chat.z.ai/space/u01ja6suarb0-ppt](https://chat.z.ai/space/u01ja6suarb0-ppt)\n\nhttps://reddit.com/link/1mh6zja/video/0kgfqae7gygf1/player\n\nGEMINI 2.5 Pro DR:  \n[https://gemini.google.com/share/ca95257c1a48](https://gemini.google.com/share/ca95257c1a48)\n\nhttps://reddit.com/link/1mh6zja/video/gmg5vfk2eygf1/player\n\n",
          "author_fullname": "t2_3fg55rsm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 AI Sliders vs Gemini 2.5 Pro Deep Research Infographics",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "gmg5vfk2eygf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1mh6zja/asset/gmg5vfk2eygf1/DASHPlaylist.mpd?a=1756989093%2CODk5OWE5ZDYzZTA4YTdiYzUxNTM3MjQ5MzRkMDY0MTc0OWIxOTJhNmVmNDZhYjA0NjM4MDA3MjBkYmE1NmQ5MA%3D%3D&amp;v=1&amp;f=sd",
              "x": 1278,
              "y": 720,
              "hlsUrl": "https://v.redd.it/link/1mh6zja/asset/gmg5vfk2eygf1/HLSPlaylist.m3u8?a=1756989093%2CMmM5MDE4ODQ1YmNiZmE4MDdiYmZkNjMzYzU0ODQyYjY2ODFiYzFhN2VlNDNhNDJhNjYxM2ZjNmFmYmUxMGI2Nw%3D%3D&amp;v=1&amp;f=sd",
              "id": "gmg5vfk2eygf1",
              "isGif": false
            },
            "0kgfqae7gygf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1mh6zja/asset/0kgfqae7gygf1/DASHPlaylist.mpd?a=1756989093%2COWY3NmU1OWEyZmUzZjQ5M2Q1ZjE1NjBhNjQ2MTZhZDdiN2FlOTc5Yjc1OTY4NDhiNGJkZDFiNGI0N2NiOTM5OQ%3D%3D&amp;v=1&amp;f=sd",
              "x": 1278,
              "y": 720,
              "hlsUrl": "https://v.redd.it/link/1mh6zja/asset/0kgfqae7gygf1/HLSPlaylist.m3u8?a=1756989093%2CZDcxNDA2M2YzZDA0YzkxZjZmM2QwODMyZjY0NjFmMDM2YzNmZWQ0YmI0MWFlNzZjNmVmMjEyYjZkNjI1NWIzNQ%3D%3D&amp;v=1&amp;f=sd",
              "id": "0kgfqae7gygf1",
              "isGif": false
            }
          },
          "name": "t3_1mh6zja",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 48,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 48,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=be50e0facc73717d00af311d55a802e1d4f67b46",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1754292525,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using Gemini 2.5 Pro Deep Research with infographics since release, but I tried GLM-4.5&amp;#39;s slides the past few days... and wow, I actually might prefer it now.&lt;/p&gt;\n\n&lt;p&gt;Here is example of same topic:&lt;/p&gt;\n\n&lt;p&gt;GLM 4.5 AI Slides:&lt;br/&gt;\n&lt;a href=\"https://chat.z.ai/space/u01ja6suarb0-ppt\"&gt;https://chat.z.ai/space/u01ja6suarb0-ppt&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1mh6zja/video/0kgfqae7gygf1/player\"&gt;https://reddit.com/link/1mh6zja/video/0kgfqae7gygf1/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GEMINI 2.5 Pro DR:&lt;br/&gt;\n&lt;a href=\"https://gemini.google.com/share/ca95257c1a48\"&gt;https://gemini.google.com/share/ca95257c1a48&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1mh6zja/video/gmg5vfk2eygf1/player\"&gt;https://reddit.com/link/1mh6zja/video/gmg5vfk2eygf1/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?auto=webp&amp;s=06f19448d458a949198ac72d6d7c73d5e6463785",
                  "width": 400,
                  "height": 400
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=731547beb9c0ce796d8f8edd4b883c564da2c39b",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63a6eef195d7537bf441a643dbcaf760056822a2",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=abcfb3d145a4837cd123c1d5c55d56b5eaefd529",
                    "width": 320,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mh6zja",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "z1xto",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh6zja/glm_45_ai_sliders_vs_gemini_25_pro_deep_research/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh6zja/glm_45_ai_sliders_vs_gemini_25_pro_deep_research/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754292525,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Task example, with the question being \"What was net sales by reportable segment in europe in 2016?\" an a table in a text format like the following:\n\n    | | | | | | | | | | | | | | | | |   \n     | 2018|  | Change|  | 2017|  | Change|  | 2016  \n    Net Sales by Reportable Segment:|  |  |  |  |  |  |  |  |    \n    Americas| $| 112,093|  \n    |  | 16|  %|  | $| 96,600|   \n    |  | 12|  %|  | $| 86,613|   \n      \n    Europe| 62,420|  \n    |  | 14|  %|  | 54,938|   \n    |  | 10|  %|  | 49,952|   \n      \n    Greater China| 51,942|  \n    |  | 16|  %|  | 44,764|   \n    |  | (8| )%|  | 48,492|   \n      \n    Japan| 21,733|  \n    |  | 23|  %|  | 17,733|   \n    |  | 5|  %|  | 16,928|   \n    ...\n\nI'd like to find a model that can run quickly on a single GPU and can handle this task. gemma-12b (and I'm sure others) can do it, but ideally there are smaller models that can handle this sort of QA reliably. I tried tinyllama but those don't work very well.\n\nI've also tried some of the huggingface roberta-style models (purely extractive), but those don't seem to work well on this specific task, which is why I've been testing LLMs mainly. The markup is similar to what the html2text python library provides, so I guess I could finetune on existing QA/table datasets though (by converting the tables to this text format first). If you have any ideas regarding this, please share. Thank you.",
          "author_fullname": "t2_122pue6ib1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Finding a local model for text table QA",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhz4jl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754366370,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Task example, with the question being &amp;quot;What was net sales by reportable segment in europe in 2016?&amp;quot; an a table in a text format like the following:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;| | | | | | | | | | | | | | | | |   \n | 2018|  | Change|  | 2017|  | Change|  | 2016  \nNet Sales by Reportable Segment:|  |  |  |  |  |  |  |  |    \nAmericas| $| 112,093|  \n|  | 16|  %|  | $| 96,600|   \n|  | 12|  %|  | $| 86,613|   \n\nEurope| 62,420|  \n|  | 14|  %|  | 54,938|   \n|  | 10|  %|  | 49,952|   \n\nGreater China| 51,942|  \n|  | 16|  %|  | 44,764|   \n|  | (8| )%|  | 48,492|   \n\nJapan| 21,733|  \n|  | 23|  %|  | 17,733|   \n|  | 5|  %|  | 16,928|   \n...\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;d like to find a model that can run quickly on a single GPU and can handle this task. gemma-12b (and I&amp;#39;m sure others) can do it, but ideally there are smaller models that can handle this sort of QA reliably. I tried tinyllama but those don&amp;#39;t work very well.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also tried some of the huggingface roberta-style models (purely extractive), but those don&amp;#39;t seem to work well on this specific task, which is why I&amp;#39;ve been testing LLMs mainly. The markup is similar to what the html2text python library provides, so I guess I could finetune on existing QA/table datasets though (by converting the tables to this text format first). If you have any ideas regarding this, please share. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhz4jl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sea_Pomegranate_7803",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhz4jl/finding_a_local_model_for_text_table_qa/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhz4jl/finding_a_local_model_for_text_table_qa/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754366370,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone, I'm a student working on a project involving fine-tuning the **Gemma 3 4B Vision** model using **Unsloth** on a local WSL setup with a single NVIDIA RTX 4090. I'm running into a major issue where the save\\_pretrained\\_gguf function is taking **over 480 minutes** with no output, and I could really use some help troubleshooting this for my project deadline!  \n\n\n# Setup Details\n\n* **Environment**: Local WSL (tried on two different WSL machines)\n* **GPU**: Single NVIDIA RTX 4090 (confirmed with nvidia-smi)\n* **Model**: Gemma 3 4B Vision (using the vision notebook from Unsloth)\n* **Unsloth Version**: Latest (updated via pip install --upgrade unsloth unsloth\\_zoo)\n* **Other Versions**: Latest TRL, Transformers, and PyTorch\n* **Trainer**: SFTTrainer\n* **Code Snippet**:\n\n&amp;#8203;\n\n    model.save_pretrained_gguf(\"-eye-v1\", quantization_method=\"q4_k_m\")\n\nhttps://preview.redd.it/1ptvi5n1h4hf1.png?width=2443&amp;format=png&amp;auto=webp&amp;s=0bebe14a8f0bcfa11c82c21459e17be76fcc5f1f\n\nThe save\\_pretrained\\_gguf function for converting the fine-tuned model to GGUF format (with q4\\_k\\_m quantization) has been running for **over 8 hours** without completing or producing any output. I’ve tested this on two separate WSL machines, and the issue persists. No error messages are shown, but the process just hangs indefinitely.  \nthanks in advance !",
          "author_fullname": "t2_e4ojre534",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Student Unsloth Help] Save to GGUF Taking Forever with Gemma 3 4B Vision + Unsloth on WSL (Single 4090)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 46,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "1ptvi5n1h4hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 36,
                  "x": 108,
                  "u": "https://preview.redd.it/1ptvi5n1h4hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7bfb9ea527858591eb0b623579d8e9bfcd4d8ebc"
                },
                {
                  "y": 72,
                  "x": 216,
                  "u": "https://preview.redd.it/1ptvi5n1h4hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4bf37b3e767578e0622e6b784a6bb7dcfe9fbc97"
                },
                {
                  "y": 107,
                  "x": 320,
                  "u": "https://preview.redd.it/1ptvi5n1h4hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=78340c00de7b7b152f228d82d0f1791e0e73e27d"
                },
                {
                  "y": 214,
                  "x": 640,
                  "u": "https://preview.redd.it/1ptvi5n1h4hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7a246b9e5f76abbc705a645cccde20e016264aa"
                },
                {
                  "y": 321,
                  "x": 960,
                  "u": "https://preview.redd.it/1ptvi5n1h4hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8bcc3554f93101caa3f2c40d43e1933f1d755b27"
                },
                {
                  "y": 361,
                  "x": 1080,
                  "u": "https://preview.redd.it/1ptvi5n1h4hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a3354fd3b968faaca5e6cc36da6fa2f1ed0a75ba"
                }
              ],
              "s": {
                "y": 818,
                "x": 2443,
                "u": "https://preview.redd.it/1ptvi5n1h4hf1.png?width=2443&amp;format=png&amp;auto=webp&amp;s=0bebe14a8f0bcfa11c82c21459e17be76fcc5f1f"
              },
              "id": "1ptvi5n1h4hf1"
            }
          },
          "name": "t3_1mhz2sc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/7PXhEUB2aMKe-cmqZEGIO5saO8iivH8buydeI3lcRXk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754366222,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I&amp;#39;m a student working on a project involving fine-tuning the &lt;strong&gt;Gemma 3 4B Vision&lt;/strong&gt; model using &lt;strong&gt;Unsloth&lt;/strong&gt; on a local WSL setup with a single NVIDIA RTX 4090. I&amp;#39;m running into a major issue where the save_pretrained_gguf function is taking &lt;strong&gt;over 480 minutes&lt;/strong&gt; with no output, and I could really use some help troubleshooting this for my project deadline!  &lt;/p&gt;\n\n&lt;h1&gt;Setup Details&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: Local WSL (tried on two different WSL machines)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: Single NVIDIA RTX 4090 (confirmed with nvidia-smi)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Model&lt;/strong&gt;: Gemma 3 4B Vision (using the vision notebook from Unsloth)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Unsloth Version&lt;/strong&gt;: Latest (updated via pip install --upgrade unsloth unsloth_zoo)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Other Versions&lt;/strong&gt;: Latest TRL, Transformers, and PyTorch&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Trainer&lt;/strong&gt;: SFTTrainer&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Code Snippet&lt;/strong&gt;:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;model.save_pretrained_gguf(&amp;quot;-eye-v1&amp;quot;, quantization_method=&amp;quot;q4_k_m&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1ptvi5n1h4hf1.png?width=2443&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bebe14a8f0bcfa11c82c21459e17be76fcc5f1f\"&gt;https://preview.redd.it/1ptvi5n1h4hf1.png?width=2443&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bebe14a8f0bcfa11c82c21459e17be76fcc5f1f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The save_pretrained_gguf function for converting the fine-tuned model to GGUF format (with q4_k_m quantization) has been running for &lt;strong&gt;over 8 hours&lt;/strong&gt; without completing or producing any output. I’ve tested this on two separate WSL machines, and the issue persists. No error messages are shown, but the process just hangs indefinitely.&lt;br/&gt;\nthanks in advance !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhz2sc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LeastExperience1579",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhz2sc/student_unsloth_help_save_to_gguf_taking_forever/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhz2sc/student_unsloth_help_save_to_gguf_taking_forever/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754366222,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI'm planning a new build primarily for running a 72B model using vLLM (qwen 72b vl) with FP8.I think about  using four RTX 5090s for the highest throughput. I was also thinking about one rtx pro 6000 but the inference speed is much slower and the cost similar. I get about 3x the throughput with 4x 5090 (tested via [runpod.io](http://runpod.io)), but I've hit a wall concerning the case/chassis and the physical connectivity.  Also my budget is limited, I would like to keep it below 15k. \n\nI'd appreciate a sanity check on the whole build, especially on the feasibility of housing and connecting four of these GPUs. Any comments, critiques, or suggestions are more than welcome!\n\n**The Goal**\n\n* **Primary:** Run a 72B parameter model with FP8 quantisation using vLLM on a dedicated set of GPUs.\n* **Secondary:** Use separate GPUs for other tasks\n\n**Core System Components**\n\n* **CPU:** AMD EPYC 7402P (24 Cores, 48 Threads, 180W TDP) (\\~300 CHF)\n* **Motherboard:** ASRock Rack ROMED8-2T (Socket SP3, 8x DDR4, 7x PCIe 4.0 x16 slots) (\\~750 CHF) (  I chose a motherboard with pcie 4 because i will not use the setup for fine tuning and the pricing of the cpu + motherboard is much cheaper and i already have ram for it. )\n* **RAM:** 512 GB DDR4 ECC (Already have)\n* **Storage:** 2TB - 4TB PCIe 4.0 NVMe SSD (\\~300 CHF)\n* **CPU Cooler:** A high-performance air cooler compatible with AMD SP3, like a Noctua TR4-SP3. (\\~100 CHF)\n* **PSU:** Fortron Cannon Pro 2500W (80+ Platinum). Should be enough even with all GPUs. Could also add a second in case not or the ASUS Pro WS 3000W if its released soon (\\~500 CHF)\n\n**GPU Configuration**\n\n* **For the LLM:**\n   * **4x NVIDIA RTX 5090** (\\~8,000 CHF)\n   * I plan to **power-limit each RTX 5090 to 300-350W**. \n* **For other tasks**\n   * 2x NVIDIA RTX 5080 (\\~2,400 CHF total)\n\n**My Main Questions &amp; Problems**\n\n1. Case / Chassis\\*\\*:\\*\\* This is my biggest problem. What's the best way to house 4x RTX 5090s (plus 2x 5080s)? An open-air frame seems likely, but which ones work well for this many cards? Is there a server chassis that could handle the spacing, where it would eventually also be possible to add more later ?\n2. GPU Connectivity: The motherboard has 7x PCIe 4.0 x16 slots. I'll need to use 6 of them, whats the best way to connect them, pcie riser cables, are there any problems fixing the gpu just in front or do i need  any kind of gpu support or is the pcie bracket strong enough?\n3. General Sanity Check: Looking at the whole picture, does this setup look ok? \n4. Do you think using the first of the cases below would work with riser cables and the above listed hardware ? \n\nThanks in advance for your time!\n\nFor the cases so far i thought about: \n\n1. [https://www.amazon.de/-/en/Mining-Support-Supply-Currency-Bitcoin-Black/dp/B094H1Z8RB/ref=sr\\_1\\_1?crid=1W2DBFC5AP24X&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2\\_D32-G2a\\_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm\\_Dg-ZLK-GZpvwX1\\_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD\\_-7BaQ&amp;dib\\_tag=se&amp;keywords=mining+rig&amp;qid=1754349402&amp;sprefix=mining+rig%2Caps%2C177&amp;sr=8-1](https://www.amazon.de/-/en/Mining-Support-Supply-Currency-Bitcoin-Black/dp/B094H1Z8RB/ref=sr_1_1?crid=1W2DBFC5AP24X&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;dib_tag=se&amp;keywords=mining+rig&amp;qid=1754349402&amp;sprefix=mining+rig%2Caps%2C177&amp;sr=8-1)\n2. [ttps://www.amazon.de/-/en/Mining-Currency-Ethereum-Bitcoin-Support-Black/dp/B09DGKLKY3/ref=sr\\_1\\_6?crid=7NH56ZDDBUQV&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2\\_D32-G2a\\_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm\\_Dg-ZLK-GZpvwX1\\_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD\\_-7BaQ&amp;dib\\_tag=se&amp;keywords=mining+rig&amp;qid=1754348992&amp;sprefix=mining+ri%2Caps%2C116&amp;sr=8-6](https://www.amazon.de/-/en/Mining-Currency-Ethereum-Bitcoin-Support-Black/dp/B09DGKLKY3/ref=sr_1_6?crid=7NH56ZDDBUQV&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;dib_tag=se&amp;keywords=mining+rig&amp;qid=1754348992&amp;sprefix=mining+ri%2Caps%2C116&amp;sr=8-6)\n3. [https://www.amazon.de/-/en/MININGEEK-Raised-Support-Cooling-Supplies-black/dp/B0D1G91MKC/ref=sr\\_1\\_3?crid=1W2DBFC5AP24X&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2\\_D32-G2a\\_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm\\_Dg-ZLK-GZpvwX1\\_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD\\_-7BaQ&amp;dib\\_tag=se&amp;keywords=mining+rig&amp;qid=1754349402&amp;sprefix=mining+rig%2Caps%2C177&amp;sr=8-3](https://www.amazon.de/-/en/MININGEEK-Raised-Support-Cooling-Supplies-black/dp/B0D1G91MKC/ref=sr_1_3?crid=1W2DBFC5AP24X&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;dib_tag=se&amp;keywords=mining+rig&amp;qid=1754349402&amp;sprefix=mining+rig%2Caps%2C177&amp;sr=8-3)",
          "author_fullname": "t2_oaw1i0pr4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help building an price efficient inference server (no fine tuning) + multi 5090 setup",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhu9tx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754352667,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning a new build primarily for running a 72B model using vLLM (qwen 72b vl) with FP8.I think about  using four RTX 5090s for the highest throughput. I was also thinking about one rtx pro 6000 but the inference speed is much slower and the cost similar. I get about 3x the throughput with 4x 5090 (tested via &lt;a href=\"http://runpod.io\"&gt;runpod.io&lt;/a&gt;), but I&amp;#39;ve hit a wall concerning the case/chassis and the physical connectivity.  Also my budget is limited, I would like to keep it below 15k. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d appreciate a sanity check on the whole build, especially on the feasibility of housing and connecting four of these GPUs. Any comments, critiques, or suggestions are more than welcome!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Goal&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Primary:&lt;/strong&gt; Run a 72B parameter model with FP8 quantisation using vLLM on a dedicated set of GPUs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Secondary:&lt;/strong&gt; Use separate GPUs for other tasks&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Core System Components&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD EPYC 7402P (24 Cores, 48 Threads, 180W TDP) (~300 CHF)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; ASRock Rack ROMED8-2T (Socket SP3, 8x DDR4, 7x PCIe 4.0 x16 slots) (~750 CHF) (  I chose a motherboard with pcie 4 because i will not use the setup for fine tuning and the pricing of the cpu + motherboard is much cheaper and i already have ram for it. )&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 512 GB DDR4 ECC (Already have)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Storage:&lt;/strong&gt; 2TB - 4TB PCIe 4.0 NVMe SSD (~300 CHF)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CPU Cooler:&lt;/strong&gt; A high-performance air cooler compatible with AMD SP3, like a Noctua TR4-SP3. (~100 CHF)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;PSU:&lt;/strong&gt; Fortron Cannon Pro 2500W (80+ Platinum). Should be enough even with all GPUs. Could also add a second in case not or the ASUS Pro WS 3000W if its released soon (~500 CHF)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;GPU Configuration&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;For the LLM:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;4x NVIDIA RTX 5090&lt;/strong&gt; (~8,000 CHF)&lt;/li&gt;\n&lt;li&gt;I plan to &lt;strong&gt;power-limit each RTX 5090 to 300-350W&lt;/strong&gt;. &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;For other tasks&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;2x NVIDIA RTX 5080 (~2,400 CHF total)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;My Main Questions &amp;amp; Problems&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Case / Chassis**:** This is my biggest problem. What&amp;#39;s the best way to house 4x RTX 5090s (plus 2x 5080s)? An open-air frame seems likely, but which ones work well for this many cards? Is there a server chassis that could handle the spacing, where it would eventually also be possible to add more later ?&lt;/li&gt;\n&lt;li&gt;GPU Connectivity: The motherboard has 7x PCIe 4.0 x16 slots. I&amp;#39;ll need to use 6 of them, whats the best way to connect them, pcie riser cables, are there any problems fixing the gpu just in front or do i need  any kind of gpu support or is the pcie bracket strong enough?&lt;/li&gt;\n&lt;li&gt;General Sanity Check: Looking at the whole picture, does this setup look ok? &lt;/li&gt;\n&lt;li&gt;Do you think using the first of the cases below would work with riser cables and the above listed hardware ? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance for your time!&lt;/p&gt;\n\n&lt;p&gt;For the cases so far i thought about: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.de/-/en/Mining-Support-Supply-Currency-Bitcoin-Black/dp/B094H1Z8RB/ref=sr_1_1?crid=1W2DBFC5AP24X&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754349402&amp;amp;sprefix=mining+rig%2Caps%2C177&amp;amp;sr=8-1\"&gt;https://www.amazon.de/-/en/Mining-Support-Supply-Currency-Bitcoin-Black/dp/B094H1Z8RB/ref=sr_1_1?crid=1W2DBFC5AP24X&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754349402&amp;amp;sprefix=mining+rig%2Caps%2C177&amp;amp;sr=8-1&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.de/-/en/Mining-Currency-Ethereum-Bitcoin-Support-Black/dp/B09DGKLKY3/ref=sr_1_6?crid=7NH56ZDDBUQV&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754348992&amp;amp;sprefix=mining+ri%2Caps%2C116&amp;amp;sr=8-6\"&gt;ttps://www.amazon.de/-/en/Mining-Currency-Ethereum-Bitcoin-Support-Black/dp/B09DGKLKY3/ref=sr_1_6?crid=7NH56ZDDBUQV&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754348992&amp;amp;sprefix=mining+ri%2Caps%2C116&amp;amp;sr=8-6&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.de/-/en/MININGEEK-Raised-Support-Cooling-Supplies-black/dp/B0D1G91MKC/ref=sr_1_3?crid=1W2DBFC5AP24X&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754349402&amp;amp;sprefix=mining+rig%2Caps%2C177&amp;amp;sr=8-3\"&gt;https://www.amazon.de/-/en/MININGEEK-Raised-Support-Cooling-Supplies-black/dp/B0D1G91MKC/ref=sr_1_3?crid=1W2DBFC5AP24X&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754349402&amp;amp;sprefix=mining+rig%2Caps%2C177&amp;amp;sr=8-3&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?auto=webp&amp;s=9b598b3fe915c4e340a1d2be347d6ada11f361b7",
                  "width": 2400,
                  "height": 1260
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2ec29473ad9a43f57f6de38e719603168628711",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3fbe80d3cfbb71c2262379cd9b070f60a3559377",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=756815133007c791068d5b797184c842b074ab75",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4feb605e519121401324c6cc277e71f0c83948d",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad4525174d8ac6007a90f9bd6e65c6a7aa6a406c",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fece91f94e327af10d6bd9f2be0f89eeec62d4cb",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhu9tx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Civil-Image5411",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754352667,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "YuE: Open Full-song Music Generation Foundation Model, something similar to [Suno.ai](http://Suno.ai) but open",
          "author_fullname": "t2_etmr2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Music Foundation Models for Full-Song Generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mha439",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754304531,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "map-yue.github.io",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;YuE: Open Full-song Music Generation Foundation Model, something similar to &lt;a href=\"http://Suno.ai\"&gt;Suno.ai&lt;/a&gt; but open&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://map-yue.github.io/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mha439",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "phone_radio_tv",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mha439/open_music_foundation_models_for_fullsong/",
          "stickied": false,
          "url": "https://map-yue.github.io/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754304531,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a beast of a machine: [https://www.reddit.com/r/nvidia/comments/1mf0yal/2xl40s\\_2x6000\\_ada\\_4xrtx\\_6000\\_pro\\_build/](https://www.reddit.com/r/nvidia/comments/1mf0yal/2xl40s_2x6000_ada_4xrtx_6000_pro_build/)\n\n  \nHowever, most of the time I am running heavy CUDA workloads.  I want to run LLM locally on CPU and only use the GPU if it isn't currently being used.  I've been using ollama but I think its giving me some memory troubles (segfaults) when running two workloads at the same time.\n\n  \nIs there an inference engine that I can prioritize what resources to use and when?",
          "author_fullname": "t2_15x8q5kwup",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best tool to prioritize workloads sharing with LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhyn3c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754364878,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a beast of a machine: &lt;a href=\"https://www.reddit.com/r/nvidia/comments/1mf0yal/2xl40s_2x6000_ada_4xrtx_6000_pro_build/\"&gt;https://www.reddit.com/r/nvidia/comments/1mf0yal/2xl40s_2x6000_ada_4xrtx_6000_pro_build/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;However, most of the time I am running heavy CUDA workloads.  I want to run LLM locally on CPU and only use the GPU if it isn&amp;#39;t currently being used.  I&amp;#39;ve been using ollama but I think its giving me some memory troubles (segfaults) when running two workloads at the same time.&lt;/p&gt;\n\n&lt;p&gt;Is there an inference engine that I can prioritize what resources to use and when?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhyn3c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ill_Recipe7620",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhyn3c/best_tool_to_prioritize_workloads_sharing_with_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhyn3c/best_tool_to_prioritize_workloads_sharing_with_llm/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754364878,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1urq336nt3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Raw text file not starting Lora training",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi2izf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/HzTp1WwrMfcDvm71xzczzZdG5LXU7oQkB5mDYcYM7Mo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754378289,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/m3madkmdj5hf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/m3madkmdj5hf1.jpeg?auto=webp&amp;s=7759c5d74f172b0e87ee6cd4efd7d561d886c541",
                  "width": 1842,
                  "height": 4096
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/m3madkmdj5hf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa95790d9d2e01003c57cb82e3e4b115d76f73a6",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/m3madkmdj5hf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a9a6ee9f11c8b7132f83ee9b70e8337b46726b9",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://preview.redd.it/m3madkmdj5hf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c28df658e85ad2e60ae8262bc5977142a7584ebe",
                    "width": 320,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/m3madkmdj5hf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e6daab7df76d1e0d5487bf276484bb932d3a514",
                    "width": 640,
                    "height": 1280
                  },
                  {
                    "url": "https://preview.redd.it/m3madkmdj5hf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6083facdc0f28a09257ede70b1afe5a47eadee9f",
                    "width": 960,
                    "height": 1920
                  },
                  {
                    "url": "https://preview.redd.it/m3madkmdj5hf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5026e76861db3bd1ffebb90763d83cf811d52cde",
                    "width": 1080,
                    "height": 2160
                  }
                ],
                "variants": {},
                "id": "GRzeufP-ztJe-fwF-rg7WEvEUnZzozqsUZRtyQGiQCE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi2izf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vulgar1171",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi2izf/raw_text_file_not_starting_lora_training/",
          "stickied": false,
          "url": "https://i.redd.it/m3madkmdj5hf1.jpeg",
          "subreddit_subscribers": 510540,
          "created_utc": 1754378289,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We started using LiteLLM a few months back to route across OpenAI and Anthropic. It worked well during dev and light load tests. But as soon as we crossed around 300 requests per second, things started to break:\n\n* Some requests randomly timed out or took way longer than others, even with the same provider\n* Logs didn’t show much, and tracing failures across providers was difficult\n* When we tried running it behind a load balancer, we ran into strange behavior with state\n* Fallbacks didn’t always trigger reliably when a provider was down or rate-limited\n* We tried plugging in Prometheus, but visibility into request flow was limited\n\nThe architecture is simple, which helps at first, but that simplicity makes it hard to scale without building extra tooling around it.\n\nWhile looking for alternatives, I came across a few self-hosted ones. Most were either too early or too complex to set up. Eager to know if there are other better alternatives to litellm that work well in prod. Is anyone building their own gateway, or using something more stable?",
          "author_fullname": "t2_1p9vds0za6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LiteLLM started breaking down for us past 300 RPS, what are folks using in prod?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh99hu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754301478,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We started using LiteLLM a few months back to route across OpenAI and Anthropic. It worked well during dev and light load tests. But as soon as we crossed around 300 requests per second, things started to break:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Some requests randomly timed out or took way longer than others, even with the same provider&lt;/li&gt;\n&lt;li&gt;Logs didn’t show much, and tracing failures across providers was difficult&lt;/li&gt;\n&lt;li&gt;When we tried running it behind a load balancer, we ran into strange behavior with state&lt;/li&gt;\n&lt;li&gt;Fallbacks didn’t always trigger reliably when a provider was down or rate-limited&lt;/li&gt;\n&lt;li&gt;We tried plugging in Prometheus, but visibility into request flow was limited&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The architecture is simple, which helps at first, but that simplicity makes it hard to scale without building extra tooling around it.&lt;/p&gt;\n\n&lt;p&gt;While looking for alternatives, I came across a few self-hosted ones. Most were either too early or too complex to set up. Eager to know if there are other better alternatives to litellm that work well in prod. Is anyone building their own gateway, or using something more stable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh99hu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Otherwise_Flan7339",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754301478,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI'm a student working on a summer project involving multimodal models, and I’m currently testing **Gemma 3 Vision** with **Unsloth**. I used the **official vision inference notebook** (no major changes), loaded the model using `FastVisionModel.for_inference()`, and passed an image + prompt, but the output is just **nonsense** — totally unrelated or hallucinated responses. My setup:\n\n* **Model**: `unsloth/gemma3-4b-pt`\n* **Framework**: Unsloth\n* **Vision loader**: `FastVisionModel.for_inference()`\n* **Prompt**: Tried variations like greeting\n\nI also correctly loaded the model with chat templete\n\nhttps://preview.redd.it/egrbbd6834hf1.png?width=1830&amp;format=png&amp;auto=webp&amp;s=f5883b395e578afa58b958793ea4e5f41aa0bb70\n\nhttps://preview.redd.it/yz88rd6834hf1.png?width=1614&amp;format=png&amp;auto=webp&amp;s=29ac8caf848a6f1c50af3e0e105bc4cc1ac27eff\n\nAny advice or working example would be a huge help 🙏Thank you ",
          "author_fullname": "t2_e4ojre534",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Student Project Help] Gemma 3 Vision (Unsloth) giving nonsense output — used official notebook",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 118,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "egrbbd6834hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 91,
                  "x": 108,
                  "u": "https://preview.redd.it/egrbbd6834hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=261a3d6e81bcf665a819a49fe5b1db8e0ac82c7a"
                },
                {
                  "y": 182,
                  "x": 216,
                  "u": "https://preview.redd.it/egrbbd6834hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6a957a953a202ec19129ba4998ba4d138c3f73a6"
                },
                {
                  "y": 270,
                  "x": 320,
                  "u": "https://preview.redd.it/egrbbd6834hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8541b617dcc1315e3d084dbf85e2fea05c1bc1d7"
                },
                {
                  "y": 540,
                  "x": 640,
                  "u": "https://preview.redd.it/egrbbd6834hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3968299fad5fdfa6d75ae979d9318de5b37f11ac"
                },
                {
                  "y": 811,
                  "x": 960,
                  "u": "https://preview.redd.it/egrbbd6834hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=976b94d4d80b39bb36e62964b1abf3c4a2bfdf40"
                },
                {
                  "y": 912,
                  "x": 1080,
                  "u": "https://preview.redd.it/egrbbd6834hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=337dba84a2e29943c7a303282c776434e66b4598"
                }
              ],
              "s": {
                "y": 1546,
                "x": 1830,
                "u": "https://preview.redd.it/egrbbd6834hf1.png?width=1830&amp;format=png&amp;auto=webp&amp;s=f5883b395e578afa58b958793ea4e5f41aa0bb70"
              },
              "id": "egrbbd6834hf1"
            },
            "yz88rd6834hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/yz88rd6834hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3ca9eb4fc3a97b598103bc68df55f274dded6e32"
                },
                {
                  "y": 188,
                  "x": 216,
                  "u": "https://preview.redd.it/yz88rd6834hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=838db8c95f647f2f7ecc9b75ff5fd505d795f2ac"
                },
                {
                  "y": 279,
                  "x": 320,
                  "u": "https://preview.redd.it/yz88rd6834hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ca6bc15c228045564b6fbf5b16a0c813db03e139"
                },
                {
                  "y": 558,
                  "x": 640,
                  "u": "https://preview.redd.it/yz88rd6834hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8cc14162e21d6d513130765e0982ffe90d7a6334"
                },
                {
                  "y": 837,
                  "x": 960,
                  "u": "https://preview.redd.it/yz88rd6834hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=be4aad92afed36ad48c2b25dcdda373f1791d275"
                },
                {
                  "y": 942,
                  "x": 1080,
                  "u": "https://preview.redd.it/yz88rd6834hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5743b5b2962e6c085796e6bcb8d28263215cccb3"
                }
              ],
              "s": {
                "y": 1408,
                "x": 1614,
                "u": "https://preview.redd.it/yz88rd6834hf1.png?width=1614&amp;format=png&amp;auto=webp&amp;s=29ac8caf848a6f1c50af3e0e105bc4cc1ac27eff"
              },
              "id": "yz88rd6834hf1"
            }
          },
          "name": "t3_1mhx8cn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/n5fLnU8XA0Ape_gIgDRLZQQZ8f1LHO-swJNXz4ozKDo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754360797,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a student working on a summer project involving multimodal models, and I’m currently testing &lt;strong&gt;Gemma 3 Vision&lt;/strong&gt; with &lt;strong&gt;Unsloth&lt;/strong&gt;. I used the &lt;strong&gt;official vision inference notebook&lt;/strong&gt; (no major changes), loaded the model using &lt;code&gt;FastVisionModel.for_inference()&lt;/code&gt;, and passed an image + prompt, but the output is just &lt;strong&gt;nonsense&lt;/strong&gt; — totally unrelated or hallucinated responses. My setup:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Model&lt;/strong&gt;: &lt;code&gt;unsloth/gemma3-4b-pt&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Framework&lt;/strong&gt;: Unsloth&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Vision loader&lt;/strong&gt;: &lt;code&gt;FastVisionModel.for_inference()&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Prompt&lt;/strong&gt;: Tried variations like greeting&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I also correctly loaded the model with chat templete&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/egrbbd6834hf1.png?width=1830&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5883b395e578afa58b958793ea4e5f41aa0bb70\"&gt;https://preview.redd.it/egrbbd6834hf1.png?width=1830&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5883b395e578afa58b958793ea4e5f41aa0bb70&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yz88rd6834hf1.png?width=1614&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29ac8caf848a6f1c50af3e0e105bc4cc1ac27eff\"&gt;https://preview.redd.it/yz88rd6834hf1.png?width=1614&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29ac8caf848a6f1c50af3e0e105bc4cc1ac27eff&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any advice or working example would be a huge help 🙏Thank you &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhx8cn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LeastExperience1579",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhx8cn/student_project_help_gemma_3_vision_unsloth/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhx8cn/student_project_help_gemma_3_vision_unsloth/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754360797,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "NVIDIA AI-Q, a blueprint for building AI agents with advanced reasoning skills, is now the top-rated open, portable AI agent in the **LLM with search category** on Hugging Face’s [Deep Research Bench leaderboard](https://huggingface.co/spaces/Ayanami0730/DeepResearch-Leaderboard). \n\nAI-Q, the top open and portable AI agent for deep research, uses NVIDIA Llama Nemotron for advanced reasoning and NeMo Retriever for RAG, informed by an organization’s internal data. With AI-Q, organizations can build AI agents that operate anywhere—data center and cloud. AI-Q enables the creation of fully customizable, local AI agents with reduced latency that meet security requirements for regulated industries.\n\nHear more from NVIDIA product leader, Adel El Hallak 📺 [https://www.youtube.com/shorts/dd\\_pJchCxTg](https://www.youtube.com/shorts/dd_pJchCxTg)\n\nDevelopers - get started here  ➡️ [https://build.nvidia.com/nvidia/aiq](https://build.nvidia.com/nvidia/aiq)\n\nhttps://i.redd.it/jgv068r4h1hf1.gif\n\n",
          "author_fullname": "t2_1vf7k06t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA AI-Q Achieves Top Score for Open, Portable AI Deep Research (LLM with Search Category)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "jgv068r4h1hf1": {
              "status": "valid",
              "e": "AnimatedImage",
              "m": "image/gif",
              "p": [
                {
                  "y": 63,
                  "x": 108,
                  "u": "https://preview.redd.it/jgv068r4h1hf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=41ecd26c4ebfb0888b6b090467afa0998ae2c023"
                },
                {
                  "y": 126,
                  "x": 216,
                  "u": "https://preview.redd.it/jgv068r4h1hf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=415fa49038eccc7eea61affaa6d07fede972065f"
                },
                {
                  "y": 188,
                  "x": 320,
                  "u": "https://preview.redd.it/jgv068r4h1hf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=e12c4df6af6e828ac7d05dd0ba31b034ab18e10d"
                },
                {
                  "y": 376,
                  "x": 640,
                  "u": "https://preview.redd.it/jgv068r4h1hf1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=1305c534a3741fa2776638f279c1c2db02ba503f"
                }
              ],
              "s": {
                "y": 470,
                "gif": "https://i.redd.it/jgv068r4h1hf1.gif",
                "mp4": "https://preview.redd.it/jgv068r4h1hf1.gif?format=mp4&amp;s=04e2364284527d682e9f9bb0a7601502685d606c",
                "x": 800
              },
              "id": "jgv068r4h1hf1"
            }
          },
          "name": "t3_1mhk4it",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/axz8MpGbRHTMyaCQ3GQH5ICNpeA557SBPWKFqbOeoHY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754329131,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;NVIDIA AI-Q, a blueprint for building AI agents with advanced reasoning skills, is now the top-rated open, portable AI agent in the &lt;strong&gt;LLM with search category&lt;/strong&gt; on Hugging Face’s &lt;a href=\"https://huggingface.co/spaces/Ayanami0730/DeepResearch-Leaderboard\"&gt;Deep Research Bench leaderboard&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;AI-Q, the top open and portable AI agent for deep research, uses NVIDIA Llama Nemotron for advanced reasoning and NeMo Retriever for RAG, informed by an organization’s internal data. With AI-Q, organizations can build AI agents that operate anywhere—data center and cloud. AI-Q enables the creation of fully customizable, local AI agents with reduced latency that meet security requirements for regulated industries.&lt;/p&gt;\n\n&lt;p&gt;Hear more from NVIDIA product leader, Adel El Hallak 📺 &lt;a href=\"https://www.youtube.com/shorts/dd_pJchCxTg\"&gt;https://www.youtube.com/shorts/dd_pJchCxTg&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Developers - get started here  ➡️ &lt;a href=\"https://build.nvidia.com/nvidia/aiq\"&gt;https://build.nvidia.com/nvidia/aiq&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.redd.it/jgv068r4h1hf1.gif\"&gt;https://i.redd.it/jgv068r4h1hf1.gif&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mhk4it",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PDXcoder2000",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhk4it/nvidia_aiq_achieves_top_score_for_open_portable/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhk4it/nvidia_aiq_achieves_top_score_for_open_portable/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754329131,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Lately I started using more local LLMs again but after playing around with the latest Qwen MOE with A3B I found out the hard way how fast it falls apart due to hallucination and similar, especially when context gets a little bit longer (were talking \\~1k t). Might be because the model is just not good, because of the quant or the quant provider. In any case, I wanna stop with this \"vibe testing\" and have some up-to-date eval I can use to at least compare the basics. I know there are datasets and eval libs but i was looking for more for a \"full pacakge\" (that uses these eval libs).\n\nDoes anyone has a nice project already, ideally python, to share?\n\nSome requirements:  \n\n* Goal would be really to compare local models and their quants, not make general tests - here we have enough benchmarks already\n* Works with localmodels and their apis (e.g. Ollama/litellm) - I dont mind something foundational for the \"LLM as a judge\" though\n* Dataset as mentioned should check the foundations, like reasoning, halluzinations, instruction following... nothing too wild but with focus on longer contexts not just simple questions\n* datasets shouldn't be too big as I dont want to spend too much on running incl judge LLMs\n* Its not professional - doesnt mean  it cant use professional libs if its not overkill\n\n  \nI was actually working in different areas with datasets and evals (eg i like \"inspect ai\") but datasets were often very special or technical for certain cases. Or others try to solve everything and half of it is not working (lm evaluation harness). Its generally surprising how many datasets just suck or have issues.  \n  \nAnd there  must be someone better (hopefully) putting their working code out already. Otherwise I will probably try to get something going (again)",
          "author_fullname": "t2_1bpvzzmckh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Evalproject for Local LLMs &amp; Quants",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mho569",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754337887,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lately I started using more local LLMs again but after playing around with the latest Qwen MOE with A3B I found out the hard way how fast it falls apart due to hallucination and similar, especially when context gets a little bit longer (were talking ~1k t). Might be because the model is just not good, because of the quant or the quant provider. In any case, I wanna stop with this &amp;quot;vibe testing&amp;quot; and have some up-to-date eval I can use to at least compare the basics. I know there are datasets and eval libs but i was looking for more for a &amp;quot;full pacakge&amp;quot; (that uses these eval libs).&lt;/p&gt;\n\n&lt;p&gt;Does anyone has a nice project already, ideally python, to share?&lt;/p&gt;\n\n&lt;p&gt;Some requirements:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Goal would be really to compare local models and their quants, not make general tests - here we have enough benchmarks already&lt;/li&gt;\n&lt;li&gt;Works with localmodels and their apis (e.g. Ollama/litellm) - I dont mind something foundational for the &amp;quot;LLM as a judge&amp;quot; though&lt;/li&gt;\n&lt;li&gt;Dataset as mentioned should check the foundations, like reasoning, halluzinations, instruction following... nothing too wild but with focus on longer contexts not just simple questions&lt;/li&gt;\n&lt;li&gt;datasets shouldn&amp;#39;t be too big as I dont want to spend too much on running incl judge LLMs&lt;/li&gt;\n&lt;li&gt;Its not professional - doesnt mean  it cant use professional libs if its not overkill&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I was actually working in different areas with datasets and evals (eg i like &amp;quot;inspect ai&amp;quot;) but datasets were often very special or technical for certain cases. Or others try to solve everything and half of it is not working (lm evaluation harness). Its generally surprising how many datasets just suck or have issues.  &lt;/p&gt;\n\n&lt;p&gt;And there  must be someone better (hopefully) putting their working code out already. Otherwise I will probably try to get something going (again)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mho569",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nore_se_kra",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mho569/evalproject_for_local_llms_quants/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mho569/evalproject_for_local_llms_quants/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754337887,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is there a way to export or backup an AnythingLLM workspace/RAG? I have one that is well developed and want to deploy it so others can mess around with it, but since it keeps track of chat history, I want a backup and a way to import it into a new workspace if the interaction changes its dynamic too much.\n\nAlso just in general want to back up the workspaces as checkpoints periodically. Any advice appreciated including if I should be using something besides AnythingLLM to suit my needs better.",
          "author_fullname": "t2_36loo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Export/Backup AnythingLLM Workspace?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhrey9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754345379,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to export or backup an AnythingLLM workspace/RAG? I have one that is well developed and want to deploy it so others can mess around with it, but since it keeps track of chat history, I want a backup and a way to import it into a new workspace if the interaction changes its dynamic too much.&lt;/p&gt;\n\n&lt;p&gt;Also just in general want to back up the workspaces as checkpoints periodically. Any advice appreciated including if I should be using something besides AnythingLLM to suit my needs better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhrey9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nuvious",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhrey9/exportbackup_anythingllm_workspace/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhrey9/exportbackup_anythingllm_workspace/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754345379,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am in quest of finding SOTA document parser for PDF/Docx files. I have about 100k pages with tables, text, images(with text) that I want to convert to markdown format.\n\nWhat is the best open source document parser available right now? That reaches near to Azure document intelligence accruacy.\n\nI have explored\n\n* Doclin\n* Marker\n* Pymupdf\n\nWhich one would be best to use in production?\n\n",
          "author_fullname": "t2_1d34fueda3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best document parser",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhe2h9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754315591,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in quest of finding SOTA document parser for PDF/Docx files. I have about 100k pages with tables, text, images(with text) that I want to convert to markdown format.&lt;/p&gt;\n\n&lt;p&gt;What is the best open source document parser available right now? That reaches near to Azure document intelligence accruacy.&lt;/p&gt;\n\n&lt;p&gt;I have explored&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Doclin&lt;/li&gt;\n&lt;li&gt;Marker&lt;/li&gt;\n&lt;li&gt;Pymupdf&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Which one would be best to use in production?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhe2h9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aiwtl",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhe2h9/best_document_parser/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhe2h9/best_document_parser/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754315591,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In the era of LLM, however it is still hard for llm to synthetize the specific domain data. I found the diversity is very terrible for these generation data from LLM. Through reading some paper, I mean a self-play data generation agent may be useful. It need a data generation block and  a self-evaluation block. However, how to design the architecture? what should do for each block in this architecture? How to design the prompt for each block?",
          "author_fullname": "t2_1uydkhxx4z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "how to do a self-play data generation system",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhwb9g",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754358283,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the era of LLM, however it is still hard for llm to synthetize the specific domain data. I found the diversity is very terrible for these generation data from LLM. Through reading some paper, I mean a self-play data generation agent may be useful. It need a data generation block and  a self-evaluation block. However, how to design the architecture? what should do for each block in this architecture? How to design the prompt for each block?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhwb9g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tangbasky",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhwb9g/how_to_do_a_selfplay_data_generation_system/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhwb9g/how_to_do_a_selfplay_data_generation_system/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754358283,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "is this normal? what's happening?",
          "author_fullname": "t2_10ht66",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tried Mistral-Small3.1-24B-Instruct with Open-WebUI and got this",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhprfk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/q6BWsH-KvJTpvwlyck_74vRNRFaH4daHSQc5f90f3sA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754341536,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;is this normal? what&amp;#39;s happening?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/pbw43rp0i2hf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/pbw43rp0i2hf1.png?auto=webp&amp;s=52d1741047ef0a7eacc835dc94fc56ff87517dab",
                  "width": 1370,
                  "height": 740
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/pbw43rp0i2hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=637c5a65b67429fd45aa9db51e2e1e85724fd95c",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://preview.redd.it/pbw43rp0i2hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=30e5f3801fdc684c2f52aa184c6a96872f777711",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://preview.redd.it/pbw43rp0i2hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0017a34f4e1c768c1c0ed5b857d4ed080cb6149e",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://preview.redd.it/pbw43rp0i2hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dec9bddad59cf0ab62244b5cd3b50ce24b0b0606",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://preview.redd.it/pbw43rp0i2hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=56206801dbad385c1997e40bd4bea26c2583798b",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://preview.redd.it/pbw43rp0i2hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=22026bb62a35e0dfd07cd2bd645185074b1f18e6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "XFVhJi6M0vTP6FPybQEHVyZ6TDvWBudwA77SB9h0H50"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhprfk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Juanouo",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhprfk/tried_mistralsmall3124binstruct_with_openwebui/",
          "stickied": false,
          "url": "https://i.redd.it/pbw43rp0i2hf1.png",
          "subreddit_subscribers": 510540,
          "created_utc": 1754341536,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What's the best approach for this? Tried it in open webui with ollama backend but it's too slow.\n\nAll docs are pdf, all done with ocr so it's all just text. Ingestion to knowledgebase is the blocker.\n\nAnybody done this and what was the best approach for you?",
          "author_fullname": "t2_mu8eykc30",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RAG with 30k documents, some with 300 pages each.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mha1g1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754328080,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754304270,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the best approach for this? Tried it in open webui with ollama backend but it&amp;#39;s too slow.&lt;/p&gt;\n\n&lt;p&gt;All docs are pdf, all done with ocr so it&amp;#39;s all just text. Ingestion to knowledgebase is the blocker.&lt;/p&gt;\n\n&lt;p&gt;Anybody done this and what was the best approach for you?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mha1g1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dennisitnet",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mha1g1/rag_with_30k_documents_some_with_300_pages_each/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mha1g1/rag_with_30k_documents_some_with_300_pages_each/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754304270,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;format=png&amp;auto=webp&amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a\n\nHorizon Beta is OpenAI",
          "author_fullname": "t2_h0z59zgo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta is OpenAI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 40,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "m30x07j54vgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 31,
                  "x": 108,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=588d4a9c04c05e66d8b3c8ba8b93d7c827f14b20"
                },
                {
                  "y": 62,
                  "x": 216,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=446e608e2e3bf06827203dc313f98d8ec1704b9d"
                },
                {
                  "y": 92,
                  "x": 320,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f5cab485a7892fcf45b7a6bf1ecfa12925d59cf"
                },
                {
                  "y": 184,
                  "x": 640,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fda4acd30425eae2bddb9d9f82ce6241eaf9641"
                },
                {
                  "y": 276,
                  "x": 960,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e8f560871bebab02cfbd9ae3af3f79ac6d3819a"
                },
                {
                  "y": 310,
                  "x": 1080,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b6c803879173681737db389051fa4d599d2f19a0"
                }
              ],
              "s": {
                "y": 364,
                "x": 1266,
                "u": "https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;format=png&amp;auto=webp&amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a"
              },
              "id": "m30x07j54vgf1"
            }
          },
          "name": "t3_1mgtboa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 177,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 177,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/inyC6dLBuynY6QZPK34zrtaIVdVEKly7ofVVWlBmYW4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754252212,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a\"&gt;https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Horizon Beta is OpenAI&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgtboa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MiddleLobster9191",
          "discussion_type": null,
          "num_comments": 68,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754252212,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys!\n\nSo recently as a learning exercise I tuned a Qwen3 model for coding task. I was now interested in understanding how to properly benchmark these tunes models using wellknown benchmarks. But, I'm a bit unsure about how this is exactly done, and was curious about how this is typically done in the industry.\n\nDo each of these big tech companies usually have their own internal benchmarking frameworks/strategies? Or are there popular tools or frameworks that are widely used across both the community and in industry? Since I'm a bit new to the field would like to know what you guys think, what you've used and seen during your learning, etc. Thanks a lot!! :))",
          "author_fullname": "t2_1uzbjk5ofl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do people in industry benchmark models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhieis",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754325391,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys!&lt;/p&gt;\n\n&lt;p&gt;So recently as a learning exercise I tuned a Qwen3 model for coding task. I was now interested in understanding how to properly benchmark these tunes models using wellknown benchmarks. But, I&amp;#39;m a bit unsure about how this is exactly done, and was curious about how this is typically done in the industry.&lt;/p&gt;\n\n&lt;p&gt;Do each of these big tech companies usually have their own internal benchmarking frameworks/strategies? Or are there popular tools or frameworks that are widely used across both the community and in industry? Since I&amp;#39;m a bit new to the field would like to know what you guys think, what you&amp;#39;ve used and seen during your learning, etc. Thanks a lot!! :))&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhieis",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Spiritual_Process575",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhieis/how_do_people_in_industry_benchmark_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhieis/how_do_people_in_industry_benchmark_models/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754325391,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Finally got to finish a weekend project from a couple of months ago. \n\nThis is a small extension that can use a local LLM (any OpenAI-compatible endpoint is supported) to neutralise the clickbaits on the webpages you visit. It works reasonably well with models of Llama 3.2 3B class and above. Works in Chrome and Firefox (you can also install to Edge manually).\n\nFull source and configuration guide is on GitHub: [https://github.com/av/unhype](https://github.com/av/unhype) ",
          "author_fullname": "t2_o7p5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Use local LLM to neutralise the headers on the web",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 89,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgkiti",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bd9e9e",
          "ups": 501,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/niaha18uctgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1688,
              "scrubber_media_url": "https://v.redd.it/niaha18uctgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/niaha18uctgf1/DASHPlaylist.mpd?a=1756989093%2CYmFmOWNkYTg1OWI4NjE3MWY4YmZmMjFkZGM1MjM3MmNmYTc4YTEwMDMzOWJhNTcxZmIwOGU1OTZmN2YxN2ZkYQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 30,
              "hls_url": "https://v.redd.it/niaha18uctgf1/HLSPlaylist.m3u8?a=1756989093%2CZjI3YmNhMjEyMzljZjNjZmFmNjlkOGM3MDdkOTZlNWZjOGY2ZDgyNmZhMjAxMTE4NmNkYjAzN2M5YzJlNDg0OQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 501,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=140&amp;height=89&amp;crop=140:89,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=f692b7d05bb54cb77646e5499a279385018af33e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Alpaca"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754230985,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Finally got to finish a weekend project from a couple of months ago. &lt;/p&gt;\n\n&lt;p&gt;This is a small extension that can use a local LLM (any OpenAI-compatible endpoint is supported) to neutralise the clickbaits on the webpages you visit. It works reasonably well with models of Llama 3.2 3B class and above. Works in Chrome and Firefox (you can also install to Edge manually).&lt;/p&gt;\n\n&lt;p&gt;Full source and configuration guide is on GitHub: &lt;a href=\"https://github.com/av/unhype\"&gt;https://github.com/av/unhype&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/niaha18uctgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?format=pjpg&amp;auto=webp&amp;s=e3f711ff053fccc48764a295b7ef68533d8f7f9a",
                  "width": 1688,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=47b5550c7dd6476d0d3e442d2ca5c2a382f74e92",
                    "width": 108,
                    "height": 69
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8828ff7d3eb9b270e88f2ec7ff4973818f896e8e",
                    "width": 216,
                    "height": 138
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bc01544a8c25b4174c98c4519803a567aa98c887",
                    "width": 320,
                    "height": 204
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9c9498bdb1588d50d25b53d82635a1092a82f4ef",
                    "width": 640,
                    "height": 409
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=402ac5eba7ed08a00bc9e012b6a4bd69a46e4608",
                    "width": 960,
                    "height": 614
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=374d1819675e3e0f60cf4790383435325e543d5f",
                    "width": 1080,
                    "height": 690
                  }
                ],
                "variants": {},
                "id": "NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Alpaca",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgkiti",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Everlier",
          "discussion_type": null,
          "num_comments": 70,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mgkiti/use_local_llm_to_neutralise_the_headers_on_the_web/",
          "stickied": false,
          "url": "https://v.redd.it/niaha18uctgf1",
          "subreddit_subscribers": 510540,
          "created_utc": 1754230985,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/niaha18uctgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1688,
              "scrubber_media_url": "https://v.redd.it/niaha18uctgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/niaha18uctgf1/DASHPlaylist.mpd?a=1756989093%2CYmFmOWNkYTg1OWI4NjE3MWY4YmZmMjFkZGM1MjM3MmNmYTc4YTEwMDMzOWJhNTcxZmIwOGU1OTZmN2YxN2ZkYQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 30,
              "hls_url": "https://v.redd.it/niaha18uctgf1/HLSPlaylist.m3u8?a=1756989093%2CZjI3YmNhMjEyMzljZjNjZmFmNjlkOGM3MDdkOTZlNWZjOGY2ZDgyNmZhMjAxMTE4NmNkYjAzN2M5YzJlNDg0OQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Its listed at 0 Cost , but all my chats with have incurred a charge  . Any one else facing the same issues  ? Is it normal, i am new to this , am i missing something obvious here\n\n[Charged for chat usage](https://preview.redd.it/5kjvnxyla2hf1.png?width=1352&amp;format=png&amp;auto=webp&amp;s=c5ff5aab4d6b7e2bfc49efcb17717f9b7e429058)\n\n[Zero Price Shown](https://preview.redd.it/iahlrp1ha2hf1.png?width=1308&amp;format=png&amp;auto=webp&amp;s=cea90795f67ec7a06d376b3952551496d287f7b7)",
          "author_fullname": "t2_1hssure7vp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta Free or not on Openrouter",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 31,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "iahlrp1ha2hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 37,
                  "x": 108,
                  "u": "https://preview.redd.it/iahlrp1ha2hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d51017ab88a27516cf2dcc2084dd46cccbc24a1b"
                },
                {
                  "y": 75,
                  "x": 216,
                  "u": "https://preview.redd.it/iahlrp1ha2hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f237dd54d71dc0acf955b6e923be85eeebf41d56"
                },
                {
                  "y": 111,
                  "x": 320,
                  "u": "https://preview.redd.it/iahlrp1ha2hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f70b9d422c2a6c33a3b9c773c19f43ab80392597"
                },
                {
                  "y": 223,
                  "x": 640,
                  "u": "https://preview.redd.it/iahlrp1ha2hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dc55f6b3073f641f9013a9c91a84bfa03b1eee3e"
                },
                {
                  "y": 335,
                  "x": 960,
                  "u": "https://preview.redd.it/iahlrp1ha2hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad913e55bef2644d69fdfe5d25fb35fdd933c6f0"
                },
                {
                  "y": 377,
                  "x": 1080,
                  "u": "https://preview.redd.it/iahlrp1ha2hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=74533f2ab0814df19511eda1702d88530dcea25b"
                }
              ],
              "s": {
                "y": 457,
                "x": 1308,
                "u": "https://preview.redd.it/iahlrp1ha2hf1.png?width=1308&amp;format=png&amp;auto=webp&amp;s=cea90795f67ec7a06d376b3952551496d287f7b7"
              },
              "id": "iahlrp1ha2hf1"
            },
            "5kjvnxyla2hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 24,
                  "x": 108,
                  "u": "https://preview.redd.it/5kjvnxyla2hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=96fd790a4623b3276477b4f524ead18e4b6951a3"
                },
                {
                  "y": 49,
                  "x": 216,
                  "u": "https://preview.redd.it/5kjvnxyla2hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7143cd80ade2b4bd45933fce80454d84032fe256"
                },
                {
                  "y": 72,
                  "x": 320,
                  "u": "https://preview.redd.it/5kjvnxyla2hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1efc7a6227fe3b506499af729e91bffb9d7bc401"
                },
                {
                  "y": 145,
                  "x": 640,
                  "u": "https://preview.redd.it/5kjvnxyla2hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e9584be7ca124f495764a2075a44e7aee64a2b82"
                },
                {
                  "y": 218,
                  "x": 960,
                  "u": "https://preview.redd.it/5kjvnxyla2hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ddce746dfb93dd097db757b3921cf2df168372b2"
                },
                {
                  "y": 246,
                  "x": 1080,
                  "u": "https://preview.redd.it/5kjvnxyla2hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=57de2844d6c5980c4e84a70d7864b6f94eea94b1"
                }
              ],
              "s": {
                "y": 308,
                "x": 1352,
                "u": "https://preview.redd.it/5kjvnxyla2hf1.png?width=1352&amp;format=png&amp;auto=webp&amp;s=c5ff5aab4d6b7e2bfc49efcb17717f9b7e429058"
              },
              "id": "5kjvnxyla2hf1"
            }
          },
          "name": "t3_1mhok5i",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/-4sl9nlNlThRhCXHlpD0quR49YKqs0-ZQWK2eZGeDNs.jpg",
          "edited": 1754339031,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754338835,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Its listed at 0 Cost , but all my chats with have incurred a charge  . Any one else facing the same issues  ? Is it normal, i am new to this , am i missing something obvious here&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5kjvnxyla2hf1.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5ff5aab4d6b7e2bfc49efcb17717f9b7e429058\"&gt;Charged for chat usage&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/iahlrp1ha2hf1.png?width=1308&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cea90795f67ec7a06d376b3952551496d287f7b7\"&gt;Zero Price Shown&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhok5i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Training-Surround228",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhok5i/horizon_beta_free_or_not_on_openrouter/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhok5i/horizon_beta_free_or_not_on_openrouter/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754338835,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Qwen3 32b on a 3090Ti = 38tps\n\nI was expecting more? Like at least 50tps and more like 60? Am I tripping?\n\n    C:\\&gt;llama-bench.exe -m Qwen_Qwen3-32B-GGUF\\Qwen_Qwen3-32B-Q4_K_L.gguf --flash-attn 1\n    ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no\n    ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n    ggml_cuda_init: found 1 CUDA devices:\n    Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes\n    \n    | model | size | params | backend | ngl | fa | test | t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n    | qwen3 32B Q4_K - Medium | 18.94 GiB | 32.76 B | CUDA,RPC | 99 | 1 | pp512 | 1442.69 ± 17.38 |\n    | qwen3 32B Q4_K - Medium | 18.94 GiB | 32.76 B | CUDA,RPC | 99 | 1 | tg128 | 38.48 ± 0.06 |\n    build: 5aa1105d (6082)",
          "author_fullname": "t2_by77ogdhr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "3090Ti - 38 tokens/sec?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhhpy9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754332894,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754323876,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3 32b on a 3090Ti = 38tps&lt;/p&gt;\n\n&lt;p&gt;I was expecting more? Like at least 50tps and more like 60? Am I tripping?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;C:\\&amp;gt;llama-bench.exe -m Qwen_Qwen3-32B-GGUF\\Qwen_Qwen3-32B-Q4_K_L.gguf --flash-attn 1\nggml_cuda_init: GGML_CUDA_FORCE_MMQ: no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\nDevice 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes\n\n| model | size | params | backend | ngl | fa | test | t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n| qwen3 32B Q4_K - Medium | 18.94 GiB | 32.76 B | CUDA,RPC | 99 | 1 | pp512 | 1442.69 ± 17.38 |\n| qwen3 32B Q4_K - Medium | 18.94 GiB | 32.76 B | CUDA,RPC | 99 | 1 | tg128 | 38.48 ± 0.06 |\nbuild: 5aa1105d (6082)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhhpy9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Secure_Reflection409",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhhpy9/3090ti_38_tokenssec/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhhpy9/3090ti_38_tokenssec/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754323876,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If there was a dataset or pages from Wikipedia that is in .xml format, what do you use to change into an alpaca format like .json?",
          "author_fullname": "t2_1urq336nt3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I convert a .xml file to a .json file to train my LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mi0wkg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754372269,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If there was a dataset or pages from Wikipedia that is in .xml format, what do you use to change into an alpaca format like .json?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mi0wkg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vulgar1171",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mi0wkg/how_do_i_convert_a_xml_file_to_a_json_file_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi0wkg/how_do_i_convert_a_xml_file_to_a_json_file_to/",
          "subreddit_subscribers": 510540,
          "created_utc": 1754372269,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}