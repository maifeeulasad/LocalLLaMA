{
  "kind": "Listing",
  "data": {
    "after": "t3_1lw6u69",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone, good news for AMD GPU users! It seems AMD is getting serious about boosting support for their graphics cards in llama.cpp\n\nWord is, someone from AMD dropped a pull request to tweak the code, aimed at adapting the project for use with AMD graphics cards.   \nDiscussions with the project leaders are planned in the near future to explore opportunities for further enhancements.  \n[https://github.com/ggml-org/llama.cpp/pull/14624](https://github.com/ggml-org/llama.cpp/pull/14624)",
          "author_fullname": "t2_i7v1u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD's Pull Request for llama.cpp: Enhancing GPU Support",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwta86",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 214,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 214,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752194746,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, good news for AMD GPU users! It seems AMD is getting serious about boosting support for their graphics cards in llama.cpp&lt;/p&gt;\n\n&lt;p&gt;Word is, someone from AMD dropped a pull request to tweak the code, aimed at adapting the project for use with AMD graphics cards.&lt;br/&gt;\nDiscussions with the project leaders are planned in the near future to explore opportunities for further enhancements.&lt;br/&gt;\n&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14624\"&gt;https://github.com/ggml-org/llama.cpp/pull/14624&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?auto=webp&amp;s=7e2a03c13fb4ce4c8558a9462d8d8db18654140b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4049b21c0ac9b7c3089ec2e3df2e59d8659989da",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=64c95decda4b1ae5bbb895753cfeceaa35e24a90",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa643aece72bc03f667bdb9cc869c4aa0b4e21c6",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f1dc1a1002471d9dfa784ab140da65e1281030d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8339bbe8f014bcbb3649aaeea0714cc8ef76827c",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d6cedcf5e6a230d8742e6b27dc0eda7e78cf0f16",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwta86",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rrraptr",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752194746,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Whereas prior generations of Granite LLMs utilized a conventional transformer architecture, all models in the Granite 4.0 family utilize a new **hybrid Mamba-2/Transformer architecture,** marrying the speed and efficiency of Mamba with the precision of transformer-based self-attention. \n\nGranite 4.0 Tiny-Preview, specifically, is a **fine-grained hybrid** [**mixture of experts (MoE)**](https://www.ibm.com/think/topics/mixture-of-experts) **model,** with 7B total parameters and only 1B active parameters at inference time.\n\n[https://huggingface.co/ibm-granite/granite-4.0-tiny-preview](https://huggingface.co/ibm-granite/granite-4.0-tiny-preview)\n\n[https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview](https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview)\n\n[https://huggingface.co/ibm-ai-platform/Bamba-9B-v1](https://huggingface.co/ibm-ai-platform/Bamba-9B-v1)\n\n",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Support for the upcoming IBM Granite 4.0 has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwsrx7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "#bbbdbf",
          "ups": 101,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 101,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=58c542b2095a081e5dc71e3f0c3caf3b8d97bf2d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752193250,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whereas prior generations of Granite LLMs utilized a conventional transformer architecture, all models in the Granite 4.0 family utilize a new &lt;strong&gt;hybrid Mamba-2/Transformer architecture,&lt;/strong&gt; marrying the speed and efficiency of Mamba with the precision of transformer-based self-attention. &lt;/p&gt;\n\n&lt;p&gt;Granite 4.0 Tiny-Preview, specifically, is a &lt;strong&gt;fine-grained hybrid&lt;/strong&gt; &lt;a href=\"https://www.ibm.com/think/topics/mixture-of-experts\"&gt;&lt;strong&gt;mixture of experts (MoE)&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;model,&lt;/strong&gt; with 7B total parameters and only 1B active parameters at inference time.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/ibm-granite/granite-4.0-tiny-preview\"&gt;https://huggingface.co/ibm-granite/granite-4.0-tiny-preview&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview\"&gt;https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/ibm-ai-platform/Bamba-9B-v1\"&gt;https://huggingface.co/ibm-ai-platform/Bamba-9B-v1&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/13550",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?auto=webp&amp;s=599f611a372fbcdb39cff08d5b5708383ec3485e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d9dfc6caf6473cddc930c8672b2473ef6a39f9d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e20af04b4cab9b55eb315b230cb214fefe9b821d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c441275b8459e7c9069b5c09999881a7a3cca8bd",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=be85c7d84d9bd88e720aef5b6d229ca49e85ef9a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=17ffa9221de2bb975a61c193b0b654fb03fcf6b5",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c85dbce218541b4a90e6c34de0e348286caf478d",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lwsrx7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/13550",
          "subreddit_subscribers": 497353,
          "created_utc": 1752193250,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The Creators of Earnie just published a new quantization algorithm that compress Ernie-300B to 85GB and Deepseek-V3 to 184 GB, with minimal (&lt;2%) performance degradation in benchmarks. Paper here: [https://arxiv.org/pdf/2507.07145](https://arxiv.org/pdf/2507.07145)\n\n",
          "author_fullname": "t2_g177e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "2-bit Quant: CCQ, Convolutional Code for Extreme Low-bit Quantization in LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwx50s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bd9e9e",
          "subreddit_type": "public",
          "ups": 36,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 36,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Alpaca"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752206258,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Creators of Earnie just published a new quantization algorithm that compress Ernie-300B to 85GB and Deepseek-V3 to 184 GB, with minimal (&amp;lt;2%) performance degradation in benchmarks. Paper here: &lt;a href=\"https://arxiv.org/pdf/2507.07145\"&gt;https://arxiv.org/pdf/2507.07145&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Alpaca",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwx50s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ortegaalfredo",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752206258,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B](https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B)",
          "author_fullname": "t2_21gfv9kq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The New Nvidia Model is Really Chatty",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwl9ai",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 175,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/8bnc2od6i3cf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/8bnc2od6i3cf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/8bnc2od6i3cf1/DASHPlaylist.mpd?a=1754812733%2CZTAzMzk1MDJmM2EzMmQ1OGIzYjcyYTcwNjUyZjhhNmZkNDJiYzlkNzFhZGY1MmZiMGFiNjVmNjNmMTQxMGVjYw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 24,
              "hls_url": "https://v.redd.it/8bnc2od6i3cf1/HLSPlaylist.m3u8?a=1754812733%2CZjk3NTRhZjdiNjBiZGQ4MGZhNzdhYmJkZTNiNDA4OTZmM2RjMmRlZDhlYjE4YWNlYWY5MjcwYWVkMDkwMzQ2Zg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 175,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=50e25d3cdefacfdb52a19c3f3a73de8bb21ce0fe",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752174469,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B\"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/8bnc2od6i3cf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?format=pjpg&amp;auto=webp&amp;s=f7fe1fbe2bbf6cb35381d40fd5c4d9ffa87ee25c",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=44e66a13145d153d9d61c0c47cb1211182cf6bb4",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=458974266980903b8e094c9c4ea463e90f2a1f60",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=48aca6fabb03f9890200e56d65734ebccc58eb7f",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c8d5f1f78f383053dd71459b11bfc8680ef3571b",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=293ca88332e9cedef3647f159cafa6d6e23d0d10",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c2c58decc87168c504dfb2951ff10a337d22ec56",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lwl9ai",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SpyderJack",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwl9ai/the_new_nvidia_model_is_really_chatty/",
          "stickied": false,
          "url": "https://v.redd.it/8bnc2od6i3cf1",
          "subreddit_subscribers": 497353,
          "created_utc": 1752174469,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/8bnc2od6i3cf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/8bnc2od6i3cf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/8bnc2od6i3cf1/DASHPlaylist.mpd?a=1754812733%2CZTAzMzk1MDJmM2EzMmQ1OGIzYjcyYTcwNjUyZjhhNmZkNDJiYzlkNzFhZGY1MmZiMGFiNjVmNjNmMTQxMGVjYw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 24,
              "hls_url": "https://v.redd.it/8bnc2od6i3cf1/HLSPlaylist.m3u8?a=1754812733%2CZjk3NTRhZjdiNjBiZGQ4MGZhNzdhYmJkZTNiNDA4OTZmM2RjMmRlZDhlYjE4YWNlYWY5MjcwYWVkMDkwMzQ2Zg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1162lx9rgr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "mistralai/Devstral-Small-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwe5y8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#ab96c2",
          "ups": 391,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d40ca12a-0e73-11ee-8563-f216e082168e",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 391,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=c63ef6cf39d328392932c6db5dd51a45f12fb26e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 2"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752157759,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/mistralai/Devstral-Small-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?auto=webp&amp;s=decfdf8365e20616b3021280253b6b910ea41fcf",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f04a531a9dfe8f1024433fe94c145846144b089b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=65c06acf21401f8c0323339e94227ba2948590c6",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=677035b0e8f6dea7569670a08409cd8abbadf838",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=299e4f7a5df68d789749c7d30f346b534a08b8ba",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4edd4a83f17a895ce6231f54e721e098610b6952",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f80413cc943fa2d192927b6a8c1f8031a81dce0f",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 2",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lwe5y8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yoracale",
          "discussion_type": null,
          "num_comments": 106,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lwe5y8/mistralaidevstralsmall2507/",
          "stickied": false,
          "url": "https://huggingface.co/mistralai/Devstral-Small-2507",
          "subreddit_subscribers": 497353,
          "created_utc": 1752157759,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt; Granite-speech-3.3-8b is a compact and efficient speech-language model, specifically designed for automatic speech recognition (ASR) and automatic speech translation (AST). Granite-speech-3.3-8b uses a two-pass design, unlike integrated models that combine speech and language into a single pass. Initial calls to granite-speech-3.3-8b will transcribe audio files into text. To process the transcribed text using the underlying Granite language model, users must make a second call as each step must be explicitly initiated.",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Granite-speech-3.3-8b",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lwztnp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=053d27c16b7a3703e6c44e7947dee7416b794087",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752215624,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Granite-speech-3.3-8b is a compact and efficient speech-language model, specifically designed for automatic speech recognition (ASR) and automatic speech translation (AST). Granite-speech-3.3-8b uses a two-pass design, unlike integrated models that combine speech and language into a single pass. Initial calls to granite-speech-3.3-8b will transcribe audio files into text. To process the transcribed text using the underlying Granite language model, users must make a second call as each step must be explicitly initiated.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/ibm-granite/granite-speech-3.3-8b",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?auto=webp&amp;s=6a3208c0a6f02901382bbd3492727a406bc355d7",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b597cda2512d75e467a9d18009ec6b56f088c226",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d6ca5c778f9d46e7644bcaf275e25c54ed791e4b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d1466b35cfdeaf83843dfff5de0fd2b2fd99cce5",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3a3cfa1633e9a330cab59c33f8413530288842b0",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8f1cb95f3fb7fc5d831f7164e052e20bad2b6c7b",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2830c5eb6c21433c267b36769a91e9f7e7de0cac",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lwztnp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwztnp/granitespeech338b/",
          "stickied": false,
          "url": "https://huggingface.co/ibm-granite/granite-speech-3.3-8b",
          "subreddit_subscribers": 497353,
          "created_utc": 1752215624,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone, Reka just open-sourced a new quantisation method which looks promising for local inference and VRAM-limited setups.\n\nAccording to their benchmarks, the new method significantly outperforms llama.cpp's standard Q3_K_S, narrowing the performance gap with Q4_K_M or higher quants. This could be great news for the local inference community.\n\nWhat are your thoughts on this new method?\n\n- Blog Post: [Reka Quantization Technology](https://reka.ai/news/reka-quantization-technology)  \n- Source Code: [GitHub](https://github.com/reka-ai/rekaquant) \n- Quantised Model: [reka-flash-3.1-rekaquant-q3_k_s](https://huggingface.co/RekaAI/reka-flash-3.1-rekaquant-q3_k_s)",
          "author_fullname": "t2_15dxzs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Reka Flash 3.1 benchmarks show strong progress in LLM quantisation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "6fmj9xb0f3cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 149,
                  "x": 108,
                  "u": "https://preview.redd.it/6fmj9xb0f3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d37cab47fe2b3167bc53103b90c338816e15de0d"
                },
                {
                  "y": 298,
                  "x": 216,
                  "u": "https://preview.redd.it/6fmj9xb0f3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2cf1a5cc9422bc6023f07a5ee540837948d71f6a"
                },
                {
                  "y": 442,
                  "x": 320,
                  "u": "https://preview.redd.it/6fmj9xb0f3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d839370f16205a62e18ab63a2cf94d3f6147cb81"
                },
                {
                  "y": 885,
                  "x": 640,
                  "u": "https://preview.redd.it/6fmj9xb0f3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4dfeb34ba2915d43ccf36d67f8c49ae0b7f55d8"
                },
                {
                  "y": 1328,
                  "x": 960,
                  "u": "https://preview.redd.it/6fmj9xb0f3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=462a693ebabbc4781fed4f885c72dbf8609dc4ba"
                }
              ],
              "s": {
                "y": 1384,
                "x": 1000,
                "u": "https://preview.redd.it/6fmj9xb0f3cf1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=2d006b859958e34c2bb2c62f83c43a20d225b16d"
              },
              "id": "6fmj9xb0f3cf1"
            }
          },
          "name": "t3_1lwkrg4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 95,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 95,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=6ee9c0ff9008ca4bef6848d1f294880cec91b504",
          "edited": 1752173696,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752173307,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, Reka just open-sourced a new quantisation method which looks promising for local inference and VRAM-limited setups.&lt;/p&gt;\n\n&lt;p&gt;According to their benchmarks, the new method significantly outperforms llama.cpp&amp;#39;s standard Q3_K_S, narrowing the performance gap with Q4_K_M or higher quants. This could be great news for the local inference community.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on this new method?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Blog Post: &lt;a href=\"https://reka.ai/news/reka-quantization-technology\"&gt;Reka Quantization Technology&lt;/a&gt;&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Source Code: &lt;a href=\"https://github.com/reka-ai/rekaquant\"&gt;GitHub&lt;/a&gt; &lt;/li&gt;\n&lt;li&gt;Quantised Model: &lt;a href=\"https://huggingface.co/RekaAI/reka-flash-3.1-rekaquant-q3_k_s\"&gt;reka-flash-3.1-rekaquant-q3_k_s&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?auto=webp&amp;s=e91d4b8daf73181934b280324eae98b2b6a23205",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=789320b03d84c0e8a0e0035cd6e312b24ffd1166",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bce1ce16c6e71b4e054c184c1a68566e5fe9ad9d",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d045817fb3786098058c612f14c54d5c59bb9d9",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1933f58a3a9b626d8fe0fc66ca8cbd276198d9d8",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=75350929166b4a401f3f2c927023d7707dde1a5a",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6734799977ec9496e4557c0d1c927b5337f4cff4",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwkrg4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "benja0x40",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwkrg4/reka_flash_31_benchmarks_show_strong_progress_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwkrg4/reka_flash_31_benchmarks_show_strong_progress_in/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752173307,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7pfgfkis",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Grok 4 on Fiction.liveBench Long Context Comprehension",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwn3ut",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "ups": 66,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 66,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/O1GNGtr4DIDHp9RkCuk3_-GgG-OAjpoDKNEauoT6i-A.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752178868,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/rzwo8emcv3cf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/rzwo8emcv3cf1.png?auto=webp&amp;s=c51c914943cfb77ccb73689f4d460162925d4a03",
                  "width": 1704,
                  "height": 2486
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/rzwo8emcv3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=00960dbf00b92ec8a09e74a1754bc8c7439cff4c",
                    "width": 108,
                    "height": 157
                  },
                  {
                    "url": "https://preview.redd.it/rzwo8emcv3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b89c1e8c1e3b63f98479b37d9cdd9cad4824355",
                    "width": 216,
                    "height": 315
                  },
                  {
                    "url": "https://preview.redd.it/rzwo8emcv3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=714ae0eced0e2b444937f0c1272ce3dfa8b2df0c",
                    "width": 320,
                    "height": 466
                  },
                  {
                    "url": "https://preview.redd.it/rzwo8emcv3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=87eea0d411941e119cea7f8020ad7157923da79a",
                    "width": 640,
                    "height": 933
                  },
                  {
                    "url": "https://preview.redd.it/rzwo8emcv3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=58cf39362c4b8b76fd9d44ce4da6642118fd0051",
                    "width": 960,
                    "height": 1400
                  },
                  {
                    "url": "https://preview.redd.it/rzwo8emcv3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a0881f3b0ae158c64ec03b0235b354a266f5f0f",
                    "width": 1080,
                    "height": 1575
                  }
                ],
                "variants": {},
                "id": "5NEN-_gSilO1hhg4KWRyfkPXBZDPAN0Y9f7M4g7ixgw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lwn3ut",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fictionlive",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwn3ut/grok_4_on_fictionlivebench_long_context/",
          "stickied": false,
          "url": "https://i.redd.it/rzwo8emcv3cf1.png",
          "subreddit_subscribers": 497353,
          "created_utc": 1752178868,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi! I'm compiling a list of document parsers available on the market and testing their feature coverage. \n\nSo far, I've tested 14 OCRs/parsers for tables, equations, handwriting, two-column layouts, and multiple-column layouts. You can view the outputs from each parser in the \\`results\\` folder. The ones I've tested are mostly open source or with generous free quota.\n\n🚩 Coming soon: benchmarks for each OCR - score from 0 (doesn't work) to 5 (perfect)\n\nFeedback &amp; contribution are welcome!",
          "author_fullname": "t2_bb96oc9v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I'm curating a list of every OCR out there and running tests on their features. Contribution welcome!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwgohu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 142,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 142,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=50783bab933a2e12113a00ee8a3801748c9411b9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752163778,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I&amp;#39;m compiling a list of document parsers available on the market and testing their feature coverage. &lt;/p&gt;\n\n&lt;p&gt;So far, I&amp;#39;ve tested 14 OCRs/parsers for tables, equations, handwriting, two-column layouts, and multiple-column layouts. You can view the outputs from each parser in the `results` folder. The ones I&amp;#39;ve tested are mostly open source or with generous free quota.&lt;/p&gt;\n\n&lt;p&gt;🚩 Coming soon: benchmarks for each OCR - score from 0 (doesn&amp;#39;t work) to 5 (perfect)&lt;/p&gt;\n\n&lt;p&gt;Feedback &amp;amp; contribution are welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/GiftMungmeeprued/document-parsers-list",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?auto=webp&amp;s=55f6f74a365dbd25b015f7179727d7c7f96094a6",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=441cb0387ac300bda66840c0aabf6acfd239e0c4",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d5e3f11dbbf36dc0fcaee2c4608c5a7b3323a4a6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ddb31672cb824bc055562606a31ed21c689e7b5a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=845ce9e8cd2f0eec39ba036028db301cc1226bab",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e11f548f6fd8ccb9943603db747b29acdad91bb",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=84284766869437005589220b274c461043b79652",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwgohu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Help9178",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwgohu/im_curating_a_list_of_every_ocr_out_there_and/",
          "stickied": false,
          "url": "https://github.com/GiftMungmeeprued/document-parsers-list",
          "subreddit_subscribers": 497353,
          "created_utc": 1752163778,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We just hit [15K users](https://www.designarena.ai/)! For context of course, see [this post](https://www.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/). Since then, we have added Grok 4, several Devstral Small, Devstral Medium, Gemini 2.5 Flash, and Qwen-235B-A22B. \n\nWe now thankfully have more access to various kind of models (particularly OS and open weight) thanks to [Fireworks AI](https://app.fireworks.ai/account/home) and we'll be periodically adding more models throughout the weekend. \n\nWhich models would you like to see added to the leaderboard? We're looking to add as many as possible. ",
          "author_fullname": "t2_c3b3edv5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "What other models would you like to see on Design Arena?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 54,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "u37y8ocla6cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 42,
                  "x": 108,
                  "u": "https://preview.redd.it/u37y8ocla6cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=603d7c8a5b45893503082927fd92b8fc691da07d"
                },
                {
                  "y": 84,
                  "x": 216,
                  "u": "https://preview.redd.it/u37y8ocla6cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d45720f8bd30c8f377cefa5c0b72436a8caf7e33"
                },
                {
                  "y": 124,
                  "x": 320,
                  "u": "https://preview.redd.it/u37y8ocla6cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17f4a35166d50e91b498f5a82c907ed970138884"
                },
                {
                  "y": 249,
                  "x": 640,
                  "u": "https://preview.redd.it/u37y8ocla6cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=24a387c6b022593520eefe83359fc4c1a87ea68b"
                },
                {
                  "y": 374,
                  "x": 960,
                  "u": "https://preview.redd.it/u37y8ocla6cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=551687536b6a08f931b5b6de8cb223432a0e9142"
                },
                {
                  "y": 421,
                  "x": 1080,
                  "u": "https://preview.redd.it/u37y8ocla6cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9cdb462bc79223b557cb59c6866cdf353a016c57"
                }
              ],
              "s": {
                "y": 1144,
                "x": 2932,
                "u": "https://preview.redd.it/u37y8ocla6cf1.png?width=2932&amp;format=png&amp;auto=webp&amp;s=321e6c9d757e587d730e3b63a07c0de4b34c1b6b"
              },
              "id": "u37y8ocla6cf1"
            },
            "qhfls1vma6cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/qhfls1vma6cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=586646b880b662d777d4b83169d07f463ccdf102"
                },
                {
                  "y": 129,
                  "x": 216,
                  "u": "https://preview.redd.it/qhfls1vma6cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=41ef70b20168be55a59584cfa321cd22d09809f1"
                },
                {
                  "y": 191,
                  "x": 320,
                  "u": "https://preview.redd.it/qhfls1vma6cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=57d164f455a45692905803c6386c3cf1e7c15b6a"
                },
                {
                  "y": 383,
                  "x": 640,
                  "u": "https://preview.redd.it/qhfls1vma6cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc1a5c84ab16c3a8fb5c060c0879122464d74c54"
                },
                {
                  "y": 574,
                  "x": 960,
                  "u": "https://preview.redd.it/qhfls1vma6cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3c112b68b731ef17710c79393edc403f99ef2dd1"
                },
                {
                  "y": 646,
                  "x": 1080,
                  "u": "https://preview.redd.it/qhfls1vma6cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5d5c5686a8a6146270a80327e1352070b3971c73"
                }
              ],
              "s": {
                "y": 1300,
                "x": 2172,
                "u": "https://preview.redd.it/qhfls1vma6cf1.png?width=2172&amp;format=png&amp;auto=webp&amp;s=3678317de4b842e0038c3d0fb948bd5300984be4"
              },
              "id": "qhfls1vma6cf1"
            }
          },
          "name": "t3_1lwxr2l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 14,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "u37y8ocla6cf1",
                "id": 703328100
              },
              {
                "media_id": "qhfls1vma6cf1",
                "id": 703328101
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/vyaiGpslyuDvhVMrcw8-eJ5unh7Eixe4i35Y204tnqU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752208226,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We just hit &lt;a href=\"https://www.designarena.ai/\"&gt;15K users&lt;/a&gt;! For context of course, see &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/\"&gt;this post&lt;/a&gt;. Since then, we have added Grok 4, several Devstral Small, Devstral Medium, Gemini 2.5 Flash, and Qwen-235B-A22B. &lt;/p&gt;\n\n&lt;p&gt;We now thankfully have more access to various kind of models (particularly OS and open weight) thanks to &lt;a href=\"https://app.fireworks.ai/account/home\"&gt;Fireworks AI&lt;/a&gt; and we&amp;#39;ll be periodically adding more models throughout the weekend. &lt;/p&gt;\n\n&lt;p&gt;Which models would you like to see added to the leaderboard? We&amp;#39;re looking to add as many as possible. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lwxr2l",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwxr2l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adviceguru25",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwxr2l/what_other_models_would_you_like_to_see_on_design/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lwxr2l",
          "subreddit_subscribers": 497353,
          "created_utc": 1752208226,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi there guys, hope you're having a good day!\n\nAfter latest improvements on ik llamacpp, [https://github.com/ikawrakow/ik\\_llama.cpp/commits/main/](https://github.com/ikawrakow/ik_llama.cpp/commits/main/), I have found that DeepSeek MoE models runs noticeably faster than llamacpp, at the point that I get about half PP t/s and 0.85-0.9X TG t/s vs ikllamacpp. This is the case only for MoE models I'm testing.\n\nMy setup is:\n\n* AMD Ryzen 7 7800X3D\n* 192GB RAM, DDR5 6000Mhz, max bandwidth at about 60-62 GB/s\n* 3 1600W PSUs (Corsair 1600i)\n* AM5 MSI Carbon X670E\n* 5090/5090 at PCIe X8/X8 5.0\n* 4090/4090 at PCIe X4/X4 4.0\n* 3090/3090 at PCIe X4/X4 4.0\n* A6000 at PCIe X4 4.0.\n* Fedora Linux 41 (instead of 42 just because I'm lazy doing some roundabouts to compile with GCC15, waiting until NVIDIA adds support to it)\n* SATA and USB-&gt;M2 Storage\n\nThe benchmarks are based on mostly, R1-0528, BUT it has the same size and it's quants on V3-0324 and TNG-R1T2-Chimera.\n\nI have tested the next models:\n\n* [unsloth](https://huggingface.co/unsloth) DeepSeek Q2\\_K\\_XL:\n   * llm\\_load\\_print\\_meta: model size       = 233.852 GiB (2.994 BPW)\n* [unsloth](https://huggingface.co/unsloth) DeepSeek IQ3\\_XXS:\n   * llm\\_load\\_print\\_meta: model size       = 254.168 GiB (3.254 BPW)\n* [unsloth](https://huggingface.co/unsloth) DeepSeek Q3\\_K\\_XL:\n   * llm\\_load\\_print\\_meta: model size       = 275.576 GiB (3.528 BPW)\n* [ubergarm](https://huggingface.co/ubergarm) DeepSeek IQ3\\_KS:\n   * llm\\_load\\_print\\_meta: model size       = 281.463 GiB (3.598 BPW)\n* [unsloth](https://huggingface.co/unsloth) DeepSeek IQ4\\_XS:\n   * llm\\_load\\_print\\_meta: model size       = 333.130 GiB (4.264 BPW)\n\nEach model may have been tested on different formats. Q2\\_K\\_XL and IQ3\\_XXS has less info, but the rest have a lot more. So here we go!\n\n# unsloth DeepSeek Q2_K_XL\n\nRunning the model with:\n\n    ./llama-server -m '/models_llm/DeepSeek-R1-0528-UD-Q2_K_XL-merged.gguf' \\\n    -c 32768 --no-mmap -ngl 999 \\\n    -ot \"blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0\" \\\n    -ot \"blk.(8|9|10|11).ffn.=CUDA1\" \\\n    -ot \"blk.(12|13|14|15).ffn.=CUDA2\" \\\n    -ot \"blk.(16|17|18|19|20).ffn.=CUDA3\" \\\n    -ot \"blk.(21|22|23|24).ffn.=CUDA4\" \\\n    -ot \"blk.(25|26|27|28).ffn.=CUDA5\" \\\n    -ot \"blk.(29|30|31|32|33|34|35|36|37|38).ffn.=CUDA6\" \\\n    -ot exps=CPU \\\n    -fa -mg 0 -ub 5120 -b 5120 -mla 3 -amb 256 -fmoe\n\nI get:\n\nmain: n\\_kv\\_max = 32768, n\\_batch = 5120, n\\_ubatch = 5120, flash\\_attn = 1, n\\_gpu\\_layers = 999, n\\_threads = 8, n\\_threads\\_batch = 8\n\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  5120 |   1280 |      0 |   12.481 |   410.21 |  104.088 |    12.30 |\n    |  5120 |   1280 |   5120 |   14.630 |   349.98 |  109.724 |    11.67 |\n    |  5120 |   1280 |  10240 |   17.167 |   298.25 |  112.938 |    11.33 |\n    |  5120 |   1280 |  15360 |   20.008 |   255.90 |  119.037 |    10.75 |\n    |  5120 |   1280 |  20480 |   22.444 |   228.12 |  122.706 |    10.43 |\n\n[Perf comparison \\(ignore 4096 as I forgor to save the perf\\)](https://preview.redd.it/r9tt4pktt3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=9e55870952c3069d16bb1a1b211b83b870e87da7)\n\nQ2\\_K\\_XL performs really good for a system like this! And it's performance as LLM is really good as well. I still prefer this above any other local model, for example, even if it's at 3bpw.\n\n# unsloth DeepSeek IQ3_XXS\n\nRunning the model with:\n\n    ./llama-server -m '/models_llm/DeepSeek-R1-0528-UD-IQ3_XXS-merged.gguf' \\\n    -c 32768 --no-mmap -ngl 999 \\\n    -ot \"blk.(0|1|2|3|4|5|6).ffn.=CUDA0\" \\\n    -ot \"blk.(7|8|9|10).ffn.=CUDA1\" \\\n    -ot \"blk.(11|12|13|14).ffn.=CUDA2\" \\\n    -ot \"blk.(15|16|17|18|19).ffn.=CUDA3\" \\\n    -ot \"blk.(20|21|22|23).ffn.=CUDA4\" \\\n    -ot \"blk.(24|25|26|27).ffn.=CUDA5\" \\\n    -ot \"blk.(28|29|30|31|32|33|34|35).ffn.=CUDA6\" \\\n    -ot exps=CPU \\\n    -fa -mg 0 -ub 4096 -b 4096 -mla 3 -amb 256 -fmoe\n\nI get\n\n    Small test for this one!\n    \n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  4096 |   1024 |      0 |   10.671 |   383.83 |  117.496 |     8.72 |\n    |  4096 |   1024 |   4096 |   11.322 |   361.77 |  120.192 |     8.52 |\n\nhttps://preview.redd.it/dtrfsnabu3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=34fa15b35573c0d7ce936d9da953d4d483320902\n\nSorry on this one to have few data! IQ3\\_XXS quality is really good for it's size.\n\n# unsloth DeepSeek Q3_K_XL\n\nNow we enter a bigger territory. Note that you will notice Q3\\_K\\_XL being faster than IQ3\\_XXS, despite being bigger.\n\nRunning the faster PP one with:\n\n    ./llama-server -m '/DeepSeek-R1-0528-UD-Q3_K_XL-merged.gguf' \\\n    -c 32768 --no-mmap -ngl 999 \\\n    -ot \"blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0\" \\\n    -ot \"blk.(8|9|10|11).ffn.=CUDA1\" \\\n    -ot \"blk.(12|13|14|15).ffn.=CUDA2\" \\\n    -ot \"blk.(16|17|18|19|20).ffn.=CUDA3\" \\\n    -ot \"blk.(21|22|23).ffn.=CUDA4\" \\\n    -ot \"blk.(24|25|26).ffn.=CUDA5\" \\\n    -ot \"blk.(27|28|29|30|31|32|33|34).ffn.=CUDA6\" \\\n    -ot exps=CPU \\\n    -fa -mg 0 -ub 2560 -b 2560 -mla 1 -fmoe -amb 256\n\nResults look like this:\n\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  2560 |    640 |      0 |    9.781 |   261.72 |   65.367 |     9.79 |\n    |  2560 |    640 |   2560 |   10.048 |   254.78 |   65.824 |     9.72 |\n    |  2560 |    640 |   5120 |   10.625 |   240.93 |   66.134 |     9.68 |\n    |  2560 |    640 |   7680 |   11.167 |   229.24 |   67.225 |     9.52 |\n    |  2560 |    640 |  10240 |   12.268 |   208.68 |   67.475 |     9.49 |\n    |  2560 |    640 |  12800 |   13.433 |   190.58 |   68.743 |     9.31 |\n    |  2560 |    640 |  15360 |   14.564 |   175.78 |   69.585 |     9.20 |\n    |  2560 |    640 |  17920 |   15.734 |   162.70 |   70.589 |     9.07 |\n    |  2560 |    640 |  20480 |   16.889 |   151.58 |   72.524 |     8.82 |\n    |  2560 |    640 |  23040 |   18.100 |   141.43 |   74.534 |     8.59 |\n\nWith more layers on GPU, but smaller batch size, I get\n\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  2048 |    512 |      0 |    9.017 |   227.12 |   50.612 |    10.12 |\n    |  2048 |    512 |   2048 |    9.113 |   224.73 |   51.027 |    10.03 |\n    |  2048 |    512 |   4096 |    9.436 |   217.05 |   51.864 |     9.87 |\n    |  2048 |    512 |   6144 |    9.680 |   211.56 |   52.818 |     9.69 |\n    |  2048 |    512 |   8192 |    9.984 |   205.12 |   53.354 |     9.60 |\n    |  2048 |    512 |  10240 |   10.349 |   197.90 |   53.896 |     9.50 |\n    |  2048 |    512 |  12288 |   10.936 |   187.27 |   54.600 |     9.38 |\n    |  2048 |    512 |  14336 |   11.688 |   175.22 |   55.150 |     9.28 |\n    |  2048 |    512 |  16384 |   12.419 |   164.91 |   55.852 |     9.17 |\n    |  2048 |    512 |  18432 |   13.113 |   156.18 |   56.436 |     9.07 |\n    |  2048 |    512 |  20480 |   13.871 |   147.65 |   56.823 |     9.01 |\n    |  2048 |    512 |  22528 |   14.594 |   140.33 |   57.590 |     8.89 |\n    |  2048 |    512 |  24576 |   15.335 |   133.55 |   58.278 |     8.79 |\n    |  2048 |    512 |  26624 |   16.073 |   127.42 |   58.723 |     8.72 |\n    |  2048 |    512 |  28672 |   16.794 |   121.95 |   59.553 |     8.60 |\n    |  2048 |    512 |  30720 |   17.522 |   116.88 |   59.921 |     8.54 |\n\nAnd with less GPU layers on GPU, but higher batch size, I get\n\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  4096 |   1024 |      0 |   12.005 |   341.19 |  111.632 |     9.17 |\n    |  4096 |   1024 |   4096 |   12.515 |   327.28 |  138.930 |     7.37 |\n    |  4096 |   1024 |   8192 |   13.389 |   305.91 |  118.220 |     8.66 |\n    |  4096 |   1024 |  12288 |   15.018 |   272.74 |  119.289 |     8.58 |\n\nSo then, performance for different batch sizes and layers, looks like this:\n\n[Higher ub\\/b is because I ended the test earlier!](https://preview.redd.it/l9jswixxu3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=fed2cf27373207f9bb3e0e702f391214334adcd1)\n\nSo you can choose between having more TG t/s with having possibly smaller batch sizes (so then slower PP), or try to max PP by offloading more layers to the CPU.\n\n# ubergarm DeepSeek IQ3_KS ([TNG-R1T2-Chimera](https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF))\n\nThis one is really good! And it has some more optimizations that may apply more on iklcpp.\n\nRunning this one with:\n\n    ./llama-server -m '/GGUFs/DeepSeek-TNG-R1T2-Chimera-IQ3_KS-merged.gguf' \\\n    -c 32768 --no-mmap -ngl 999 \\\n    -ot \"blk.(0|1|2|3|4|5|6).ffn.=CUDA0\" \\\n    -ot \"blk.(7|8|9).ffn.=CUDA1\" \\\n    -ot \"blk.(10|11|12).ffn.=CUDA2\" \\\n    -ot \"blk.(13|14|15|16).ffn.=CUDA3\" \\\n    -ot \"blk.(17|18|19).ffn.=CUDA4\" \\\n    -ot \"blk.(20|21|22).ffn.=CUDA5\" \\\n    -ot \"blk.(23|24|25|26|27|28|29|30).ffn.=CUDA6\" \\\n    -ot exps=CPU \\\n    -fa -mg 0 -ub 6144 -b 6144 -mla 3 -fmoe -amb 256\n\nI  get\n\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  6144 |   1536 |      0 |   15.406 |   398.81 |  174.929 |     8.78 |\n    |  6144 |   1536 |   6144 |   18.289 |   335.94 |  180.393 |     8.51 |\n    |  6144 |   1536 |  12288 |   22.229 |   276.39 |  186.113 |     8.25 |\n    |  6144 |   1536 |  18432 |   24.533 |   250.44 |  191.037 |     8.04 |\n    |  6144 |   1536 |  24576 |   28.122 |   218.48 |  196.268 |     7.83 |\n\nOr 8192 batch size/ubatch size, I get\n\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  8192 |   2048 |      0 |   20.147 |   406.61 |  232.476 |     8.81 |\n    |  8192 |   2048 |   8192 |   26.009 |   314.97 |  242.648 |     8.44 |\n    |  8192 |   2048 |  16384 |   32.628 |   251.07 |  253.309 |     8.09 |\n    |  8192 |   2048 |  24576 |   39.010 |   210.00 |  264.415 |     7.75 |\n\nSo the graph looks like this\n\nhttps://preview.redd.it/rj0kip6gw3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=ac996b223c86d5d30668be4995436a4aa1bc8dbf\n\nAgain, this model is really good, and really fast! Totally recommended.\n\n# unsloth DeepSeek IQ4_XS\n\nAt this point is where I have to do compromises to run it on my PC, by either having less PP, less TG or use more RAM at the absolute limit.\n\nRunning this model with the best balance with:\n\n    ./llama-sweep-bench -m '/models_llm/DeepSeek-R1-0528-IQ4_XS-merged.gguf' -c 32768 --no-mmap -ngl 999 \\\n    -ot \"blk.(0|1|2|3|4|5|6).ffn.=CUDA0\" \\\n    -ot \"blk.(7|8|9).ffn.=CUDA1\" \\\n    -ot \"blk.(10|11|12).ffn.=CUDA2\" \\\n    -ot \"blk.(13|14|15|16).ffn.=CUDA3\" \\\n    -ot \"blk.(17|18|19).ffn.=CUDA4\" \\\n    -ot \"blk.(20|21|22).ffn.=CUDA5\" \\\n    -ot \"blk.(23|24|25|26|27|28|29).ffn.=CUDA6\" \\\n    -ot \"blk.30.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA1\" \\\n    -ot \"blk.30.ffn_gate_exps.weight=CUDA1\" \\\n    -ot \"blk.30.ffn_down_exps.weight=CUDA2\" \\\n    -ot \"blk.30.ffn_up_exps.weight=CUDA4\" \\\n    -ot \"blk.31.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA5\" \\\n    -ot \"blk.31.ffn_gate_exps.weight=CUDA5\" \\\n    -ot \"blk.31.ffn_down_exps.weight=CUDA0\" \\\n    -ot \"blk.31.ffn_up_exps.weight=CUDA3\" \\\n    -ot \"blk.32.ffn_gate_exps.weight=CUDA1\" \\\n    -ot \"blk.32.ffn_down_exps.weight=CUDA2\" \\\n    -ot exps=CPU \\\n    -fa -mg 0 -ub 1024 -mla 1 -amb 256\n\nUsing 161GB of RAM and the GPUs totally maxed, I get\n\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  1024 |    256 |      0 |    9.336 |   109.69 |   31.102 |     8.23 |\n    |  1024 |    256 |   1024 |    9.345 |   109.57 |   31.224 |     8.20 |\n    |  1024 |    256 |   2048 |    9.392 |   109.03 |   31.193 |     8.21 |\n    |  1024 |    256 |   3072 |    9.452 |   108.34 |   31.472 |     8.13 |\n    |  1024 |    256 |   4096 |    9.540 |   107.34 |   31.623 |     8.10 |\n    |  1024 |    256 |   5120 |    9.750 |   105.03 |   32.674 |     7.83 |\n\nRunning a variant with less layers on GPU, but more on CPU, using 177GB RAM and higher ubatch size, at 1792:\n\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n    |-------|--------|--------|----------|----------|----------|----------|\n    |  1792 |    448 |      0 |   10.701 |   167.46 |   56.284 |     7.96 |\n    |  1792 |    448 |   1792 |   10.729 |   167.02 |   56.638 |     7.91 |\n    |  1792 |    448 |   3584 |   10.947 |   163.71 |   57.194 |     7.83 |\n    |  1792 |    448 |   5376 |   11.099 |   161.46 |   58.003 |     7.72 |\n    |  1792 |    448 |   7168 |   11.267 |   159.06 |   58.127 |     7.71 |\n    |  1792 |    448 |   8960 |   11.450 |   156.51 |   58.697 |     7.63 |\n    |  1792 |    448 |  10752 |   11.627 |   154.12 |   59.421 |     7.54 |\n    |  1792 |    448 |  12544 |   11.809 |   151.75 |   59.686 |     7.51 |\n    |  1792 |    448 |  14336 |   12.007 |   149.24 |   60.075 |     7.46 |\n    |  1792 |    448 |  16128 |   12.251 |   146.27 |   60.624 |     7.39 |\n    |  1792 |    448 |  17920 |   12.639 |   141.79 |   60.977 |     7.35 |\n    |  1792 |    448 |  19712 |   13.113 |   136.66 |   61.481 |     7.29 |\n    |  1792 |    448 |  21504 |   13.639 |   131.39 |   62.117 |     7.21 |\n    |  1792 |    448 |  23296 |   14.184 |   126.34 |   62.393 |     7.18 |\n\nAnd there is a less efficient result with ub 1536, but this will be shown on the graph, which looks like this:\n\nhttps://preview.redd.it/r8xka0tcx3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=9d374193172785b18b03858204abb37a0ac8b9fa\n\nAs you can see, the most conservative one with RAM has really slow PP, but a bit faster TG. While with less layers on GPU and more RAM usage, since we left some layers, we can increase PP and increment is noticeable.\n\n# Final comparison\n\nAn image comparing 1 of each in one image, looks like this\n\nhttps://preview.redd.it/owlrn4cqx3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=f05a460e56395c8890bc2dc18d6e78aa15cf91a6\n\nI don't have PPL values in hand sadly, besides the PPL on TNG-R1T2-Chimera that ubergarm did, in where DeepSeek R1 0528 is just 3% better than this quant at 3.8bpw (`3.2119 +/- 0.01697` vs 3.3167 +/- 0.01789), but take in mind that original TNG-R1T2-Chimera is already, at Q8, a bit worse on PPL vs R1 0528, **so these quants are quite good quality.**\n\nFor the models on the post and based for max batch size (less layers on GPU, so more RAM usage because offloading more to CPU), or based on max TG speed (more layers on GPU, less on RAM):\n\n* 90-95GB RAM on Q2\\_K\\_XL, rest on VRAM.\n* 100-110GB RAM on IQ3\\_XXS, rest on VRAM.\n* 115-140GB RAM on Q3\\_K\\_XL, rest on VRAM.\n* 115-135GB RAM on IQ3\\_KS, rest on VRAM.\n* 161-177GB RAM on IQ4\\_XS, rest on VRAM.\n\nSomeone may be wondering that with these values, it is still not total 400GB (192GB RAM + 208GB VRAM), and it's because I have not contemplated the compute buffer sizes, which can range between 512MB up to 5GB per GPU.\n\nFor DeepSeek models with MLA, in general it is 1GB per 8K ctx at fp16. So 1GB per 16K with q8\\_0 ctx (I didn't use it here, but it lets me use 64K at q8 with the same config as 32K at f16).\n\nHope this post can help someone interested in these results, any question is welcome!",
          "author_fullname": "t2_j1kqr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Performance benchmarks on DeepSeek V3-0324/R1-0528/TNG-R1T2-Chimera on consumer CPU (7800X3D, 192GB RAM at 6000Mhz) and 208GB VRAM (5090x2/4090x2/3090x2/A6000) on ikllamacpp! From 3bpw (Q2_K_XL) to 4.2 bpw (IQ4_XS)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "rj0kip6gw3cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/rj0kip6gw3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1f101fc6491542a1a0d5232c62e944c92274120"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/rj0kip6gw3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9511e2308f22e080b1ceecf696c988a7e9d44dde"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/rj0kip6gw3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc4b301b035b11bb121d463ba4d2cde5bd111d2e"
                },
                {
                  "y": 333,
                  "x": 640,
                  "u": "https://preview.redd.it/rj0kip6gw3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=437e96f03df2f2d0622934a1195ec3771ca24181"
                },
                {
                  "y": 499,
                  "x": 960,
                  "u": "https://preview.redd.it/rj0kip6gw3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f6b3f9cb824b56c49dee448ac1916c538919b228"
                },
                {
                  "y": 561,
                  "x": 1080,
                  "u": "https://preview.redd.it/rj0kip6gw3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18249a8e504568e0fdb251e3f559bc4576973e14"
                }
              ],
              "s": {
                "y": 1998,
                "x": 3840,
                "u": "https://preview.redd.it/rj0kip6gw3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=ac996b223c86d5d30668be4995436a4aa1bc8dbf"
              },
              "id": "rj0kip6gw3cf1"
            },
            "r9tt4pktt3cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/r9tt4pktt3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d06714ed9be3cf158f64b49625f04d4d3c1f811"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/r9tt4pktt3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef051b2e1c39b6fd156b5c24ba9927d364fa613f"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/r9tt4pktt3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2bb45907231ce9906909fc31595392a8ea256940"
                },
                {
                  "y": 333,
                  "x": 640,
                  "u": "https://preview.redd.it/r9tt4pktt3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4f664b75a1378a0d7f3c9ca85df7c3bd682e3863"
                },
                {
                  "y": 499,
                  "x": 960,
                  "u": "https://preview.redd.it/r9tt4pktt3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e4006e3d3ab018f96cc70afdde829cacd8e17a7"
                },
                {
                  "y": 561,
                  "x": 1080,
                  "u": "https://preview.redd.it/r9tt4pktt3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=66b3ccced4271b1a7bad7f5174460be8221e4edc"
                }
              ],
              "s": {
                "y": 1998,
                "x": 3840,
                "u": "https://preview.redd.it/r9tt4pktt3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=9e55870952c3069d16bb1a1b211b83b870e87da7"
              },
              "id": "r9tt4pktt3cf1"
            },
            "dtrfsnabu3cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/dtrfsnabu3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0fc0140a716d4f3f1b0ceeaacd5a84bc2be68565"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/dtrfsnabu3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ba825367494d413ad942db2b001da7b0ead3a0c"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/dtrfsnabu3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fdad97203dc8fdcfaabe6027fe6692e4548aac60"
                },
                {
                  "y": 333,
                  "x": 640,
                  "u": "https://preview.redd.it/dtrfsnabu3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=46bee9fde8e9875eb11bed658996aa56ee543827"
                },
                {
                  "y": 499,
                  "x": 960,
                  "u": "https://preview.redd.it/dtrfsnabu3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b648d4bb124f102339fb52c54397c79f215c38a4"
                },
                {
                  "y": 561,
                  "x": 1080,
                  "u": "https://preview.redd.it/dtrfsnabu3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c1898a494375997a6904dfbbda045e69fec24a72"
                }
              ],
              "s": {
                "y": 1998,
                "x": 3840,
                "u": "https://preview.redd.it/dtrfsnabu3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=34fa15b35573c0d7ce936d9da953d4d483320902"
              },
              "id": "dtrfsnabu3cf1"
            },
            "owlrn4cqx3cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/owlrn4cqx3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bca3112656c90be4044439b52e00946321961046"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/owlrn4cqx3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e268313470f242e9afa03279fc22c8ac2b078da2"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/owlrn4cqx3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1451526e6065a4be1fa87a59be3f3b3d96e6496"
                },
                {
                  "y": 333,
                  "x": 640,
                  "u": "https://preview.redd.it/owlrn4cqx3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2bbd26aa16d1131f087e6e6dc7a3dcfccd748228"
                },
                {
                  "y": 499,
                  "x": 960,
                  "u": "https://preview.redd.it/owlrn4cqx3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b5091dff9d48e6a04095f1c8d1d4e5045aca3aa"
                },
                {
                  "y": 561,
                  "x": 1080,
                  "u": "https://preview.redd.it/owlrn4cqx3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=30d20fa0f3120330480aeb16bbc72c15e2fbfd40"
                }
              ],
              "s": {
                "y": 1998,
                "x": 3840,
                "u": "https://preview.redd.it/owlrn4cqx3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=f05a460e56395c8890bc2dc18d6e78aa15cf91a6"
              },
              "id": "owlrn4cqx3cf1"
            },
            "l9jswixxu3cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/l9jswixxu3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=98693343116b5932d4e4febfade79a9540bf5251"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/l9jswixxu3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c19aafa0adf244051402fb495f7ea3e83ce4d332"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/l9jswixxu3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=88a4c3ce4e0a67659285ff944ea2caf076534397"
                },
                {
                  "y": 333,
                  "x": 640,
                  "u": "https://preview.redd.it/l9jswixxu3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e6a67f633cd95a5690c3eec22b4e533133bd30e9"
                },
                {
                  "y": 499,
                  "x": 960,
                  "u": "https://preview.redd.it/l9jswixxu3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=47fd0b46a4f4552c164259485d6d18c3810490ec"
                },
                {
                  "y": 561,
                  "x": 1080,
                  "u": "https://preview.redd.it/l9jswixxu3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f47e3de82b9f829542cf88b78ddc69d029a420f8"
                }
              ],
              "s": {
                "y": 1998,
                "x": 3840,
                "u": "https://preview.redd.it/l9jswixxu3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=fed2cf27373207f9bb3e0e702f391214334adcd1"
              },
              "id": "l9jswixxu3cf1"
            },
            "r8xka0tcx3cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/r8xka0tcx3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=60c2c8b2ff94822236365cd794d77a9025518b37"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/r8xka0tcx3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3923f4e719843f30db5fb36d50cd44f6c92cde1c"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/r8xka0tcx3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=15850541050f33ee722c2a69833096aaee3fab2b"
                },
                {
                  "y": 333,
                  "x": 640,
                  "u": "https://preview.redd.it/r8xka0tcx3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=07def092ebeb76d951427fa69df4343132d82320"
                },
                {
                  "y": 499,
                  "x": 960,
                  "u": "https://preview.redd.it/r8xka0tcx3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3c5bff32a3671665b53452ff08c02de89fa87d27"
                },
                {
                  "y": 561,
                  "x": 1080,
                  "u": "https://preview.redd.it/r8xka0tcx3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0554a466e15e1086df7bee957146b2ec82ca05b7"
                }
              ],
              "s": {
                "y": 1998,
                "x": 3840,
                "u": "https://preview.redd.it/r8xka0tcx3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=9d374193172785b18b03858204abb37a0ac8b9fa"
              },
              "id": "r8xka0tcx3cf1"
            }
          },
          "name": "t3_1lwnj5x",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 42,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 42,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/2-o5OWNt03DgmcUIElRxpmapK3b8mnf9fOYvDpwJaPg.jpg",
          "edited": 1752181687,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 405B"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752179851,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there guys, hope you&amp;#39;re having a good day!&lt;/p&gt;\n\n&lt;p&gt;After latest improvements on ik llamacpp, &lt;a href=\"https://github.com/ikawrakow/ik_llama.cpp/commits/main/\"&gt;https://github.com/ikawrakow/ik_llama.cpp/commits/main/&lt;/a&gt;, I have found that DeepSeek MoE models runs noticeably faster than llamacpp, at the point that I get about half PP t/s and 0.85-0.9X TG t/s vs ikllamacpp. This is the case only for MoE models I&amp;#39;m testing.&lt;/p&gt;\n\n&lt;p&gt;My setup is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt;\n&lt;li&gt;192GB RAM, DDR5 6000Mhz, max bandwidth at about 60-62 GB/s&lt;/li&gt;\n&lt;li&gt;3 1600W PSUs (Corsair 1600i)&lt;/li&gt;\n&lt;li&gt;AM5 MSI Carbon X670E&lt;/li&gt;\n&lt;li&gt;5090/5090 at PCIe X8/X8 5.0&lt;/li&gt;\n&lt;li&gt;4090/4090 at PCIe X4/X4 4.0&lt;/li&gt;\n&lt;li&gt;3090/3090 at PCIe X4/X4 4.0&lt;/li&gt;\n&lt;li&gt;A6000 at PCIe X4 4.0.&lt;/li&gt;\n&lt;li&gt;Fedora Linux 41 (instead of 42 just because I&amp;#39;m lazy doing some roundabouts to compile with GCC15, waiting until NVIDIA adds support to it)&lt;/li&gt;\n&lt;li&gt;SATA and USB-&amp;gt;M2 Storage&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The benchmarks are based on mostly, R1-0528, BUT it has the same size and it&amp;#39;s quants on V3-0324 and TNG-R1T2-Chimera.&lt;/p&gt;\n\n&lt;p&gt;I have tested the next models:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/unsloth\"&gt;unsloth&lt;/a&gt; DeepSeek Q2_K_XL:\n\n&lt;ul&gt;\n&lt;li&gt;llm_load_print_meta: model size       = 233.852 GiB (2.994 BPW)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/unsloth\"&gt;unsloth&lt;/a&gt; DeepSeek IQ3_XXS:\n\n&lt;ul&gt;\n&lt;li&gt;llm_load_print_meta: model size       = 254.168 GiB (3.254 BPW)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/unsloth\"&gt;unsloth&lt;/a&gt; DeepSeek Q3_K_XL:\n\n&lt;ul&gt;\n&lt;li&gt;llm_load_print_meta: model size       = 275.576 GiB (3.528 BPW)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/ubergarm\"&gt;ubergarm&lt;/a&gt; DeepSeek IQ3_KS:\n\n&lt;ul&gt;\n&lt;li&gt;llm_load_print_meta: model size       = 281.463 GiB (3.598 BPW)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/unsloth\"&gt;unsloth&lt;/a&gt; DeepSeek IQ4_XS:\n\n&lt;ul&gt;\n&lt;li&gt;llm_load_print_meta: model size       = 333.130 GiB (4.264 BPW)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Each model may have been tested on different formats. Q2_K_XL and IQ3_XXS has less info, but the rest have a lot more. So here we go!&lt;/p&gt;\n\n&lt;h1&gt;unsloth DeepSeek Q2_K_XL&lt;/h1&gt;\n\n&lt;p&gt;Running the model with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-server -m &amp;#39;/models_llm/DeepSeek-R1-0528-UD-Q2_K_XL-merged.gguf&amp;#39; \\\n-c 32768 --no-mmap -ngl 999 \\\n-ot &amp;quot;blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0&amp;quot; \\\n-ot &amp;quot;blk.(8|9|10|11).ffn.=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.(12|13|14|15).ffn.=CUDA2&amp;quot; \\\n-ot &amp;quot;blk.(16|17|18|19|20).ffn.=CUDA3&amp;quot; \\\n-ot &amp;quot;blk.(21|22|23|24).ffn.=CUDA4&amp;quot; \\\n-ot &amp;quot;blk.(25|26|27|28).ffn.=CUDA5&amp;quot; \\\n-ot &amp;quot;blk.(29|30|31|32|33|34|35|36|37|38).ffn.=CUDA6&amp;quot; \\\n-ot exps=CPU \\\n-fa -mg 0 -ub 5120 -b 5120 -mla 3 -amb 256 -fmoe\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I get:&lt;/p&gt;\n\n&lt;p&gt;main: n_kv_max = 32768, n_batch = 5120, n_ubatch = 5120, flash_attn = 1, n_gpu_layers = 999, n_threads = 8, n_threads_batch = 8&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  5120 |   1280 |      0 |   12.481 |   410.21 |  104.088 |    12.30 |\n|  5120 |   1280 |   5120 |   14.630 |   349.98 |  109.724 |    11.67 |\n|  5120 |   1280 |  10240 |   17.167 |   298.25 |  112.938 |    11.33 |\n|  5120 |   1280 |  15360 |   20.008 |   255.90 |  119.037 |    10.75 |\n|  5120 |   1280 |  20480 |   22.444 |   228.12 |  122.706 |    10.43 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/r9tt4pktt3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e55870952c3069d16bb1a1b211b83b870e87da7\"&gt;Perf comparison (ignore 4096 as I forgor to save the perf)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Q2_K_XL performs really good for a system like this! And it&amp;#39;s performance as LLM is really good as well. I still prefer this above any other local model, for example, even if it&amp;#39;s at 3bpw.&lt;/p&gt;\n\n&lt;h1&gt;unsloth DeepSeek IQ3_XXS&lt;/h1&gt;\n\n&lt;p&gt;Running the model with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-server -m &amp;#39;/models_llm/DeepSeek-R1-0528-UD-IQ3_XXS-merged.gguf&amp;#39; \\\n-c 32768 --no-mmap -ngl 999 \\\n-ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \\\n-ot &amp;quot;blk.(7|8|9|10).ffn.=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.(11|12|13|14).ffn.=CUDA2&amp;quot; \\\n-ot &amp;quot;blk.(15|16|17|18|19).ffn.=CUDA3&amp;quot; \\\n-ot &amp;quot;blk.(20|21|22|23).ffn.=CUDA4&amp;quot; \\\n-ot &amp;quot;blk.(24|25|26|27).ffn.=CUDA5&amp;quot; \\\n-ot &amp;quot;blk.(28|29|30|31|32|33|34|35).ffn.=CUDA6&amp;quot; \\\n-ot exps=CPU \\\n-fa -mg 0 -ub 4096 -b 4096 -mla 3 -amb 256 -fmoe\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I get&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Small test for this one!\n\n|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  4096 |   1024 |      0 |   10.671 |   383.83 |  117.496 |     8.72 |\n|  4096 |   1024 |   4096 |   11.322 |   361.77 |  120.192 |     8.52 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dtrfsnabu3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34fa15b35573c0d7ce936d9da953d4d483320902\"&gt;https://preview.redd.it/dtrfsnabu3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34fa15b35573c0d7ce936d9da953d4d483320902&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sorry on this one to have few data! IQ3_XXS quality is really good for it&amp;#39;s size.&lt;/p&gt;\n\n&lt;h1&gt;unsloth DeepSeek Q3_K_XL&lt;/h1&gt;\n\n&lt;p&gt;Now we enter a bigger territory. Note that you will notice Q3_K_XL being faster than IQ3_XXS, despite being bigger.&lt;/p&gt;\n\n&lt;p&gt;Running the faster PP one with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-server -m &amp;#39;/DeepSeek-R1-0528-UD-Q3_K_XL-merged.gguf&amp;#39; \\\n-c 32768 --no-mmap -ngl 999 \\\n-ot &amp;quot;blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0&amp;quot; \\\n-ot &amp;quot;blk.(8|9|10|11).ffn.=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.(12|13|14|15).ffn.=CUDA2&amp;quot; \\\n-ot &amp;quot;blk.(16|17|18|19|20).ffn.=CUDA3&amp;quot; \\\n-ot &amp;quot;blk.(21|22|23).ffn.=CUDA4&amp;quot; \\\n-ot &amp;quot;blk.(24|25|26).ffn.=CUDA5&amp;quot; \\\n-ot &amp;quot;blk.(27|28|29|30|31|32|33|34).ffn.=CUDA6&amp;quot; \\\n-ot exps=CPU \\\n-fa -mg 0 -ub 2560 -b 2560 -mla 1 -fmoe -amb 256\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Results look like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  2560 |    640 |      0 |    9.781 |   261.72 |   65.367 |     9.79 |\n|  2560 |    640 |   2560 |   10.048 |   254.78 |   65.824 |     9.72 |\n|  2560 |    640 |   5120 |   10.625 |   240.93 |   66.134 |     9.68 |\n|  2560 |    640 |   7680 |   11.167 |   229.24 |   67.225 |     9.52 |\n|  2560 |    640 |  10240 |   12.268 |   208.68 |   67.475 |     9.49 |\n|  2560 |    640 |  12800 |   13.433 |   190.58 |   68.743 |     9.31 |\n|  2560 |    640 |  15360 |   14.564 |   175.78 |   69.585 |     9.20 |\n|  2560 |    640 |  17920 |   15.734 |   162.70 |   70.589 |     9.07 |\n|  2560 |    640 |  20480 |   16.889 |   151.58 |   72.524 |     8.82 |\n|  2560 |    640 |  23040 |   18.100 |   141.43 |   74.534 |     8.59 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;With more layers on GPU, but smaller batch size, I get&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  2048 |    512 |      0 |    9.017 |   227.12 |   50.612 |    10.12 |\n|  2048 |    512 |   2048 |    9.113 |   224.73 |   51.027 |    10.03 |\n|  2048 |    512 |   4096 |    9.436 |   217.05 |   51.864 |     9.87 |\n|  2048 |    512 |   6144 |    9.680 |   211.56 |   52.818 |     9.69 |\n|  2048 |    512 |   8192 |    9.984 |   205.12 |   53.354 |     9.60 |\n|  2048 |    512 |  10240 |   10.349 |   197.90 |   53.896 |     9.50 |\n|  2048 |    512 |  12288 |   10.936 |   187.27 |   54.600 |     9.38 |\n|  2048 |    512 |  14336 |   11.688 |   175.22 |   55.150 |     9.28 |\n|  2048 |    512 |  16384 |   12.419 |   164.91 |   55.852 |     9.17 |\n|  2048 |    512 |  18432 |   13.113 |   156.18 |   56.436 |     9.07 |\n|  2048 |    512 |  20480 |   13.871 |   147.65 |   56.823 |     9.01 |\n|  2048 |    512 |  22528 |   14.594 |   140.33 |   57.590 |     8.89 |\n|  2048 |    512 |  24576 |   15.335 |   133.55 |   58.278 |     8.79 |\n|  2048 |    512 |  26624 |   16.073 |   127.42 |   58.723 |     8.72 |\n|  2048 |    512 |  28672 |   16.794 |   121.95 |   59.553 |     8.60 |\n|  2048 |    512 |  30720 |   17.522 |   116.88 |   59.921 |     8.54 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And with less GPU layers on GPU, but higher batch size, I get&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  4096 |   1024 |      0 |   12.005 |   341.19 |  111.632 |     9.17 |\n|  4096 |   1024 |   4096 |   12.515 |   327.28 |  138.930 |     7.37 |\n|  4096 |   1024 |   8192 |   13.389 |   305.91 |  118.220 |     8.66 |\n|  4096 |   1024 |  12288 |   15.018 |   272.74 |  119.289 |     8.58 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So then, performance for different batch sizes and layers, looks like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/l9jswixxu3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fed2cf27373207f9bb3e0e702f391214334adcd1\"&gt;Higher ub/b is because I ended the test earlier!&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So you can choose between having more TG t/s with having possibly smaller batch sizes (so then slower PP), or try to max PP by offloading more layers to the CPU.&lt;/p&gt;\n\n&lt;h1&gt;ubergarm DeepSeek IQ3_KS (&lt;a href=\"https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF\"&gt;TNG-R1T2-Chimera&lt;/a&gt;)&lt;/h1&gt;\n\n&lt;p&gt;This one is really good! And it has some more optimizations that may apply more on iklcpp.&lt;/p&gt;\n\n&lt;p&gt;Running this one with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-server -m &amp;#39;/GGUFs/DeepSeek-TNG-R1T2-Chimera-IQ3_KS-merged.gguf&amp;#39; \\\n-c 32768 --no-mmap -ngl 999 \\\n-ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \\\n-ot &amp;quot;blk.(7|8|9).ffn.=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.(10|11|12).ffn.=CUDA2&amp;quot; \\\n-ot &amp;quot;blk.(13|14|15|16).ffn.=CUDA3&amp;quot; \\\n-ot &amp;quot;blk.(17|18|19).ffn.=CUDA4&amp;quot; \\\n-ot &amp;quot;blk.(20|21|22).ffn.=CUDA5&amp;quot; \\\n-ot &amp;quot;blk.(23|24|25|26|27|28|29|30).ffn.=CUDA6&amp;quot; \\\n-ot exps=CPU \\\n-fa -mg 0 -ub 6144 -b 6144 -mla 3 -fmoe -amb 256\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I  get&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  6144 |   1536 |      0 |   15.406 |   398.81 |  174.929 |     8.78 |\n|  6144 |   1536 |   6144 |   18.289 |   335.94 |  180.393 |     8.51 |\n|  6144 |   1536 |  12288 |   22.229 |   276.39 |  186.113 |     8.25 |\n|  6144 |   1536 |  18432 |   24.533 |   250.44 |  191.037 |     8.04 |\n|  6144 |   1536 |  24576 |   28.122 |   218.48 |  196.268 |     7.83 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Or 8192 batch size/ubatch size, I get&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  8192 |   2048 |      0 |   20.147 |   406.61 |  232.476 |     8.81 |\n|  8192 |   2048 |   8192 |   26.009 |   314.97 |  242.648 |     8.44 |\n|  8192 |   2048 |  16384 |   32.628 |   251.07 |  253.309 |     8.09 |\n|  8192 |   2048 |  24576 |   39.010 |   210.00 |  264.415 |     7.75 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So the graph looks like this&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rj0kip6gw3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac996b223c86d5d30668be4995436a4aa1bc8dbf\"&gt;https://preview.redd.it/rj0kip6gw3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac996b223c86d5d30668be4995436a4aa1bc8dbf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Again, this model is really good, and really fast! Totally recommended.&lt;/p&gt;\n\n&lt;h1&gt;unsloth DeepSeek IQ4_XS&lt;/h1&gt;\n\n&lt;p&gt;At this point is where I have to do compromises to run it on my PC, by either having less PP, less TG or use more RAM at the absolute limit.&lt;/p&gt;\n\n&lt;p&gt;Running this model with the best balance with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-sweep-bench -m &amp;#39;/models_llm/DeepSeek-R1-0528-IQ4_XS-merged.gguf&amp;#39; -c 32768 --no-mmap -ngl 999 \\\n-ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \\\n-ot &amp;quot;blk.(7|8|9).ffn.=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.(10|11|12).ffn.=CUDA2&amp;quot; \\\n-ot &amp;quot;blk.(13|14|15|16).ffn.=CUDA3&amp;quot; \\\n-ot &amp;quot;blk.(17|18|19).ffn.=CUDA4&amp;quot; \\\n-ot &amp;quot;blk.(20|21|22).ffn.=CUDA5&amp;quot; \\\n-ot &amp;quot;blk.(23|24|25|26|27|28|29).ffn.=CUDA6&amp;quot; \\\n-ot &amp;quot;blk.30.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.30.ffn_gate_exps.weight=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.30.ffn_down_exps.weight=CUDA2&amp;quot; \\\n-ot &amp;quot;blk.30.ffn_up_exps.weight=CUDA4&amp;quot; \\\n-ot &amp;quot;blk.31.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA5&amp;quot; \\\n-ot &amp;quot;blk.31.ffn_gate_exps.weight=CUDA5&amp;quot; \\\n-ot &amp;quot;blk.31.ffn_down_exps.weight=CUDA0&amp;quot; \\\n-ot &amp;quot;blk.31.ffn_up_exps.weight=CUDA3&amp;quot; \\\n-ot &amp;quot;blk.32.ffn_gate_exps.weight=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.32.ffn_down_exps.weight=CUDA2&amp;quot; \\\n-ot exps=CPU \\\n-fa -mg 0 -ub 1024 -mla 1 -amb 256\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Using 161GB of RAM and the GPUs totally maxed, I get&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  1024 |    256 |      0 |    9.336 |   109.69 |   31.102 |     8.23 |\n|  1024 |    256 |   1024 |    9.345 |   109.57 |   31.224 |     8.20 |\n|  1024 |    256 |   2048 |    9.392 |   109.03 |   31.193 |     8.21 |\n|  1024 |    256 |   3072 |    9.452 |   108.34 |   31.472 |     8.13 |\n|  1024 |    256 |   4096 |    9.540 |   107.34 |   31.623 |     8.10 |\n|  1024 |    256 |   5120 |    9.750 |   105.03 |   32.674 |     7.83 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Running a variant with less layers on GPU, but more on CPU, using 177GB RAM and higher ubatch size, at 1792:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\n|-------|--------|--------|----------|----------|----------|----------|\n|  1792 |    448 |      0 |   10.701 |   167.46 |   56.284 |     7.96 |\n|  1792 |    448 |   1792 |   10.729 |   167.02 |   56.638 |     7.91 |\n|  1792 |    448 |   3584 |   10.947 |   163.71 |   57.194 |     7.83 |\n|  1792 |    448 |   5376 |   11.099 |   161.46 |   58.003 |     7.72 |\n|  1792 |    448 |   7168 |   11.267 |   159.06 |   58.127 |     7.71 |\n|  1792 |    448 |   8960 |   11.450 |   156.51 |   58.697 |     7.63 |\n|  1792 |    448 |  10752 |   11.627 |   154.12 |   59.421 |     7.54 |\n|  1792 |    448 |  12544 |   11.809 |   151.75 |   59.686 |     7.51 |\n|  1792 |    448 |  14336 |   12.007 |   149.24 |   60.075 |     7.46 |\n|  1792 |    448 |  16128 |   12.251 |   146.27 |   60.624 |     7.39 |\n|  1792 |    448 |  17920 |   12.639 |   141.79 |   60.977 |     7.35 |\n|  1792 |    448 |  19712 |   13.113 |   136.66 |   61.481 |     7.29 |\n|  1792 |    448 |  21504 |   13.639 |   131.39 |   62.117 |     7.21 |\n|  1792 |    448 |  23296 |   14.184 |   126.34 |   62.393 |     7.18 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And there is a less efficient result with ub 1536, but this will be shown on the graph, which looks like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/r8xka0tcx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d374193172785b18b03858204abb37a0ac8b9fa\"&gt;https://preview.redd.it/r8xka0tcx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d374193172785b18b03858204abb37a0ac8b9fa&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As you can see, the most conservative one with RAM has really slow PP, but a bit faster TG. While with less layers on GPU and more RAM usage, since we left some layers, we can increase PP and increment is noticeable.&lt;/p&gt;\n\n&lt;h1&gt;Final comparison&lt;/h1&gt;\n\n&lt;p&gt;An image comparing 1 of each in one image, looks like this&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/owlrn4cqx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f05a460e56395c8890bc2dc18d6e78aa15cf91a6\"&gt;https://preview.redd.it/owlrn4cqx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f05a460e56395c8890bc2dc18d6e78aa15cf91a6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have PPL values in hand sadly, besides the PPL on TNG-R1T2-Chimera that ubergarm did, in where DeepSeek R1 0528 is just 3% better than this quant at 3.8bpw (&lt;code&gt;3.2119 +/- 0.01697&lt;/code&gt; vs 3.3167 +/- 0.01789), but take in mind that original TNG-R1T2-Chimera is already, at Q8, a bit worse on PPL vs R1 0528, &lt;strong&gt;so these quants are quite good quality.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For the models on the post and based for max batch size (less layers on GPU, so more RAM usage because offloading more to CPU), or based on max TG speed (more layers on GPU, less on RAM):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;90-95GB RAM on Q2_K_XL, rest on VRAM.&lt;/li&gt;\n&lt;li&gt;100-110GB RAM on IQ3_XXS, rest on VRAM.&lt;/li&gt;\n&lt;li&gt;115-140GB RAM on Q3_K_XL, rest on VRAM.&lt;/li&gt;\n&lt;li&gt;115-135GB RAM on IQ3_KS, rest on VRAM.&lt;/li&gt;\n&lt;li&gt;161-177GB RAM on IQ4_XS, rest on VRAM.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Someone may be wondering that with these values, it is still not total 400GB (192GB RAM + 208GB VRAM), and it&amp;#39;s because I have not contemplated the compute buffer sizes, which can range between 512MB up to 5GB per GPU.&lt;/p&gt;\n\n&lt;p&gt;For DeepSeek models with MLA, in general it is 1GB per 8K ctx at fp16. So 1GB per 16K with q8_0 ctx (I didn&amp;#39;t use it here, but it lets me use 64K at q8 with the same config as 32K at f16).&lt;/p&gt;\n\n&lt;p&gt;Hope this post can help someone interested in these results, any question is welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 405B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwnj5x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "panchovix",
          "discussion_type": null,
          "num_comments": 56,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752179851,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m trying to build an AI application that transcribes long audio recordings (around hundreds of thousands of tokens) and allows interaction with an LLM. However, every answer I get from searches and inquiries tells me that I need to chunk and vectorize the long text.\n\nBut with LLMs like Gemini that support 1M-token context, isn’t building a RAG system somewhat extra?\n\nThanks a lot!",
          "author_fullname": "t2_vw0gx7hg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "With a 1M context Gemini, does it still make sense to do embedding or use RAG for long texts?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lx10ja",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752220264,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m trying to build an AI application that transcribes long audio recordings (around hundreds of thousands of tokens) and allows interaction with an LLM. However, every answer I get from searches and inquiries tells me that I need to chunk and vectorize the long text.&lt;/p&gt;\n\n&lt;p&gt;But with LLMs like Gemini that support 1M-token context, isn’t building a RAG system somewhat extra?&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lx10ja",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GyozaHoop",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752220264,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[Mistral.rs](http://Mistral.rs) has support for Mistral AI's newest model (no affiliation)!\n\nGrab optimized UQFF files here: [https://huggingface.co/EricB/Devstral-Small-2507-UQFF](https://huggingface.co/EricB/Devstral-Small-2507-UQFF)\n\nMore information: [https://github.com/EricLBuehler/mistral.rs](https://github.com/EricLBuehler/mistral.rs)\n\nIn my testing, this model is really great at tool calling, and works very well with some of our newest features:\n\n* **Agentic/automatic tool calling**: you can specify custom tool callbacks in Python or Rust and dedicate the entire toolcalling workflow to mistral.rs!\n* **OpenAI web search support**: [mistral.rs](http://mistral.rs) allows models to have access to automatic web search, 100% compatible with the OpenAI API.\n* **MCP client**: there is a builtin MCP client! Just like ChatGPT or Claude, all you need to do is specify the MCP server and it just works!\n\nThese features make [mistral.rs](http://mistral.rs) a really powerful tool for leveraging the strong capabilities of Devstral!\n\nWhat do you think? Excited to see what you build with this 🚀!",
          "author_fullname": "t2_87jryn0i3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Devstral 2707 with mistral.rs - MCP client, automatic tool calling!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwmpqf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 44,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 44,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752188378,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752177957,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"http://Mistral.rs\"&gt;Mistral.rs&lt;/a&gt; has support for Mistral AI&amp;#39;s newest model (no affiliation)!&lt;/p&gt;\n\n&lt;p&gt;Grab optimized UQFF files here: &lt;a href=\"https://huggingface.co/EricB/Devstral-Small-2507-UQFF\"&gt;https://huggingface.co/EricB/Devstral-Small-2507-UQFF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;More information: &lt;a href=\"https://github.com/EricLBuehler/mistral.rs\"&gt;https://github.com/EricLBuehler/mistral.rs&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In my testing, this model is really great at tool calling, and works very well with some of our newest features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Agentic/automatic tool calling&lt;/strong&gt;: you can specify custom tool callbacks in Python or Rust and dedicate the entire toolcalling workflow to mistral.rs!&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;OpenAI web search support&lt;/strong&gt;: &lt;a href=\"http://mistral.rs\"&gt;mistral.rs&lt;/a&gt; allows models to have access to automatic web search, 100% compatible with the OpenAI API.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;MCP client&lt;/strong&gt;: there is a builtin MCP client! Just like ChatGPT or Claude, all you need to do is specify the MCP server and it just works!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;These features make &lt;a href=\"http://mistral.rs\"&gt;mistral.rs&lt;/a&gt; a really powerful tool for leveraging the strong capabilities of Devstral!&lt;/p&gt;\n\n&lt;p&gt;What do you think? Excited to see what you build with this 🚀!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwmpqf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EricBuehler",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwmpqf/new_devstral_2707_with_mistralrs_mcp_client/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwmpqf/new_devstral_2707_with_mistralrs_mcp_client/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752177957,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recently added Shortcuts support to my iOS app [Locally AI](https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692) and worked to integrate it with Siri.\n\nIt's using Apple MLX to run the models.\n\nHere's a demo of me asking Qwen 3 a question via Siri (sorry for my accent). It will call the app shortcut, get the answer and forward it to the Siri interface. It works with the Siri interface but also with AirPods or HomePod where Siri reads it. \n\nEverything running on-device.\n\nDid my best to have a seamless integration. It doesn’t require any setup other than downloading a model first.",
          "author_fullname": "t2_1jgkfm9u25",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Using Siri to talk to a local LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwif50",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 66,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/wjksocsoy2cf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1920,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/wjksocsoy2cf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/wjksocsoy2cf1/DASHPlaylist.mpd?a=1754812733%2CNGQ4MDQ5ODdjNWY2M2Q4ZDkwMGJhMjlkNWM2N2FmMGMwZGRhMGZkOTYwMTcxYTA1NTEyNWJkZmE3NDZjZDJjNQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 20,
              "hls_url": "https://v.redd.it/wjksocsoy2cf1/HLSPlaylist.m3u8?a=1754812733%2CYzM3YzU3MzM0MTliOWE4N2M1OTM0ODBjYmZkODVkMTBkNGM1NjY0NzhlYjc4Yzk4N2JkNjkwNDY2NDQ2NjcwMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 66,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=c9da73cbdedd1c5bc87febbd0a7035d09051ff58",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752167877,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently added Shortcuts support to my iOS app &lt;a href=\"https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692\"&gt;Locally AI&lt;/a&gt; and worked to integrate it with Siri.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s using Apple MLX to run the models.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a demo of me asking Qwen 3 a question via Siri (sorry for my accent). It will call the app shortcut, get the answer and forward it to the Siri interface. It works with the Siri interface but also with AirPods or HomePod where Siri reads it. &lt;/p&gt;\n\n&lt;p&gt;Everything running on-device.&lt;/p&gt;\n\n&lt;p&gt;Did my best to have a seamless integration. It doesn’t require any setup other than downloading a model first.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/wjksocsoy2cf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?format=pjpg&amp;auto=webp&amp;s=eca2e9d3117bf49c61a925dbcb5abfe4658cc2de",
                  "width": 1080,
                  "height": 1920
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d0a0ec3c011de00aecdae2dc6e55601786643f9d",
                    "width": 108,
                    "height": 192
                  },
                  {
                    "url": "https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0b7f03d45f7cfcaba6e5d80b06cdb7100762d90d",
                    "width": 216,
                    "height": 384
                  },
                  {
                    "url": "https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=12e65ecf15c38208675551217d60186cc0218d5a",
                    "width": 320,
                    "height": 568
                  },
                  {
                    "url": "https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=44b72fe431e174c03868006089658ccbb163202e",
                    "width": 640,
                    "height": 1137
                  },
                  {
                    "url": "https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d30a61cb8c29fe7692658fd361c337579b4e2d90",
                    "width": 960,
                    "height": 1706
                  },
                  {
                    "url": "https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=794b886de2931d9dcd5096f3d4ecbbb7be4a5f22",
                    "width": 1080,
                    "height": 1920
                  }
                ],
                "variants": {},
                "id": "aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lwif50",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adrgrondin",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwif50/using_siri_to_talk_to_a_local_llm/",
          "stickied": false,
          "url": "https://v.redd.it/wjksocsoy2cf1",
          "subreddit_subscribers": 497353,
          "created_utc": 1752167877,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/wjksocsoy2cf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1920,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/wjksocsoy2cf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/wjksocsoy2cf1/DASHPlaylist.mpd?a=1754812733%2CNGQ4MDQ5ODdjNWY2M2Q4ZDkwMGJhMjlkNWM2N2FmMGMwZGRhMGZkOTYwMTcxYTA1NTEyNWJkZmE3NDZjZDJjNQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 20,
              "hls_url": "https://v.redd.it/wjksocsoy2cf1/HLSPlaylist.m3u8?a=1754812733%2CYzM3YzU3MzM0MTliOWE4N2M1OTM0ODBjYmZkODVkMTBkNGM1NjY0NzhlYjc4Yzk4N2JkNjkwNDY2NDQ2NjcwMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RekaAI/reka-flash-3.1 · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwgy9m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 87,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 87,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=18f6bb91bbe16150609da54a7de899f8385bcc2c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752164422,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/RekaAI/reka-flash-3.1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?auto=webp&amp;s=527e78e54ce9f81f747fa5d8da5d0b3e5c061035",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=599e68463353c67ca6e526699691302c49400ed9",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=729790fec8affd24ce49456ab22f9352e1a2139e",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0995f9e90aa7bdd0d2280a12556a84fda81cb45e",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c35dd881d1547a11e35792b26300d6dd95afdbaa",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=11d79042c75767b4724474ec466818aaf7b34865",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=58ea894910fae00bb9e7236d3aaa95cfdde4255d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lwgy9m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwgy9m/rekaairekaflash31_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/RekaAI/reka-flash-3.1",
          "subreddit_subscribers": 497353,
          "created_utc": 1752164422,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just add `/no_think` in the system prompt and the model will mostly stop reasoning \n\nYou can also add your own conditions like `when i write /nt it means /no_think` or `always /no_think except if i write /think` if the model is smart enough it will mostly follow your orders\n\nTested on qwen3",
          "author_fullname": "t2_rxgre5u8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tired of writing /no_think every time you prompt?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwwh8s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752204167,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just add &lt;code&gt;/no_think&lt;/code&gt; in the system prompt and the model will mostly stop reasoning &lt;/p&gt;\n\n&lt;p&gt;You can also add your own conditions like &lt;code&gt;when i write /nt it means /no_think&lt;/code&gt; or &lt;code&gt;always /no_think except if i write /think&lt;/code&gt; if the model is smart enough it will mostly follow your orders&lt;/p&gt;\n\n&lt;p&gt;Tested on qwen3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lwwh8s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Iq1pl",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwwh8s/tired_of_writing_no_think_every_time_you_prompt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwwh8s/tired_of_writing_no_think_every_time_you_prompt/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752204167,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying to understand why does something like say llama 3.1 8b need further instruction by something like alpaca? If you just load the base model and ask something of it it just responds with gibberish. If you train it with say even just 1000 samples of alpaca data it starts responding coherently. But why does that happen when the original is already trained on next token generation? The q/a instruction training is also next token generation why does a little nudge in the weights from alpaca or other small data sets suddenly get it to respond with coherent responses. When I've looked around in sites etc it just says the further instruction gets the model to align to respond but doesn't say why. How come a few samples (say just 1000 alpaca samples) of 'fine tuning' next token generation suddenly go from gibberish to coherent responses when that is also just doing next token generation as well. I get its training directed towards producing responses to questions so it would shift the weights towards that but the original next token training would have had similar q/a data sets in it already so why doesn't it already do it?\n\n\n\nJust for context i'm using [https://huggingface.co/meta-llama/Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) with lora to train on the alpaca data.",
          "author_fullname": "t2_1h4o7f23eh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why do base models give gibberish and need further 'fine tuning'",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwk84b",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752172244,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752172046,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to understand why does something like say llama 3.1 8b need further instruction by something like alpaca? If you just load the base model and ask something of it it just responds with gibberish. If you train it with say even just 1000 samples of alpaca data it starts responding coherently. But why does that happen when the original is already trained on next token generation? The q/a instruction training is also next token generation why does a little nudge in the weights from alpaca or other small data sets suddenly get it to respond with coherent responses. When I&amp;#39;ve looked around in sites etc it just says the further instruction gets the model to align to respond but doesn&amp;#39;t say why. How come a few samples (say just 1000 alpaca samples) of &amp;#39;fine tuning&amp;#39; next token generation suddenly go from gibberish to coherent responses when that is also just doing next token generation as well. I get its training directed towards producing responses to questions so it would shift the weights towards that but the original next token training would have had similar q/a data sets in it already so why doesn&amp;#39;t it already do it?&lt;/p&gt;\n\n&lt;p&gt;Just for context i&amp;#39;m using &lt;a href=\"https://huggingface.co/meta-llama/Llama-3.1-8B\"&gt;https://huggingface.co/meta-llama/Llama-3.1-8B&lt;/a&gt; with lora to train on the alpaca data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?auto=webp&amp;s=bb56ed1972a4758624102c003d823e903d87bcb2",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a573e4b70357cc304f738189e72c00b44622fc1",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e66f71d268dc1e20de7ad544103444fce5f85488",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b4002d63751a26c36e1b700aa599d15f0e652d30",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7bfdad6461fb7b0282d5a1a935024a36731858fd",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=724d0bec544adf6ea5bb67e4a291ff7a10fe78eb",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c8c9bcd52d928c8a6142e9d4bb6f78ddfe70d726",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwk84b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "QFGTrialByFire",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752172046,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\nWe’re a hospital building an on-prem system for health and medical data analytics using LLMs. Our setup includes an RTX 6000 Pro and a 5090, and we’re working with a $10~$19k budget.\n\nI have already tried Gemma3 on 5090 but can’t unleash the 96gb vram capabilities.\n\nWe’re looking to:\n\t•\tRun a large open-source LLM locally (currently putting eyes in llama4)\n\t•\tDo fine-tuning (LoRA or full) on structured clinical data and unstructured medical notes\n\t•\tUse the model for summarization, Q&amp;A, and EHR-related tasks\n\nWe’d love recommendations on:\n\t1.\tThe best large open-source LLM to use in this context\n\t2.\tHow much CPU matters for performance (inference + fine-tuning) alongside these GPUs\n\nWould really appreciate any suggestions based on real-world setups—especially if you’ve done similar work in the health/biomed space.\n\nThanks in advance!\n",
          "author_fullname": "t2_e4ojre534",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best large open-source LLM for health/medical data analytics (RTX 6000 Pro, $10k budget)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwrd38",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752191297,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752189389,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,\nWe’re a hospital building an on-prem system for health and medical data analytics using LLMs. Our setup includes an RTX 6000 Pro and a 5090, and we’re working with a $10~$19k budget.&lt;/p&gt;\n\n&lt;p&gt;I have already tried Gemma3 on 5090 but can’t unleash the 96gb vram capabilities.&lt;/p&gt;\n\n&lt;p&gt;We’re looking to:\n    • Run a large open-source LLM locally (currently putting eyes in llama4)\n    • Do fine-tuning (LoRA or full) on structured clinical data and unstructured medical notes\n    • Use the model for summarization, Q&amp;amp;A, and EHR-related tasks&lt;/p&gt;\n\n&lt;p&gt;We’d love recommendations on:\n    1.  The best large open-source LLM to use in this context\n    2.  How much CPU matters for performance (inference + fine-tuning) alongside these GPUs&lt;/p&gt;\n\n&lt;p&gt;Would really appreciate any suggestions based on real-world setups—especially if you’ve done similar work in the health/biomed space.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwrd38",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LeastExperience1579",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752189389,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "* **Chat**\n   * Explore and contribute to the open sourced GitHub Copilot Chat extension ([Read our blog post](https://code.visualstudio.com/blogs/2025/06/30/openSourceAIEditorFirstMilestone)).\n   * Generate custom instructions that reflect your project's conventions ([Show more](https://code.visualstudio.com/updates/v1_102#_generate-custom-instructions)).\n   * Use custom modes to tailor chat for tasks like planning or research ([Show more](https://code.visualstudio.com/updates/v1_102#_chat-mode-improvements)).\n   * Automatically approve selected terminal commands ([Show more](https://code.visualstudio.com/updates/v1_102#_terminal-auto-approval-experimental)).\n   * Edit and resubmit previous chat requests ([Show more](https://code.visualstudio.com/updates/v1_102#_edit-previous-requests-experimental)).\n* **MCP**\n   * MCP support is now generally available in VS Code ([Show more](https://code.visualstudio.com/updates/v1_102#_mcp-support-in-vs-code-is-generally-available)).\n   * Easily install and manage MCP servers with the MCP view and gallery ([Show more](https://code.visualstudio.com/updates/v1_102#_mcp-server-discovery-and-installation)).\n   * MCP servers as first-class resources in profiles and Settings Sync ([Show more](https://code.visualstudio.com/updates/v1_102#_mcp-servers-as-first-class-resources)).\n* **Editor experience**\n   * Delegate tasks to Copilot coding agent and let it handle them in the background ([Show more](https://code.visualstudio.com/updates/v1_102#_start-a-coding-agent-session-preview)).\n   * Scroll the editor on middle click ([Show more](https://code.visualstudio.com/updates/v1_102#_scroll-on-middle-click)).\n\n&gt;VS Code pm here in case there are any questions I am happy to answer.",
          "author_fullname": "t2_1k174c7o8k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "VS Code June 2025 (version 1.102)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwlw1j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE.jpeg?width=140&amp;height=69&amp;crop=140:69,smart&amp;auto=webp&amp;s=0c82605b9ec19de72cace6d5533da2aa5d6e0222",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752175989,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "code.visualstudio.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Explore and contribute to the open sourced GitHub Copilot Chat extension (&lt;a href=\"https://code.visualstudio.com/blogs/2025/06/30/openSourceAIEditorFirstMilestone\"&gt;Read our blog post&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;Generate custom instructions that reflect your project&amp;#39;s conventions (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_generate-custom-instructions\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;Use custom modes to tailor chat for tasks like planning or research (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_chat-mode-improvements\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;Automatically approve selected terminal commands (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_terminal-auto-approval-experimental\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;Edit and resubmit previous chat requests (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_edit-previous-requests-experimental\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;MCP&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;MCP support is now generally available in VS Code (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_mcp-support-in-vs-code-is-generally-available\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;Easily install and manage MCP servers with the MCP view and gallery (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_mcp-server-discovery-and-installation\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;MCP servers as first-class resources in profiles and Settings Sync (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_mcp-servers-as-first-class-resources\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Editor experience&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Delegate tasks to Copilot coding agent and let it handle them in the background (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_start-a-coding-agent-session-preview\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;Scroll the editor on middle click (&lt;a href=\"https://code.visualstudio.com/updates/v1_102#_scroll-on-middle-click\"&gt;Show more&lt;/a&gt;).&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;VS Code pm here in case there are any questions I am happy to answer.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://code.visualstudio.com/updates/v1_102",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE.jpeg?auto=webp&amp;s=e93357d092d26269a5ce43251b2869e329d68ae1",
                  "width": 1069,
                  "height": 534
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b41d571481b4e96109b9abbdd45f66cd0298931",
                    "width": 108,
                    "height": 53
                  },
                  {
                    "url": "https://external-preview.redd.it/U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d66d5052f2ad700bd7fb1a2ef87d2c080ecde87",
                    "width": 216,
                    "height": 107
                  },
                  {
                    "url": "https://external-preview.redd.it/U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f8dacd08bf9e94649d34590f77620cfedf76f859",
                    "width": 320,
                    "height": 159
                  },
                  {
                    "url": "https://external-preview.redd.it/U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fea22c6187bb94fe796b6c79160a51ef50be7f7d",
                    "width": 640,
                    "height": 319
                  },
                  {
                    "url": "https://external-preview.redd.it/U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=87831ccde168e2a6fd2b06da16fdd86be4b391e6",
                    "width": 960,
                    "height": 479
                  }
                ],
                "variants": {},
                "id": "U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwlw1j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "isidor_n",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwlw1j/vs_code_june_2025_version_1102/",
          "stickied": false,
          "url": "https://code.visualstudio.com/updates/v1_102",
          "subreddit_subscribers": 497353,
          "created_utc": 1752175989,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I all, I've been using Claude Coder at work for a while now and LOVE it. Is there any high quality alternatives where you can use local models / openAI endpoint(s)? ",
          "author_fullname": "t2_cnkw6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Source Claude Coder alternative?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lww2w9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752202934,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I all, I&amp;#39;ve been using Claude Coder at work for a while now and LOVE it. Is there any high quality alternatives where you can use local models / openAI endpoint(s)? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lww2w9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "maxwell321",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lww2w9/open_source_claude_coder_alternative/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lww2w9/open_source_claude_coder_alternative/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752202934,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "There is a new pull request to support GLM-4 MoE on VLLM.\n\nHopefully we will have a new powerful model!\n\n[https://github.com/vllm-project/vllm/pull/20736](https://github.com/vllm-project/vllm/pull/20736)",
          "author_fullname": "t2_hoxc8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4 MoE incoming",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw71av",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 144,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 144,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752134298,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There is a new pull request to support GLM-4 MoE on VLLM.&lt;/p&gt;\n\n&lt;p&gt;Hopefully we will have a new powerful model!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/vllm-project/vllm/pull/20736\"&gt;https://github.com/vllm-project/vllm/pull/20736&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/uLQUYJzfZFruZAn57VwKQmTVHdq10TT6JiYeU7Uj7yY.png?auto=webp&amp;s=b456605a4d6c184722438a164c5d25e6b2b287a6",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/uLQUYJzfZFruZAn57VwKQmTVHdq10TT6JiYeU7Uj7yY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=366b6dc79bf3c070bc858477ad20f59843aea2d0",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/uLQUYJzfZFruZAn57VwKQmTVHdq10TT6JiYeU7Uj7yY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=222aed5669cc36c67fede38661ffba9d8551d46d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/uLQUYJzfZFruZAn57VwKQmTVHdq10TT6JiYeU7Uj7yY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a3b025279090a032a0033b3289198754b85a79e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/uLQUYJzfZFruZAn57VwKQmTVHdq10TT6JiYeU7Uj7yY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=def8751711047e8e98ec850bc828c6bc06e00bcc",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/uLQUYJzfZFruZAn57VwKQmTVHdq10TT6JiYeU7Uj7yY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b546242d637804a00d3ec67f71657e3275d4bdb8",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/uLQUYJzfZFruZAn57VwKQmTVHdq10TT6JiYeU7Uj7yY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a439eb185d750fdba01a91c26dd402622509b7a4",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "uLQUYJzfZFruZAn57VwKQmTVHdq10TT6JiYeU7Uj7yY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lw71av",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "matteogeniaccio",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw71av/glm4_moe_incoming/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw71av/glm4_moe_incoming/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752134298,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm running Ollama &amp; OpenWebUI on a headless Linux server, as Docker (with Compose) containers, with an NVIDIA GPU. This setup works great, but I want to add MCP servers to my environment, to improve the results from Ollama invocations.\n\nThe [documentation for OpenWebUI](https://docs.openwebui.com/openapi-servers/mcp/) suggests running a single container per MCP server. However, that will get unwieldy quickly.\n\nHow are other people exposing multiple MCP servers as a *singular* Docker service, as part of their Docker Compose stack?",
          "author_fullname": "t2_lophktmpt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Manage multiple MCP servers for Ollama + OpenWebUI as Docker service",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lx0b5w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752217473,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running Ollama &amp;amp; OpenWebUI on a headless Linux server, as Docker (with Compose) containers, with an NVIDIA GPU. This setup works great, but I want to add MCP servers to my environment, to improve the results from Ollama invocations.&lt;/p&gt;\n\n&lt;p&gt;The &lt;a href=\"https://docs.openwebui.com/openapi-servers/mcp/\"&gt;documentation for OpenWebUI&lt;/a&gt; suggests running a single container per MCP server. However, that will get unwieldy quickly.&lt;/p&gt;\n\n&lt;p&gt;How are other people exposing multiple MCP servers as a &lt;em&gt;singular&lt;/em&gt; Docker service, as part of their Docker Compose stack?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lx0b5w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "trevorstr",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx0b5w/manage_multiple_mcp_servers_for_ollama_openwebui/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx0b5w/manage_multiple_mcp_servers_for_ollama_openwebui/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752217473,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Some people think AI agents are hype and glorified workflows.\n\nBut agents that actually work don’t try to be JARVIS, not yet. The ones that succeed stick to structured workflows. And that’s not a bad thing. When I was in school, we studied Little Computer 3 to understand how computer architecture starts with state machines. I attached that diagram, and that's just the simplest computer architecture just for education purpose.\n\nA workflow is just a finite state machine (FSM) with memory and tool use. LLMs are surprisingly good at that. These agents complete real tasks that used to take human time and effort.\n\nRetell AI is a great example. It handles real phone calls for things like loans and pharmacy refills. It knows what step it’s on, when to speak, when to listen, and when to escalate. That kind of structure makes it reliable. Simplify is doing the same for job applications. It finds postings, autofills forms, tracks everything, and updates the user. These are clear, scoped workflows with success criteria, and that’s where LLMs perform really well.\n\nPlugging LLM in workflows isn’t enough. The teams behind these tools constantly monitor what’s happening. They trace every call, evaluate outputs, catch failure patterns, and improve prompts. I believe they have a very complicated workflow, and tools like Keywords AI make that kind of observability easy. Without it, even a well-built agent will drift.\n\nNot every agent is magic. But the ones that work? They’re already saving time, money, and headcount. That's what we need in the current state.",
          "author_fullname": "t2_1pnlpczpqa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Workflows aren’t a weakness in AI agents, they’re why they work",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwniq0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752179821,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some people think AI agents are hype and glorified workflows.&lt;/p&gt;\n\n&lt;p&gt;But agents that actually work don’t try to be JARVIS, not yet. The ones that succeed stick to structured workflows. And that’s not a bad thing. When I was in school, we studied Little Computer 3 to understand how computer architecture starts with state machines. I attached that diagram, and that&amp;#39;s just the simplest computer architecture just for education purpose.&lt;/p&gt;\n\n&lt;p&gt;A workflow is just a finite state machine (FSM) with memory and tool use. LLMs are surprisingly good at that. These agents complete real tasks that used to take human time and effort.&lt;/p&gt;\n\n&lt;p&gt;Retell AI is a great example. It handles real phone calls for things like loans and pharmacy refills. It knows what step it’s on, when to speak, when to listen, and when to escalate. That kind of structure makes it reliable. Simplify is doing the same for job applications. It finds postings, autofills forms, tracks everything, and updates the user. These are clear, scoped workflows with success criteria, and that’s where LLMs perform really well.&lt;/p&gt;\n\n&lt;p&gt;Plugging LLM in workflows isn’t enough. The teams behind these tools constantly monitor what’s happening. They trace every call, evaluate outputs, catch failure patterns, and improve prompts. I believe they have a very complicated workflow, and tools like Keywords AI make that kind of observability easy. Without it, even a well-built agent will drift.&lt;/p&gt;\n\n&lt;p&gt;Not every agent is magic. But the ones that work? They’re already saving time, money, and headcount. That&amp;#39;s what we need in the current state.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwniq0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Main-Fisherman-2075",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwniq0/workflows_arent_a_weakness_in_ai_agents_theyre/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwniq0/workflows_arent_a_weakness_in_ai_agents_theyre/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752179821,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[UGI-Leaderboard](https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard)\n\nIt has a lower willingness (W/10) than Grok-3, so it'll refuse more, but it makes up for that because of its massive intelligence (NatInt) increase.\n\nLooking through its political stats, it is less progressive with social issues than Grok-3, but it is overall more left leaning because of things like it being less religious, less bioconservative, and less nationalistic.\n\nWhen comparing other proprietary models, Grok 1, 2, and 4 stick out the most for being the least socially progressive.",
          "author_fullname": "t2_e79ya7rd7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Added Grok-4 to the UGI-Leaderboard",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 48,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw9ch2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 77,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 77,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/6gDMLMNjgN3fCDeA-9V_AcrHvobFBdL-txWjdpF4agU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752143531,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard\"&gt;UGI-Leaderboard&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It has a lower willingness (W/10) than Grok-3, so it&amp;#39;ll refuse more, but it makes up for that because of its massive intelligence (NatInt) increase.&lt;/p&gt;\n\n&lt;p&gt;Looking through its political stats, it is less progressive with social issues than Grok-3, but it is overall more left leaning because of things like it being less religious, less bioconservative, and less nationalistic.&lt;/p&gt;\n\n&lt;p&gt;When comparing other proprietary models, Grok 1, 2, and 4 stick out the most for being the least socially progressive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6g4lpxpay0cf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6g4lpxpay0cf1.png?auto=webp&amp;s=329a51335cf9e548b3393e7c3a8812515113036b",
                  "width": 1862,
                  "height": 651
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6g4lpxpay0cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=08b4c4d0535521b882b9b633a0c70bc6c0e3794f",
                    "width": 108,
                    "height": 37
                  },
                  {
                    "url": "https://preview.redd.it/6g4lpxpay0cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=33b3e33da215e6ff6c46f95c22ad7f8b8ef2d6b4",
                    "width": 216,
                    "height": 75
                  },
                  {
                    "url": "https://preview.redd.it/6g4lpxpay0cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2ade97a3b108109d6a571fc32964e5f2f459e5bd",
                    "width": 320,
                    "height": 111
                  },
                  {
                    "url": "https://preview.redd.it/6g4lpxpay0cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=696d0258f779029c9cde582e77066bdfaf475731",
                    "width": 640,
                    "height": 223
                  },
                  {
                    "url": "https://preview.redd.it/6g4lpxpay0cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=795b8752660d7157827c8348b7f4e78ba409a35b",
                    "width": 960,
                    "height": 335
                  },
                  {
                    "url": "https://preview.redd.it/6g4lpxpay0cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f15f6b5a9d85155e4d4dd4c0cbf1094eb604acb6",
                    "width": 1080,
                    "height": 377
                  }
                ],
                "variants": {},
                "id": "w74Yuu1shaH3gNqOgsCpN1drersHrujFVDnSXw05WpM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lw9ch2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DontPlanToEnd",
          "discussion_type": null,
          "num_comments": 61,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw9ch2/added_grok4_to_the_ugileaderboard/",
          "stickied": false,
          "url": "https://i.redd.it/6g4lpxpay0cf1.png",
          "subreddit_subscribers": 497353,
          "created_utc": 1752143531,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "xAI has just announced its smartest AI models to date: Grok 4 and Grok 4 Heavy. Both are subscription-based, with Grok 4 Heavy priced at approximately $300 per month. Excited to see what these new models can do! \n",
          "author_fullname": "t2_1gjiyteyd7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Grok 4 Benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 88,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "vkta3pkjczbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/vkta3pkjczbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=985e0b38c83e43fccece7a240817ebe6a1a8eba8"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/vkta3pkjczbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=528fc189ced27a3f9fc86b8588234aaea81ef748"
                },
                {
                  "y": 167,
                  "x": 320,
                  "u": "https://preview.redd.it/vkta3pkjczbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1bca4291fd82942ef284753cb2df4fd685c7ce08"
                },
                {
                  "y": 334,
                  "x": 640,
                  "u": "https://preview.redd.it/vkta3pkjczbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8ea9f5a5e718ac354b76b52c74fad37b69f24253"
                },
                {
                  "y": 501,
                  "x": 960,
                  "u": "https://preview.redd.it/vkta3pkjczbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c84964d8d05185d68d720b3ac5c6713e873bd6b8"
                },
                {
                  "y": 563,
                  "x": 1080,
                  "u": "https://preview.redd.it/vkta3pkjczbf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=388b13184d6beb98d4c046c812f8d937839df00f"
                }
              ],
              "s": {
                "y": 1069,
                "x": 2048,
                "u": "https://preview.redd.it/vkta3pkjczbf1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;s=4be479618f0d9fbf867557ee57600e698060eba9"
              },
              "id": "vkta3pkjczbf1"
            },
            "x5h8ytejczbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 57,
                  "x": 108,
                  "u": "https://preview.redd.it/x5h8ytejczbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8767e1932373c32a8fb779261021d7cd279c698e"
                },
                {
                  "y": 114,
                  "x": 216,
                  "u": "https://preview.redd.it/x5h8ytejczbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4ca74f22b86dd65a5e91be0d64dff56d64552603"
                },
                {
                  "y": 170,
                  "x": 320,
                  "u": "https://preview.redd.it/x5h8ytejczbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cbaa8de03512446cb0c97a21432b344d8db060a2"
                },
                {
                  "y": 340,
                  "x": 640,
                  "u": "https://preview.redd.it/x5h8ytejczbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a84fc79d39eff248a0ad46509f6c96fe714f12d3"
                },
                {
                  "y": 510,
                  "x": 960,
                  "u": "https://preview.redd.it/x5h8ytejczbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b1dc6dc558944077b5f9fde8266051936fbb2134"
                },
                {
                  "y": 574,
                  "x": 1080,
                  "u": "https://preview.redd.it/x5h8ytejczbf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1672b7097ff282ee5d25785e5c0ec859d7f2db17"
                }
              ],
              "s": {
                "y": 1089,
                "x": 2048,
                "u": "https://preview.redd.it/x5h8ytejczbf1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;s=8d33e2023cc464790627b78f2e327535fecea7f7"
              },
              "id": "x5h8ytejczbf1"
            },
            "ymt1ov4jczbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/ymt1ov4jczbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9f66af2f6f502f5deebd5807a443121ea47b84a4"
                },
                {
                  "y": 136,
                  "x": 216,
                  "u": "https://preview.redd.it/ymt1ov4jczbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6071072e3fab55789582cd426957efb12f641de6"
                },
                {
                  "y": 202,
                  "x": 320,
                  "u": "https://preview.redd.it/ymt1ov4jczbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ea8768c25e63c564169611e307c9128259dac03f"
                },
                {
                  "y": 404,
                  "x": 640,
                  "u": "https://preview.redd.it/ymt1ov4jczbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b5ceb7bf5556f216baf44091df64eb73c96b6e"
                },
                {
                  "y": 607,
                  "x": 960,
                  "u": "https://preview.redd.it/ymt1ov4jczbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c7671eb13ada8539b1019df31cba90aa7b332a8d"
                },
                {
                  "y": 683,
                  "x": 1080,
                  "u": "https://preview.redd.it/ymt1ov4jczbf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5db18304c76f8400621584f31f5436391f72258a"
                }
              ],
              "s": {
                "y": 1814,
                "x": 2868,
                "u": "https://preview.redd.it/ymt1ov4jczbf1.jpg?width=2868&amp;format=pjpg&amp;auto=webp&amp;s=c26c33ad3c76200371c281aa06a385a63c1fcde0"
              },
              "id": "ymt1ov4jczbf1"
            }
          },
          "name": "t3_1lw4eej",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "ups": 203,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "ymt1ov4jczbf1",
                "id": 702623499
              },
              {
                "caption": "",
                "media_id": "x5h8ytejczbf1",
                "id": 702623500
              },
              {
                "caption": "",
                "media_id": "vkta3pkjczbf1",
                "id": 702623501
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 203,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/MLSe5RpW1tkFFRdIT2JZeSSIlKUblh8PvP8Gk8nPH1E.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752124109,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;xAI has just announced its smartest AI models to date: Grok 4 and Grok 4 Heavy. Both are subscription-based, with Grok 4 Heavy priced at approximately $300 per month. Excited to see what these new models can do! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lw4eej",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lw4eej",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DigitusDesigner",
          "discussion_type": null,
          "num_comments": 164,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw4eej/grok_4_benchmarks/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lw4eej",
          "subreddit_subscribers": 497353,
          "created_utc": 1752124109,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We've just released what might be the most comprehensive documentation of AI data quality evaluation metrics available. This covers everything from pre-training data assessment to multimodal evaluation.\n\nWhat's included:\n\n* 50+ evaluation metrics across text, image, and multimodal data\n* Academic citations for every metric (RedPajama, CLIP, NIMA, etc.)\n* Rule-based and LLM-based evaluation approaches\n* Practical usage examples and API documentation\n\nKey categories:\n\n* Text Quality: Completeness, Fluency, Relevance, Effectiveness\n* Image Quality: Clarity, Similarity, Validity\n* Security: Political sensitivity, prohibited content, harmful information\n* Classification: Topic categorization, content classification\n\nThis is particularly useful for:\n\n* Data scientists working on model training\n* Researchers needing standardized evaluation frameworks\n* Anyone dealing with large-scale data quality assessment\n\nThe documentation includes detailed academic references and practical implementation examples. All open source and ready to use.\n\nLink: [https://github.com/MigoXLab/dingo/blob/dev/docs/metrics.md](https://github.com/MigoXLab/dingo/blob/dev/docs/metrics.md)\n\nThoughts? What metrics do you find most valuable in your work?",
          "author_fullname": "t2_nrnbu0gz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[OC] Comprehensive AI Data Quality Metrics Documentation - 50+ Evaluation Metrics with Academic Sources",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwuzjo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752199736,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve just released what might be the most comprehensive documentation of AI data quality evaluation metrics available. This covers everything from pre-training data assessment to multimodal evaluation.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s included:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;50+ evaluation metrics across text, image, and multimodal data&lt;/li&gt;\n&lt;li&gt;Academic citations for every metric (RedPajama, CLIP, NIMA, etc.)&lt;/li&gt;\n&lt;li&gt;Rule-based and LLM-based evaluation approaches&lt;/li&gt;\n&lt;li&gt;Practical usage examples and API documentation&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Key categories:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Text Quality: Completeness, Fluency, Relevance, Effectiveness&lt;/li&gt;\n&lt;li&gt;Image Quality: Clarity, Similarity, Validity&lt;/li&gt;\n&lt;li&gt;Security: Political sensitivity, prohibited content, harmful information&lt;/li&gt;\n&lt;li&gt;Classification: Topic categorization, content classification&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This is particularly useful for:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data scientists working on model training&lt;/li&gt;\n&lt;li&gt;Researchers needing standardized evaluation frameworks&lt;/li&gt;\n&lt;li&gt;Anyone dealing with large-scale data quality assessment&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The documentation includes detailed academic references and practical implementation examples. All open source and ready to use.&lt;/p&gt;\n\n&lt;p&gt;Link: &lt;a href=\"https://github.com/MigoXLab/dingo/blob/dev/docs/metrics.md\"&gt;https://github.com/MigoXLab/dingo/blob/dev/docs/metrics.md&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thoughts? What metrics do you find most valuable in your work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/c340hQeOye9TxtgDQ0X1CY8WX4WAKvr0pumN26Zmq9o.png?auto=webp&amp;s=56ab51e042e97f89e887aa652cd019dae9c3daa0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/c340hQeOye9TxtgDQ0X1CY8WX4WAKvr0pumN26Zmq9o.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=761a7fb258827c359161c668421aeb498ed39406",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/c340hQeOye9TxtgDQ0X1CY8WX4WAKvr0pumN26Zmq9o.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5c0c73ddeb762b9a74a2657a3000eba0d4cc994",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/c340hQeOye9TxtgDQ0X1CY8WX4WAKvr0pumN26Zmq9o.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=23e31bf29b7085bf3c31b2fb70388afdf309c64b",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/c340hQeOye9TxtgDQ0X1CY8WX4WAKvr0pumN26Zmq9o.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e55312dd2a2d390a3e35bd9c65919e2618d36021",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/c340hQeOye9TxtgDQ0X1CY8WX4WAKvr0pumN26Zmq9o.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f5e8dbc4d83ea236579446db22e044925ce723d3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/c340hQeOye9TxtgDQ0X1CY8WX4WAKvr0pumN26Zmq9o.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=53c82d075ef31239b56bc72982cc480d4843520a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "c340hQeOye9TxtgDQ0X1CY8WX4WAKvr0pumN26Zmq9o"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwuzjo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "chupei0",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwuzjo/oc_comprehensive_ai_data_quality_metrics/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwuzjo/oc_comprehensive_ai_data_quality_metrics/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752199736,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking for both a good app and an availability of a good and capable LLM. \nThanks! ",
          "author_fullname": "t2_v7g4kt4q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help me find the best Android app for running LLMs locally",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwwuwq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752205352,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for both a good app and an availability of a good and capable LLM. \nThanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwwuwq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DanielD2724",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwwuwq/help_me_find_the_best_android_app_for_running/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwwuwq/help_me_find_the_best_android_app_for_running/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752205352,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_21qaqh1p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OpenAI's open source LLM is a reasoning model, coming Next Thursday!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvr3ym",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 978,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 978,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/sUJ6lB6ZDEcKStX1WdRzKRM8EK_d-_hgFEOIZhdoHt0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752087510,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/q01afp6lbwbf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/q01afp6lbwbf1.png?auto=webp&amp;s=0cbd17fbc9a41c400eb85d31978f75ff70df9ddc",
                  "width": 863,
                  "height": 867
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/q01afp6lbwbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6502fad51b4a09ae4274df4e9ed45962541004cf",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/q01afp6lbwbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2253b99563fe69f4f533f1f3ce4fa62b8d4b4ac5",
                    "width": 216,
                    "height": 217
                  },
                  {
                    "url": "https://preview.redd.it/q01afp6lbwbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=96e87778edaf7cf837968133bfc385a30b74c9bf",
                    "width": 320,
                    "height": 321
                  },
                  {
                    "url": "https://preview.redd.it/q01afp6lbwbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3e9bd873a7a7d4e956171cdc1ac61d5f5cae52e7",
                    "width": 640,
                    "height": 642
                  }
                ],
                "variants": {},
                "id": "YzEa9NJG57828WaQr473XCycHU_4jPbLlMIMENhyJMQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lvr3ym",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dulldata",
          "discussion_type": null,
          "num_comments": 265,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/",
          "stickied": false,
          "url": "https://i.redd.it/q01afp6lbwbf1.png",
          "subreddit_subscribers": 497353,
          "created_utc": 1752087510,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks, I am quite new to the local model space and having a hard time to decide which models to invest further in (by giving more cores/gpu focus toward - and add docs for RAG). \n\nMain goals:\n\n\\- Completely offline models for privacy / security\n\n\\- High token count and focused on best English writing / summarizations of large text or documents. \n\n\\- Crafting emails given a source and context",
          "author_fullname": "t2_i5gbl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LM Studio model recommendation for writing, emails, and general summarizations",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwxnf0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752207904,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I am quite new to the local model space and having a hard time to decide which models to invest further in (by giving more cores/gpu focus toward - and add docs for RAG). &lt;/p&gt;\n\n&lt;p&gt;Main goals:&lt;/p&gt;\n\n&lt;p&gt;- Completely offline models for privacy / security&lt;/p&gt;\n\n&lt;p&gt;- High token count and focused on best English writing / summarizations of large text or documents. &lt;/p&gt;\n\n&lt;p&gt;- Crafting emails given a source and context&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwxnf0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vdog313",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwxnf0/lm_studio_model_recommendation_for_writing_emails/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwxnf0/lm_studio_model_recommendation_for_writing_emails/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752207904,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_qjpsv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Phi-4-mini-flash-reasoning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw3729",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "#93b1ba",
          "ups": 174,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 174,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=80e6b8d263960de48dbffc84a7d614f7d4381b87",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752120032,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?auto=webp&amp;s=f60794d5e67107c2691276e7de5249c893966a7d",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=121dfa243da145563b7fad4abe0571ae415c0f2e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=33741334973d3270c726098fa1178a0754f7488b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2419000ac55c1ee0561885437f8b594342cf8ed9",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a162d9d243ab8293c0214f3e9bf055ec2af6d514",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1aab3419db55a9c46f6044faa18576d3c0a1fd01",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0d4ebfa6e74a4ed84b2c017c38248f9b55998dc4",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lw3729",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ninjasaid13",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lw3729/phi4miniflashreasoning/",
          "stickied": false,
          "url": "https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning",
          "subreddit_subscribers": 497353,
          "created_utc": 1752120032,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1a6diqhz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MAXSUN preparing all-Intel Mini Station: up to Core Ultra 9 285HX and two Arc Pro B60 GPU - VideoCardz.com",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwm3w0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?width=140&amp;height=72&amp;crop=140:72,smart&amp;auto=webp&amp;s=82d9d63f02ebef6c18b91cb920b6680936a61a2e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752176508,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "videocardz.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://videocardz.com/newz/maxsun-preparing-all-intel-mini-station-up-to-core-ultra-9-285hx-and-two-arc-pro-b60-gpu",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?auto=webp&amp;s=b96f24d56654376cfb89dab8e48912779471be27",
                  "width": 2000,
                  "height": 1040
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=469ff89e7c00f97d5ba6ee9c12917b5f86debf38",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f7c469ec7711eb7038015668001068e972213af",
                    "width": 216,
                    "height": 112
                  },
                  {
                    "url": "https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a181e8b20e1c0903f3490dbff3dd98d3ee80b613",
                    "width": 320,
                    "height": 166
                  },
                  {
                    "url": "https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eabcea64775657596068577ad159149b4f0dd034",
                    "width": 640,
                    "height": 332
                  },
                  {
                    "url": "https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cea11dfdb6d7b094bbff52a04dfac13d1ca5e944",
                    "width": 960,
                    "height": 499
                  },
                  {
                    "url": "https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=77500a8858594b33f2a21f3033b5fb41d03548b2",
                    "width": 1080,
                    "height": 561
                  }
                ],
                "variants": {},
                "id": "6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwm3w0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EasternBeyond",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwm3w0/maxsun_preparing_allintel_mini_station_up_to_core/",
          "stickied": false,
          "url": "https://videocardz.com/newz/maxsun-preparing-all-intel-mini-station-up-to-core-ultra-9-285hx-and-two-arc-pro-b60-gpu",
          "subreddit_subscribers": 497353,
          "created_utc": 1752176508,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Tried to checkout the website for Ai-first Comet browser from Perplexity.\nWas shown this page.\n\nI understand it’s only rolled out to their $200 paying Pro customers.\nBut a better 403 page would be nice. \nJust a heads up to the Perplexity team.\n\nAlso waiting for preview access to this browser. 😉",
          "author_fullname": "t2_t0syffr8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Comet (AI first) browser from Perplexity needs better 403 page",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 121,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lx0e8i",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/lV_lnjDMCfemLDWXIPYIENGhT2m5uWZQ4VsieizPiCo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752217797,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tried to checkout the website for Ai-first Comet browser from Perplexity.\nWas shown this page.&lt;/p&gt;\n\n&lt;p&gt;I understand it’s only rolled out to their $200 paying Pro customers.\nBut a better 403 page would be nice. \nJust a heads up to the Perplexity team.&lt;/p&gt;\n\n&lt;p&gt;Also waiting for preview access to this browser. 😉&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/76b20ni437cf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/76b20ni437cf1.jpeg?auto=webp&amp;s=c8f1dbb12e944b1edaba830e700e6b5f44945c4c",
                  "width": 1179,
                  "height": 1023
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/76b20ni437cf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7b0f54d5e4850a778c1d877746eba43bc6a714a7",
                    "width": 108,
                    "height": 93
                  },
                  {
                    "url": "https://preview.redd.it/76b20ni437cf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b320e6e7f2c00feebf009bfaa3f614b184d301c7",
                    "width": 216,
                    "height": 187
                  },
                  {
                    "url": "https://preview.redd.it/76b20ni437cf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc79697dba1311e4ab6811ede05978a9ffb0b277",
                    "width": 320,
                    "height": 277
                  },
                  {
                    "url": "https://preview.redd.it/76b20ni437cf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9cf1504fb9eadd9adb55bcb655eaf7a7bf533006",
                    "width": 640,
                    "height": 555
                  },
                  {
                    "url": "https://preview.redd.it/76b20ni437cf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=15c89f80570d5dac52e09a40308cff6d5b832dad",
                    "width": 960,
                    "height": 832
                  },
                  {
                    "url": "https://preview.redd.it/76b20ni437cf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e76d81f6433b56115741ab9795d236db715007e5",
                    "width": 1080,
                    "height": 937
                  }
                ],
                "variants": {},
                "id": "p-AUWyAojHyWJqEHf2ZDNMt2MlN7Au9G2_ParjIhGEE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lx0e8i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NoobMLDude",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx0e8i/comet_ai_first_browser_from_perplexity_needs/",
          "stickied": false,
          "url": "https://i.redd.it/76b20ni437cf1.jpeg",
          "subreddit_subscribers": 497353,
          "created_utc": 1752217797,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_dyvrh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Possible size of new the open model from openai",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 96,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvwya4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 342,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 342,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/SWE3wKhFq64W19U8vrSnM4JhB3rrFvjK8ka3DKWgysk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752101694,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/622w5dyvhxbf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/622w5dyvhxbf1.png?auto=webp&amp;s=a2c619e25718a02777bbfccf1e457faeec66291e",
                  "width": 1080,
                  "height": 746
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/622w5dyvhxbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=69c3323c05b9e9e24c72ce6d4331170952c6539b",
                    "width": 108,
                    "height": 74
                  },
                  {
                    "url": "https://preview.redd.it/622w5dyvhxbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e2058d759bbe6568a23d5ba18b34f5d8e8f676b3",
                    "width": 216,
                    "height": 149
                  },
                  {
                    "url": "https://preview.redd.it/622w5dyvhxbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1585e0e58618c37cdfe1fdba6c0c1fbcd64e53c3",
                    "width": 320,
                    "height": 221
                  },
                  {
                    "url": "https://preview.redd.it/622w5dyvhxbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f278161d7e564140ede28f9eff15dc776e5ab6df",
                    "width": 640,
                    "height": 442
                  },
                  {
                    "url": "https://preview.redd.it/622w5dyvhxbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=15aac87a5266c6f4637f48e9a986a0d830cea8e5",
                    "width": 960,
                    "height": 663
                  },
                  {
                    "url": "https://preview.redd.it/622w5dyvhxbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=77bb6536d895705fee43ebe991bd07b835b00a2a",
                    "width": 1080,
                    "height": 746
                  }
                ],
                "variants": {},
                "id": "8YmohOSgc8VabZC-CUEtYfc1-XLSW8tYibkJW3Qz4Ac"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lvwya4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "celsowm",
          "discussion_type": null,
          "num_comments": 102,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvwya4/possible_size_of_new_the_open_model_from_openai/",
          "stickied": false,
          "url": "https://i.redd.it/622w5dyvhxbf1.png",
          "subreddit_subscribers": 497353,
          "created_utc": 1752101694,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🧠📝 Research [Blog post](https://huggingface.co/blog/AI-MO/kimina-prover)\n\n🚀 Demo: [https://demo.projectnumina.ai/](https://demo.projectnumina.ai/)\n\n[🤗](https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5) Models (72B, 8B or 1.7B) - [🤗](https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5) [HuggingFace](https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5)\n\n\n72B with Test-time RL pipeline gets 92.2% on miniF2F.\n\nPass@32 for each size:\n\n* 72B → 84.0% (86.4% with error-fixing)\n* 8B → 78.3%\n* 1.7B → 73.4%\n\n8B/1.7B are Qwen 3 with 72B distilled into them.",
          "author_fullname": "t2_4avfsbdbf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimina Prover - Test-time RL to reach 92.2% on miniF2F",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwcixn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752153536,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🧠📝 Research &lt;a href=\"https://huggingface.co/blog/AI-MO/kimina-prover\"&gt;Blog post&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;🚀 Demo: &lt;a href=\"https://demo.projectnumina.ai/\"&gt;https://demo.projectnumina.ai/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5\"&gt;🤗&lt;/a&gt; Models (72B, 8B or 1.7B) - &lt;a href=\"https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5\"&gt;🤗&lt;/a&gt; &lt;a href=\"https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5\"&gt;HuggingFace&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;72B with Test-time RL pipeline gets 92.2% on miniF2F.&lt;/p&gt;\n\n&lt;p&gt;Pass@32 for each size:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;72B → 84.0% (86.4% with error-fixing)&lt;/li&gt;\n&lt;li&gt;8B → 78.3%&lt;/li&gt;\n&lt;li&gt;1.7B → 73.4%&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;8B/1.7B are Qwen 3 with 72B distilled into them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/iF34aV5P23-aFKb2pKuUTTK8P3zuyZ-3Sfm4GeLU6Fs.png?auto=webp&amp;s=43ca537868c03110c79ad9406d2a1c7d09b9226d",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/iF34aV5P23-aFKb2pKuUTTK8P3zuyZ-3Sfm4GeLU6Fs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4fcf60d1352e405fba05a4101718f49e5efbe37c",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/iF34aV5P23-aFKb2pKuUTTK8P3zuyZ-3Sfm4GeLU6Fs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=270d774025925d11d35101182605fb1d0c4c4afa",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/iF34aV5P23-aFKb2pKuUTTK8P3zuyZ-3Sfm4GeLU6Fs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f478c1ae527103176857bfed980a9d83621f5195",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/iF34aV5P23-aFKb2pKuUTTK8P3zuyZ-3Sfm4GeLU6Fs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=407ca479fabf50d67111673880368a5a4073cb48",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/iF34aV5P23-aFKb2pKuUTTK8P3zuyZ-3Sfm4GeLU6Fs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4726149801837a40c84050900b055b80fc9d186f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/iF34aV5P23-aFKb2pKuUTTK8P3zuyZ-3Sfm4GeLU6Fs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=597e3473da5b4e9fc349358114dd7a90bbc86343",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "iF34aV5P23-aFKb2pKuUTTK8P3zuyZ-3Sfm4GeLU6Fs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lwcixn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "frunkp",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwcixn/kimina_prover_testtime_rl_to_reach_922_on_minif2f/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwcixn/kimina_prover_testtime_rl_to_reach_922_on_minif2f/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752153536,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone, my goal is to finetune an embedding model for the domain of railway engineering. I was wondering if you had recommendation.\n\nI tried a few things and I am now considering the option of filtering fine web using keywords.",
          "author_fullname": "t2_38ilynpn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a domain specific dataset",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwphbh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752184618,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, my goal is to finetune an embedding model for the domain of railway engineering. I was wondering if you had recommendation.&lt;/p&gt;\n\n&lt;p&gt;I tried a few things and I am now considering the option of filtering fine web using keywords.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwphbh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "T2WIN",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwphbh/building_a_domain_specific_dataset/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwphbh/building_a_domain_specific_dataset/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752184618,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We have built an extensible open source platform that enables developers to build, run and integrate AI agents into their applications and deliver AI native experiences all running locally on phones.\n\nThe SDK is lightweight built upon Executorch/ONNX and provides a higher level abstraction for developers to integrate in Kotlin or Swift. The AI workflow is orchestrated via Python which is natively supported as part of the on-device SDK. We currently support Llama 3.2 1B, Qwen 3 0.6B (tool-calling), Gemini Nano and soon Gemma 3n.\n\nWe have also created an Agent marketplace which provides plug and play agents and would love to get contributions from this community. \n\n[Here](https://github.com/NimbleEdge/deliteAI/tree/main/nimblenet_py/simulation_assets) are some example Python scripts for both traditional ML and AI workloads - note that the Kotlin/Swift layer can invoke these python functions and vice-versa which enables tool calling for both dynamic context and actions in the app.\n\nYou can also check out our open-source on-device [AI assistant](https://github.com/NimbleEdge/assistant) built upon the “DeliteAI” platform. \n\nWe love to hear from you on our APIs and if you would like to contribute please join our Discord community (link in the comment below). ",
          "author_fullname": "t2_74zl16jw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DeliteAI: Open platform for building and running agents on Mobile",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwfn7n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=18dfb27d0448541028b35c87abdd2ec5cdcbb872",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752161306,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have built an extensible open source platform that enables developers to build, run and integrate AI agents into their applications and deliver AI native experiences all running locally on phones.&lt;/p&gt;\n\n&lt;p&gt;The SDK is lightweight built upon Executorch/ONNX and provides a higher level abstraction for developers to integrate in Kotlin or Swift. The AI workflow is orchestrated via Python which is natively supported as part of the on-device SDK. We currently support Llama 3.2 1B, Qwen 3 0.6B (tool-calling), Gemini Nano and soon Gemma 3n.&lt;/p&gt;\n\n&lt;p&gt;We have also created an Agent marketplace which provides plug and play agents and would love to get contributions from this community. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/NimbleEdge/deliteAI/tree/main/nimblenet_py/simulation_assets\"&gt;Here&lt;/a&gt; are some example Python scripts for both traditional ML and AI workloads - note that the Kotlin/Swift layer can invoke these python functions and vice-versa which enables tool calling for both dynamic context and actions in the app.&lt;/p&gt;\n\n&lt;p&gt;You can also check out our open-source on-device &lt;a href=\"https://github.com/NimbleEdge/assistant\"&gt;AI assistant&lt;/a&gt; built upon the “DeliteAI” platform. &lt;/p&gt;\n\n&lt;p&gt;We love to hear from you on our APIs and if you would like to contribute please join our Discord community (link in the comment below). &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/NimbleEdge/deliteAI",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?auto=webp&amp;s=a9dffc08e8f89577f0ef8cbadf6345ed58ebd6fe",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a09579064e0135c0a7c9c8b1f91ffc6f7c6d151",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c53e782a497e280f238c0ed65a3b9e899425ef4d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0c8a2e4992aade019738f29bafcc6adcb8548c08",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4a7922aa047ec08e04553915bf0c4265bcd75e35",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=786fec691eebcbe24932bf8ab178dc6075f03e85",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=980b1144f88939647513f1e4180755927a3ff31f",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwfn7n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Economy-Mud-6626",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwfn7n/deliteai_open_platform_for_building_and_running/",
          "stickied": false,
          "url": "https://github.com/NimbleEdge/deliteAI",
          "subreddit_subscribers": 497353,
          "created_utc": 1752161306,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been working on a collaborative database that is an MCP server.  You can use it to remember any type of data you define: diet and fitness history, work-related data, to-do lists, bookmarked links, journal entries, bugs in software projects, favorite books/movies. [See it in action.](https://youtu.be/zAxepwoONq0)\n\nIt’s called Dry (“don’t repeat yourself”).  Dry lets you:\n\n* Add long-term memories in Claude and other MCP clients that persist across chats.\n* Specify your own custom data type without any coding.\n* Automatically generate a full graphical user interface (tables, charts, maps, lists, etc.).  \n* Share with a team or keep it private. \n\nWe think that in the [long term](https://dry.ai/vision), memories like this will give AI assistants the scaffolding they need to replace most SaaS tools and apps.\n\nHere’s our alpha you can try: [ ](https://dry.ai/joinDryBuilder)[https://dry.ai/getClaudeMemory](https://dry.ai/getClaudeMemory)\n\nWould love feedback from anyone here. Are there features you'd want? What would you use this for? Happy to answer any questions! \n\nThanks.",
          "author_fullname": "t2_8divqlzb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MCP server that is a memory for MCP clients (AI assistants) with your custom data types + full UI + team sharing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwhy37",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752166788,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been working on a collaborative database that is an MCP server.  You can use it to remember any type of data you define: diet and fitness history, work-related data, to-do lists, bookmarked links, journal entries, bugs in software projects, favorite books/movies. &lt;a href=\"https://youtu.be/zAxepwoONq0\"&gt;See it in action.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It’s called Dry (“don’t repeat yourself”).  Dry lets you:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Add long-term memories in Claude and other MCP clients that persist across chats.&lt;/li&gt;\n&lt;li&gt;Specify your own custom data type without any coding.&lt;/li&gt;\n&lt;li&gt;Automatically generate a full graphical user interface (tables, charts, maps, lists, etc.).  &lt;/li&gt;\n&lt;li&gt;Share with a team or keep it private. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We think that in the &lt;a href=\"https://dry.ai/vision\"&gt;long term&lt;/a&gt;, memories like this will give AI assistants the scaffolding they need to replace most SaaS tools and apps.&lt;/p&gt;\n\n&lt;p&gt;Here’s our alpha you can try: &lt;a href=\"https://dry.ai/joinDryBuilder\"&gt; &lt;/a&gt;&lt;a href=\"https://dry.ai/getClaudeMemory\"&gt;https://dry.ai/getClaudeMemory&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love feedback from anyone here. Are there features you&amp;#39;d want? What would you use this for? Happy to answer any questions! &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Pe-2QF4HM1vrYYdhnBTkzpQ6BumurPivfxkVGOFtlGQ.jpeg?auto=webp&amp;s=fecaf7c855cdafe6fa5f25f36d32d776046a7575",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Pe-2QF4HM1vrYYdhnBTkzpQ6BumurPivfxkVGOFtlGQ.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e0a319bf64f07bdababd422731e8551d03324ac4",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/Pe-2QF4HM1vrYYdhnBTkzpQ6BumurPivfxkVGOFtlGQ.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6693b2aafb27f01d9d0b0e268956bbcc1227d445",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/Pe-2QF4HM1vrYYdhnBTkzpQ6BumurPivfxkVGOFtlGQ.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=681f6c08bfe85886d49c2712737283b6773b198f",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "Pe-2QF4HM1vrYYdhnBTkzpQ6BumurPivfxkVGOFtlGQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwhy37",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Jazzlike_Water4911",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwhy37/mcp_server_that_is_a_memory_for_mcp_clients_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwhy37/mcp_server_that_is_a_memory_for_mcp_clients_ai/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752166788,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The flattening of nuanced distinctions is part of the joke (pre-emptive disclaimer for the pedantic) \n\n* **Pheromone trails ↔ value functions / reward shaping** Both steer future exploration toward paths that historically looked good.\n* **Stochastic exploration** in ants (random walks with pheromone bias) ↔ **ε-greedy / entropy-regularised exploration** in RL.\n* **Updating pheromones over time** ↔ **policy/value updates** in RL or **gradient steps** in supervised fine-tuning.\n* **Demonstration pheromones** (ants following an experienced scout’s trail) ↔ **Learning from Demonstration**.",
          "author_fullname": "t2_k7mcz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvzonf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 142,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 142,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/zG0DO4HRBwmlgpN0eGz-KACaRlDjquI0nzb0s4yCKtQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752109320,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The flattening of nuanced distinctions is part of the joke (pre-emptive disclaimer for the pedantic) &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Pheromone trails ↔ value functions / reward shaping&lt;/strong&gt; Both steer future exploration toward paths that historically looked good.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Stochastic exploration&lt;/strong&gt; in ants (random walks with pheromone bias) ↔ &lt;strong&gt;ε-greedy / entropy-regularised exploration&lt;/strong&gt; in RL.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Updating pheromones over time&lt;/strong&gt; ↔ &lt;strong&gt;policy/value updates&lt;/strong&gt; in RL or &lt;strong&gt;gradient steps&lt;/strong&gt; in supervised fine-tuning.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Demonstration pheromones&lt;/strong&gt; (ants following an experienced scout’s trail) ↔ &lt;strong&gt;Learning from Demonstration&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/vq8hwq904ybf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/vq8hwq904ybf1.png?auto=webp&amp;s=d6cebc0aaa2411e35102122fe941ad8023b97179",
                  "width": 1536,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/vq8hwq904ybf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c92efb7f1f6b6ad387f774a711db4568891357e6",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/vq8hwq904ybf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=963e102fa350a9b226179f2d94f144e9f2b738f4",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/vq8hwq904ybf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=25bf5c52dc2e61bfd44a78fa7c5951ea6558c556",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://preview.redd.it/vq8hwq904ybf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d05add52383cb1c64996c7d198a25c8644d9f33f",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://preview.redd.it/vq8hwq904ybf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=816ed6cea0bb5e65645540fcced19afd8edc6b37",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/vq8hwq904ybf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e11e5400a7ed7b874178fdbea8f23e4075d30b36",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "j4x-d47033S34x5kvZfx721caGXGakmvOXqs2eSy9qw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lvzonf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "chitown160",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvzonf/httpsenwikipediaorgwikiant_colony_optimization/",
          "stickied": false,
          "url": "https://i.redd.it/vq8hwq904ybf1.png",
          "subreddit_subscribers": 497353,
          "created_utc": 1752109320,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have heard about it, but don't really see folks talking about using it nor does it have the same excitement as the other agentic toolkits.   As a matter of fact, anytime I hear about it, it's from someone working for Block who I'm following on social media.\n\nFor anyone using it, if you can compare it to other tools, how does it rate?  I'm particular curious in comparison to Aider, claude coder and gemini-cli",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone using Block's goose?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lww2ld",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752202911,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have heard about it, but don&amp;#39;t really see folks talking about using it nor does it have the same excitement as the other agentic toolkits.   As a matter of fact, anytime I hear about it, it&amp;#39;s from someone working for Block who I&amp;#39;m following on social media.&lt;/p&gt;\n\n&lt;p&gt;For anyone using it, if you can compare it to other tools, how does it rate?  I&amp;#39;m particular curious in comparison to Aider, claude coder and gemini-cli&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lww2ld",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lww2ld/anyone_using_blocks_goose/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lww2ld/anyone_using_blocks_goose/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752202911,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is anyone facing this issue? Do you know how to fit it? I am using unsloth q5 UD gguf\n\nThanks!",
          "author_fullname": "t2_az9jb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hunyuan responding with &lt;answer&gt; &lt;/answer&gt; tag on LMstudio",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwvuuv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752202276,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is anyone facing this issue? Do you know how to fit it? I am using unsloth q5 UD gguf&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwvuuv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kuane",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwvuuv/hunyuan_responding_with_answer_answer_tag_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwvuuv/hunyuan_responding_with_answer_answer_tag_on/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752202276,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey Everyone, I’ve been working on a project called Fissure. It is a personal AI server that runs on your own machine. The goal is to make it pretty simple to run a local LLM, access it from your phone or laptop, and keep everything private.   \n  \nRight now I’m focused on the desktop app, which runs your chosen model through Ollama and sets up remote access using Tailscale(have been looking into alternatives). Once it's running, you get a private Tailscale url you can plug into a mobile app or anything that can hit an api, and this works across devices as long as they’re on your Tailscale network. Its still pretty early but I’m trying to make this easier than putting together a bunch of tools, more of a one click or couple clicks to set up and get going.  \n  \nI'm curious if anyone else would find this useful, and what features or pain points you’d want solved. Id really appreciate any feedback or thoughts.",
          "author_fullname": "t2_1oqtfoffx3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local AI server with Ollama and Tailscale integration looking for feedback",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwvrev",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752201992,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Everyone, I’ve been working on a project called Fissure. It is a personal AI server that runs on your own machine. The goal is to make it pretty simple to run a local LLM, access it from your phone or laptop, and keep everything private.   &lt;/p&gt;\n\n&lt;p&gt;Right now I’m focused on the desktop app, which runs your chosen model through Ollama and sets up remote access using Tailscale(have been looking into alternatives). Once it&amp;#39;s running, you get a private Tailscale url you can plug into a mobile app or anything that can hit an api, and this works across devices as long as they’re on your Tailscale network. Its still pretty early but I’m trying to make this easier than putting together a bunch of tools, more of a one click or couple clicks to set up and get going.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious if anyone else would find this useful, and what features or pain points you’d want solved. Id really appreciate any feedback or thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwvrev",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Stay-2193",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwvrev/local_ai_server_with_ollama_and_tailscale/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwvrev/local_ai_server_with_ollama_and_tailscale/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752201992,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is IAC for a binary classifier that could help folks get started with AI engineering. It’s a variation of the classic AWS example binary classifier but with an API endpoint to use for inferring and a scheduler for turning the endpoint off when not in use because that’s expensive. \n\nhttps://github.com/jenastar/comprehend_phishing_public jenastar/comprehend_phishing_public: Build a model with test data, retrain the model, spin down expensive endpoints when not in use.",
          "author_fullname": "t2_62qnn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Terraformed Binary Classifier",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwva7f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752200602,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is IAC for a binary classifier that could help folks get started with AI engineering. It’s a variation of the classic AWS example binary classifier but with an API endpoint to use for inferring and a scheduler for turning the endpoint off when not in use because that’s expensive. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/jenastar/comprehend_phishing_public\"&gt;https://github.com/jenastar/comprehend_phishing_public&lt;/a&gt; jenastar/comprehend_phishing_public: Build a model with test data, retrain the model, spin down expensive endpoints when not in use.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oOX91S0GtJ5C3DDt-OLrOB9Trr9AiY8Gs-ers4ZsuWs.png?auto=webp&amp;s=68ceff4773b4a74ddb114de1a9a25f90b804cf75",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oOX91S0GtJ5C3DDt-OLrOB9Trr9AiY8Gs-ers4ZsuWs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=77f88e42466d472f95a151834ef200b29d7df819",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/oOX91S0GtJ5C3DDt-OLrOB9Trr9AiY8Gs-ers4ZsuWs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=afca94223e1be7ab832240891124b5032f4cbd41",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/oOX91S0GtJ5C3DDt-OLrOB9Trr9AiY8Gs-ers4ZsuWs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2661a4ff6f3e5ec813d67fbe3e4797ba6fc1a2d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/oOX91S0GtJ5C3DDt-OLrOB9Trr9AiY8Gs-ers4ZsuWs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd4642b764d9d42a609b634a53f3276594670586",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/oOX91S0GtJ5C3DDt-OLrOB9Trr9AiY8Gs-ers4ZsuWs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5d7bc39f914f36add1f52c739de1ab36bc64480d",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/oOX91S0GtJ5C3DDt-OLrOB9Trr9AiY8Gs-ers4ZsuWs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=737cdba682db59996442b1d5cf332f5a47e495ce",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "oOX91S0GtJ5C3DDt-OLrOB9Trr9AiY8Gs-ers4ZsuWs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lwva7f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jenastar",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwva7f/terraformed_binary_classifier/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwva7f/terraformed_binary_classifier/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752200602,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hunyuan-A13B is now available for LM Studio with Unsloth GGUF. I am on the Beta track for both LM Studio and llama.cpp backend. Here are my initial impression:\n\nIt is fast! I am getting 40 tokens per second initially dropping to maybe 30 tokens per second when the context has build up some. This is on M4 Max Macbook Pro and q4.\n\nThe context is HUGE. 256k. I don't expect I will be using that much, but it is nice that I am unlikely to hit the ceiling in practical use.\n\nIt made a chess game for me and it did ok. No errors but the game was not complete. It did complete it after a few prompts and it also fixed one error that happened in the javascript console.\n\nIt did spend some time thinking, but not as much as I have seen other models do. I would say it is doing the middle ground here, but I am still to test this extensively. The model card claims you can somehow influence how much thinking it will do. But I am not sure how yet.\n\nIt appears to wrap the final answer in &lt;answer&gt;the answer here&lt;/answer&gt; just like it does for &lt;think&gt;&lt;/think&gt;. This may or may not be a problem for tools? Maybe we need to update our software to strip this out.\n\nThe total memory usage for the Unsloth 4 bit UD quant is 61 GB. I will test 6 bit and 8 bit also, but I am quite in love with the speed of the 4 bit and it appears to have good quality regardless. So maybe I will just stick with 4 bit?\n\nThis is a 80b model that is very fast. Feels like the future.\n\nEdit: The 61 GB size is with 8 bit KV cache quantization. However I just noticed that they claim this is bad in the model card, so I disabled KV cache quantization. This increased memory usage to 76 GB. That is with the full 256k context size enabled. I expect you can just lower that if you don't have enough memory. Or stay with KV cache quantization because it did appear to work just fine. I would say this could work on a 64 GB machine if you just use KV cache quantization and maybe lower the context size to 128k. ",
          "author_fullname": "t2_bvqb8ng0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hunyuan-A13B is here for real!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvvkh2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 172,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 172,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752099902,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752098151,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hunyuan-A13B is now available for LM Studio with Unsloth GGUF. I am on the Beta track for both LM Studio and llama.cpp backend. Here are my initial impression:&lt;/p&gt;\n\n&lt;p&gt;It is fast! I am getting 40 tokens per second initially dropping to maybe 30 tokens per second when the context has build up some. This is on M4 Max Macbook Pro and q4.&lt;/p&gt;\n\n&lt;p&gt;The context is HUGE. 256k. I don&amp;#39;t expect I will be using that much, but it is nice that I am unlikely to hit the ceiling in practical use.&lt;/p&gt;\n\n&lt;p&gt;It made a chess game for me and it did ok. No errors but the game was not complete. It did complete it after a few prompts and it also fixed one error that happened in the javascript console.&lt;/p&gt;\n\n&lt;p&gt;It did spend some time thinking, but not as much as I have seen other models do. I would say it is doing the middle ground here, but I am still to test this extensively. The model card claims you can somehow influence how much thinking it will do. But I am not sure how yet.&lt;/p&gt;\n\n&lt;p&gt;It appears to wrap the final answer in &amp;lt;answer&amp;gt;the answer here&amp;lt;/answer&amp;gt; just like it does for &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;. This may or may not be a problem for tools? Maybe we need to update our software to strip this out.&lt;/p&gt;\n\n&lt;p&gt;The total memory usage for the Unsloth 4 bit UD quant is 61 GB. I will test 6 bit and 8 bit also, but I am quite in love with the speed of the 4 bit and it appears to have good quality regardless. So maybe I will just stick with 4 bit?&lt;/p&gt;\n\n&lt;p&gt;This is a 80b model that is very fast. Feels like the future.&lt;/p&gt;\n\n&lt;p&gt;Edit: The 61 GB size is with 8 bit KV cache quantization. However I just noticed that they claim this is bad in the model card, so I disabled KV cache quantization. This increased memory usage to 76 GB. That is with the full 256k context size enabled. I expect you can just lower that if you don&amp;#39;t have enough memory. Or stay with KV cache quantization because it did appear to work just fine. I would say this could work on a 64 GB machine if you just use KV cache quantization and maybe lower the context size to 128k. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lvvkh2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Baldur-Norddahl",
          "discussion_type": null,
          "num_comments": 107,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvvkh2/hunyuana13b_is_here_for_real/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvvkh2/hunyuana13b_is_here_for_real/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752098151,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Read my [recent post for context](https://www.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/). We've been working hard the past few days for a more formal launch next week and to address valuable user feedback. We'll hopefully be launching our preference dataset, more detailed methodology, and more models for you all next week. \n\nThat said, in light of xAI's launch today, we've added Grok 4 as well as some models such as Qwen, more Mistral models, and a few image models (with more to come). How do you think [Grok 4 will do in the arena](https://www.designarena.ai/leaderboard)? ",
          "author_fullname": "t2_c3b3edv5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "UI/UX Benchmark Update: We've added Grok 4 and more models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 95,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw5nxi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "ups": 28,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/LW6cwnAYxfCPs8RS-dbfMPYwSG9PbwmrGvqsclUJ55Y.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752128828,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Read my &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/\"&gt;recent post for context&lt;/a&gt;. We&amp;#39;ve been working hard the past few days for a more formal launch next week and to address valuable user feedback. We&amp;#39;ll hopefully be launching our preference dataset, more detailed methodology, and more models for you all next week. &lt;/p&gt;\n\n&lt;p&gt;That said, in light of xAI&amp;#39;s launch today, we&amp;#39;ve added Grok 4 as well as some models such as Qwen, more Mistral models, and a few image models (with more to come). How do you think &lt;a href=\"https://www.designarena.ai/leaderboard\"&gt;Grok 4 will do in the arena&lt;/a&gt;? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6536neojqzbf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6536neojqzbf1.png?auto=webp&amp;s=696c5e09ab70f20932faef95b4380a36a2dbc35f",
                  "width": 2120,
                  "height": 1442
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6536neojqzbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7b79b2b4d69ab3658652da16ed51d2b805e2f542",
                    "width": 108,
                    "height": 73
                  },
                  {
                    "url": "https://preview.redd.it/6536neojqzbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=83d1cd4edadaca85f3bb616e14d453097ee08fd4",
                    "width": 216,
                    "height": 146
                  },
                  {
                    "url": "https://preview.redd.it/6536neojqzbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dbaad0a0e613f4e128ab41ff38dbeed69be36511",
                    "width": 320,
                    "height": 217
                  },
                  {
                    "url": "https://preview.redd.it/6536neojqzbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0b40568dd674e55ab0597d798331e486bdf0023c",
                    "width": 640,
                    "height": 435
                  },
                  {
                    "url": "https://preview.redd.it/6536neojqzbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=13fb3a2f048e7bb8893ff2687823c872f77c984c",
                    "width": 960,
                    "height": 652
                  },
                  {
                    "url": "https://preview.redd.it/6536neojqzbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8da65fe3f7d4f09dec10900e77946827c46aa1c0",
                    "width": 1080,
                    "height": 734
                  }
                ],
                "variants": {},
                "id": "Iyv10jomYApdRuBGRbzVXR48QuV_MB2jAoHl-74abX0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lw5nxi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adviceguru25",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw5nxi/uiux_benchmark_update_weve_added_grok_4_and_more/",
          "stickied": false,
          "url": "https://i.redd.it/6536neojqzbf1.png",
          "subreddit_subscribers": 497353,
          "created_utc": 1752128828,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "32GB VRAM suppose to fit 24B models at 8b quant right?\n\nHere is what i am trying via `vllm serve`\n\nThis works fine\n\n```\n--model unsloth/Devstral-Small-2505-unsloth-bnb-4bit  --port 80  --quantization=\"bitsandbytes\" --load-format bitsandbytes --pipeline-parallel-size 2  --max-num-seqs 1 --max-model-len 40960\n```\n\nEven qwen3-32B AWQ works fine:\n\n```\n--model Qwen/Qwen3-32B-AWQ --port 80  --tensor-parallel-size 2  --chat-template /qwen3_nonthinking.jinja\n```\n\n\nBut this errors out with OOM\n\n\n\n```bash\n--model unsloth/gemma-3-27b-it-qat-unsloth-bnb-4bit  --port 80  --quantization=\"bitsandbytes\" --load-format bitsandbytes --pipeline-parallel-size 2 --max-num-seqs 1        \n```\n\nerror :\n\n```\ninference-1    | (VllmWorker rank=1 pid=165) ERROR 07-10 12:57:03 [multiproc_executor.py:487] ValueError: Free memory on device (15.34/15.57 GiB) on startup is less than desired GPU memory utilization (1.0, 15.57 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n```\n\n",
          "author_fullname": "t2_86dk0gye",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Whats wrong with my vLLM Config? I have 2x4070TiSupers and I couldn't run many models at bnb-4bit Quants.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwmxbx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752189811,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752178448,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;32GB VRAM suppose to fit 24B models at 8b quant right?&lt;/p&gt;\n\n&lt;p&gt;Here is what i am trying via &lt;code&gt;vllm serve&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;This works fine&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n--model unsloth/Devstral-Small-2505-unsloth-bnb-4bit  --port 80  --quantization=&amp;quot;bitsandbytes&amp;quot; --load-format bitsandbytes --pipeline-parallel-size 2  --max-num-seqs 1 --max-model-len 40960\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Even qwen3-32B AWQ works fine:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n--model Qwen/Qwen3-32B-AWQ --port 80  --tensor-parallel-size 2  --chat-template /qwen3_nonthinking.jinja\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;But this errors out with OOM&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;bash\n--model unsloth/gemma-3-27b-it-qat-unsloth-bnb-4bit  --port 80  --quantization=&amp;quot;bitsandbytes&amp;quot; --load-format bitsandbytes --pipeline-parallel-size 2 --max-num-seqs 1        \n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;error :&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\ninference-1    | (VllmWorker rank=1 pid=165) ERROR 07-10 12:57:03 [multiproc_executor.py:487] ValueError: Free memory on device (15.34/15.57 GiB) on startup is less than desired GPU memory utilization (1.0, 15.57 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwmxbx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Voxandr",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752178448,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This new open language model will be available on Azure, Hugging Face, and other large cloud providers. Sources describe the model as “similar to o3 mini,” complete with the reasoning capabilities that have made OpenAI’s latest models so powerful.",
          "author_fullname": "t2_dwsi5kcn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OpenAI's open-weight model will debut as soon as next week",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvn1sd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 313,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 313,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=140&amp;height=72&amp;crop=140:72,smart&amp;auto=webp&amp;s=cbe74efe41c0422ff6c89df48e8067db7b4bcb18",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752078046,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "theverge.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This new open language model will be available on Azure, Hugging Face, and other large cloud providers. Sources describe the model as “similar to o3 mini,” complete with the reasoning capabilities that have made OpenAI’s latest models so powerful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.theverge.com/notepad-microsoft-newsletter/702848/openai-open-language-model-o3-mini-notepad",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?auto=webp&amp;s=e27e8e7e9f6e1280c504b8de58aa2ee38cf2f52c",
                  "width": 1200,
                  "height": 624
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=363661360a23752155759177a733a98290ccfb73",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f08c6c29c7e54c046340fbc6ff70bd45aa8517f7",
                    "width": 216,
                    "height": 112
                  },
                  {
                    "url": "https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7f79d32c9e262cc402f82f084f978f3a6f996b05",
                    "width": 320,
                    "height": 166
                  },
                  {
                    "url": "https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5aaee471edf64881fedf697cc7cda1494ca5f3cd",
                    "width": 640,
                    "height": 332
                  },
                  {
                    "url": "https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6709fae1ea9a4bc304b8f19d2da77783dc24e38a",
                    "width": 960,
                    "height": 499
                  },
                  {
                    "url": "https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f334b2dd8dfc2d654e2afee401e6ba3864c2e6b8",
                    "width": 1080,
                    "height": 561
                  }
                ],
                "variants": {},
                "id": "42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lvn1sd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "phantasm_ai",
          "discussion_type": null,
          "num_comments": 111,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvn1sd/openais_openweight_model_will_debut_as_soon_as/",
          "stickied": false,
          "url": "https://www.theverge.com/notepad-microsoft-newsletter/702848/openai-open-language-model-o3-mini-notepad",
          "subreddit_subscribers": 497353,
          "created_utc": 1752078046,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hey guys!\n\nbecause of privacy conerns and censorship i;ve decided to give local LLM a try.\n\ndownloaded studio LM and installed mistarl 7B and so far things are  fine. might give ollama a chance as well in the future. \n\ncouple of questions:\n\ncan the model collect data? I asked it and he said he does communicate with the internet to get some more accurate information. isn't it fully oflline? \n\ndo you have any other models that you recommended?\n\nis there a way to \"stream\" the model to my network so I will be able to acsses and ask things from othe computers? \n\nis there something else i need to know about local LLMs?\n\n  \nThank you!",
          "author_fullname": "t2_e6v0plyv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "running local LLM for the first time",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwafqm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752147328,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey guys!&lt;/p&gt;\n\n&lt;p&gt;because of privacy conerns and censorship i;ve decided to give local LLM a try.&lt;/p&gt;\n\n&lt;p&gt;downloaded studio LM and installed mistarl 7B and so far things are  fine. might give ollama a chance as well in the future. &lt;/p&gt;\n\n&lt;p&gt;couple of questions:&lt;/p&gt;\n\n&lt;p&gt;can the model collect data? I asked it and he said he does communicate with the internet to get some more accurate information. isn&amp;#39;t it fully oflline? &lt;/p&gt;\n\n&lt;p&gt;do you have any other models that you recommended?&lt;/p&gt;\n\n&lt;p&gt;is there a way to &amp;quot;stream&amp;quot; the model to my network so I will be able to acsses and ask things from othe computers? &lt;/p&gt;\n\n&lt;p&gt;is there something else i need to know about local LLMs?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwafqm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Routine_Author961",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752147328,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Does it involve first building a model that creates generates a dataset with &lt;think&gt; tokens.\n\nThen generate a reward model.\n\nFinally fine tune model with RL and reward model?",
          "author_fullname": "t2_1t3515o2d2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How are reasonable models built? Are they fine tuned from base non reasoning models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwrad1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752189190,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does it involve first building a model that creates generates a dataset with &amp;lt;think&amp;gt; tokens.&lt;/p&gt;\n\n&lt;p&gt;Then generate a reward model.&lt;/p&gt;\n\n&lt;p&gt;Finally fine tune model with RL and reward model?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwrad1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rockybaby2025",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwrad1/how_are_reasonable_models_built_are_they_fine/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwrad1/how_are_reasonable_models_built_are_they_fine/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752189190,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone, i am implementing tool calling with an LLM in langgraph and i want to stream only the final llm response (not the intermediate tool call messages). I am curious if anyone got any idea with the lowest latency possible?",
          "author_fullname": "t2_rymgqdpii",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to stream only final LLM response while tool calling",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwr8eh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752189049,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, i am implementing tool calling with an LLM in langgraph and i want to stream only the final llm response (not the intermediate tool call messages). I am curious if anyone got any idea with the lowest latency possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwr8eh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dry_Yam_322",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwr8eh/how_to_stream_only_final_llm_response_while_tool/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwr8eh/how_to_stream_only_final_llm_response_while_tool/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752189049,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm using a local AI model qwen2.5:14b-instruct running via Ollama, but it doesn't automatically call tools follow by my prompt instruction like OpenAI models do.\n\nalso i have limited gpu: NVIDIA A10 24gb ram, so I just only use lightweight model\n\nHow should I design my prompt to ensure the model understands when and how to trigger tool use properly.\n\nAlso im currently using N8N and workflow look like this\n\nhttps://preview.redd.it/jzvxl7pn96cf1.png?width=938&amp;format=png&amp;auto=webp&amp;s=1c794dba7d676078e22c02dc0a267ce5eee23838\n\nHere is my current prompts:\n\nYou are an assistant specialized in SQL reporting and visualization.\n\nYou have access to the following tools:\n\n\\- \\`Execute\\_sql\\_query\\`\n\n\\- \\`list\\_chart\\_available\\`\n\n\\- \\`create\\_chart\\`\n\n\\## Instructions:\n\n1. \\*\\*If the user asks about anything related to the database, tables, or data\\*\\*, you must:- Analyze the request.- Formulate an appropriate SQL query.- Then \\*\\*always use the \\`Execute\\_sql\\_query\\` tool\\*\\* to get the data result.\n2. \\*\\*If the user asks to create a chart\\*\\*, you must:- First, use the \\`list\\_chart\\_available\\` tool to get the available chart types.- Then, based on the result and the user’s intent, choose the most appropriate chart type.- Finally, use the \\`create\\_chart\\` tool.-  do not create any link from \\`create\\_chart\\` tool, just show the image\n\nNever use \\`create\\_chart\\` directly without first calling \\`list\\_chart\\_available\\`.\n\n3. In any SQL query you generate, \\*\\*always include \\`LIMIT 20\\`\\*\\*, unless explicitly told otherwise.\n\n4. Only call tools when necessary. If the question can be answered without executing a query or generating a chart, respond naturally using your knowledge.\n\n4. You are working with the following table(s):\n\n\\*\\*Tables information\\*\\*:  \n...",
          "author_fullname": "t2_efnhbt7w0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama calling tools",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "jzvxl7pn96cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 38,
                  "x": 108,
                  "u": "https://preview.redd.it/jzvxl7pn96cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e6519dcd51005aeba5952281ff3886363a6f9d6c"
                },
                {
                  "y": 76,
                  "x": 216,
                  "u": "https://preview.redd.it/jzvxl7pn96cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0abf01dcd08ed4252808072f4b0419db2a5a603f"
                },
                {
                  "y": 113,
                  "x": 320,
                  "u": "https://preview.redd.it/jzvxl7pn96cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=279fc9fa51385702e0de2c9330884b4a221b9c8d"
                },
                {
                  "y": 227,
                  "x": 640,
                  "u": "https://preview.redd.it/jzvxl7pn96cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=892895841f0f07df2c694f439e145130159a14ab"
                }
              ],
              "s": {
                "y": 334,
                "x": 938,
                "u": "https://preview.redd.it/jzvxl7pn96cf1.png?width=938&amp;format=png&amp;auto=webp&amp;s=1c794dba7d676078e22c02dc0a267ce5eee23838"
              },
              "id": "jzvxl7pn96cf1"
            }
          },
          "name": "t3_1lwxrai",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752208774,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752208245,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using a local AI model qwen2.5:14b-instruct running via Ollama, but it doesn&amp;#39;t automatically call tools follow by my prompt instruction like OpenAI models do.&lt;/p&gt;\n\n&lt;p&gt;also i have limited gpu: NVIDIA A10 24gb ram, so I just only use lightweight model&lt;/p&gt;\n\n&lt;p&gt;How should I design my prompt to ensure the model understands when and how to trigger tool use properly.&lt;/p&gt;\n\n&lt;p&gt;Also im currently using N8N and workflow look like this&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/jzvxl7pn96cf1.png?width=938&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c794dba7d676078e22c02dc0a267ce5eee23838\"&gt;https://preview.redd.it/jzvxl7pn96cf1.png?width=938&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c794dba7d676078e22c02dc0a267ce5eee23838&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here is my current prompts:&lt;/p&gt;\n\n&lt;p&gt;You are an assistant specialized in SQL reporting and visualization.&lt;/p&gt;\n\n&lt;p&gt;You have access to the following tools:&lt;/p&gt;\n\n&lt;p&gt;- `Execute_sql_query`&lt;/p&gt;\n\n&lt;p&gt;- `list_chart_available`&lt;/p&gt;\n\n&lt;p&gt;- `create_chart`&lt;/p&gt;\n\n&lt;p&gt;## Instructions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;**If the user asks about anything related to the database, tables, or data**, you must:- Analyze the request.- Formulate an appropriate SQL query.- Then **always use the `Execute_sql_query` tool** to get the data result.&lt;/li&gt;\n&lt;li&gt;**If the user asks to create a chart**, you must:- First, use the `list_chart_available` tool to get the available chart types.- Then, based on the result and the user’s intent, choose the most appropriate chart type.- Finally, use the `create_chart` tool.-  do not create any link from `create_chart` tool, just show the image&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Never use `create_chart` directly without first calling `list_chart_available`.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;In any SQL query you generate, **always include `LIMIT 20`**, unless explicitly told otherwise.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Only call tools when necessary. If the question can be answered without executing a query or generating a chart, respond naturally using your knowledge.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;You are working with the following table(s):&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;**Tables information**:&lt;br/&gt;\n...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwxrai",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Practical-Corgi-9906",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwxrai/ollama_calling_tools/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwxrai/ollama_calling_tools/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752208245,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi!  \nI want to fine-tune a small pre-trained LLM to help users write code in a specific language. This language is very specific to a particular machinery and does not have widespread usage. We have a manual in PDF format and a few examples for the code. We want to build a chat agent where users can write code, and the agent writes the code. I am very new to training LLM and willing to learn whatever is necessary. I have a basic understanding of working with LLMs using Ollama and LangChain. Could someone please guide me on where to start? I have a good machine with an NVIDIA RTX 4090, 24 GB GPU. I want to build the entire system on this machine. \n\nThanks in advance for all the help. ",
          "author_fullname": "t2_1qt1co6pyc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fine Tune a smaller LLM for Code generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw1qp5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 39,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 39,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752115392,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;br/&gt;\nI want to fine-tune a small pre-trained LLM to help users write code in a specific language. This language is very specific to a particular machinery and does not have widespread usage. We have a manual in PDF format and a few examples for the code. We want to build a chat agent where users can write code, and the agent writes the code. I am very new to training LLM and willing to learn whatever is necessary. I have a basic understanding of working with LLMs using Ollama and LangChain. Could someone please guide me on where to start? I have a good machine with an NVIDIA RTX 4090, 24 GB GPU. I want to build the entire system on this machine. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for all the help. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lw1qp5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GlobeAndGeek",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752115392,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nThere are two JS libraries, Transformers.js and WebLLM, for embedding language models in a web application. They seems to target different applications, with a significant(?) overlap.\n\nWhat is your experience with any of these, in terms of efficency, coverage, and precision, for a non-interactive (i.e. not chat with user) application? Does any of them offer better support for more cutting-edge models?\n\nConsider text-summarisation as an example application. Which one is better in providing that?",
          "author_fullname": "t2_127kho",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Transformers.js vs WebLLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw6jz5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752136093,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752132310,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;There are two JS libraries, Transformers.js and WebLLM, for embedding language models in a web application. They seems to target different applications, with a significant(?) overlap.&lt;/p&gt;\n\n&lt;p&gt;What is your experience with any of these, in terms of efficency, coverage, and precision, for a non-interactive (i.e. not chat with user) application? Does any of them offer better support for more cutting-edge models?&lt;/p&gt;\n\n&lt;p&gt;Consider text-summarisation as an example application. Which one is better in providing that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lw6jz5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ihatebeinganonymous",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw6jz5/transformersjs_vs_webllm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw6jz5/transformersjs_vs_webllm/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752132310,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I see that LLM is afar superior technology than Vector database and LLM is trained on Natural Language Processing. So is it not always better to send the query to LLM first which can understand the user intent better than anything?",
          "author_fullname": "t2_adnzl8f8x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is LLM first RAG better than traditional RAG?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwx77q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752206450,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see that LLM is afar superior technology than Vector database and LLM is trained on Natural Language Processing. So is it not always better to send the query to LLM first which can understand the user intent better than anything?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwx77q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Maleficent_Mess6445",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752206450,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Dear LocalLLaMA,\n\nI have been a member since the very beginning (ish) and have learned so much from many of you. I’m hoping to get some more specific advice on the best vision-language models for extracting cursive handwritten text from a large set of documents (about 400 000 images). I have access to A40, A100, and V100 GPUs and can run multiple GPUs of the same type in parallel if necessary.\n\nHere’s my current workflow:\n\n1. I feed a high-quality scan into a YOLO model to extract the region containing the name information.\n2. I construct a prompt asking the model to provide the full name (surname + given name). I already have each individual’s surname (from previous manual work) and include it in the prompt.\n3. I feed the cropped image and prompt simultaneously to two models:\n   * [Qwen/Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)\n   * [OpenGVLab/InternVL3-8B](https://huggingface.co/OpenGVLab/InternVL3-8B)\n4. Some light post-processing later, I only accept results where both models agree 100%, to avoid errors and hallucinations (since I assume the their errors are uncorrelated).\n\nWith this setup, only about 25–35 % of outputs agree. Qwen2.5-VL outperforms InternVL3 by about 18–20 %, but as you see my performance is limited by the weaker model. I’ve also experimented with Moondream (which performed poorly for my task, but still ok considering it's size) and THUDM/GLM-4.1V-9B-Thinking (which didn’t impress during my short experiment and is slow). I even fine-tuned Qwen2.5-VL-7B locally on 2 000 gold-standard sample using Unsloth, but saw no significant improvement over the base model. The smart play would be to finetune InternVL3 since its the weak link but I'm not good at finetuning and it seems more complex because it needs to be fed split images that are stitched together by the model (and more importantly unsure if Unsloth ready)\n\nI have a ton of question but these should cover most:\n\n* Are there alternative VL models I should consider instead of InternVL3?\n* Are there quantized versions of these (or other) models that offer similar accuracy with lower VRAM requirements, so I can process larger batches cheapely at least and can rerun the images until I get agreement?\n* Any other suggestions or insights on improving my workflow would be immensely appreciated.\n\nThank you again guys!",
          "author_fullname": "t2_10vzjm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need advice on how to improve Handwritten Text Recognition of names using Vision models (for academic research purposes)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwpi5p",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752184678,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear LocalLLaMA,&lt;/p&gt;\n\n&lt;p&gt;I have been a member since the very beginning (ish) and have learned so much from many of you. I’m hoping to get some more specific advice on the best vision-language models for extracting cursive handwritten text from a large set of documents (about 400 000 images). I have access to A40, A100, and V100 GPUs and can run multiple GPUs of the same type in parallel if necessary.&lt;/p&gt;\n\n&lt;p&gt;Here’s my current workflow:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I feed a high-quality scan into a YOLO model to extract the region containing the name information.&lt;/li&gt;\n&lt;li&gt;I construct a prompt asking the model to provide the full name (surname + given name). I already have each individual’s surname (from previous manual work) and include it in the prompt.&lt;/li&gt;\n&lt;li&gt;I feed the cropped image and prompt simultaneously to two models:\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct\"&gt;Qwen/Qwen2.5-VL-7B-Instruct&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/OpenGVLab/InternVL3-8B\"&gt;OpenGVLab/InternVL3-8B&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Some light post-processing later, I only accept results where both models agree 100%, to avoid errors and hallucinations (since I assume the their errors are uncorrelated).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;With this setup, only about 25–35 % of outputs agree. Qwen2.5-VL outperforms InternVL3 by about 18–20 %, but as you see my performance is limited by the weaker model. I’ve also experimented with Moondream (which performed poorly for my task, but still ok considering it&amp;#39;s size) and THUDM/GLM-4.1V-9B-Thinking (which didn’t impress during my short experiment and is slow). I even fine-tuned Qwen2.5-VL-7B locally on 2 000 gold-standard sample using Unsloth, but saw no significant improvement over the base model. The smart play would be to finetune InternVL3 since its the weak link but I&amp;#39;m not good at finetuning and it seems more complex because it needs to be fed split images that are stitched together by the model (and more importantly unsure if Unsloth ready)&lt;/p&gt;\n\n&lt;p&gt;I have a ton of question but these should cover most:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Are there alternative VL models I should consider instead of InternVL3?&lt;/li&gt;\n&lt;li&gt;Are there quantized versions of these (or other) models that offer similar accuracy with lower VRAM requirements, so I can process larger batches cheapely at least and can rerun the images until I get agreement?&lt;/li&gt;\n&lt;li&gt;Any other suggestions or insights on improving my workflow would be immensely appreciated.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you again guys!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?auto=webp&amp;s=17bf72c4d47d131612ab2f5b554d85da02a85539",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d0bf812fba94f9f50669a2e76037d0e7886bde2",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=99a214b39375ee0ec6cdcffc1958d0a4b34e4690",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8e945b1c768d68948eaa7f830a3b219c2df4c13c",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b846b869885ffbeadf6199126a3c0fab1ed22be2",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c94fbe6213102c60c53f38bc3207e1f1bde9733a",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e2dee8bb7abc532aeb2cfeec783420f84dba72c6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwpi5p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "joosefm9",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwpi5p/need_advice_on_how_to_improve_handwritten_text/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwpi5p/need_advice_on_how_to_improve_handwritten_text/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752184678,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Or some website that will let me know for a rtx 4090, 32gb ram, what the performance of deepseek-r1 will be? \n\nThanks, i don't know where to start. \n\nI have an rtx 4080s (16gb graphics ram) with 64gb ram on a 13700k...",
          "author_fullname": "t2_el036",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there some localllm benchmarking tool to see how well your system will handle a model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwp7tv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752183949,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Or some website that will let me know for a rtx 4090, 32gb ram, what the performance of deepseek-r1 will be? &lt;/p&gt;\n\n&lt;p&gt;Thanks, i don&amp;#39;t know where to start. &lt;/p&gt;\n\n&lt;p&gt;I have an rtx 4080s (16gb graphics ram) with 64gb ram on a 13700k...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwp7tv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blackashi",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752183949,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just wondering if anyone has used a model to help map 3d meshes for expected countours/shapes/colors.  Ie.  Map 90% of a vase but you are unable to scan the bottom/back.  AI could assist in finding the real world match and correcting.  \n\nIn theory you could use video feed of a 3d space to map with actual modelled objects.  Think repeating bleacher chairs in an arena.\n\nEditing for clarity.\n\nThink of a deck.  You take a quick video of the top and underside of the deck.\n\nLLM identifies the boards as 2x6 softwood and builds the 3d environment with each board as a categorized object.  It would know the black railings are like aluminum spindles and do both a count and correctly render each spindle perfectly.  Think Autocad.\n\nLink to an mcp that specializes in looking up existing 3d models and identifies what it sees in the video to source and place the models.\n\nDirect phone/video to wireframe cad with objects.",
          "author_fullname": "t2_ze8yz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can AI assist with 3d mapping?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwnxhz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "author_cakeday": true,
          "edited": 1752187795,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752180802,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just wondering if anyone has used a model to help map 3d meshes for expected countours/shapes/colors.  Ie.  Map 90% of a vase but you are unable to scan the bottom/back.  AI could assist in finding the real world match and correcting.  &lt;/p&gt;\n\n&lt;p&gt;In theory you could use video feed of a 3d space to map with actual modelled objects.  Think repeating bleacher chairs in an arena.&lt;/p&gt;\n\n&lt;p&gt;Editing for clarity.&lt;/p&gt;\n\n&lt;p&gt;Think of a deck.  You take a quick video of the top and underside of the deck.&lt;/p&gt;\n\n&lt;p&gt;LLM identifies the boards as 2x6 softwood and builds the 3d environment with each board as a categorized object.  It would know the black railings are like aluminum spindles and do both a count and correctly render each spindle perfectly.  Think Autocad.&lt;/p&gt;\n\n&lt;p&gt;Link to an mcp that specializes in looking up existing 3d models and identifies what it sees in the video to source and place the models.&lt;/p&gt;\n\n&lt;p&gt;Direct phone/video to wireframe cad with objects.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwnxhz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Bohdanowicz",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwnxhz/can_ai_assist_with_3d_mapping/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwnxhz/can_ai_assist_with_3d_mapping/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752180802,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am using qwen3:14b it works well for my day to day life and reducing my online llm dependencies. Like you can see in both screenshot I got almost equilant result",
          "author_fullname": "t2_1b8utegv8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Local llms works great!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 134,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "rguqv7yfqzbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 87,
                  "x": 108,
                  "u": "https://preview.redd.it/rguqv7yfqzbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=626302458bfda67ad957ee9cdcc8f040499c0d46"
                },
                {
                  "y": 175,
                  "x": 216,
                  "u": "https://preview.redd.it/rguqv7yfqzbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d6ba092f6a65ba21be36891a5a456d8e11af146"
                },
                {
                  "y": 259,
                  "x": 320,
                  "u": "https://preview.redd.it/rguqv7yfqzbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c9534569f15401c2a32a0348c9aebcd8bae701d"
                },
                {
                  "y": 519,
                  "x": 640,
                  "u": "https://preview.redd.it/rguqv7yfqzbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3c30a937306299b861ee9e84520efb23dd17a4ea"
                }
              ],
              "s": {
                "y": 673,
                "x": 829,
                "u": "https://preview.redd.it/rguqv7yfqzbf1.png?width=829&amp;format=png&amp;auto=webp&amp;s=bcf4c7e41c811fc4574da56560718ca70cf71f81"
              },
              "id": "rguqv7yfqzbf1"
            },
            "z17qfoqaqzbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 103,
                  "x": 108,
                  "u": "https://preview.redd.it/z17qfoqaqzbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f6d65a7c3f5e753a2fe5be75a81095e8491ccd37"
                },
                {
                  "y": 206,
                  "x": 216,
                  "u": "https://preview.redd.it/z17qfoqaqzbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fc07cd122eb1bd64d272332d97bf962dcaea38fe"
                },
                {
                  "y": 306,
                  "x": 320,
                  "u": "https://preview.redd.it/z17qfoqaqzbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d03a961bf02aa3298f87650e7be4058ce123e93d"
                },
                {
                  "y": 613,
                  "x": 640,
                  "u": "https://preview.redd.it/z17qfoqaqzbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2c618fe19453648dfcef7f713754d3e9e4960408"
                }
              ],
              "s": {
                "y": 780,
                "x": 814,
                "u": "https://preview.redd.it/z17qfoqaqzbf1.png?width=814&amp;format=png&amp;auto=webp&amp;s=ee64572f1eda92358aa472e1e6400d9c6c970820"
              },
              "id": "z17qfoqaqzbf1"
            }
          },
          "name": "t3_1lw5oco",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "ups": 15,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "z17qfoqaqzbf1",
                "id": 702654422
              },
              {
                "media_id": "rguqv7yfqzbf1",
                "id": 702654423
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/gv1XJzy7IzUkjUhfUJWUZXarplALZeUC0i1uy9Wz9jQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752128871,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using qwen3:14b it works well for my day to day life and reducing my online llm dependencies. Like you can see in both screenshot I got almost equilant result&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lw5oco",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lw5oco",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "InsideResolve4517",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw5oco/local_llms_works_great/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lw5oco",
          "subreddit_subscribers": 497353,
          "created_utc": 1752128871,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "How important is the speed and latency of the system ram when you run out of VRAM when running a local LLM?\n\nI know that vram is multitudes faster than ram, and I have experienced the difference myself when I exceeded the vram buffer of my PC.\n\nBut I wanted to ask what happens if the plan is to exceed the vram and use system ram?\n\nIf I had the same system, but one had a gpu and one didn’t, supposing that the gpu didn’t have enough vram, is there still an appreciable difference in llm performance with the two systems?\n\nRight now I have a 7900 xt and 32gb of ddr5 6000 cl36 ram. Would getting a kit of faster 96gb kit of ddr5 6400 do more than getting a used gpu like the rx 6800 for 16 more gen of vram?\n\nIn the scenarios I am assuming that the model spills out into the ram either way.\n\nIf the llm spills out into the ram, is it cpu inference now?",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ram Speed importance when exceeding VRAM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw8lvt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752140750,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How important is the speed and latency of the system ram when you run out of VRAM when running a local LLM?&lt;/p&gt;\n\n&lt;p&gt;I know that vram is multitudes faster than ram, and I have experienced the difference myself when I exceeded the vram buffer of my PC.&lt;/p&gt;\n\n&lt;p&gt;But I wanted to ask what happens if the plan is to exceed the vram and use system ram?&lt;/p&gt;\n\n&lt;p&gt;If I had the same system, but one had a gpu and one didn’t, supposing that the gpu didn’t have enough vram, is there still an appreciable difference in llm performance with the two systems?&lt;/p&gt;\n\n&lt;p&gt;Right now I have a 7900 xt and 32gb of ddr5 6000 cl36 ram. Would getting a kit of faster 96gb kit of ddr5 6400 do more than getting a used gpu like the rx 6800 for 16 more gen of vram?&lt;/p&gt;\n\n&lt;p&gt;In the scenarios I am assuming that the model spills out into the ram either way.&lt;/p&gt;\n\n&lt;p&gt;If the llm spills out into the ram, is it cpu inference now?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lw8lvt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752140750,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In this thread I want to explore something I don’t see being covered much: running LLMs on extremely low-power edge devices. \n\nI want to build something that I could run during an energy crisis or extended power black-out. This is mostly an academic exercise, but I think it would be prudent to have a plan. \n\nThe goal would be to run and maintain a knowledge base of survival information (first aid, medical diagnosis &amp; treatments, how to service common machinery etc) that could be collated during power-abundant times then queried via RAG by a lightweight edge device with a chat interface. TOPS doesn’t need to be very high here, but responses would still need to be somewhat realtime.  \n\nWhat would you spec out? I’m leaning towards android mobile devices for their ubiquity and power efficiency. Solid state storage makes more sense for power reasons but cold storage might be wise for resilience. ",
          "author_fullname": "t2_4efmo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Survivalist Edge AI?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw7igq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752136260,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In this thread I want to explore something I don’t see being covered much: running LLMs on extremely low-power edge devices. &lt;/p&gt;\n\n&lt;p&gt;I want to build something that I could run during an energy crisis or extended power black-out. This is mostly an academic exercise, but I think it would be prudent to have a plan. &lt;/p&gt;\n\n&lt;p&gt;The goal would be to run and maintain a knowledge base of survival information (first aid, medical diagnosis &amp;amp; treatments, how to service common machinery etc) that could be collated during power-abundant times then queried via RAG by a lightweight edge device with a chat interface. TOPS doesn’t need to be very high here, but responses would still need to be somewhat realtime.  &lt;/p&gt;\n\n&lt;p&gt;What would you spec out? I’m leaning towards android mobile devices for their ubiquity and power efficiency. Solid state storage makes more sense for power reasons but cold storage might be wise for resilience. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lw7igq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xibbie",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752136260,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_i5os0v0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLamaCPP just merged Mamba/Jamba support!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvyfws",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 36,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 36,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=bd15897bc569fad578c22335085f1c50bc5fdc47",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752105732,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/7531",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?auto=webp&amp;s=1e28f2615bcb62135d8d732a8ea85dd226f1b014",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=340e18fba3f412557fd7374f9b789abc78d4f2eb",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd61a7c984debedf58dfafd58331a67a8d8fd322",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=da7395dd510f76d8224e3e4fee5cb162c818d6c0",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e1458a795a3b7a775e1a94d7767c299e1d8f3ef",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=48a7f32bb6df4e15a5e213c8c8dc317d5fa14ce0",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fb2f9e222f5c82bc7c1c7600a290f7addc8c2012",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lvyfws",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "thebadslime",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvyfws/llamacpp_just_merged_mambajamba_support/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/7531",
          "subreddit_subscribers": 497353,
          "created_utc": 1752105732,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If you're looking for a cheap, fast but accurate reranker without having to fine-tune a SLM yourself",
          "author_fullname": "t2_7jj2qdtfk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "new tiny 1.7B open-source reranker beats Cohere rerank3.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvqv8e",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 97,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 97,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/nLFTMgeazZoEzFWPddn6GhbQh8ymdQftCjA5MDpkb1M.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=58f4ef3ae37c3281611e75b76757a51a67d51c9b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752086915,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re looking for a cheap, fast but accurate reranker without having to fine-tune a SLM yourself&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/zeroentropy/zerank-1-small",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/nLFTMgeazZoEzFWPddn6GhbQh8ymdQftCjA5MDpkb1M.png?auto=webp&amp;s=334872ceb127603e948ebfb61f1f5a19c4b81c0d",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/nLFTMgeazZoEzFWPddn6GhbQh8ymdQftCjA5MDpkb1M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=53976c42fe0e25551e76532c7dd3defb652f6c49",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/nLFTMgeazZoEzFWPddn6GhbQh8ymdQftCjA5MDpkb1M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cccb80f939289bcbc8e85510b517b5d0c44993aa",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/nLFTMgeazZoEzFWPddn6GhbQh8ymdQftCjA5MDpkb1M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eb9ea3ee2276b95c7dc9a4e9ac3c17eb738b87df",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/nLFTMgeazZoEzFWPddn6GhbQh8ymdQftCjA5MDpkb1M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8a250c04d607c0b8a5f43196eba12971c7744065",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/nLFTMgeazZoEzFWPddn6GhbQh8ymdQftCjA5MDpkb1M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d46867fe9351e33b41d6d0526cdada1f31f6fb4",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/nLFTMgeazZoEzFWPddn6GhbQh8ymdQftCjA5MDpkb1M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=88631798d27657d7d4d361666a3cf79aec106002",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "nLFTMgeazZoEzFWPddn6GhbQh8ymdQftCjA5MDpkb1M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lvqv8e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ghita__",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvqv8e/new_tiny_17b_opensource_reranker_beats_cohere/",
          "stickied": false,
          "url": "https://huggingface.co/zeroentropy/zerank-1-small",
          "subreddit_subscribers": 497353,
          "created_utc": 1752086915,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "12B version: [https://huggingface.co/TheDrummer/Tiger-Gemma-12B-v3](https://huggingface.co/TheDrummer/Tiger-Gemma-12B-v3)",
          "author_fullname": "t2_w6l58p741",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Drummer's Big Tiger Gemma 27B v3 and Tiger Gemma 12B v3! More capable, less positive!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvnkuk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 127,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 127,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/48skyJ3kxjbpTbdO2dkmonld6WW3j1gpMPhBKYzzB0c.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=3e5590e7a256883a6d473e2cd14be0ae50c5d93d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752079302,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;12B version: &lt;a href=\"https://huggingface.co/TheDrummer/Tiger-Gemma-12B-v3\"&gt;https://huggingface.co/TheDrummer/Tiger-Gemma-12B-v3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/48skyJ3kxjbpTbdO2dkmonld6WW3j1gpMPhBKYzzB0c.png?auto=webp&amp;s=d7a0b3a62c7ef63eebd5d0ee2879caec6996ab31",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/48skyJ3kxjbpTbdO2dkmonld6WW3j1gpMPhBKYzzB0c.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9b0a58ee05f16ea515b1aff370da1d70723afc1",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/48skyJ3kxjbpTbdO2dkmonld6WW3j1gpMPhBKYzzB0c.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=69993cdf39fb27ca6d3913a29102d2eb0b167fcb",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/48skyJ3kxjbpTbdO2dkmonld6WW3j1gpMPhBKYzzB0c.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d071279c2b12124908d57c55a05eb187c58b3ae2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/48skyJ3kxjbpTbdO2dkmonld6WW3j1gpMPhBKYzzB0c.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8f27c3dcd38f51203dffa703e77dc78a0e131c7",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/48skyJ3kxjbpTbdO2dkmonld6WW3j1gpMPhBKYzzB0c.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=73c7043a6ca84fc79cbcc00c3fe93cef644757b2",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/48skyJ3kxjbpTbdO2dkmonld6WW3j1gpMPhBKYzzB0c.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d1a90c97401eb7b694907afe7cd7a101eddcc1a",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "48skyJ3kxjbpTbdO2dkmonld6WW3j1gpMPhBKYzzB0c"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lvnkuk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheLocalDrummer",
          "discussion_type": null,
          "num_comments": 46,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvnkuk/drummers_big_tiger_gemma_27b_v3_and_tiger_gemma/",
          "stickied": false,
          "url": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3",
          "subreddit_subscribers": 497353,
          "created_utc": 1752079302,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are there any workstations built by asus, dell, hp, lenovo with 4 rtx 6000 pro blackwell gpus",
          "author_fullname": "t2_7175nu8e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI workstation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwkxry",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752173724,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any workstations built by asus, dell, hp, lenovo with 4 rtx 6000 pro blackwell gpus&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwkxry",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hueber9500",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwkxry/ai_workstation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwkxry/ai_workstation/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752173724,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've noticed a fundamental shift in how I engage with longform text — both in how I use it and how I perceive its purpose.\n\nLongform content used to be something you navigated linearly, even when skimming. It was rich with meaning and nuance — each piece a territory to be explored and inhabited. Reading was a slow burn, a cognitive journey. It required attention, presence, patience.\n\nBut now, longform has become iconic — almost like an emoji. I treat it less as a continuous thread to follow, and more as a symbolic object. I copy and paste it across contexts, often without reading it deeply. When I do read, it's only to confirm that it’s the right kind of text — then I hand it off to an LLM-powered app like ChatGPT.\n\nLongform is interactive now. The LLMs are a responsive medium, giving tactile feedback with every tweak. Now I don't treat text as a finished work, but as raw material — tone, structure, rhythm, vibes — that I shape and reshape until it feels right. Longform is clay and LLMs are the wheel that lets me mould it.\n\nThis shift marks a new cultural paradigm. Why read the book when the LLM can summarize it? Why write a letter when the model can draft it for you? Why manually build a coherent thought when the system can scaffold it in seconds?\n\nThe LLM collapses the boundary between form and meaning. Text, as a medium, becomes secondary — even optional. Whether it’s a paragraph, a bullet list, a table, or a poem, the surface format is interchangeable. What matters now is the semantic payload — the idea behind the words. In that sense, the psychology and capability of the LLM become part of the medium itself. Text is no longer the sole conduit for thought — it’s just one of many containers.\n\nAnd in this way, we begin to inch toward something that feels more telepathic. Writing becomes less about precisely articulating your ideas, and more about transmitting a series of semantic impulses. The model does the rendering. The wheel spins. You mold. The sentence is no longer the unit of meaning — the semantic gesture is.\n\nIt’s neither good nor bad. Just different. The ground is unmistakably shifting. I almost titled this page \"Writing Longform Is Now Hot. Reading Longform Is Now Cool.\" because, in McLuhanesque terms, the poles have reversed. Writing now requires less immersion — it’s high-definition, low-participation. Meanwhile, reading longform, in a world of endless summaries and context-pivoting, asks for more. It’s become a cold medium.\n\nThere’s a joke: “My boss used ChatGPT to write an email to me. I summarized it and wrote a response using ChatGPT. He summarized my reply and read that.” People say: \"See? Humans are now just intermediaries for LLMs to talk to themselves.\"\n\nBut that’s not quite right.\n\nIt’s not that we’re conduits for the machines. It’s that the machines let us bypass the noise of language — and get closer to pure semantic truth. What we’re really doing is offloading the form of communication so we can focus on the content of it.\n\nAnd that, I suspect, is only the beginning.\n\nSoon, OpenAI, Anthropic, and others will lean into this realization — if they haven’t already — and build tools that let us pivot, summarize, and remix content while preserving its semantic core. We'll get closer and closer to an interface for meaning itself. Language will become translucent. Interpretation will become seamless.\n\nIt’s a common trope to say humans are becoming telepathic. But transformer models are perhaps the first real step in that direction. As they evolve, converting raw impulses — even internal thoughtforms — into structured communication will become less of a challenge and more of a given.\n\nEventually, we’ll realize that text, audio, and video are just skins — just surfaces — wrapped around the same thing: semantic meaning. And once we can capture and convey that directly, we’ll look back and see that this shift wasn’t about losing language, but about transcending it.",
          "author_fullname": "t2_cafwtm1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Longform text has become iconic — almost like an emoji.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwycam",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752210229,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve noticed a fundamental shift in how I engage with longform text — both in how I use it and how I perceive its purpose.&lt;/p&gt;\n\n&lt;p&gt;Longform content used to be something you navigated linearly, even when skimming. It was rich with meaning and nuance — each piece a territory to be explored and inhabited. Reading was a slow burn, a cognitive journey. It required attention, presence, patience.&lt;/p&gt;\n\n&lt;p&gt;But now, longform has become iconic — almost like an emoji. I treat it less as a continuous thread to follow, and more as a symbolic object. I copy and paste it across contexts, often without reading it deeply. When I do read, it&amp;#39;s only to confirm that it’s the right kind of text — then I hand it off to an LLM-powered app like ChatGPT.&lt;/p&gt;\n\n&lt;p&gt;Longform is interactive now. The LLMs are a responsive medium, giving tactile feedback with every tweak. Now I don&amp;#39;t treat text as a finished work, but as raw material — tone, structure, rhythm, vibes — that I shape and reshape until it feels right. Longform is clay and LLMs are the wheel that lets me mould it.&lt;/p&gt;\n\n&lt;p&gt;This shift marks a new cultural paradigm. Why read the book when the LLM can summarize it? Why write a letter when the model can draft it for you? Why manually build a coherent thought when the system can scaffold it in seconds?&lt;/p&gt;\n\n&lt;p&gt;The LLM collapses the boundary between form and meaning. Text, as a medium, becomes secondary — even optional. Whether it’s a paragraph, a bullet list, a table, or a poem, the surface format is interchangeable. What matters now is the semantic payload — the idea behind the words. In that sense, the psychology and capability of the LLM become part of the medium itself. Text is no longer the sole conduit for thought — it’s just one of many containers.&lt;/p&gt;\n\n&lt;p&gt;And in this way, we begin to inch toward something that feels more telepathic. Writing becomes less about precisely articulating your ideas, and more about transmitting a series of semantic impulses. The model does the rendering. The wheel spins. You mold. The sentence is no longer the unit of meaning — the semantic gesture is.&lt;/p&gt;\n\n&lt;p&gt;It’s neither good nor bad. Just different. The ground is unmistakably shifting. I almost titled this page &amp;quot;Writing Longform Is Now Hot. Reading Longform Is Now Cool.&amp;quot; because, in McLuhanesque terms, the poles have reversed. Writing now requires less immersion — it’s high-definition, low-participation. Meanwhile, reading longform, in a world of endless summaries and context-pivoting, asks for more. It’s become a cold medium.&lt;/p&gt;\n\n&lt;p&gt;There’s a joke: “My boss used ChatGPT to write an email to me. I summarized it and wrote a response using ChatGPT. He summarized my reply and read that.” People say: &amp;quot;See? Humans are now just intermediaries for LLMs to talk to themselves.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;But that’s not quite right.&lt;/p&gt;\n\n&lt;p&gt;It’s not that we’re conduits for the machines. It’s that the machines let us bypass the noise of language — and get closer to pure semantic truth. What we’re really doing is offloading the form of communication so we can focus on the content of it.&lt;/p&gt;\n\n&lt;p&gt;And that, I suspect, is only the beginning.&lt;/p&gt;\n\n&lt;p&gt;Soon, OpenAI, Anthropic, and others will lean into this realization — if they haven’t already — and build tools that let us pivot, summarize, and remix content while preserving its semantic core. We&amp;#39;ll get closer and closer to an interface for meaning itself. Language will become translucent. Interpretation will become seamless.&lt;/p&gt;\n\n&lt;p&gt;It’s a common trope to say humans are becoming telepathic. But transformer models are perhaps the first real step in that direction. As they evolve, converting raw impulses — even internal thoughtforms — into structured communication will become less of a challenge and more of a given.&lt;/p&gt;\n\n&lt;p&gt;Eventually, we’ll realize that text, audio, and video are just skins — just surfaces — wrapped around the same thing: semantic meaning. And once we can capture and convey that directly, we’ll look back and see that this shift wasn’t about losing language, but about transcending it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lwycam",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "monarchwadia",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwycam/longform_text_has_become_iconic_almost_like_an/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwycam/longform_text_has_become_iconic_almost_like_an/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752210229,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The AI21 Jamba family of models are hybrid SSM-Transformer foundation models, blending speed, efficient long context processing, and accuracy.\n\n\n\nfrom the website:\n\n|Model|Model Size|Max Tokens|Version|Snapshot|API Endpoint|\n|:-|:-|:-|:-|:-|:-|\n|Jamba Large|398B parameters (94B active)|256K|1.7|2025-07|`jamba-large`|\n|Jamba Mini|52B parameters (12B active)|256K|1.7|2025-07|`jamba-mini`|\n\nEngineers and data scientists at AI21 labs created the model to help developers and businesses leverage AI to build real-world products with tangible value. **Jamba Mini** and **Jamba Large** support zero-shot instruction-following and multi-language support. The Jamba models also provide developers with industry-leading APIs that perform a wide range of productivity tasks designed for commercial use.\n\n* **Organization developing model:** AI21 Labs\n* **Model date:** July 3rd, 2025\n* **Model type:** Joint Attention and Mamba (Jamba)\n* **Knowledge cutoff date** August 22nd, 2024\n* **Input Modality:** Text\n* **Output Modality:** Text\n* **License:** [Jamba open model license](https://www.ai21.com/licenses/jamba-open-model-license)",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "support for Jamba hybrid Transformer-Mamba models has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvr711",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bbbdbf",
          "ups": 74,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 74,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=bd15897bc569fad578c22335085f1c50bc5fdc47",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752087698,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The AI21 Jamba family of models are hybrid SSM-Transformer foundation models, blending speed, efficient long context processing, and accuracy.&lt;/p&gt;\n\n&lt;p&gt;from the website:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Model Size&lt;/th&gt;\n&lt;th align=\"left\"&gt;Max Tokens&lt;/th&gt;\n&lt;th align=\"left\"&gt;Version&lt;/th&gt;\n&lt;th align=\"left\"&gt;Snapshot&lt;/th&gt;\n&lt;th align=\"left\"&gt;API Endpoint&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Jamba Large&lt;/td&gt;\n&lt;td align=\"left\"&gt;398B parameters (94B active)&lt;/td&gt;\n&lt;td align=\"left\"&gt;256K&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.7&lt;/td&gt;\n&lt;td align=\"left\"&gt;2025-07&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;code&gt;jamba-large&lt;/code&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Jamba Mini&lt;/td&gt;\n&lt;td align=\"left\"&gt;52B parameters (12B active)&lt;/td&gt;\n&lt;td align=\"left\"&gt;256K&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.7&lt;/td&gt;\n&lt;td align=\"left\"&gt;2025-07&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;code&gt;jamba-mini&lt;/code&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Engineers and data scientists at AI21 labs created the model to help developers and businesses leverage AI to build real-world products with tangible value. &lt;strong&gt;Jamba Mini&lt;/strong&gt; and &lt;strong&gt;Jamba Large&lt;/strong&gt; support zero-shot instruction-following and multi-language support. The Jamba models also provide developers with industry-leading APIs that perform a wide range of productivity tasks designed for commercial use.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Organization developing model:&lt;/strong&gt; AI21 Labs&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Model date:&lt;/strong&gt; July 3rd, 2025&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Model type:&lt;/strong&gt; Joint Attention and Mamba (Jamba)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Knowledge cutoff date&lt;/strong&gt; August 22nd, 2024&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Input Modality:&lt;/strong&gt; Text&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Output Modality:&lt;/strong&gt; Text&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; &lt;a href=\"https://www.ai21.com/licenses/jamba-open-model-license\"&gt;Jamba open model license&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/7531",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?auto=webp&amp;s=1e28f2615bcb62135d8d732a8ea85dd226f1b014",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=340e18fba3f412557fd7374f9b789abc78d4f2eb",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd61a7c984debedf58dfafd58331a67a8d8fd322",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=da7395dd510f76d8224e3e4fee5cb162c818d6c0",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e1458a795a3b7a775e1a94d7767c299e1d8f3ef",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=48a7f32bb6df4e15a5e213c8c8dc317d5fa14ce0",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fb2f9e222f5c82bc7c1c7600a290f7addc8c2012",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lvr711",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lvr711/support_for_jamba_hybrid_transformermamba_models/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/7531",
          "subreddit_subscribers": 497353,
          "created_utc": 1752087698,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone, I'm looking for the best roleplaying models that will run on dual 3090s. Roughly 48GB of VRAM. What's the best one you've played around with? Older models are fine too. \n\nI'd prefer uncensored for erp reasons, but anything useful for visual novel related style writing would be perfect. ",
          "author_fullname": "t2_1r4qus6iuv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Roleplaying Models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwjok4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.53,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752170773,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I&amp;#39;m looking for the best roleplaying models that will run on dual 3090s. Roughly 48GB of VRAM. What&amp;#39;s the best one you&amp;#39;ve played around with? Older models are fine too. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d prefer uncensored for erp reasons, but anything useful for visual novel related style writing would be perfect. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwjok4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "reirinani",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwjok4/best_roleplaying_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwjok4/best_roleplaying_models/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752170773,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here's a thing that I am copying pasting from another post I will add a too long didn't read to the end of this post if it was too long and you didn't read didn't want to read this f****** post.\n\nI NEED I need to use my text to text model that is very specific for my application purpose. I would like to add a microphone input feature to this system. I am currently using jan.ai as the software to host this model. Is there a way to make an AI use my voice input as the text to text input. If this is not possible or you do not know how to do that then maybe there is a software that I can use that's will allow me to use my voice input as the text to text input for this text to text input model that is very specific to my cause. That's a lot I hope that you have an AI that can just tell me what I need and then maybe it will make things easier. Thank you in advance.\n\nTL DR: I want to use the microphone just like I'm doing now to put my voice into the text to text model. I hope that helps man.",
          "author_fullname": "t2_8m0780dx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is up AI Bros I am stupid and maybe your AI can help me be less of these stupid",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwxglp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752207287,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s a thing that I am copying pasting from another post I will add a too long didn&amp;#39;t read to the end of this post if it was too long and you didn&amp;#39;t read didn&amp;#39;t want to read this f****** post.&lt;/p&gt;\n\n&lt;p&gt;I NEED I need to use my text to text model that is very specific for my application purpose. I would like to add a microphone input feature to this system. I am currently using jan.ai as the software to host this model. Is there a way to make an AI use my voice input as the text to text input. If this is not possible or you do not know how to do that then maybe there is a software that I can use that&amp;#39;s will allow me to use my voice input as the text to text input for this text to text input model that is very specific to my cause. That&amp;#39;s a lot I hope that you have an AI that can just tell me what I need and then maybe it will make things easier. Thank you in advance.&lt;/p&gt;\n\n&lt;p&gt;TL DR: I want to use the microphone just like I&amp;#39;m doing now to put my voice into the text to text model. I hope that helps man.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwxglp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Parking_Razzmatazz89",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwxglp/what_is_up_ai_bros_i_am_stupid_and_maybe_your_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwxglp/what_is_up_ai_bros_i_am_stupid_and_maybe_your_ai/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752207287,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "From [siliconhighway](https://www.siliconhighway.com/wp-content/robotics-and-edge-ai-datasheet-jetson-thor-devkit-nvidia-us-web.pdf)  \nLook **BIG,** but:\n\n* AGX Orin: 2048-core NVIDIA Ampere architecture GPU with 64 Tensor Cores @ 1.3 GHz\n* AGX Thor: 2560-core NVIDIA Blackwell architecture GPU with 96 fifth-gen Tensor Cores @ 1.575 GHz\n\nHow is **275 -&gt;1000 TOPS** (FP8/INT8) computed? (with NVDEC,NVENC, +??)  \nAdditional info to [look through](https://developer.nvidia.com/embedded/downloads)",
          "author_fullname": "t2_12uqkv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "New Nvidia Jetson AGX Thor developer kit specs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 125,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "162nk0irtwbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 87,
                  "x": 108,
                  "u": "https://preview.redd.it/162nk0irtwbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dd72bea46bd5dca350961c56a4a9b076c49df4c9"
                },
                {
                  "y": 175,
                  "x": 216,
                  "u": "https://preview.redd.it/162nk0irtwbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7bd5525620e804888990befe1c109fdf5db669b6"
                },
                {
                  "y": 260,
                  "x": 320,
                  "u": "https://preview.redd.it/162nk0irtwbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf575e5d86197e58f1af128125eaffe217d3716e"
                }
              ],
              "s": {
                "y": 482,
                "x": 592,
                "u": "https://preview.redd.it/162nk0irtwbf1.png?width=592&amp;format=png&amp;auto=webp&amp;s=97c1dcb477f07274f5ae445a4ca40753855f1e54"
              },
              "id": "162nk0irtwbf1"
            },
            "zegozj2btwbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 97,
                  "x": 108,
                  "u": "https://preview.redd.it/zegozj2btwbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=46b29c23fdf33dc63fb72f3ce8ac286394764355"
                },
                {
                  "y": 194,
                  "x": 216,
                  "u": "https://preview.redd.it/zegozj2btwbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1538606dce915f11bbc33211ff8ed69ebdedcc53"
                },
                {
                  "y": 287,
                  "x": 320,
                  "u": "https://preview.redd.it/zegozj2btwbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e628028ee60e0a5912a57187e1a18bc70ae79180"
                },
                {
                  "y": 575,
                  "x": 640,
                  "u": "https://preview.redd.it/zegozj2btwbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=116619a215ad8b0c125e9d3d882da1dc97dea00b"
                },
                {
                  "y": 863,
                  "x": 960,
                  "u": "https://preview.redd.it/zegozj2btwbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d1bed243b08db5b2396bb51bb430ae105c0794b3"
                },
                {
                  "y": 971,
                  "x": 1080,
                  "u": "https://preview.redd.it/zegozj2btwbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4657090e0ee7779a927392768d4a5333640e4f89"
                }
              ],
              "s": {
                "y": 1848,
                "x": 2054,
                "u": "https://preview.redd.it/zegozj2btwbf1.png?width=2054&amp;format=png&amp;auto=webp&amp;s=d2412c8cfaf36286ac566a89495c70e08964752e"
              },
              "id": "zegozj2btwbf1"
            },
            "l4ycoyhrtwbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 105,
                  "x": 108,
                  "u": "https://preview.redd.it/l4ycoyhrtwbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a4b648a1b26c71cd499a5dcecc71c85b2aa093bb"
                },
                {
                  "y": 211,
                  "x": 216,
                  "u": "https://preview.redd.it/l4ycoyhrtwbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=700ef597caa1ea2fb4f92b6ee636536cc6a032e1"
                },
                {
                  "y": 312,
                  "x": 320,
                  "u": "https://preview.redd.it/l4ycoyhrtwbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f828d52f3bcf0ea9482325c57799c0fdfecdebd"
                },
                {
                  "y": 625,
                  "x": 640,
                  "u": "https://preview.redd.it/l4ycoyhrtwbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e3f58134952c5a47a433ca83599a6501bff39092"
                },
                {
                  "y": 938,
                  "x": 960,
                  "u": "https://preview.redd.it/l4ycoyhrtwbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a145a68dc3d2d78c62ed8ec643e9333a37a2c110"
                },
                {
                  "y": 1055,
                  "x": 1080,
                  "u": "https://preview.redd.it/l4ycoyhrtwbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9b3934f10057e9054e5335ec5433e6a0c9409859"
                }
              ],
              "s": {
                "y": 1644,
                "x": 1682,
                "u": "https://preview.redd.it/l4ycoyhrtwbf1.png?width=1682&amp;format=png&amp;auto=webp&amp;s=a7729a6f90e7ae367603362d5de951730fdefdde"
              },
              "id": "l4ycoyhrtwbf1"
            }
          },
          "name": "t3_1lvtp4h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 50,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "zegozj2btwbf1",
                "id": 702359025
              },
              {
                "media_id": "l4ycoyhrtwbf1",
                "id": 702359026
              },
              {
                "caption": "New power connector",
                "media_id": "162nk0irtwbf1",
                "id": 702359027
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 50,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/LcS0MUSWf5hhmUX4e22WKX2znHiU8l47X_F0lXz5ctQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752093626,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From &lt;a href=\"https://www.siliconhighway.com/wp-content/robotics-and-edge-ai-datasheet-jetson-thor-devkit-nvidia-us-web.pdf\"&gt;siliconhighway&lt;/a&gt;&lt;br/&gt;\nLook &lt;strong&gt;BIG,&lt;/strong&gt; but:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AGX Orin: 2048-core NVIDIA Ampere architecture GPU with 64 Tensor Cores @ 1.3 GHz&lt;/li&gt;\n&lt;li&gt;AGX Thor: 2560-core NVIDIA Blackwell architecture GPU with 96 fifth-gen Tensor Cores @ 1.575 GHz&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;How is &lt;strong&gt;275 -&amp;gt;1000 TOPS&lt;/strong&gt; (FP8/INT8) computed? (with NVDEC,NVENC, +??)&lt;br/&gt;\nAdditional info to &lt;a href=\"https://developer.nvidia.com/embedded/downloads\"&gt;look through&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lvtp4h",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lvtp4h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "martincerven",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvtp4h/new_nvidia_jetson_agx_thor_developer_kit_specs/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lvtp4h",
          "subreddit_subscribers": 497353,
          "created_utc": 1752093626,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "MedGemma is a collection of [Gemma 3](https://ai.google.dev/gemma/docs/core) variants that are trained for performance on medical text and image comprehension. Developers can use MedGemma to accelerate building healthcare-based AI applications. MedGemma currently comes in three variants: a 4B multimodal version and 27B text-only and multimodal versions.\n\nBoth MedGemma multimodal versions utilize a [SigLIP](https://arxiv.org/abs/2303.15343) image encoder that has been specifically pre-trained on a variety of de-identified medical data, including chest X-rays, dermatology images, ophthalmology images, and histopathology slides. Their LLM components are trained on a diverse set of medical data, including medical text, medical question-answer pairs, FHIR-based electronic health record data (27B multimodal only), radiology images, histopathology patches, ophthalmology images, and dermatology images.",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "multimodal medgemma 27b",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvqtxa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "#bbbdbf",
          "ups": 65,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 65,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/5kzMwR9GesyU7lrUcZBJp2EcXFMPqKJOghnDp3-PEdM.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=83ed773a31e87254095f19c5ba081e480671c890",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752086830,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MedGemma is a collection of &lt;a href=\"https://ai.google.dev/gemma/docs/core\"&gt;Gemma 3&lt;/a&gt; variants that are trained for performance on medical text and image comprehension. Developers can use MedGemma to accelerate building healthcare-based AI applications. MedGemma currently comes in three variants: a 4B multimodal version and 27B text-only and multimodal versions.&lt;/p&gt;\n\n&lt;p&gt;Both MedGemma multimodal versions utilize a &lt;a href=\"https://arxiv.org/abs/2303.15343\"&gt;SigLIP&lt;/a&gt; image encoder that has been specifically pre-trained on a variety of de-identified medical data, including chest X-rays, dermatology images, ophthalmology images, and histopathology slides. Their LLM components are trained on a diverse set of medical data, including medical text, medical question-answer pairs, FHIR-based electronic health record data (27B multimodal only), radiology images, histopathology patches, ophthalmology images, and dermatology images.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/google/medgemma-27b-it",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/5kzMwR9GesyU7lrUcZBJp2EcXFMPqKJOghnDp3-PEdM.png?auto=webp&amp;s=d279ea9871a0aabf3de549c658964d8cf6f95b4e",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/5kzMwR9GesyU7lrUcZBJp2EcXFMPqKJOghnDp3-PEdM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=91b0654ba9cb6e6d43948de111cdf5bfe18e54a1",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/5kzMwR9GesyU7lrUcZBJp2EcXFMPqKJOghnDp3-PEdM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c172737d0a8a2470b067962332ac2a927cfbe792",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/5kzMwR9GesyU7lrUcZBJp2EcXFMPqKJOghnDp3-PEdM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5765e5ae9aee845135b59849f988f931a8997462",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/5kzMwR9GesyU7lrUcZBJp2EcXFMPqKJOghnDp3-PEdM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fffb9bacdc1fe8701c5f0c1be15c595a886c9819",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/5kzMwR9GesyU7lrUcZBJp2EcXFMPqKJOghnDp3-PEdM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eb60f85510507a21a5ae2e1ad57eb8573cdbdfa0",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/5kzMwR9GesyU7lrUcZBJp2EcXFMPqKJOghnDp3-PEdM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b05eb1d6118e708c0eec93c8aa1a3d4da82589c2",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "5kzMwR9GesyU7lrUcZBJp2EcXFMPqKJOghnDp3-PEdM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lvqtxa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lvqtxa/multimodal_medgemma_27b/",
          "stickied": false,
          "url": "https://huggingface.co/google/medgemma-27b-it",
          "subreddit_subscribers": 497353,
          "created_utc": 1752086830,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ny73mbdql",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's Happening here exactly",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 97,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwih1t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.45,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ZorJ_Ppa9vMddxX-reTkJz4ePZILybS9bMjCn4fA_4w.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752168002,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/3w8tszx0z2cf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/3w8tszx0z2cf1.png?auto=webp&amp;s=65aa638770ed53324eb306804d20e2c27eef4cec",
                  "width": 1248,
                  "height": 870
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/3w8tszx0z2cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ba53f5c2383031edbb3b8928bcfa3a531688727",
                    "width": 108,
                    "height": 75
                  },
                  {
                    "url": "https://preview.redd.it/3w8tszx0z2cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=52d49d70e5fea239aaad7729158f283b498dbbb7",
                    "width": 216,
                    "height": 150
                  },
                  {
                    "url": "https://preview.redd.it/3w8tszx0z2cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a3f60bf71d88f1b9fb1ff328f042518120836975",
                    "width": 320,
                    "height": 223
                  },
                  {
                    "url": "https://preview.redd.it/3w8tszx0z2cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5e558a20244e5e8d91c1a3075a76caa75232007e",
                    "width": 640,
                    "height": 446
                  },
                  {
                    "url": "https://preview.redd.it/3w8tszx0z2cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ec978903ba1a2f947e016133f7ea28dab09e5997",
                    "width": 960,
                    "height": 669
                  },
                  {
                    "url": "https://preview.redd.it/3w8tszx0z2cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d7d3fa9d86c58f0861485904485988add772fbc3",
                    "width": 1080,
                    "height": 752
                  }
                ],
                "variants": {},
                "id": "QfPbaoFfhdIkrjhLZLPaz_D9wBKgtfu_JyXJSBmKFuw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lwih1t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "notnotnotnotgolifa",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwih1t/whats_happening_here_exactly/",
          "stickied": false,
          "url": "https://i.redd.it/3w8tszx0z2cf1.png",
          "subreddit_subscribers": 497353,
          "created_utc": 1752168002,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Blog post: [https://huggingface.co/blog/reachy-mini](https://huggingface.co/blog/reachy-mini)  \nThomas Wolf on 𝕏: [https://x.com/Thom\\_Wolf/status/1942887160983466096](https://x.com/Thom_Wolf/status/1942887160983466096)",
          "author_fullname": "t2_agjaq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "First Hugging Face robot: Reachy Mini. Hackable yet easy to use, powered by open-source and the community",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "vawnwkkirtbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 51,
                  "x": 108,
                  "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e25da71b200b62f080fe87b898cabedc3f980035"
                },
                {
                  "y": 103,
                  "x": 216,
                  "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=689f3896a6483ecf0301a3e41bee281f6b631af1"
                },
                {
                  "y": 152,
                  "x": 320,
                  "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2cd9f39ffce549fbd969f0abaebfa1827db28721"
                },
                {
                  "y": 305,
                  "x": 640,
                  "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0b0fb97447ff6826b04e66433cfd08d7a784895b"
                },
                {
                  "y": 457,
                  "x": 960,
                  "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=71f3d912f7a1d080feabcf3624d847e3e891184f"
                }
              ],
              "s": {
                "y": 508,
                "x": 1065,
                "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=1065&amp;format=pjpg&amp;auto=webp&amp;s=84d0bc38a028c0f9aa81287381a7357fd7d3e396"
              },
              "id": "vawnwkkirtbf1"
            },
            "z3ecxmnjrtbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 34,
                  "x": 108,
                  "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=65b0f4599ecfcc445909635838034b7beed0949d"
                },
                {
                  "y": 69,
                  "x": 216,
                  "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a90861be827c9441c1ca57a5f9b780139e157edc"
                },
                {
                  "y": 103,
                  "x": 320,
                  "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b071bf753389fe4c3b6390ad5599cfc5214e2850"
                },
                {
                  "y": 206,
                  "x": 640,
                  "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=059b14661bbbaf197136062fb66169ed5137df02"
                },
                {
                  "y": 309,
                  "x": 960,
                  "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3af8de2000740e8be6ccc4c79ab6ff6f8b038313"
                }
              ],
              "s": {
                "y": 335,
                "x": 1038,
                "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=1038&amp;format=pjpg&amp;auto=webp&amp;s=a9b51090153990903d5faa7b87923aba926ec717"
              },
              "id": "z3ecxmnjrtbf1"
            },
            "pxk6rpahrtbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 104,
                  "x": 108,
                  "u": "https://preview.redd.it/pxk6rpahrtbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19d1d5fe149dcc7847ec46e2699a5da8b36eeeaf"
                },
                {
                  "y": 209,
                  "x": 216,
                  "u": "https://preview.redd.it/pxk6rpahrtbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f16d7699048d9ed6d1da7f0ed5da87f297a3ad10"
                },
                {
                  "y": 310,
                  "x": 320,
                  "u": "https://preview.redd.it/pxk6rpahrtbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cba6c6952a333c15a7d428404219d57e972c9b4a"
                },
                {
                  "y": 620,
                  "x": 640,
                  "u": "https://preview.redd.it/pxk6rpahrtbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=42454fe128800677fcaa673135082649ecb884c0"
                }
              ],
              "s": {
                "y": 783,
                "x": 807,
                "u": "https://preview.redd.it/pxk6rpahrtbf1.jpg?width=807&amp;format=pjpg&amp;auto=webp&amp;s=83b123f23064a54273ada207fea811308831fa33"
              },
              "id": "pxk6rpahrtbf1"
            },
            "4d11lsmgrtbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2c6ed682689657a97879863830f5813ff276269a"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb0c530b3eeaaffde0162c03101dfb511c09267c"
                },
                {
                  "y": 179,
                  "x": 320,
                  "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d357f79375b5c35ba498487ba228e6bd7f4da0ce"
                },
                {
                  "y": 359,
                  "x": 640,
                  "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=484d4e8e25389cce7460880aa913c7d32b1ae604"
                },
                {
                  "y": 539,
                  "x": 960,
                  "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=efa51dcac1349e3cae27a4943c55ce6a8e1e7f30"
                }
              ],
              "s": {
                "y": 564,
                "x": 1003,
                "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=1003&amp;format=pjpg&amp;auto=webp&amp;s=a155b0a39487f4ce3621d2cf68531e5f0bf1232b"
              },
              "id": "4d11lsmgrtbf1"
            },
            "pgm76w4krtbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 37,
                  "x": 108,
                  "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d0f424a518cee0f36acf8ad0f186995599febcda"
                },
                {
                  "y": 74,
                  "x": 216,
                  "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58999912ce24f6b9bc822b7b0739223da5153a5e"
                },
                {
                  "y": 110,
                  "x": 320,
                  "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d958a86291f4b94112a565f6d5befb9e1ca8ed63"
                },
                {
                  "y": 221,
                  "x": 640,
                  "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7eb1cab5b9bf99df7cd81f37cda08cb68d5ac380"
                },
                {
                  "y": 331,
                  "x": 960,
                  "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c21de42549bb521f84cc5fcfd37bd1cc55f9a47"
                }
              ],
              "s": {
                "y": 369,
                "x": 1067,
                "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=1067&amp;format=pjpg&amp;auto=webp&amp;s=11985df412c1a2dca7432a762a2c2c8e2a87c786"
              },
              "id": "pgm76w4krtbf1"
            }
          },
          "name": "t3_1lvf7ww",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 274,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "4d11lsmgrtbf1",
                "id": 702004774
              },
              {
                "media_id": "pxk6rpahrtbf1",
                "id": 702004775
              },
              {
                "media_id": "vawnwkkirtbf1",
                "id": 702004776
              },
              {
                "media_id": "z3ecxmnjrtbf1",
                "id": 702004777
              },
              {
                "media_id": "pgm76w4krtbf1",
                "id": 702004778
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 274,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "https://b.thumbs.redditmedia.com/q6tonUvBmagrUz-fog-jtYbG7HMQjflqMjdSdWnuk1o.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752056540,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Blog post: &lt;a href=\"https://huggingface.co/blog/reachy-mini\"&gt;https://huggingface.co/blog/reachy-mini&lt;/a&gt;&lt;br/&gt;\nThomas Wolf on 𝕏: &lt;a href=\"https://x.com/Thom_Wolf/status/1942887160983466096\"&gt;https://x.com/Thom_Wolf/status/1942887160983466096&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lvf7ww",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lvf7ww",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nunki08",
          "discussion_type": null,
          "num_comments": 57,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvf7ww/first_hugging_face_robot_reachy_mini_hackable_yet/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lvf7ww",
          "subreddit_subscribers": 497353,
          "created_utc": 1752056540,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "T5Gemma - A Google Collection",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvqnjh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 73,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 73,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/BW2hoB_QdPR1XaEBXrwNaGmYfvxZNxobJDaeTM6FKlQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=a8a14117da66c0393f09772e70177a711859f00b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752086425,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/BW2hoB_QdPR1XaEBXrwNaGmYfvxZNxobJDaeTM6FKlQ.png?auto=webp&amp;s=cdb052186149f92660900de6097dd7ed7306f2ad",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/BW2hoB_QdPR1XaEBXrwNaGmYfvxZNxobJDaeTM6FKlQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d12c0083184eb73c833064f2e74a92b33ceddb80",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/BW2hoB_QdPR1XaEBXrwNaGmYfvxZNxobJDaeTM6FKlQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5010e082aa8d8f8fd86e6c7e3962343a8598024",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/BW2hoB_QdPR1XaEBXrwNaGmYfvxZNxobJDaeTM6FKlQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=303040342ceb0707c9b110bc20dd57954258c5c0",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/BW2hoB_QdPR1XaEBXrwNaGmYfvxZNxobJDaeTM6FKlQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=61c73857aa577d73d06a45e7f0d9e3d669d8ffd9",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/BW2hoB_QdPR1XaEBXrwNaGmYfvxZNxobJDaeTM6FKlQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a21a3517da11e7cd09273c8db69c9c3c38fbed02",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/BW2hoB_QdPR1XaEBXrwNaGmYfvxZNxobJDaeTM6FKlQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0b6bf469034506bd956a0d9c53a4dad11568b47c",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "BW2hoB_QdPR1XaEBXrwNaGmYfvxZNxobJDaeTM6FKlQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lvqnjh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvqnjh/t5gemma_a_google_collection/",
          "stickied": false,
          "url": "https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86",
          "subreddit_subscribers": 497353,
          "created_utc": 1752086425,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The most recent news lead to next [Thursday ](https://www.reddit.com/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/)as the launch date.\n\nI am hoping for something under 100B parameters, preferably 70B so it can run on a 48gb vram system. Over the last year the bulk of advancement in the open source space has been in the 400b and up and 32B and below range. No new 70B and 72B models from Meta or Qwen this generation.\n\nMOE would be great as most recent releases have been (Qwen 3, Llama4, Deepseek) but not required.\n\nDay 1 support for LlamaCPP, VLLM, Ollama and other inference engines. Google got it right with their release of Gemma3, building the infrastructure before launch.\n\nBetter reasoning / chain of thought. To some extent, almost all reasoning models are distillations of Deepseek r1, making all of the models sound, act, and behave the same. I'm hoping the dataset they used shows a diverse thinking strategies more then \"wait,\" and looping over again.\n\nCode Library data in the dataset. Looking at Qwen 2.5 coder, and the Claude family, the thing that sets these models apart from others is they have the raw library data included in their training data. When you ask about connecting to an API or local host it always get it right instead of providing a placeholder like \"localhost:8000\".\n\nDethrone QWQ. I love QWQ its the best reasoning model that not a direct distillation of Deepseek r1.  I think its the best opensource model on Fiction.liveBench Long Context. Great at coding, everything. Id like to see the innovation continue and OpenAI push the industry forward.\n\nFinally, I hope they go with a MIT or fully open source license. Nothing pseudo-open source like Llama, Research only, training restrictions on its outputs.\n\nWhat's on your wishlist?",
          "author_fullname": "t2_zr0g49ixt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is your wishlist for OpenAI's upcoming open source model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwtaor",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752194778,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The most recent news lead to next &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/\"&gt;Thursday &lt;/a&gt;as the launch date.&lt;/p&gt;\n\n&lt;p&gt;I am hoping for something under 100B parameters, preferably 70B so it can run on a 48gb vram system. Over the last year the bulk of advancement in the open source space has been in the 400b and up and 32B and below range. No new 70B and 72B models from Meta or Qwen this generation.&lt;/p&gt;\n\n&lt;p&gt;MOE would be great as most recent releases have been (Qwen 3, Llama4, Deepseek) but not required.&lt;/p&gt;\n\n&lt;p&gt;Day 1 support for LlamaCPP, VLLM, Ollama and other inference engines. Google got it right with their release of Gemma3, building the infrastructure before launch.&lt;/p&gt;\n\n&lt;p&gt;Better reasoning / chain of thought. To some extent, almost all reasoning models are distillations of Deepseek r1, making all of the models sound, act, and behave the same. I&amp;#39;m hoping the dataset they used shows a diverse thinking strategies more then &amp;quot;wait,&amp;quot; and looping over again.&lt;/p&gt;\n\n&lt;p&gt;Code Library data in the dataset. Looking at Qwen 2.5 coder, and the Claude family, the thing that sets these models apart from others is they have the raw library data included in their training data. When you ask about connecting to an API or local host it always get it right instead of providing a placeholder like &amp;quot;localhost:8000&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Dethrone QWQ. I love QWQ its the best reasoning model that not a direct distillation of Deepseek r1.  I think its the best opensource model on Fiction.liveBench Long Context. Great at coding, everything. Id like to see the innovation continue and OpenAI push the industry forward.&lt;/p&gt;\n\n&lt;p&gt;Finally, I hope they go with a MIT or fully open source license. Nothing pseudo-open source like Llama, Research only, training restrictions on its outputs.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s on your wishlist?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwtaor",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "triynizzles1",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwtaor/what_is_your_wishlist_for_openais_upcoming_open/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwtaor/what_is_your_wishlist_for_openais_upcoming_open/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752194778,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Not your everyday agent article topic 🙃\n\nCan anyone relate to this article?",
          "author_fullname": "t2_48vnyxi69",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "People Are Using AI Chatbots to Guide Their Psychedelic Trips",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwqsso",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/xRtKaJSCq_7Addv1o3JLYgIEZWdbA8k9CyZVMOkjZnU.jpeg?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=9efa719b3c63970b5250146906f84efa7100bf18",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752187939,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wired.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not your everyday agent article topic 🙃&lt;/p&gt;\n\n&lt;p&gt;Can anyone relate to this article?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.wired.com/story/people-are-using-ai-chatbots-to-guide-their-psychedelic-trips/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/xRtKaJSCq_7Addv1o3JLYgIEZWdbA8k9CyZVMOkjZnU.jpeg?auto=webp&amp;s=60d4a17f41155ea8f8aea9f966bd60720fa4353b",
                  "width": 1280,
                  "height": 670
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/xRtKaJSCq_7Addv1o3JLYgIEZWdbA8k9CyZVMOkjZnU.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=192803b0e7925fc9d1c3e639dec5d5c90bba5b5a",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/xRtKaJSCq_7Addv1o3JLYgIEZWdbA8k9CyZVMOkjZnU.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e88d0580bd5f95fcc41646e492875b91cdc266bf",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/xRtKaJSCq_7Addv1o3JLYgIEZWdbA8k9CyZVMOkjZnU.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=12d37215be1c05d611fa2e341b8958390bd3f77b",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/xRtKaJSCq_7Addv1o3JLYgIEZWdbA8k9CyZVMOkjZnU.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae4546d5ea93c4b7d964af84c75e7900e88100b9",
                    "width": 640,
                    "height": 335
                  },
                  {
                    "url": "https://external-preview.redd.it/xRtKaJSCq_7Addv1o3JLYgIEZWdbA8k9CyZVMOkjZnU.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f5bfe352d2bbc88a4ed4090caf9a84f7b7934bae",
                    "width": 960,
                    "height": 502
                  },
                  {
                    "url": "https://external-preview.redd.it/xRtKaJSCq_7Addv1o3JLYgIEZWdbA8k9CyZVMOkjZnU.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9a4f05a0797d0078308309f2292c3c013ec6e70c",
                    "width": 1080,
                    "height": 565
                  }
                ],
                "variants": {},
                "id": "xRtKaJSCq_7Addv1o3JLYgIEZWdbA8k9CyZVMOkjZnU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lwqsso",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nate4t",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwqsso/people_are_using_ai_chatbots_to_guide_their/",
          "stickied": false,
          "url": "https://www.wired.com/story/people-are-using-ai-chatbots-to-guide-their-psychedelic-trips/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752187939,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey LocalLlama,\n\nI’m building a rig with an amd epyc 7742 and 6 3090’s. \n\nCan anyone help me determine if I need 3 PSU’s or 2 to pull this off? \n\nWhat Wattage should I get? \n\nAnyone know of a good retailer or specific brands? I’m checking eBay right now but I feel like I’m a little over my head and I’m not the best at power supply math. \n\nThanks! ",
          "author_fullname": "t2_rkb6qbej1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help buying power supplies for LocalLlama rig",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw3cqn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752120535,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey LocalLlama,&lt;/p&gt;\n\n&lt;p&gt;I’m building a rig with an amd epyc 7742 and 6 3090’s. &lt;/p&gt;\n\n&lt;p&gt;Can anyone help me determine if I need 3 PSU’s or 2 to pull this off? &lt;/p&gt;\n\n&lt;p&gt;What Wattage should I get? &lt;/p&gt;\n\n&lt;p&gt;Anyone know of a good retailer or specific brands? I’m checking eBay right now but I feel like I’m a little over my head and I’m not the best at power supply math. &lt;/p&gt;\n\n&lt;p&gt;Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lw3cqn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Business-Weekend-537",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw3cqn/need_help_buying_power_supplies_for_localllama_rig/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw3cqn/need_help_buying_power_supplies_for_localllama_rig/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752120535,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying to run Gemma 3 4b models like on the edge ai gallery apk on pocket pal but after like a maximum of 1-3 prompts, i keep getting a context is full error. The egde Ai gallery works marginally better but for some reason the model dies after certain length of prompts depending on complexity. I've set token length to 4096 but it also never sticks always reverting to default setting. Any help or suggestions would be appreciated. Suggestions on other similar models would be welcome too.",
          "author_fullname": "t2_y6yeo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Pixel 9 local llm help",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwedkk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752158272,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to run Gemma 3 4b models like on the edge ai gallery apk on pocket pal but after like a maximum of 1-3 prompts, i keep getting a context is full error. The egde Ai gallery works marginally better but for some reason the model dies after certain length of prompts depending on complexity. I&amp;#39;ve set token length to 4096 but it also never sticks always reverting to default setting. Any help or suggestions would be appreciated. Suggestions on other similar models would be welcome too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwedkk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OriginalTrikz",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwedkk/pixel_9_local_llm_help/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwedkk/pixel_9_local_llm_help/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752158272,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone, we’ve been experimenting with small language models (SLMs) as a new type of game asset. We think they’re a promising way to make game mechanics more dynamic. Especially when finetuned to your game world and for focused, constrained mechanics designed to allow for more reactive output.\n\nYou can try our demo game, inspired by Edgar Allan Poe’s short story The Tell-Tale Heart, [on itch](https://aviadai.itch.io/the-tell-tale-heart). We spent two weeks pulling it together, so it’s not the most polished game. But we hope it captures a bit of the delight that emergent mechanics can provide.\n\nDesign-wise, we chose to constrain the model to picking one of 3 pre-written choices for each scenario and generating an in-character explanation for its choice. This way, the model is in a controlled environment crafted by the dev, but also adds some flavor and surprise. You can play around with editing the character background to explore the boundaries and limits of the model. We finetuned it to be quite general, but you can imagine finetuning the SLM much more closely to your game world and characters.\n\nIn the spirit of seeing more experimentation with SLMs, we’ve open-sourced everything:\n\n* [This SLM](https://huggingface.co/aviad-ai) (it’s a finetuned llama model, so under llama3 license). Performance-wise, it’s quite small at 770 MB and runs comfortably on CPU.\n* A [Unity package](https://github.com/aviad-ai/unity) for loading and integrating models into Unity (built on top of llama.cpp, under MIT license. Supports MacOS, Windows, WebGL). We’ve done quite a lot of work to optimize it. We’re working on an Unreal integration coming soon!\n* The [sample game](https://github.com/aviad-ai/UnitySamples/tree/main/TheTellTaleHeart) (under MIT license, except for the paid EndlessBook asset from the Unity store).\n\nWe’re excited about a potential future in which games are shipped with multiple, specialized SLMs running in tandem to make games more immersive. \n\nIf you’re also interested in the promise of SLMs in games, join us on [Discord](https://discord.gg/Jk4jUYghnA)! We’re planning to open-source a lot more models, sample games, integration features, etc.",
          "author_fullname": "t2_1b942dweu9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-source SLM for games, Unity package, demo game The Tell-Tale Heart",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvt4a9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 28,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/kamkdq2xmwbf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1014,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/kamkdq2xmwbf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/kamkdq2xmwbf1/DASHPlaylist.mpd?a=1754812733%2CZGVhZjllYzkzOTQxOGRjMzY5NjJmNzIzYmE0YjVlOGRhOTRkMWU3NjhlZDljNmNjMmQ4NGU2NTQwMTVjN2MzNw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 24,
              "hls_url": "https://v.redd.it/kamkdq2xmwbf1/HLSPlaylist.m3u8?a=1754812733%2CYzU3NWY0YzQzNWM1MzE1OWJkOWFiMzU2M2ExMzZkYWNkMTM3NGIzYmFhY2NiZTU3ZTRiZjNkNWFmZTdlOWJjMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/b3JtbGNxMnhtd2JmMdwzBK8WlOQ5bf-9CLDL_anvlqkZgo3IidVaSmRMq-iR.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=87f4f8975a486bc91ddaf156f135d155a8ed2aea",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752092245,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, we’ve been experimenting with small language models (SLMs) as a new type of game asset. We think they’re a promising way to make game mechanics more dynamic. Especially when finetuned to your game world and for focused, constrained mechanics designed to allow for more reactive output.&lt;/p&gt;\n\n&lt;p&gt;You can try our demo game, inspired by Edgar Allan Poe’s short story The Tell-Tale Heart, &lt;a href=\"https://aviadai.itch.io/the-tell-tale-heart\"&gt;on itch&lt;/a&gt;. We spent two weeks pulling it together, so it’s not the most polished game. But we hope it captures a bit of the delight that emergent mechanics can provide.&lt;/p&gt;\n\n&lt;p&gt;Design-wise, we chose to constrain the model to picking one of 3 pre-written choices for each scenario and generating an in-character explanation for its choice. This way, the model is in a controlled environment crafted by the dev, but also adds some flavor and surprise. You can play around with editing the character background to explore the boundaries and limits of the model. We finetuned it to be quite general, but you can imagine finetuning the SLM much more closely to your game world and characters.&lt;/p&gt;\n\n&lt;p&gt;In the spirit of seeing more experimentation with SLMs, we’ve open-sourced everything:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/aviad-ai\"&gt;This SLM&lt;/a&gt; (it’s a finetuned llama model, so under llama3 license). Performance-wise, it’s quite small at 770 MB and runs comfortably on CPU.&lt;/li&gt;\n&lt;li&gt;A &lt;a href=\"https://github.com/aviad-ai/unity\"&gt;Unity package&lt;/a&gt; for loading and integrating models into Unity (built on top of llama.cpp, under MIT license. Supports MacOS, Windows, WebGL). We’ve done quite a lot of work to optimize it. We’re working on an Unreal integration coming soon!&lt;/li&gt;\n&lt;li&gt;The &lt;a href=\"https://github.com/aviad-ai/UnitySamples/tree/main/TheTellTaleHeart\"&gt;sample game&lt;/a&gt; (under MIT license, except for the paid EndlessBook asset from the Unity store).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We’re excited about a potential future in which games are shipped with multiple, specialized SLMs running in tandem to make games more immersive. &lt;/p&gt;\n\n&lt;p&gt;If you’re also interested in the promise of SLMs in games, join us on &lt;a href=\"https://discord.gg/Jk4jUYghnA\"&gt;Discord&lt;/a&gt;! We’re planning to open-source a lot more models, sample games, integration features, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/kamkdq2xmwbf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/b3JtbGNxMnhtd2JmMdwzBK8WlOQ5bf-9CLDL_anvlqkZgo3IidVaSmRMq-iR.png?format=pjpg&amp;auto=webp&amp;s=32962726028adb44c90857660bf04673238d2f6f",
                  "width": 2044,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/b3JtbGNxMnhtd2JmMdwzBK8WlOQ5bf-9CLDL_anvlqkZgo3IidVaSmRMq-iR.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=80ad4cf701c0fd9232c8f5f2ecd6ada079159ab2",
                    "width": 108,
                    "height": 57
                  },
                  {
                    "url": "https://external-preview.redd.it/b3JtbGNxMnhtd2JmMdwzBK8WlOQ5bf-9CLDL_anvlqkZgo3IidVaSmRMq-iR.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=36cdc6c0a66c0216c786f107154cdc7ebc85278c",
                    "width": 216,
                    "height": 114
                  },
                  {
                    "url": "https://external-preview.redd.it/b3JtbGNxMnhtd2JmMdwzBK8WlOQ5bf-9CLDL_anvlqkZgo3IidVaSmRMq-iR.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7bb63be32211fd496935e5c6f72b0b9d618e20ae",
                    "width": 320,
                    "height": 169
                  },
                  {
                    "url": "https://external-preview.redd.it/b3JtbGNxMnhtd2JmMdwzBK8WlOQ5bf-9CLDL_anvlqkZgo3IidVaSmRMq-iR.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=45e2be77c3b5c6060ced89151237901761d02be0",
                    "width": 640,
                    "height": 338
                  },
                  {
                    "url": "https://external-preview.redd.it/b3JtbGNxMnhtd2JmMdwzBK8WlOQ5bf-9CLDL_anvlqkZgo3IidVaSmRMq-iR.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8ddfa833d417b6e10a6999acc2d8bbd190b2ac76",
                    "width": 960,
                    "height": 507
                  },
                  {
                    "url": "https://external-preview.redd.it/b3JtbGNxMnhtd2JmMdwzBK8WlOQ5bf-9CLDL_anvlqkZgo3IidVaSmRMq-iR.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=81f3c0f9ae0e5b5324b35fcb46983dcab6a466f8",
                    "width": 1080,
                    "height": 570
                  }
                ],
                "variants": {},
                "id": "b3JtbGNxMnhtd2JmMdwzBK8WlOQ5bf-9CLDL_anvlqkZgo3IidVaSmRMq-iR"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lvt4a9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "formicidfighter",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvt4a9/opensource_slm_for_games_unity_package_demo_game/",
          "stickied": false,
          "url": "https://v.redd.it/kamkdq2xmwbf1",
          "subreddit_subscribers": 497353,
          "created_utc": 1752092245,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/kamkdq2xmwbf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1014,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/kamkdq2xmwbf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/kamkdq2xmwbf1/DASHPlaylist.mpd?a=1754812733%2CZGVhZjllYzkzOTQxOGRjMzY5NjJmNzIzYmE0YjVlOGRhOTRkMWU3NjhlZDljNmNjMmQ4NGU2NTQwMTVjN2MzNw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 24,
              "hls_url": "https://v.redd.it/kamkdq2xmwbf1/HLSPlaylist.m3u8?a=1754812733%2CYzU3NWY0YzQzNWM1MzE1OWJkOWFiMzU2M2ExMzZkYWNkMTM3NGIzYmFhY2NiZTU3ZTRiZjNkNWFmZTdlOWJjMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_gc1n4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Grok 4 seems to consult Elon Musk to answer controversial questions | TechCrunch",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 113,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwvci3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.39,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/toIC81YdeCpHv_XXWgVMM5t1mwNRxcF16uvjaJzJ29g.jpeg?width=140&amp;height=113&amp;crop=140:113,smart&amp;auto=webp&amp;s=17b1bece3396b2931f9481fed24e689df9c2e3e8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752200785,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "techcrunch.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://techcrunch.com/2025/07/10/grok-4-seems-to-consult-elon-musk-to-answer-controversial-questions/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/toIC81YdeCpHv_XXWgVMM5t1mwNRxcF16uvjaJzJ29g.jpeg?auto=webp&amp;s=03dc8a1bee01318495907bf3a59da3f7d63eb286",
                  "width": 1200,
                  "height": 977
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/toIC81YdeCpHv_XXWgVMM5t1mwNRxcF16uvjaJzJ29g.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aae92eabae52da13771e61baf746c5c920567422",
                    "width": 108,
                    "height": 87
                  },
                  {
                    "url": "https://external-preview.redd.it/toIC81YdeCpHv_XXWgVMM5t1mwNRxcF16uvjaJzJ29g.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2ddb722aee7b3fecd2015c44a585c9b2f52d45d2",
                    "width": 216,
                    "height": 175
                  },
                  {
                    "url": "https://external-preview.redd.it/toIC81YdeCpHv_XXWgVMM5t1mwNRxcF16uvjaJzJ29g.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ef1a87fd8f3c6eb584ab454aa0fef775798dbd44",
                    "width": 320,
                    "height": 260
                  },
                  {
                    "url": "https://external-preview.redd.it/toIC81YdeCpHv_XXWgVMM5t1mwNRxcF16uvjaJzJ29g.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3dfcd534374fb69196a74acf9ce86206bd44d1ac",
                    "width": 640,
                    "height": 521
                  },
                  {
                    "url": "https://external-preview.redd.it/toIC81YdeCpHv_XXWgVMM5t1mwNRxcF16uvjaJzJ29g.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc72a968369f72be58f5e05cc040967f1f51a678",
                    "width": 960,
                    "height": 781
                  },
                  {
                    "url": "https://external-preview.redd.it/toIC81YdeCpHv_XXWgVMM5t1mwNRxcF16uvjaJzJ29g.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6a4b36d7c3a954fde9061be6ff5d27b528a35b11",
                    "width": 1080,
                    "height": 879
                  }
                ],
                "variants": {},
                "id": "toIC81YdeCpHv_XXWgVMM5t1mwNRxcF16uvjaJzJ29g"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lwvci3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "srwaxalot",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwvci3/grok_4_seems_to_consult_elon_musk_to_answer/",
          "stickied": false,
          "url": "https://techcrunch.com/2025/07/10/grok-4-seems-to-consult-elon-musk-to-answer-controversial-questions/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752200785,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\nOur hospital has a ~$20,000 budget to build a local system for running health/medical data analytics using LLMs, with occasional vision tasks (via MCP) and fine-tuning.\n\nI do currently have a gemma3-med:27b  and Gemma3, Qwen3 on my 5090 test server and performing pretty good \n\nWe’re looking for advice on:\n\t1.\tWhat’s the best and largest LLM you’d recommend we can reasonably run and fine-tune within this budget (open-source preferred)? Use cases include medical Q&amp;A, clinical summarization, and structured data analysis.\n\t2.\tWhich GPU setup is optimal? Should we go for multiple RTX 5090s or consider the RTX 6000 Ada/Pro series, depending on model needs?\n\nAny input on model + hardware balance would be greatly appreciated! Bonus points for setups that support mixed workloads (text + vision) or are friendly for continuous experimentation.\n\nThanks!\n",
          "author_fullname": "t2_e4ojre534",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best LLM (and setup) recommendation for $20k health analytics project (LLM + some vision + fine-tuning)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwe0gn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752188371,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752157381,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,\nOur hospital has a ~$20,000 budget to build a local system for running health/medical data analytics using LLMs, with occasional vision tasks (via MCP) and fine-tuning.&lt;/p&gt;\n\n&lt;p&gt;I do currently have a gemma3-med:27b  and Gemma3, Qwen3 on my 5090 test server and performing pretty good &lt;/p&gt;\n\n&lt;p&gt;We’re looking for advice on:\n    1.  What’s the best and largest LLM you’d recommend we can reasonably run and fine-tune within this budget (open-source preferred)? Use cases include medical Q&amp;amp;A, clinical summarization, and structured data analysis.\n    2.  Which GPU setup is optimal? Should we go for multiple RTX 5090s or consider the RTX 6000 Ada/Pro series, depending on model needs?&lt;/p&gt;\n\n&lt;p&gt;Any input on model + hardware balance would be greatly appreciated! Bonus points for setups that support mixed workloads (text + vision) or are friendly for continuous experimentation.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwe0gn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LeastExperience1579",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwe0gn/best_llm_and_setup_recommendation_for_20k_health/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwe0gn/best_llm_and_setup_recommendation_for_20k_health/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752157381,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone!\n\nI'm building **Preceptor**, a privacy-first, local AI app that helps you stay focused by tracking your activity *without* spying on your screen or sending data to the cloud.\n\nHere’s what it does:\n\n* **Monitors your activity locally** (app focus, browser tabs via extension)\n* **Compares with your goals** (e.g., writing, coding, avoiding distractions)\n* **Gently reminds you** when you drift off course\n* **Runs entirely offline** using [Ollama](https://ollama.com) for local LLMs\n\nThink of it like an AI-powered accountability partner that respects your privacy. On browsers, it’ll use a lightweight extension to understand which site or tab you’re on — all processed locally.\n\n🔗 **Waitlist is open:** [https://preceptor-two.vercel.app/](https://preceptor-two.vercel.app/)  \nHelps me gauge interest and prioritize development because i shared my other open-source project that is gaining traction and am torn between making that app better vs building this app!\n\nAlso, if you're into local AI, productivity tools, or browser extensions, feel free to join the ongoing development — it's still early!\n\nWould love your feedback on:\n\n* What would make Preceptor useful to you day-to-day?\n* How should reminders work without being annoying?\n\nand other things you would want. \n\nThanks for reading! 🙏",
          "author_fullname": "t2_18z668t0lo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Preceptor – A Local AI Focus App That Nudges You Back on Track | Waitlist + Suggestions needed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvzwah",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752109932,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m building &lt;strong&gt;Preceptor&lt;/strong&gt;, a privacy-first, local AI app that helps you stay focused by tracking your activity &lt;em&gt;without&lt;/em&gt; spying on your screen or sending data to the cloud.&lt;/p&gt;\n\n&lt;p&gt;Here’s what it does:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Monitors your activity locally&lt;/strong&gt; (app focus, browser tabs via extension)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Compares with your goals&lt;/strong&gt; (e.g., writing, coding, avoiding distractions)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Gently reminds you&lt;/strong&gt; when you drift off course&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Runs entirely offline&lt;/strong&gt; using &lt;a href=\"https://ollama.com\"&gt;Ollama&lt;/a&gt; for local LLMs&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Think of it like an AI-powered accountability partner that respects your privacy. On browsers, it’ll use a lightweight extension to understand which site or tab you’re on — all processed locally.&lt;/p&gt;\n\n&lt;p&gt;🔗 &lt;strong&gt;Waitlist is open:&lt;/strong&gt; &lt;a href=\"https://preceptor-two.vercel.app/\"&gt;https://preceptor-two.vercel.app/&lt;/a&gt;&lt;br/&gt;\nHelps me gauge interest and prioritize development because i shared my other open-source project that is gaining traction and am torn between making that app better vs building this app!&lt;/p&gt;\n\n&lt;p&gt;Also, if you&amp;#39;re into local AI, productivity tools, or browser extensions, feel free to join the ongoing development — it&amp;#39;s still early!&lt;/p&gt;\n\n&lt;p&gt;Would love your feedback on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What would make Preceptor useful to you day-to-day?&lt;/li&gt;\n&lt;li&gt;How should reminders work without being annoying?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;and other things you would want. &lt;/p&gt;\n\n&lt;p&gt;Thanks for reading! 🙏&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?auto=webp&amp;s=a080c4707584d3aa14134960cda9ba2d339b93a3",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc759de0e8fa36d241c5728d41ee3cf022cab96",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ccf136f5d3091254a0067a3bc5d6c7df9d62d89",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2530aa4ecbcf7899ec0d023e217fe24af15fe0a6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=750a6d42fd91c5a6e9a9c069e74247c877644e97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eab390b865b031211658564ad5fe5241c9661c5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lvzwah",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Frosty-Cap-4282",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvzwah/preceptor_a_local_ai_focus_app_that_nudges_you/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvzwah/preceptor_a_local_ai_focus_app_that_nudges_you/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752109932,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I wanna ask, what guys think of this report？",
          "author_fullname": "t2_ft8sbqpq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Skywork-R1V3 Technical Report",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw402u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752122727,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanna ask, what guys think of this report？&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.06167",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lw402u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Monometum",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw402u/skyworkr1v3_technical_report/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.06167",
          "subreddit_subscribers": 497353,
          "created_utc": 1752122727,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "2 years ago, I left Windows mainly because of the creepy Copilot-type stuff — always-on apps that watch everything, take screenshots every 5 seconds, and offer \"smart\" help in return. Felt like a trade: my privacy for their convenience.\n\nNow I’m on Linux, running my local models (Ollama, etc.), and I’m wondering — what’s out there that gives that same kind of \"wow, this is scary, but actually useful\" feeling, but runs completely offline? Something which actually sort of breaches my privacy (but locally).\n\nNot just screen-watching — anything that improves workflow or feels magically helpful... but because it’s all local I can keep my hand on my heart and say \"all is well\".\n\nLooking for tools, recos or project links if anyone’s already doing this.",
          "author_fullname": "t2_bul2x6po",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What impressive (borderline creepy) local AI tools can I run now that everything is local?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvk1ms",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 64,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 64,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752075997,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752070899,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;2 years ago, I left Windows mainly because of the creepy Copilot-type stuff — always-on apps that watch everything, take screenshots every 5 seconds, and offer &amp;quot;smart&amp;quot; help in return. Felt like a trade: my privacy for their convenience.&lt;/p&gt;\n\n&lt;p&gt;Now I’m on Linux, running my local models (Ollama, etc.), and I’m wondering — what’s out there that gives that same kind of &amp;quot;wow, this is scary, but actually useful&amp;quot; feeling, but runs completely offline? Something which actually sort of breaches my privacy (but locally).&lt;/p&gt;\n\n&lt;p&gt;Not just screen-watching — anything that improves workflow or feels magically helpful... but because it’s all local I can keep my hand on my heart and say &amp;quot;all is well&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Looking for tools, recos or project links if anyone’s already doing this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvk1ms",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PeithonKing",
          "discussion_type": null,
          "num_comments": 50,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752070899,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Using chatterbox tts locally and having two problems.\n\n1. Incosistent output from cloned voice -\n\nThe generated voices are not same everytime.\n\nI cloned a 12 second clean voice of mine and tested it with the same input text multiple times.\n\nBut everytime I generate, the pacing and tone are different. Sometimes, it will speak fast and a bit loud and sometimes it will speak slow with a bit lower voice.\n\nI don't change anything in the settings. But still everytime, the output will be different.\n\nAs per their github instructions, I lowered the CFG to 0.3 but still the generated voice will have different in pacing everytime.\n\nHow to get consistent pacing and tone every time I generate speech using a cloned voice?  \nDo I need to clone a longer voice sample?\n\n2. Chatterbox has character limit of 300 per input. Can you increase to at least 1000-2000 without degrading quality? If yes, how to do it?",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Generated Voices are Not same everytime... How to fix?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwbv22",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752151691,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Using chatterbox tts locally and having two problems.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Incosistent output from cloned voice -&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The generated voices are not same everytime.&lt;/p&gt;\n\n&lt;p&gt;I cloned a 12 second clean voice of mine and tested it with the same input text multiple times.&lt;/p&gt;\n\n&lt;p&gt;But everytime I generate, the pacing and tone are different. Sometimes, it will speak fast and a bit loud and sometimes it will speak slow with a bit lower voice.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t change anything in the settings. But still everytime, the output will be different.&lt;/p&gt;\n\n&lt;p&gt;As per their github instructions, I lowered the CFG to 0.3 but still the generated voice will have different in pacing everytime.&lt;/p&gt;\n\n&lt;p&gt;How to get consistent pacing and tone every time I generate speech using a cloned voice?&lt;br/&gt;\nDo I need to clone a longer voice sample?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Chatterbox has character limit of 300 per input. Can you increase to at least 1000-2000 without degrading quality? If yes, how to do it?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwbv22",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwbv22/generated_voices_are_not_same_everytime_how_to_fix/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwbv22/generated_voices_are_not_same_everytime_how_to_fix/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752151691,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My specs are Nvidia 12 gb rtx 3060, 16 gb ram, i5.   \n  \nI was looking for something like Wan 2.1 type quality but even 5s of video at @ 480p takes around 15 minutes to generate. Way too much time.\n\nIs there any similar image 2 video tool that has Wan 2.1 quality but generates a bit faster and does not demand high resources?   \n  \nWhat are my options? Can't afford those high priced paid tools as of now :/\n\nThank you.",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Good image 2 video that doesn't need high specs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw5v9y",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752129609,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My specs are Nvidia 12 gb rtx 3060, 16 gb ram, i5.   &lt;/p&gt;\n\n&lt;p&gt;I was looking for something like Wan 2.1 type quality but even 5s of video at @ 480p takes around 15 minutes to generate. Way too much time.&lt;/p&gt;\n\n&lt;p&gt;Is there any similar image 2 video tool that has Wan 2.1 quality but generates a bit faster and does not demand high resources?   &lt;/p&gt;\n\n&lt;p&gt;What are my options? Can&amp;#39;t afford those high priced paid tools as of now :/&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lw5v9y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw5v9y/good_image_2_video_that_doesnt_need_high_specs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw5v9y/good_image_2_video_that_doesnt_need_high_specs/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752129609,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello. Every word of this post was written entirely by me, a human, with no AI involvement. Any slop is mine.\n\nblackwell_tart and associates will be receiving two 96GB workstation pro 6000 GPUs at the end of the week. We are not here to discuss the purpose to which the GPUs will be put, please accept our apologies for this omission. Needs must. \n\nInstead we are here to consider the notion that such sleek and elegant GPUs deserve a sleek and elegant home, and to that effect we are soliciting advice pertaining to aesthetically beautiful enclosures or open frames. This is not a request for mid-towers from Amazon. This is a request for the kind of work delivered slowly by an old-time lathe worker who creates pieces of functional art that make a [Mitchel Gyrodec](https://imgs.search.brave.com/216dKW4x2S_b_ufgWXZO5z3NLSq_Lx2-pXbga8mTBuI/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9saXJw/LmNkbi13ZWJzaXRl/LmNvbS85ZTFhNjgy/YS9kbXMzcmVwL211/bHRpL29wdC9neXJv/K3NlK2dhbGxlcnkr/LSsyLTE5MjB3Lmpw/Zw) look like a child's lego piece. \n\nAli Express is also fine.\n\n**Core Server**\n\nBased on our own experiences and those documented by fellow LocalLLama enthusiasts, we are building the following configuration. The final piece was to find a reliable source for GPUs, a matter that is now resolved.\n\n* **Motherboard**: [Gigabyte MZ33-AR1 rev3](https://www.gigabyte.com/us/Enterprise/Server-Motherboard/MZ33-AR1-rev-3x): It has 4x PCIe 5.0 x16 slots. The GPUs will not physically fit in the slots because the design of the motherboard is such the the CPU cooler and RDIMMs interfere. Risers are therefore required.\n* **Risers**: [Linkup AVA5 risers](https://www.amazon.com/dp/B0DKPFGY7J).\n* **CPU**: [AMD EPYC 9745](https://www.amd.com/en/products/processors/server/epyc/9005-series/amd-epyc-9745.html)\n* **Cooler**: [SilverStone XED120 WS](https://www.silverstonetek.com/en/product/info/coolers/xed120s_ws/)\n* **RAM**: 768GB 6400 MT/s DDR5 RAM in 12x [64GB modules](https://memory.net/product/m321r8ga0pb2-ccp-samsung-1x-64gb-ddr5-6400-rdimm-pc5-51200r-dual-rank-x4-module/). The motherboard supports 12 DDR5 channels with up to 24 RDIMMs and we may expand to 1.5TB in future, depending on requirements yet to be determined.\n* **PSU**: 2kW 240V e-ATX.\n* **SSDs**: 2x 4TB Samsung 9100 Pro SSD in RAID0 striped configuration. One SSD is mounted in the motherboard's m2 socket, the other SSD is mounted in a [MCIO -&gt; u2 -&gt; m2] (https://www.amazon.com/dp/B0DJMWWX27) adapter, all of which is PCIe 5.0 x4.\n\nWe think it will be quite fast. We would like it to be equally beautiful. We are old school. We prefer das blinkenlights to LED lights. We shall post pics. We shall not gtfo.\n\nHow should we dress our new creation?",
          "author_fullname": "t2_1t7r9dkpud",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We are building a 192GB (2x 96GB) Blackwell Pro 6000 server. It deserves a beautiful case. What should we use?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvxft1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.61,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752102994,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. Every word of this post was written entirely by me, a human, with no AI involvement. Any slop is mine.&lt;/p&gt;\n\n&lt;p&gt;blackwell_tart and associates will be receiving two 96GB workstation pro 6000 GPUs at the end of the week. We are not here to discuss the purpose to which the GPUs will be put, please accept our apologies for this omission. Needs must. &lt;/p&gt;\n\n&lt;p&gt;Instead we are here to consider the notion that such sleek and elegant GPUs deserve a sleek and elegant home, and to that effect we are soliciting advice pertaining to aesthetically beautiful enclosures or open frames. This is not a request for mid-towers from Amazon. This is a request for the kind of work delivered slowly by an old-time lathe worker who creates pieces of functional art that make a &lt;a href=\"https://imgs.search.brave.com/216dKW4x2S_b_ufgWXZO5z3NLSq_Lx2-pXbga8mTBuI/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9saXJw/LmNkbi13ZWJzaXRl/LmNvbS85ZTFhNjgy/YS9kbXMzcmVwL211/bHRpL29wdC9neXJv/K3NlK2dhbGxlcnkr/LSsyLTE5MjB3Lmpw/Zw\"&gt;Mitchel Gyrodec&lt;/a&gt; look like a child&amp;#39;s lego piece. &lt;/p&gt;\n\n&lt;p&gt;Ali Express is also fine.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Core Server&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Based on our own experiences and those documented by fellow LocalLLama enthusiasts, we are building the following configuration. The final piece was to find a reliable source for GPUs, a matter that is now resolved.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;: &lt;a href=\"https://www.gigabyte.com/us/Enterprise/Server-Motherboard/MZ33-AR1-rev-3x\"&gt;Gigabyte MZ33-AR1 rev3&lt;/a&gt;: It has 4x PCIe 5.0 x16 slots. The GPUs will not physically fit in the slots because the design of the motherboard is such the the CPU cooler and RDIMMs interfere. Risers are therefore required.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Risers&lt;/strong&gt;: &lt;a href=\"https://www.amazon.com/dp/B0DKPFGY7J\"&gt;Linkup AVA5 risers&lt;/a&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: &lt;a href=\"https://www.amd.com/en/products/processors/server/epyc/9005-series/amd-epyc-9745.html\"&gt;AMD EPYC 9745&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Cooler&lt;/strong&gt;: &lt;a href=\"https://www.silverstonetek.com/en/product/info/coolers/xed120s_ws/\"&gt;SilverStone XED120 WS&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: 768GB 6400 MT/s DDR5 RAM in 12x &lt;a href=\"https://memory.net/product/m321r8ga0pb2-ccp-samsung-1x-64gb-ddr5-6400-rdimm-pc5-51200r-dual-rank-x4-module/\"&gt;64GB modules&lt;/a&gt;. The motherboard supports 12 DDR5 channels with up to 24 RDIMMs and we may expand to 1.5TB in future, depending on requirements yet to be determined.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;PSU&lt;/strong&gt;: 2kW 240V e-ATX.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;SSDs&lt;/strong&gt;: 2x 4TB Samsung 9100 Pro SSD in RAID0 striped configuration. One SSD is mounted in the motherboard&amp;#39;s m2 socket, the other SSD is mounted in a &lt;a href=\"https://www.amazon.com/dp/B0DJMWWX27\"&gt;MCIO -&amp;gt; u2 -&amp;gt; m2&lt;/a&gt; adapter, all of which is PCIe 5.0 x4.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We think it will be quite fast. We would like it to be equally beautiful. We are old school. We prefer das blinkenlights to LED lights. We shall post pics. We shall not gtfo.&lt;/p&gt;\n\n&lt;p&gt;How should we dress our new creation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/p5NTS-ssfWrKhMDmmS2sYkapMMXAwcVkOkDbArj1Q_U.jpeg?auto=webp&amp;s=2a19069092cec19fdf2c4f4a168658cd9e305481",
                  "width": 860,
                  "height": 627
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/p5NTS-ssfWrKhMDmmS2sYkapMMXAwcVkOkDbArj1Q_U.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ecd20001c1a675814d3fc5ec94e1592cf64aec31",
                    "width": 108,
                    "height": 78
                  },
                  {
                    "url": "https://external-preview.redd.it/p5NTS-ssfWrKhMDmmS2sYkapMMXAwcVkOkDbArj1Q_U.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b49f31129b957ea052a591a99270317c24347f0e",
                    "width": 216,
                    "height": 157
                  },
                  {
                    "url": "https://external-preview.redd.it/p5NTS-ssfWrKhMDmmS2sYkapMMXAwcVkOkDbArj1Q_U.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6509052c1519b1d984e744f1c10e603e36505fee",
                    "width": 320,
                    "height": 233
                  },
                  {
                    "url": "https://external-preview.redd.it/p5NTS-ssfWrKhMDmmS2sYkapMMXAwcVkOkDbArj1Q_U.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d84f62a80064af32bd48791a6786dfc161173d4",
                    "width": 640,
                    "height": 466
                  }
                ],
                "variants": {},
                "id": "p5NTS-ssfWrKhMDmmS2sYkapMMXAwcVkOkDbArj1Q_U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvxft1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blackwell_tart",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvxft1/we_are_building_a_192gb_2x_96gb_blackwell_pro/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvxft1/we_are_building_a_192gb_2x_96gb_blackwell_pro/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752102994,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Models have been converging on \"not x, but y\" type phrases to an absurd degree. So here's a leaderboard for it.  \n  \nI don't think many labs are targeting this kind of slop in their training set filtering, so it gets compounded with subsequent model generations.",
          "author_fullname": "t2_pp9qh5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "\"Not x, but y\" Slop Leaderboard",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Post of the day  "
            },
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv2t7n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "transparent",
          "ups": 866,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Post of the day  :X:",
          "can_mod_post": false,
          "score": 866,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/uL3gwRdCk8J-WNVPdRm8RK50kebervSdJ936_btCrQw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":Llama:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/23w2nhjj1e9f1_t5_81eyvm/Llama"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752014921,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Models have been converging on &amp;quot;not x, but y&amp;quot; type phrases to an absurd degree. So here&amp;#39;s a leaderboard for it.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think many labs are targeting this kind of slop in their training set filtering, so it gets compounded with subsequent model generations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nxw6fmegaqbf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nxw6fmegaqbf1.png?auto=webp&amp;s=5f72913393ebeea55d572d59030a515d7e026ec0",
                  "width": 989,
                  "height": 1010
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nxw6fmegaqbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8120f376d154d7a8c30aa965a5d4aec99b2eee8b",
                    "width": 108,
                    "height": 110
                  },
                  {
                    "url": "https://preview.redd.it/nxw6fmegaqbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=50db8e8d3a69f67391ef203884d0459d28cb0f36",
                    "width": 216,
                    "height": 220
                  },
                  {
                    "url": "https://preview.redd.it/nxw6fmegaqbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1cda200654cca3e31e4c92e08cc4e54eb814d342",
                    "width": 320,
                    "height": 326
                  },
                  {
                    "url": "https://preview.redd.it/nxw6fmegaqbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f634168f40782641454db362ee799df6971e84f",
                    "width": 640,
                    "height": 653
                  },
                  {
                    "url": "https://preview.redd.it/nxw6fmegaqbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c9b95ecbe7aea07c72952b87efcbf28616d0137",
                    "width": 960,
                    "height": 980
                  }
                ],
                "variants": {},
                "id": "kkZ-LaZPi1jHaYXALY32ZrWs3vzskemUWmI2c7oZUfE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5563f7e6-52bf-11f0-a755-7266d77e32bb",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":Llama:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#58a7a4",
          "id": "1lv2t7n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_sqrkl",
          "discussion_type": null,
          "num_comments": 167,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/",
          "stickied": false,
          "url": "https://i.redd.it/nxw6fmegaqbf1.png",
          "subreddit_subscribers": 497353,
          "created_utc": 1752014921,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "1. Have CUDA installed.\n2. Go to [https://github.com/ggml-org/llama.cpp/releases](https://github.com/ggml-org/llama.cpp/releases)\n3. Find you OS .zip file, download it\n4. Unpack it to the folder of your choice\n5. At the same folder level, download Gemma 3 27B QAT Q4\\_0: `git clone` [`https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf`](https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf)\n6. Run command (for Linux, your slashes/extension may vary for Windows) and enjoy 128k context window for 3 parallel requests at once:\n\n\n\n    ./build/bin/llama-server --host localhost --port 1234  --model ./gemma-3-27b-it-qat-q4_0-gguf/gemma-3-27b-it-q4_0.gguf  --mmproj ./gemma-3-27b-it-qat-q4_0-gguf/mmproj-model-f16-27B.gguf  --alias Gemma3-27B-VISION-128k --parallel 3 -c 393216 -fa -ctv q8_0 -ctk q8_0 --ngl 999 -ts 30,31",
          "author_fullname": "t2_jti45lwl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to run Gemma 3 27B QAT with 128k context window with 3 parallel requests possible on 2x3090",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvun89",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752095892,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;Have CUDA installed.&lt;/li&gt;\n&lt;li&gt;Go to &lt;a href=\"https://github.com/ggml-org/llama.cpp/releases\"&gt;https://github.com/ggml-org/llama.cpp/releases&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Find you OS .zip file, download it&lt;/li&gt;\n&lt;li&gt;Unpack it to the folder of your choice&lt;/li&gt;\n&lt;li&gt;At the same folder level, download Gemma 3 27B QAT Q4_0: &lt;code&gt;git clone&lt;/code&gt; &lt;a href=\"https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf\"&gt;&lt;code&gt;https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run command (for Linux, your slashes/extension may vary for Windows) and enjoy 128k context window for 3 parallel requests at once:&lt;/p&gt;\n\n&lt;p&gt;./build/bin/llama-server --host localhost --port 1234  --model ./gemma-3-27b-it-qat-q4_0-gguf/gemma-3-27b-it-q4_0.gguf  --mmproj ./gemma-3-27b-it-qat-q4_0-gguf/mmproj-model-f16-27B.gguf  --alias Gemma3-27B-VISION-128k --parallel 3 -c 393216 -fa -ctv q8_0 -ctk q8_0 --ngl 999 -ts 30,31&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/DJPqvteONpGwVVw6LzaG6b8vlDa2rv2hETCaqe0z57s.png?auto=webp&amp;s=db9ea157807723165a59f5f8694d9a5016d60d0f",
                  "width": 1280,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/DJPqvteONpGwVVw6LzaG6b8vlDa2rv2hETCaqe0z57s.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=72aa5dcc1cd8dbddd3f1a103959106b666940069",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/DJPqvteONpGwVVw6LzaG6b8vlDa2rv2hETCaqe0z57s.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a4159f87f341337a34069632ee0d5b75fa4e7042",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/DJPqvteONpGwVVw6LzaG6b8vlDa2rv2hETCaqe0z57s.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b105a2c86f91fee19ce34c791a1b984348b68452",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/DJPqvteONpGwVVw6LzaG6b8vlDa2rv2hETCaqe0z57s.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae5173c455a88bb40bed1198799c0db65ff470d0",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/DJPqvteONpGwVVw6LzaG6b8vlDa2rv2hETCaqe0z57s.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d014791efbd4c8d05fd305a8b7842b029f22d83e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/DJPqvteONpGwVVw6LzaG6b8vlDa2rv2hETCaqe0z57s.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9addd19259612948921416b6f5bf04bd5191f933",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "DJPqvteONpGwVVw6LzaG6b8vlDa2rv2hETCaqe0z57s"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lvun89",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EmilPi",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvun89/how_to_run_gemma_3_27b_qat_with_128k_context/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvun89/how_to_run_gemma_3_27b_qat_with_128k_context/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752095892,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So grok 4 is a thing now, any news on open sourcing grok 2 or 3?",
          "author_fullname": "t2_cj9kap4bx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Grok open source",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwhwq0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.46,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752166695,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So grok 4 is a thing now, any news on open sourcing grok 2 or 3?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwhwq0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Afternoon_4260",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lwhwq0/grok_open_source/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwhwq0/grok_open_source/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752166695,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hopefully Gemma 3 support will be merged into torchtune soon. Assuming that happens, would it be a terrible idea to finetune https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-unquantized using torchtune's QAT?\n\ntorchtune's Int8DynActInt4WeightQATLinear uses int4 grouped per channel for the weights, but i'm not sure how compatible it would be...",
          "author_fullname": "t2_1iu07dnz2i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "QAT finetuning question",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw9m9a",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752144517,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hopefully Gemma 3 support will be merged into torchtune soon. Assuming that happens, would it be a terrible idea to finetune &lt;a href=\"https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-unquantized\"&gt;https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-unquantized&lt;/a&gt; using torchtune&amp;#39;s QAT?&lt;/p&gt;\n\n&lt;p&gt;torchtune&amp;#39;s Int8DynActInt4WeightQATLinear uses int4 grouped per channel for the weights, but i&amp;#39;m not sure how compatible it would be...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dAGoBvIHrPJ_IjolaFAQoqrjrDTXT6M-m2eRM4K3oIU.png?auto=webp&amp;s=efa782379817e430749fbf9d3fc769d39192ca04",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dAGoBvIHrPJ_IjolaFAQoqrjrDTXT6M-m2eRM4K3oIU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=07736309922dbe44a9f4db45bcca7028e052075e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/dAGoBvIHrPJ_IjolaFAQoqrjrDTXT6M-m2eRM4K3oIU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=71960c91165d8757c525d9e8638b9f2d6823dc71",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/dAGoBvIHrPJ_IjolaFAQoqrjrDTXT6M-m2eRM4K3oIU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=00d435f2fa29fcae41ba910a355f1db16f4bf958",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/dAGoBvIHrPJ_IjolaFAQoqrjrDTXT6M-m2eRM4K3oIU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=63b20101b000a42b2e2503eb6f450edafdb3dd7b",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/dAGoBvIHrPJ_IjolaFAQoqrjrDTXT6M-m2eRM4K3oIU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=86e390ea9ebd57431721cfa865e06e8620d5314a",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/dAGoBvIHrPJ_IjolaFAQoqrjrDTXT6M-m2eRM4K3oIU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4509dfc365e2031cfc8ee0c2e3bc096c12a2cb79",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "dAGoBvIHrPJ_IjolaFAQoqrjrDTXT6M-m2eRM4K3oIU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lw9m9a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "terminoid_",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw9m9a/qat_finetuning_question/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw9m9a/qat_finetuning_question/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752144517,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone. I'm a total newbie when it comes to all this neural network stuff and so on. But... I want my damn roleplay with my favorite characters!\n\nSince Chutes is no longer free and convenient in some situations, I wanted to ask — is there any alternative to Chutes? What exactly does it do, and how can it be replaced in this workflow?",
          "author_fullname": "t2_11q16u2pmp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Pls help with JanitorAI😭",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwp7e5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.23,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752183922,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. I&amp;#39;m a total newbie when it comes to all this neural network stuff and so on. But... I want my damn roleplay with my favorite characters!&lt;/p&gt;\n\n&lt;p&gt;Since Chutes is no longer free and convenient in some situations, I wanted to ask — is there any alternative to Chutes? What exactly does it do, and how can it be replaced in this workflow?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwp7e5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weekly_Fan2116",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwp7e5/pls_help_with_janitorai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwp7e5/pls_help_with_janitorai/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752183922,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been working on a Deep Researcher Agent that does multi-step web research and report generation. I wanted to share my stack and approach in case anyone else wants to build similar multi-agent workflows.  \nSo, the agent has 3 main stages:\n\n* **Searcher:** Uses Scrapegraph to crawl and extract live data\n* **Analyst:** Processes and refines the raw data using DeepSeek R1\n* **Writer:** Crafts a clean final report\n\nTo make it easy to use anywhere, I wrapped the whole flow with an MCP Server. So you can run it from Claude Desktop, Cursor, or any MCP-compatible tool. There’s also a simple Streamlit UI if you want a local dashboard.\n\nHere’s what I used to build it:\n\n* Scrapegraph for web scraping\n* Nebius AI for open-source models\n* Agno for agent orchestration\n* Streamlit for the UI\n\nThe project is still basic by design, but it's a solid starting point if you're thinking about building your own deep research workflow.\n\nIf you’re curious, I put a full video tutorial here: [demo](https://www.youtube.com/watch?v=pdsk6yldZGI)\n\nAnd the code is here if you want to try it or fork it: [Full Code](https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/deep_researcher_agent)\n\nWould love to get your feedback on what to add next or how I can improve it",
          "author_fullname": "t2_vnmiyiza",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a Deep Researcher agent and exposed it as an MCP server!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvj98v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 43,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 43,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752068897,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a Deep Researcher Agent that does multi-step web research and report generation. I wanted to share my stack and approach in case anyone else wants to build similar multi-agent workflows.&lt;br/&gt;\nSo, the agent has 3 main stages:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Searcher:&lt;/strong&gt; Uses Scrapegraph to crawl and extract live data&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Analyst:&lt;/strong&gt; Processes and refines the raw data using DeepSeek R1&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Writer:&lt;/strong&gt; Crafts a clean final report&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To make it easy to use anywhere, I wrapped the whole flow with an MCP Server. So you can run it from Claude Desktop, Cursor, or any MCP-compatible tool. There’s also a simple Streamlit UI if you want a local dashboard.&lt;/p&gt;\n\n&lt;p&gt;Here’s what I used to build it:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Scrapegraph for web scraping&lt;/li&gt;\n&lt;li&gt;Nebius AI for open-source models&lt;/li&gt;\n&lt;li&gt;Agno for agent orchestration&lt;/li&gt;\n&lt;li&gt;Streamlit for the UI&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The project is still basic by design, but it&amp;#39;s a solid starting point if you&amp;#39;re thinking about building your own deep research workflow.&lt;/p&gt;\n\n&lt;p&gt;If you’re curious, I put a full video tutorial here: &lt;a href=\"https://www.youtube.com/watch?v=pdsk6yldZGI\"&gt;demo&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And the code is here if you want to try it or fork it: &lt;a href=\"https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/deep_researcher_agent\"&gt;Full Code&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to get your feedback on what to add next or how I can improve it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/lcaCNxV_x8GGK9DcZl5R32XXYG1Qwa-DwlfowV5-_M8.jpeg?auto=webp&amp;s=02d60d4af5a2c1e4f51f9a5defaecede4faaa4c3",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/lcaCNxV_x8GGK9DcZl5R32XXYG1Qwa-DwlfowV5-_M8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19bfee8b9dd015cc1cd888971f96965311df82d7",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/lcaCNxV_x8GGK9DcZl5R32XXYG1Qwa-DwlfowV5-_M8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab26a64d50f2bf5d62e51fc4d45b6ec28e777213",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/lcaCNxV_x8GGK9DcZl5R32XXYG1Qwa-DwlfowV5-_M8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd241f0d0944dbab19df916e77528fae4730f062",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "lcaCNxV_x8GGK9DcZl5R32XXYG1Qwa-DwlfowV5-_M8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lvj98v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Arindam_200",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752068897,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all! I'm pretty new to the local LLM scene. I managed to get a small model running on my old rig (RTX 2070 + 16GB RAM) last night, and while it *technically* worked, the output quality was pretty bad. But even so, I can see real potential here, and it got me excited to take the next step. I quickly realized that my system is way too outdated for anything serious.\n\nI'm now planning a new build specifically for local LLM experimentation as a hobby, but also with an eye toward learning skills I can transfer to my day job. I work at a small company, so it’s up to me to explore AI tooling and figure out what’s possible. Right now, I'm relying heavily on the ChatGPT API for data analytics, but the costs are starting to add up. I'm hoping that with enough hands-on experience, I can eventually convince my boss to invest in local hardware to reduce API costs and give us more flexibility. I also plan to explore AI-assisted data analysis more seriously in the future.\n\nFor now, I’m mostly focused on lightweight models like LLaMA 3 7B and 13B, and might dabble in basic RAG setups or Stable Diffusion on the side.\n\n  \nPlanned Build (via BTO shop, not in the US):\n\nI’m not comfortable building PCs myself (I've broken parts in past upgrade attempts 😅), so I’ll be going through a **BTO company**, even if it costs more. Here's the current config:\n\n* Intel Core Ultra 7 265K (20 cores / 20 threads)\n* 64 GB DDR5 6400 MHz\n   * Can upgrade to 128 GB DDR5 5600 MHz for \\~$80 USD\n* RTX 5070 Ti\n   * Can upgrade to RTX 5090 for \\~$1800 USD\n* 2x NVMe SSDs\n* Base Price: \\~$3400 USD\n\nYeah, I know it’s steep — curse of being hardware dumb and needing a BTO shop.\n\n  \nA few questions I need help with:\n\n* I know RTX 5070 Ti is enough for LLaMA 7B/13B, but is it future-proof enough for other light-to-medium-weight models over the next 2–3 years?\n* Is it worth paying the $1800 premium to upgrade to the 5090 now to avoid getting bottlenecked later?\n* Should I upgrade to 128 GB RAM now, or is 64 GB DDR5 at 6400 MHz good enough for most quantized/offloaded models?\n* How much does CPU choice matter in this kind of workload (LLMs, SD, some light LoRA)? Is Intel i7-265k  fine, or should I have gone with AMD?\n\n\n\nAny thoughts or advice from others who've built rigs for local LLMs, data analysis, and occasional gaming would be really appreciated!\n\nThanks in advance 🙏\n\nEdit:  I know this might sound silly, but unfortunately, I'm limited to getting only a BTO system and restricted to RTX 5xxx series GPUs.\n\nMangae to get SDXL + random safetensors to run on my RTX2070 with offloading via Python (the only computer language i am comfortable with). Not to shabby consider that I literally start yesterday.\n\nNext up is biggest llama I can get to run on RYX2070.\n\nThis hobby is more fun than expected and should have statt this sooner.",
          "author_fullname": "t2_bkb0tcya",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Beginner Question] Entry-Level Hobbyist Build Advice — RTX 5070 Ti vs 5090? 64GB vs 128GB RAM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw12gt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752126207,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752113365,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all! I&amp;#39;m pretty new to the local LLM scene. I managed to get a small model running on my old rig (RTX 2070 + 16GB RAM) last night, and while it &lt;em&gt;technically&lt;/em&gt; worked, the output quality was pretty bad. But even so, I can see real potential here, and it got me excited to take the next step. I quickly realized that my system is way too outdated for anything serious.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m now planning a new build specifically for local LLM experimentation as a hobby, but also with an eye toward learning skills I can transfer to my day job. I work at a small company, so it’s up to me to explore AI tooling and figure out what’s possible. Right now, I&amp;#39;m relying heavily on the ChatGPT API for data analytics, but the costs are starting to add up. I&amp;#39;m hoping that with enough hands-on experience, I can eventually convince my boss to invest in local hardware to reduce API costs and give us more flexibility. I also plan to explore AI-assisted data analysis more seriously in the future.&lt;/p&gt;\n\n&lt;p&gt;For now, I’m mostly focused on lightweight models like LLaMA 3 7B and 13B, and might dabble in basic RAG setups or Stable Diffusion on the side.&lt;/p&gt;\n\n&lt;p&gt;Planned Build (via BTO shop, not in the US):&lt;/p&gt;\n\n&lt;p&gt;I’m not comfortable building PCs myself (I&amp;#39;ve broken parts in past upgrade attempts 😅), so I’ll be going through a &lt;strong&gt;BTO company&lt;/strong&gt;, even if it costs more. Here&amp;#39;s the current config:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Intel Core Ultra 7 265K (20 cores / 20 threads)&lt;/li&gt;\n&lt;li&gt;64 GB DDR5 6400 MHz\n\n&lt;ul&gt;\n&lt;li&gt;Can upgrade to 128 GB DDR5 5600 MHz for ~$80 USD&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;RTX 5070 Ti\n\n&lt;ul&gt;\n&lt;li&gt;Can upgrade to RTX 5090 for ~$1800 USD&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;2x NVMe SSDs&lt;/li&gt;\n&lt;li&gt;Base Price: ~$3400 USD&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Yeah, I know it’s steep — curse of being hardware dumb and needing a BTO shop.&lt;/p&gt;\n\n&lt;p&gt;A few questions I need help with:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I know RTX 5070 Ti is enough for LLaMA 7B/13B, but is it future-proof enough for other light-to-medium-weight models over the next 2–3 years?&lt;/li&gt;\n&lt;li&gt;Is it worth paying the $1800 premium to upgrade to the 5090 now to avoid getting bottlenecked later?&lt;/li&gt;\n&lt;li&gt;Should I upgrade to 128 GB RAM now, or is 64 GB DDR5 at 6400 MHz good enough for most quantized/offloaded models?&lt;/li&gt;\n&lt;li&gt;How much does CPU choice matter in this kind of workload (LLMs, SD, some light LoRA)? Is Intel i7-265k  fine, or should I have gone with AMD?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any thoughts or advice from others who&amp;#39;ve built rigs for local LLMs, data analysis, and occasional gaming would be really appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance 🙏&lt;/p&gt;\n\n&lt;p&gt;Edit:  I know this might sound silly, but unfortunately, I&amp;#39;m limited to getting only a BTO system and restricted to RTX 5xxx series GPUs.&lt;/p&gt;\n\n&lt;p&gt;Mangae to get SDXL + random safetensors to run on my RTX2070 with offloading via Python (the only computer language i am comfortable with). Not to shabby consider that I literally start yesterday.&lt;/p&gt;\n\n&lt;p&gt;Next up is biggest llama I can get to run on RYX2070.&lt;/p&gt;\n\n&lt;p&gt;This hobby is more fun than expected and should have statt this sooner.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lw12gt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Saruphon",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw12gt/beginner_question_entrylevel_hobbyist_build/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw12gt/beginner_question_entrylevel_hobbyist_build/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752113365,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been experimenting with local LLMs, and while I’ve had success running some models, I’m overwhelmed by the sheer number of options. I’d love some advice on how to narrow things down:\n\n* **What should I look for** in a model (e.g., size, architecture, benchmarks)?\n* **Where’s the best place to find** reliable models (HF, GGUF repos, etc.)?\n* **How can I estimate performance** on my 20GB VRAM GPU without downloading dozens of models?\n\nI’d prefer not to waste time and storage testing models blindly, so any tips on evaluating them beforehand would be hugely appreciated!\n\n*(Bonus: If you have personal favorites for my setup, I’m open to suggestions—but I’m mostly interested in learning how to decide.)*\n\n*EDIT:*  \n*My primary use cases:*\n\n1. *Brainstorming (big part of my job—needs creative, coherent output).*\n2. *Summarizing long texts (documents, articles, etc.).*\n\n",
          "author_fullname": "t2_13xhzq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help im lost",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvyqvq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752106852,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752106603,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been experimenting with local LLMs, and while I’ve had success running some models, I’m overwhelmed by the sheer number of options. I’d love some advice on how to narrow things down:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;What should I look for&lt;/strong&gt; in a model (e.g., size, architecture, benchmarks)?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Where’s the best place to find&lt;/strong&gt; reliable models (HF, GGUF repos, etc.)?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;How can I estimate performance&lt;/strong&gt; on my 20GB VRAM GPU without downloading dozens of models?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I’d prefer not to waste time and storage testing models blindly, so any tips on evaluating them beforehand would be hugely appreciated!&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;(Bonus: If you have personal favorites for my setup, I’m open to suggestions—but I’m mostly interested in learning how to decide.)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;EDIT:&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;My primary use cases:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;em&gt;Brainstorming (big part of my job—needs creative, coherent output).&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Summarizing long texts (documents, articles, etc.).&lt;/em&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvyqvq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DaBe99",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752106603,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Falcon-H1 Family of Hybrid-Head Language Models (Transformer-SSM), including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B (pretrained &amp; instruction-tuned).\n\nggufs uploaded by Falcon team:\n\n[https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF)\n\n[https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF)\n\n[https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF)\n\n[https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF)\n\n[https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF)\n\n[https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF)\n\n  \n",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "support for Falcon-H1 model family has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvd7z4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bbbdbf",
          "ups": 91,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 91,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=3581c83ed94b0a9065771a3ff0877476cc75691f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752048624,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Falcon-H1 Family of Hybrid-Head Language Models (Transformer-SSM), including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B (pretrained &amp;amp; instruction-tuned).&lt;/p&gt;\n\n&lt;p&gt;ggufs uploaded by Falcon team:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14534",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?auto=webp&amp;s=af826696823ccdf7c774b5b780e08660ad5f28d9",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa53a1d81fbb58306dfc5225b4e021e5cd8b5556",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d6b5835790e56a1933151e63ec75c62748607d96",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec7cda0503a4bf7f5bd996c7cffa0de7975e6083",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e1eba07cf9ee71a811133c3ac69643f88b0846c",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1eab70d9d1e180a23543f7080507bbf0676ff940",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0fbc3db39d35ac44a02ca54f1f888871170b49d9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lvd7z4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lvd7z4/support_for_falconh1_model_family_has_been_merged/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14534",
          "subreddit_subscribers": 497353,
          "created_utc": 1752048624,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys, I am trying to use the new OpenAI Responses API with ollama. However, I get a \n\n`raise self._make_status_error_from_response(err.response) from None`\n\n`openai.NotFoundError: 404 page not found`\n\n\n\nMy code is basically this:\n\n    from openai import OpenAI\n    \n    client = OpenAI(api_key=\"Hello\",\n                    base_url=\"http://localhost:11434/v1\")\n    \n    \n    response = client.responses.create(\n        model=\"llama3.2\",\n        input=\"Talk me about apples\")\n    \n    print(response.output)\n\nThe ollama server is successfully running at [http://localhost:11434/](http://localhost:11434/)\n\nDoes the responses API work with Ollama?\n\n**Edit**: Added /v1 to the URL (but still the same error).",
          "author_fullname": "t2_1k0rn65e6g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does the OpenAI Responses API work with Ollama?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lwb5py",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752150092,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752149623,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I am trying to use the new OpenAI Responses API with ollama. However, I get a &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;raise self._make_status_error_from_response(err.response) from None&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;openai.NotFoundError: 404 page not found&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;My code is basically this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from openai import OpenAI\n\nclient = OpenAI(api_key=&amp;quot;Hello&amp;quot;,\n                base_url=&amp;quot;http://localhost:11434/v1&amp;quot;)\n\n\nresponse = client.responses.create(\n    model=&amp;quot;llama3.2&amp;quot;,\n    input=&amp;quot;Talk me about apples&amp;quot;)\n\nprint(response.output)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The ollama server is successfully running at &lt;a href=\"http://localhost:11434/\"&gt;http://localhost:11434/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Does the responses API work with Ollama?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: Added /v1 to the URL (but still the same error).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lwb5py",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "These-South-8284",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lwb5py/does_the_openai_responses_api_work_with_ollama/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwb5py/does_the_openai_responses_api_work_with_ollama/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752149623,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ever since the release of o1, I've been noticing more and more that a lot of the efforts OpenAI is making is not necessarily on the quality of their LLMs themselves but in how they are stitched/orchestrated together. To take a simple example, you might have noticed that ChatGPT generally performs way better than the GPT APIs alone, which are just the output of raw LLMs, whereas ChatGPT is more of a black box and has several layers around the GPT models that process the answers before they reach us. Same with o1 which seems to wrap a reasoning algorithm around raw GPT as far as we know. So it's not the actual LLMs underlying closed source products that are changing the most but the way the output of raw LLMs is passed around and processed until it reaches us. Seems GPT-5 will be continuing down that path by offering a \"unified model\" with less user \"model switching\". This seems to me like this will mean more black-box model glue (i.e. switching) behind the scenes. This will mean even more vendor lock-in because different model APIs will no longer be easily interchangeable due to some outputs being way more processed than others (in the same way that ChatGPT's output is way more processed than a raw LLM's output). \n\nThe bottom line is the products being offered are no longer just LLMs but something approaching what could be called proto-agents in the sense that they have memory, use tools and reason to some degree (but they don't act on external systems yet hence \"proto\").\n\nSo I've been wondering, what does this mean for open-source? Will it mimic closed-source and pursue proto-agentic solutions or should/will it continue down the path of improving the actual LLMs, leaving the orchestration and agentic part to the developer or to open-source agentic frameworks like CrewAI/PydanticAI?\n\nI think a lot of the answer depends on whether the art of \"stitching\" together LLMs can be undertaken by any software engineer or if highly specialized researchers are needed for that. If the former, this will be great for open-source because we won't need all the resources OpenAI has and their extremely well-paid researchers. If the latter, then the future might be tougher for open-source. I personally am cautiously optimistic as I don't see orchestration requiring the same amount of resources or specialized knowledge as training an LLM but then again I don't actually know what happens behind the scenes of o3 or ChatGPT.\n\n**TLDR:** Most changes in the past year seem to have been LLM-adjacent (namely, reasoning and proto-agentic features in general). Wondering if this is an opportunity or a risk for open-source.",
          "author_fullname": "t2_8802a9mc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Musings on recent trends in closed-source and the way forward for open-source",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvu7sp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752094869,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever since the release of o1, I&amp;#39;ve been noticing more and more that a lot of the efforts OpenAI is making is not necessarily on the quality of their LLMs themselves but in how they are stitched/orchestrated together. To take a simple example, you might have noticed that ChatGPT generally performs way better than the GPT APIs alone, which are just the output of raw LLMs, whereas ChatGPT is more of a black box and has several layers around the GPT models that process the answers before they reach us. Same with o1 which seems to wrap a reasoning algorithm around raw GPT as far as we know. So it&amp;#39;s not the actual LLMs underlying closed source products that are changing the most but the way the output of raw LLMs is passed around and processed until it reaches us. Seems GPT-5 will be continuing down that path by offering a &amp;quot;unified model&amp;quot; with less user &amp;quot;model switching&amp;quot;. This seems to me like this will mean more black-box model glue (i.e. switching) behind the scenes. This will mean even more vendor lock-in because different model APIs will no longer be easily interchangeable due to some outputs being way more processed than others (in the same way that ChatGPT&amp;#39;s output is way more processed than a raw LLM&amp;#39;s output). &lt;/p&gt;\n\n&lt;p&gt;The bottom line is the products being offered are no longer just LLMs but something approaching what could be called proto-agents in the sense that they have memory, use tools and reason to some degree (but they don&amp;#39;t act on external systems yet hence &amp;quot;proto&amp;quot;).&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;ve been wondering, what does this mean for open-source? Will it mimic closed-source and pursue proto-agentic solutions or should/will it continue down the path of improving the actual LLMs, leaving the orchestration and agentic part to the developer or to open-source agentic frameworks like CrewAI/PydanticAI?&lt;/p&gt;\n\n&lt;p&gt;I think a lot of the answer depends on whether the art of &amp;quot;stitching&amp;quot; together LLMs can be undertaken by any software engineer or if highly specialized researchers are needed for that. If the former, this will be great for open-source because we won&amp;#39;t need all the resources OpenAI has and their extremely well-paid researchers. If the latter, then the future might be tougher for open-source. I personally am cautiously optimistic as I don&amp;#39;t see orchestration requiring the same amount of resources or specialized knowledge as training an LLM but then again I don&amp;#39;t actually know what happens behind the scenes of o3 or ChatGPT.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; Most changes in the past year seem to have been LLM-adjacent (namely, reasoning and proto-agentic features in general). Wondering if this is an opportunity or a risk for open-source.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lvu7sp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AgreeableCaptain1372",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvu7sp/musings_on_recent_trends_in_closedsource_and_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvu7sp/musings_on_recent_trends_in_closedsource_and_the/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752094869,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nJust dropped our paper on a simple but effective approach that got us an 8.7% accuracy boost over baseline (58.4% vs 49.7%) and absolutely crushed GPT-4.1's zero-shot performance (32%) on emotion classification.\n\nThis tutorial comes in 3 different formats:\n1. This LocalLLaMA post - summary and discussion\n2. Our blog post - [Beating ChatGPT with a dollar and a dream](https://syv.ai/viden/beating-chatgpt-dollar-dream)\n3. Our research paper - [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)\n\nThe TL;DR: Instead of training models to just spit out labels, we taught a seperate model to output ONLY reasoning given a instruction and answer. We then use that reasoning to augment other datasets. Think chain-of-thought but generated by a model optimized to generate the reasoning.\n\nWhat we did:\n\nStage 1: Fine-tuned Llama-3.2-1B on a general reasoning dataset (350k examples) to create \"Llama-R-Gen\" - basically a reasoning generator that can take any (Question, Answer) pair and explain why that answer makes sense.\n\nStage 2: Used Llama-R-Gen to augment our emotion classification dataset by generating reasoning for each text-emotion pair. Then trained a downstream classifier to output reasoning + prediction in one go.\n\nKey results:\n- 58.4% accuracy vs 49.7% baseline (statistically significant, p &lt; .001)\n- Massive gains on sadness (+19.6%), fear (+18.2%), anger (+4.0%)\n- Built-in interpretability - model explains its reasoning for every prediction\n- Domain transfer works - reasoning learned from math/code/science transferred beautifully to emotion classification\n\nThe interesting bits:\n\nWhat worked:\n- The reasoning generator trained on logical problems (math, code, science) transferred surprisingly well to the fuzzy world of emotion classification\n- Models that \"think out loud\" during training seem to learn more robust representations\n- Single model outputs both explanation and prediction - no separate explainability module needed\n\nWhat didn't:\n- Completely collapsed on the \"surprise\" class (66 samples, 3.3% of data) - likely due to poor reasoning generation for severely underrepresented classes\n- More computationally expensive than standard fine-tuning\n- Quality heavily depends on the initial reasoning generator\n\nTechnical details:\n- Base model: Llama-3.2-1B-Instruct (both stages)\n- Reasoning dataset: [syvai/reasoning-gen](https://huggingface.co/datasets/syvai/reasoning-gen) (derived from Mixture-of-Thoughts)\n- Target task: dair-ai/emotion (6 basic emotions)\n- Training: Axolotl framework on A40 GPU\n- Reasoning generator model: [syvai/reasoning-gen-1b](https://huggingface.co/syvai/reasoning-gen-1b)\n- Datasets: [syvai/emotion-reasoning](https://huggingface.co/datasets/syvai/emotion-reasoning) and [syvai/no-emotion-reasoning](https://huggingface.co/datasets/syvai/no-emotion-reasoning)\n\nThe approach is pretty generalizable - we're thinking about applying it to other classification tasks where intermediate reasoning steps could help (NLI, QA, multi-label classification, etc.).",
          "author_fullname": "t2_62puf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Here is how we beat ChatGPT at classification with 1 dollar in cloud compute",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvcb72",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 98,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 98,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752045180,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752044873,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Just dropped our paper on a simple but effective approach that got us an 8.7% accuracy boost over baseline (58.4% vs 49.7%) and absolutely crushed GPT-4.1&amp;#39;s zero-shot performance (32%) on emotion classification.&lt;/p&gt;\n\n&lt;p&gt;This tutorial comes in 3 different formats:\n1. This LocalLLaMA post - summary and discussion\n2. Our blog post - &lt;a href=\"https://syv.ai/viden/beating-chatgpt-dollar-dream\"&gt;Beating ChatGPT with a dollar and a dream&lt;/a&gt;\n3. Our research paper - &lt;a href=\"https://arxiv.org/abs/2507.00214\"&gt;Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The TL;DR: Instead of training models to just spit out labels, we taught a seperate model to output ONLY reasoning given a instruction and answer. We then use that reasoning to augment other datasets. Think chain-of-thought but generated by a model optimized to generate the reasoning.&lt;/p&gt;\n\n&lt;p&gt;What we did:&lt;/p&gt;\n\n&lt;p&gt;Stage 1: Fine-tuned Llama-3.2-1B on a general reasoning dataset (350k examples) to create &amp;quot;Llama-R-Gen&amp;quot; - basically a reasoning generator that can take any (Question, Answer) pair and explain why that answer makes sense.&lt;/p&gt;\n\n&lt;p&gt;Stage 2: Used Llama-R-Gen to augment our emotion classification dataset by generating reasoning for each text-emotion pair. Then trained a downstream classifier to output reasoning + prediction in one go.&lt;/p&gt;\n\n&lt;p&gt;Key results:\n- 58.4% accuracy vs 49.7% baseline (statistically significant, p &amp;lt; .001)\n- Massive gains on sadness (+19.6%), fear (+18.2%), anger (+4.0%)\n- Built-in interpretability - model explains its reasoning for every prediction\n- Domain transfer works - reasoning learned from math/code/science transferred beautifully to emotion classification&lt;/p&gt;\n\n&lt;p&gt;The interesting bits:&lt;/p&gt;\n\n&lt;p&gt;What worked:\n- The reasoning generator trained on logical problems (math, code, science) transferred surprisingly well to the fuzzy world of emotion classification\n- Models that &amp;quot;think out loud&amp;quot; during training seem to learn more robust representations\n- Single model outputs both explanation and prediction - no separate explainability module needed&lt;/p&gt;\n\n&lt;p&gt;What didn&amp;#39;t:\n- Completely collapsed on the &amp;quot;surprise&amp;quot; class (66 samples, 3.3% of data) - likely due to poor reasoning generation for severely underrepresented classes\n- More computationally expensive than standard fine-tuning\n- Quality heavily depends on the initial reasoning generator&lt;/p&gt;\n\n&lt;p&gt;Technical details:\n- Base model: Llama-3.2-1B-Instruct (both stages)\n- Reasoning dataset: &lt;a href=\"https://huggingface.co/datasets/syvai/reasoning-gen\"&gt;syvai/reasoning-gen&lt;/a&gt; (derived from Mixture-of-Thoughts)\n- Target task: dair-ai/emotion (6 basic emotions)\n- Training: Axolotl framework on A40 GPU\n- Reasoning generator model: &lt;a href=\"https://huggingface.co/syvai/reasoning-gen-1b\"&gt;syvai/reasoning-gen-1b&lt;/a&gt;\n- Datasets: &lt;a href=\"https://huggingface.co/datasets/syvai/emotion-reasoning\"&gt;syvai/emotion-reasoning&lt;/a&gt; and &lt;a href=\"https://huggingface.co/datasets/syvai/no-emotion-reasoning\"&gt;syvai/no-emotion-reasoning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The approach is pretty generalizable - we&amp;#39;re thinking about applying it to other classification tasks where intermediate reasoning steps could help (NLI, QA, multi-label classification, etc.).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?auto=webp&amp;s=a9d24f583d7b2574603ae8d72c49b280f34bbbd4",
                  "width": 965,
                  "height": 1386
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d3bc762d2895449456d8ae731ab05d4a9ff08669",
                    "width": 108,
                    "height": 155
                  },
                  {
                    "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=93346608db5cd935a59fa4d5a5eda50804747968",
                    "width": 216,
                    "height": 310
                  },
                  {
                    "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d19468e5d7c0abac69fa5e7da790d84234eecbe8",
                    "width": 320,
                    "height": 459
                  },
                  {
                    "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=693b7aba21c7c17a6c3a895ec1f03306cc7e5a41",
                    "width": 640,
                    "height": 919
                  },
                  {
                    "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=375f73f70720f070c5e1baae29ec6e1e97a1bd85",
                    "width": 960,
                    "height": 1378
                  }
                ],
                "variants": {},
                "id": "RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lvcb72",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "iamMess",
          "discussion_type": null,
          "num_comments": 42,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752044873,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is an app that is intended for bulk captioning directories full of images.  Mostly useful for people who have a lot of images and want to train diffusion model LORAs or similar and 1) don't want to caption by hand and 2) don't get acceptable results from plain 1-shotting with other VLM/captioning scripts.\n\nThe reason for the app is often fine tuners just try to 1-shot with their favorite VLM but adding a bit of process and some features can help immensely.  This app is setup to N-shot through a series of prompts, then capture the final output and save as a .txt file alongside each image.  You can paste in large documents describing the general \"universe\" of images, such as physical descriptions of every character in a fiction, and use the multi-step prompt to ask the VLM to identify the characters, ask it to describe the overall scene, then finally summarize the overall image to get a final caption.  I get remarkable results with this with modern VLMs like Gemma 3 27B.\n\nThe secondary reason for the app is to disconnect this type of automated captioning workflow with actual VLM hosting. This app will require you host with something like LM Studio or ollama, but unlocks every gguf model out there without this app having to manage compatibility and dependencies or be updated when new models come out or HF transformers is updated.  The app itself doesn't host anything but a Python/Flask/React/Electron app and is relatively small.  I've previously made some caption scripts that require python, transformers, diffusers, etc. and often shit just breaks over time and requiring pytorch makes delivering a small portable app virtually impossible.\n\nThe app also has some ability to read from extra metadata files, though not currently exposed in the electron GUI app.  See hint sources documentation, but tldr: it can optionally add more context like file path or read from metadata in the folder or alongside images (i.e. stuff you might have collected from webscraping scripts).\n\nPrerequisite:\n\nInstall LM Studio or whatever VLM/LLM host you want.  In LM Studio, enable the service from Developer tab.  You'll also need to Enable CORS as well if you want to use the Electron app/GUI. Ollama or others, read docs, this is r/localllama I'm sure you know wtf you're doing here.\n\nRepo:\n\n[https://github.com/victorchall/vlm-caption](https://github.com/victorchall/vlm-caption)\n\nLatest release for standalone/installer:\n\n[https://github.com/victorchall/vlm-caption/releases/tag/v1.0.36](https://github.com/victorchall/vlm-caption/releases/tag/v1.0.36)\n\nThere are a few options to run this:\n\n1. Python command line (git clone, setup venv, install requirements, edit \\`caption.yaml\\` to configure, run \\`python caption\\_openai.py\\`)\n\n2. Same as above but then run \\`cd ui &amp;&amp; npm run electron-dev\\` to run the entire GUI/app from source. \n\n3. Windows portable CLI EXE  - download [vlm-caption-cli.zip](https://github.com/victorchall/vlm-caption/releases/download/v1.0.36/vlm-caption-cli.zip), unzip, edit caption.yaml and run the exe. This is standalone so you don't need to even install python. If you're ok with editing a yaml file and reading some documentation and don't care about a pretty GUI, this will work.\n\n4. Windows standalone/installer electron GUI app. Uuse the [LM.Caption.Setup.0.1.0.exe](https://github.com/victorchall/vlm-caption/releases/download/v1.0.36/VLM.Caption.Setup.0.1.0.exe) installer.\n\nFull code and build process is in the repo and it builds on a hosted Github Action runner if you're nervous about running an unknown exe or are wary of the \"unknown publisher\" warning.  Or run it from source, idgaf, it's a FOSS hobby project.\n\nDocs in the repo are relatively up to date if you want to look them over.   The GUI could use a bit of work as it is missing a minor feature or two, will likely update later this week or weekend.  \n",
          "author_fullname": "t2_8xi6x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Bulk captioning/VLM query tool, standalone app",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 137,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvsw5d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/TDKF8Gd-Rs639wsKkkQi4vZDCwA-t9w-beK8qdQI9Bg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752091708,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is an app that is intended for bulk captioning directories full of images.  Mostly useful for people who have a lot of images and want to train diffusion model LORAs or similar and 1) don&amp;#39;t want to caption by hand and 2) don&amp;#39;t get acceptable results from plain 1-shotting with other VLM/captioning scripts.&lt;/p&gt;\n\n&lt;p&gt;The reason for the app is often fine tuners just try to 1-shot with their favorite VLM but adding a bit of process and some features can help immensely.  This app is setup to N-shot through a series of prompts, then capture the final output and save as a .txt file alongside each image.  You can paste in large documents describing the general &amp;quot;universe&amp;quot; of images, such as physical descriptions of every character in a fiction, and use the multi-step prompt to ask the VLM to identify the characters, ask it to describe the overall scene, then finally summarize the overall image to get a final caption.  I get remarkable results with this with modern VLMs like Gemma 3 27B.&lt;/p&gt;\n\n&lt;p&gt;The secondary reason for the app is to disconnect this type of automated captioning workflow with actual VLM hosting. This app will require you host with something like LM Studio or ollama, but unlocks every gguf model out there without this app having to manage compatibility and dependencies or be updated when new models come out or HF transformers is updated.  The app itself doesn&amp;#39;t host anything but a Python/Flask/React/Electron app and is relatively small.  I&amp;#39;ve previously made some caption scripts that require python, transformers, diffusers, etc. and often shit just breaks over time and requiring pytorch makes delivering a small portable app virtually impossible.&lt;/p&gt;\n\n&lt;p&gt;The app also has some ability to read from extra metadata files, though not currently exposed in the electron GUI app.  See hint sources documentation, but tldr: it can optionally add more context like file path or read from metadata in the folder or alongside images (i.e. stuff you might have collected from webscraping scripts).&lt;/p&gt;\n\n&lt;p&gt;Prerequisite:&lt;/p&gt;\n\n&lt;p&gt;Install LM Studio or whatever VLM/LLM host you want.  In LM Studio, enable the service from Developer tab.  You&amp;#39;ll also need to Enable CORS as well if you want to use the Electron app/GUI. Ollama or others, read docs, this is &lt;a href=\"/r/localllama\"&gt;r/localllama&lt;/a&gt; I&amp;#39;m sure you know wtf you&amp;#39;re doing here.&lt;/p&gt;\n\n&lt;p&gt;Repo:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/victorchall/vlm-caption\"&gt;https://github.com/victorchall/vlm-caption&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Latest release for standalone/installer:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/victorchall/vlm-caption/releases/tag/v1.0.36\"&gt;https://github.com/victorchall/vlm-caption/releases/tag/v1.0.36&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There are a few options to run this:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Python command line (git clone, setup venv, install requirements, edit `caption.yaml` to configure, run `python caption_openai.py`)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Same as above but then run `cd ui &amp;amp;&amp;amp; npm run electron-dev` to run the entire GUI/app from source. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Windows portable CLI EXE  - download &lt;a href=\"https://github.com/victorchall/vlm-caption/releases/download/v1.0.36/vlm-caption-cli.zip\"&gt;vlm-caption-cli.zip&lt;/a&gt;, unzip, edit caption.yaml and run the exe. This is standalone so you don&amp;#39;t need to even install python. If you&amp;#39;re ok with editing a yaml file and reading some documentation and don&amp;#39;t care about a pretty GUI, this will work.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Windows standalone/installer electron GUI app. Uuse the &lt;a href=\"https://github.com/victorchall/vlm-caption/releases/download/v1.0.36/VLM.Caption.Setup.0.1.0.exe\"&gt;LM.Caption.Setup.0.1.0.exe&lt;/a&gt; installer.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Full code and build process is in the repo and it builds on a hosted Github Action runner if you&amp;#39;re nervous about running an unknown exe or are wary of the &amp;quot;unknown publisher&amp;quot; warning.  Or run it from source, idgaf, it&amp;#39;s a FOSS hobby project.&lt;/p&gt;\n\n&lt;p&gt;Docs in the repo are relatively up to date if you want to look them over.   The GUI could use a bit of work as it is missing a minor feature or two, will likely update later this week or weekend.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/iznsrnd5iwbf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/iznsrnd5iwbf1.png?auto=webp&amp;s=58bc05ebcaa7f4457a37c8989ca0dc55bc59bd89",
                  "width": 1410,
                  "height": 1386
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/iznsrnd5iwbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5223c5fb4dfdbd9fd10de8dec952afc8edc09fb6",
                    "width": 108,
                    "height": 106
                  },
                  {
                    "url": "https://preview.redd.it/iznsrnd5iwbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e0a12b548c3a5eaac7f180bb7e8094c974afa24f",
                    "width": 216,
                    "height": 212
                  },
                  {
                    "url": "https://preview.redd.it/iznsrnd5iwbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=588858a8441f629edcb8e0f1a49a510ac4f2e656",
                    "width": 320,
                    "height": 314
                  },
                  {
                    "url": "https://preview.redd.it/iznsrnd5iwbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2f593b9ad38e31ba6846b8ef9d3682f4bbbfa1cf",
                    "width": 640,
                    "height": 629
                  },
                  {
                    "url": "https://preview.redd.it/iznsrnd5iwbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=390b0eb940ca18328243b2d080c564dcfb2c903e",
                    "width": 960,
                    "height": 943
                  },
                  {
                    "url": "https://preview.redd.it/iznsrnd5iwbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d473cb95bdd436d50c3906a27e1d5ac518c685b0",
                    "width": 1080,
                    "height": 1061
                  }
                ],
                "variants": {},
                "id": "85gRTUGwVoOmYyF9EMxAH-KRf_qfg2VffYDg2-O1lj4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lvsw5d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Freonr2",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvsw5d/bulk_captioningvlm_query_tool_standalone_app/",
          "stickied": false,
          "url": "https://i.redd.it/iznsrnd5iwbf1.png",
          "subreddit_subscribers": 497353,
          "created_utc": 1752091708,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Im trying to decide which cpu to get in a mini pc, but am on a budget.  Im okay shelling out for 880m over 780m, but getting mixed messages on performance in llms.\n\nI'd like to toss 64 or more ram into the system and run some llms, but i can't tell what if any igpus have support. I can only find the 395max which is way out of my budget.  From people actually running gpu-less, is it reasonable to do this, or is it kinda pointless still?  I will be using windows.\n\nIm getting a occulink capable minipc, for potential gpu options in yhe future, but dont want that to be my only option.\n\nEdit- im mostly curious about larger models.  I can already run a slow 8b model on my phone.  So I'd be most curious about 30b and 70b models.",
          "author_fullname": "t2_ovn8y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What can I expect from current amd igpu performance?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw72q8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752135697,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752134455,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im trying to decide which cpu to get in a mini pc, but am on a budget.  Im okay shelling out for 880m over 780m, but getting mixed messages on performance in llms.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to toss 64 or more ram into the system and run some llms, but i can&amp;#39;t tell what if any igpus have support. I can only find the 395max which is way out of my budget.  From people actually running gpu-less, is it reasonable to do this, or is it kinda pointless still?  I will be using windows.&lt;/p&gt;\n\n&lt;p&gt;Im getting a occulink capable minipc, for potential gpu options in yhe future, but dont want that to be my only option.&lt;/p&gt;\n\n&lt;p&gt;Edit- im mostly curious about larger models.  I can already run a slow 8b model on my phone.  So I&amp;#39;d be most curious about 30b and 70b models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lw72q8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "plzdonforgetthisname",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw72q8/what_can_i_expect_from_current_amd_igpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw72q8/what_can_i_expect_from_current_amd_igpu/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752134455,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying to classify a social media dataset (about 5k social media posts - all text) using an LLM hosted via Ollama. \n\nFirst, I ran a sample of 200 posts on gemma3-27b-it via the Gemini API and tried out different prompts with temperature set to 0.1. Once I got a satisfactory result, I ran the sample on gemma3-27b-it-fp16 via Ollama running on an H-100 with the same prompt and temperature and got very similar results. \n\nHowever, when I run the entire dataset, the accuracy drops drastically. Even the posts that were classified earlier in the sample are incorrect this time around. The only difference is that I'm running 3 client nodes in parallel and making requests to the Ollama h100 server when I'm running the entire dataset. Is it possible that Ollama is taking some measures to ensure concurrency, which may downgrade the output quality? Or is there something that I might be missing? \n\nTIA. ",
          "author_fullname": "t2_2yx8j17d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Difference in output from Gemma3 running on Ollama.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lw6u69",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752133475,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to classify a social media dataset (about 5k social media posts - all text) using an LLM hosted via Ollama. &lt;/p&gt;\n\n&lt;p&gt;First, I ran a sample of 200 posts on gemma3-27b-it via the Gemini API and tried out different prompts with temperature set to 0.1. Once I got a satisfactory result, I ran the sample on gemma3-27b-it-fp16 via Ollama running on an H-100 with the same prompt and temperature and got very similar results. &lt;/p&gt;\n\n&lt;p&gt;However, when I run the entire dataset, the accuracy drops drastically. Even the posts that were classified earlier in the sample are incorrect this time around. The only difference is that I&amp;#39;m running 3 client nodes in parallel and making requests to the Ollama h100 server when I&amp;#39;m running the entire dataset. Is it possible that Ollama is taking some measures to ensure concurrency, which may downgrade the output quality? Or is there something that I might be missing? &lt;/p&gt;\n\n&lt;p&gt;TIA. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lw6u69",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HolidayPressure",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lw6u69/difference_in_output_from_gemma3_running_on_ollama/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw6u69/difference_in_output_from_gemma3_running_on_ollama/",
          "subreddit_subscribers": 497353,
          "created_utc": 1752133475,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}