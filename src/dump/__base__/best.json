{
  "kind": "Listing",
  "data": {
    "after": "t3_1m7d9d9",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "üöÄ We‚Äôre excited to introduce Qwen3-235B-A22B-Thinking-2507 ‚Äî our most advanced reasoning model yet!\n\nOver the past 3 months, we‚Äôve significantly scaled and enhanced the thinking capability of Qwen3, achieving:\n‚úÖ Improved performance in logical reasoning, math, science &amp; coding\n‚úÖ Better general skills: instruction following, tool use, alignment\n‚úÖ 256K native context for deep, long-form understanding\n\nüß† Built exclusively for thinking mode, with no need to enable it manually. The model now natively supports extended reasoning chains for maximum depth and accuracy.",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-Thinking-2507 released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vegq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 258,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 258,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/-odbY7J30GjzUj_7jlWdKXiqJnZvfwCCllktRqnbgQw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753438585,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;üöÄ We‚Äôre excited to introduce Qwen3-235B-A22B-Thinking-2507 ‚Äî our most advanced reasoning model yet!&lt;/p&gt;\n\n&lt;p&gt;Over the past 3 months, we‚Äôve significantly scaled and enhanced the thinking capability of Qwen3, achieving:\n‚úÖ Improved performance in logical reasoning, math, science &amp;amp; coding\n‚úÖ Better general skills: instruction following, tool use, alignment\n‚úÖ 256K native context for deep, long-form understanding&lt;/p&gt;\n\n&lt;p&gt;üß† Built exclusively for thinking mode, with no need to enable it manually. The model now natively supports extended reasoning chains for maximum depth and accuracy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/bvx1dbl5xzef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?auto=webp&amp;s=859c619548c1493932ad87e55f7f58a1af5e10a9",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=12b042c0d833ea5fda0bb3962502543415136139",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f46d3176f8fa24281464889257c33a01a1436c1a",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f8f68cc4715e6804c2c5837be4369857e2df0466",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f579818ebd6748b55b90f802c28f4d37095432e",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=140ba50cbe31d7a36d36327b99a4151addfcc085",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=71a7110710b3d8cba38090a6e670da187ba8f0a7",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "DoQkFL1CfB5iTwd4k2RZEMaeKWH49DDXr_m3yklloUY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8vegq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 53,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vegq/qwen3235ba22bthinking2507_released/",
          "stickied": false,
          "url": "https://i.redd.it/bvx1dbl5xzef1.jpeg",
          "subreddit_subscribers": 504253,
          "created_utc": 1753438585,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Watching everyone else drop new models while knowing you‚Äôre going to release the best open source model of all time in about 20 years.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8myxl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 739,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 739,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/RMM5ptmjVG7kaBZS0SOzo0NX4eL3tYZb179ptZ_bN9I.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753408933,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nl9jgkkzgxef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?auto=webp&amp;s=8465f34a7f504c523ccda4f16197b27156796978",
                  "width": 1024,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b85428e434a7d4cd150f23a38f934c57dbd23502",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1164a07e29c695bb059e756e08babe234ff379d6",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5315a863e1565b2b6dd93c5a571cf54fb94a33ad",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5596e2098d9a669775268db5ef71e54bd685cd0d",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d61f41e97643e7ae41cdc73d41cb27bc076e45f",
                    "width": 960,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "e_AIfm2x33q9UQnGSrgnpxiaWz6SPpbRIOgsMere00w"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m8myxl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8myxl/watching_everyone_else_drop_new_models_while/",
          "stickied": false,
          "url": "https://i.redd.it/nl9jgkkzgxef1.jpeg",
          "subreddit_subscribers": 504253,
          "created_utc": 1753408933,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looks like we will get smaller instruct and reasoning variants of Qwen3 next week. Hopefully smaller Qwen3 coder variants aswell.",
          "author_fullname": "t2_aedi2k9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Smaller Qwen Models next week!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 120,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m8w7ny",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 134,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 134,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/6kSnt6WBnJfIBSCL6q0LkQ73C2HHdl70wbVC4gMqel8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753441468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looks like we will get smaller instruct and reasoning variants of Qwen3 next week. Hopefully smaller Qwen3 coder variants aswell.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/752ts71q50ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/752ts71q50ff1.png?auto=webp&amp;s=cd553f34eb742e1d85420c6d88ba6b8cb1d3b9d6",
                  "width": 1220,
                  "height": 1052
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9445aa998ff7b1cb74e082152702795b220a5ac",
                    "width": 108,
                    "height": 93
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=de63b169d17a3c50132d99540acd46ec84af351c",
                    "width": 216,
                    "height": 186
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebafb9a27014643332158cfd5bce11fa7ce928bb",
                    "width": 320,
                    "height": 275
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=27677972e2a6faf4ae42e2c72e03cfbb90ab79cb",
                    "width": 640,
                    "height": 551
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dbb199b19983285065aac667c6d68d707942ae1d",
                    "width": 960,
                    "height": 827
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aff7f0acf9e11e6cd46908b27417dd44a8c4e224",
                    "width": 1080,
                    "height": 931
                  }
                ],
                "variants": {},
                "id": "Uxg0LnkLD_KDODwI8dd_rf7OYZXf6oVvIyFguNn4CcI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8w7ny",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "R46H4V",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8w7ny/smaller_qwen_models_next_week/",
          "stickied": false,
          "url": "https://i.redd.it/752ts71q50ff1.png",
          "subreddit_subscribers": 504253,
          "created_utc": 1753441468,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/Alibaba_Qwen/status/1948688466386280706?t=7T6_M6vN6HrK4wvLjFNVBg&amp;s=19",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Amazing qwen 3 updated thinking model just released !! Open source !",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vhp3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 73,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 73,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/F41cNbRL6zX_oMjLZyQ-1dfZZnw6cXvW4eSrGH95qqI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753438909,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/Alibaba_Qwen/status/1948688466386280706?t=7T6_M6vN6HrK4wvLjFNVBg&amp;amp;s=19\"&gt;https://x.com/Alibaba_Qwen/status/1948688466386280706?t=7T6_M6vN6HrK4wvLjFNVBg&amp;amp;s=19&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nx5d8w74yzef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?auto=webp&amp;s=96c200cb190833b2bf1f5012444f0dc3b238d209",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b98d17e59ac2e72b931b0b8fd7215c2bc7e353d",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a348a0bdfb7057916cf5942fbc64bb0dc17a34e",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=956c91eab46464e833aaeb318551a0b4b8b10b46",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d728468419b7ffc3426c85447250b3cc034f70a",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5dab0ca10ad6fe3a88aa470b1eabd3b5a53a9b77",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a7f6c6dd942072b45b5ae2a1c5871cfece1acdb7",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "KvH8TJ7zk2PBZHdgGic1Zrs7h6lhFOr63lQmnuZcFlg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8vhp3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vhp3/amazing_qwen_3_updated_thinking_model_just/",
          "stickied": false,
          "url": "https://i.redd.it/nx5d8w74yzef1.jpeg",
          "subreddit_subscribers": 504253,
          "created_utc": 1753438909,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/ \n\n‚ÄúIf you listen to the hype, it‚Äôs like we should be seeing AI doctors and AI lawyers and AI software engineers, and that‚Äôs just not true,‚Äù he says. ‚ÄúIf we can‚Äôt even get more than 10% on a contamination-free SWE-Bench, that‚Äôs the reality check for me.‚Äù",
          "author_fullname": "t2_nrsswg757",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A contamination-free coding benchmark shows AI may not be as excellent as claimed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ud84",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 47,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 47,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753434586,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/\"&gt;https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;‚ÄúIf you listen to the hype, it‚Äôs like we should be seeing AI doctors and AI lawyers and AI software engineers, and that‚Äôs just not true,‚Äù he says. ‚ÄúIf we can‚Äôt even get more than 10% on a contamination-free SWE-Bench, that‚Äôs the reality check for me.‚Äù&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?auto=webp&amp;s=2bfdda582644a1925177c0e421a4a2c6950c77a7",
                  "width": 1200,
                  "height": 675
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=faea71883e14653e5ce297d91fc495960f8b18eb",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fafd8205cf207b0508b5104593c16cf4c0ffe3de",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1cbb456de00218886e9420164f12a5eca3ff0d1",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f4b19f9c8b648f4414c2572113329cabf9dd2693",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=078b612839727b8bd391f293af598403ad688643",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c128b6e2f538773e90e5bc76e74a71f6424fcaed",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8ud84",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Creepy-Document4034",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8ud84/a_contaminationfree_coding_benchmark_shows_ai_may/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8ud84/a_contaminationfree_coding_benchmark_shows_ai_may/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753434586,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Check out this chart comparing the latest Qwen3-235B-A22B-2507 models (Instruct and Thinking) to the older versions. The improvements are huge across different tests:\n\n\t‚Ä¢\tGPQA (Graduate-level reasoning): 81 ‚Üí 71\n\t‚Ä¢\tAIME2025 (Math competition problems): 92 ‚Üí 81\n\t‚Ä¢\tLiveCodeBench v6 (Code generation and debugging): 74 ‚Üí 56\n\t‚Ä¢\tArena-Hard v2 (General problem-solving): 80 ‚Üí 62\n\nEven the new instruct version is way better than the old non-thinking one. Looks like they‚Äôve really boosted reasoning and coding skills here.\n\nWhat do you think is driving this jump, better training, bigger data, or new techniques?",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Qwen3-235B update is crushing old models in benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 81,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m8w9ah",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/wQ7SbNTBzIdOQb6lfJFHU10NFUkitTQk5yMGuXDJ-EY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753441629,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Check out this chart comparing the latest Qwen3-235B-A22B-2507 models (Instruct and Thinking) to the older versions. The improvements are huge across different tests:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;‚Ä¢ GPQA (Graduate-level reasoning): 81 ‚Üí 71\n‚Ä¢ AIME2025 (Math competition problems): 92 ‚Üí 81\n‚Ä¢ LiveCodeBench v6 (Code generation and debugging): 74 ‚Üí 56\n‚Ä¢ Arena-Hard v2 (General problem-solving): 80 ‚Üí 62\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Even the new instruct version is way better than the old non-thinking one. Looks like they‚Äôve really boosted reasoning and coding skills here.&lt;/p&gt;\n\n&lt;p&gt;What do you think is driving this jump, better training, bigger data, or new techniques?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/q009687760ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/q009687760ff1.jpeg?auto=webp&amp;s=b2eff0f0d944d9f3cc3dd7822d8f074bf89032b7",
                  "width": 2379,
                  "height": 1392
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f76378abbbe79bad791d59ab511364ecf839f4ba",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a7d7562fa1030bd11fb35ae20f3f153be32261ae",
                    "width": 216,
                    "height": 126
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dff32d54f41f3760a510fcbf21e705869a748449",
                    "width": 320,
                    "height": 187
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee6d42068b310b231eceef2e74d8ae35c50e819e",
                    "width": 640,
                    "height": 374
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d7c3a37e2d1bba48fd4f97b28bd78fe7580bb2ca",
                    "width": 960,
                    "height": 561
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8cfaba909996daa45a067d35347b915f94c15843",
                    "width": 1080,
                    "height": 631
                  }
                ],
                "variants": {},
                "id": "5z0PiohPQ5P8oWxfKaPaT1JALPktWest18Z3iN05GrQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8w9ah",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8w9ah/new_qwen3235b_update_is_crushing_old_models_in/",
          "stickied": false,
          "url": "https://i.redd.it/q009687760ff1.jpeg",
          "subreddit_subscribers": 504253,
          "created_utc": 1753441629,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Over the past three months, we have continued to scale the¬†**thinking capability**¬†of Qwen3-235B-A22B, improving both the¬†**quality and depth**¬†of reasoning. We are pleased to introduce¬†**Qwen3-235B-A22B-Thinking-2507**, featuring the following key enhancements:\n\n* **Significantly improved performance**¬†on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise ‚Äî achieving¬†**state-of-the-art results among open-source thinking models**.\n* **Markedly better general capabilities**, such as instruction following, tool usage, text generation, and alignment with human preferences.\n* **Enhanced 256K long-context understanding**¬†capabilities.",
          "author_fullname": "t2_1162lx9rgr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-235B-A22B-Thinking-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ven3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": "#ab96c2",
          "ups": 36,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d40ca12a-0e73-11ee-8563-f216e082168e",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 36,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=21733a4ca840c3530b7ab1a5843dfdeba5a3f822",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 2"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753438601,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Over the past three months, we have continued to scale the¬†&lt;strong&gt;thinking capability&lt;/strong&gt;¬†of Qwen3-235B-A22B, improving both the¬†&lt;strong&gt;quality and depth&lt;/strong&gt;¬†of reasoning. We are pleased to introduce¬†&lt;strong&gt;Qwen3-235B-A22B-Thinking-2507&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Significantly improved performance&lt;/strong&gt;¬†on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise ‚Äî achieving¬†&lt;strong&gt;state-of-the-art results among open-source thinking models&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Markedly better general capabilities&lt;/strong&gt;, such as instruction following, tool usage, text generation, and alignment with human preferences.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Enhanced 256K long-context understanding&lt;/strong&gt;¬†capabilities.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?auto=webp&amp;s=91ee507d4a4a214e9d8d575336cf37333e5678f2",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd52c9fa4a571e95dfd71b26b8e6ebff17bbc117",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=159f17b22507b591ab3268fba6357cfbc5b4d5ed",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ce88d6294a42f488b4c5238bcdd5abcbb6bd0f2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdec699720d09b0abd832855f564b348eefd2304",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a336e6ae20fea77a6e44bc4f35540e297e8cce2c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6c85c9c232a3126b98c3e0be994b7cb036c1e34d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 2",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8ven3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yoracale",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m8ven3/qwenqwen3235ba22bthinking2507/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
          "subreddit_subscribers": 504253,
          "created_utc": 1753438601,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_10pze1d3jf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Executive Order: \"Preventing Woke AI in the Federal Government\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8l648",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 213,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 213,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=9f4b098cfa68cb7be71aac428f695841b0c9cbbe",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753403766,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "whitehouse.gov",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?auto=webp&amp;s=ecf43e8e82602652ec95e06f13b6ce18da205b9c",
                  "width": 1200,
                  "height": 628
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c1e4661cbba0b6e1e232602fbabfa0384ba0123",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b84255c302c8464ea76b251e4d4ab64cac0ec723",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7c4bae3b4c97261af353a9ec64d3ef027f6deac",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb89e898879eb7adef969749433776a6f6a543ad",
                    "width": 640,
                    "height": 334
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f16221a57c07b16c8cef11acfc0eeb15f6f1254e",
                    "width": 960,
                    "height": 502
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=db29c2e5309166fabf6283791735d6762adf4b55",
                    "width": 1080,
                    "height": 565
                  }
                ],
                "variants": {},
                "id": "4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8l648",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NunyaBuzor",
          "discussion_type": null,
          "num_comments": 100,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8l648/executive_order_preventing_woke_ai_in_the_federal/",
          "stickied": false,
          "url": "https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753403766,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "its show time folks",
          "author_fullname": "t2_7g0m6735",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-235B-A22B-Thinking-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vjna",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 36,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 36,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=21733a4ca840c3530b7ab1a5843dfdeba5a3f822",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753439107,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;its show time folks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?auto=webp&amp;s=91ee507d4a4a214e9d8d575336cf37333e5678f2",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd52c9fa4a571e95dfd71b26b8e6ebff17bbc117",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=159f17b22507b591ab3268fba6357cfbc5b4d5ed",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ce88d6294a42f488b4c5238bcdd5abcbb6bd0f2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdec699720d09b0abd832855f564b348eefd2304",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a336e6ae20fea77a6e44bc4f35540e297e8cce2c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6c85c9c232a3126b98c3e0be994b7cb036c1e34d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8vjna",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ApprehensiveAd3629",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vjna/qwenqwen3235ba22bthinking2507/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
          "subreddit_subscribers": 504253,
          "created_utc": 1753439107,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/casper_hansen_/status/1948402352320360811?t=sPHOGEKIcaucRVzENlIr1g&amp;s=19",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ok next big open source model also from China only ! Which is about to release",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m88jdh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 810,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 810,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/c08j-el568SvKGYGEd0gZbFM3-WDn7gmHlrwY9mVv5E.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753373337,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/casper_hansen_/status/1948402352320360811?t=sPHOGEKIcaucRVzENlIr1g&amp;amp;s=19\"&gt;https://x.com/casper_hansen_/status/1948402352320360811?t=sPHOGEKIcaucRVzENlIr1g&amp;amp;s=19&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/j6rwug34juef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/j6rwug34juef1.png?auto=webp&amp;s=18f17b9ceaa2b5d279bcbb0bb243851740e717c4",
                  "width": 1080,
                  "height": 1419
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb9a593e1fb7f521dc0f069833d5296c3e11f7e9",
                    "width": 108,
                    "height": 141
                  },
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6484155fe20574b23faf0c91f18281d0b64f0ef8",
                    "width": 216,
                    "height": 283
                  },
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=168aa62e13429a9a7659a2b00480eeed71c2b0ec",
                    "width": 320,
                    "height": 420
                  },
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a04ad517c7ca8eeeb00ee48288d8f17c562ca63c",
                    "width": 640,
                    "height": 840
                  },
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=66e086c5c04e9638800c62f55dfe3f8b4b914e18",
                    "width": 960,
                    "height": 1261
                  },
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e86fd4d029b2f6132d30619ca2201a26cba6b494",
                    "width": 1080,
                    "height": 1419
                  }
                ],
                "variants": {},
                "id": "yDDBcSzVcQ88JLp4cep6OXeVFwV_1fmwTlslE0j_6FU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m88jdh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 140,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m88jdh/ok_next_big_open_source_model_also_from_china/",
          "stickied": false,
          "url": "https://i.redd.it/j6rwug34juef1.png",
          "subreddit_subscribers": 504253,
          "created_utc": 1753373337,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_58qturpl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Announcing the open-source release of Wan2.2. Stay tuned.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8uozu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753435882,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/Ali_TongyiLab/status/1948654675575668959?t=HLbGkqoAgFio6XLkqS8ueg&amp;s=19",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8uozu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "abdouhlili",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8uozu/announcing_the_opensource_release_of_wan22_stay/",
          "stickied": false,
          "url": "https://x.com/Ali_TongyiLab/status/1948654675575668959?t=HLbGkqoAgFio6XLkqS8ueg&amp;s=19",
          "subreddit_subscribers": 504253,
          "created_utc": 1753435882,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_fiiv6xm3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-Thinking-2507 is about to be released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 55,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8dgfu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 385,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 385,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/UaMXb3ybuMfEr_Q8-TcVTvRTejPtVI4uvfxCaqtYwhE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753384454,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6l84nwc3gvef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6l84nwc3gvef1.png?auto=webp&amp;s=57ba972c71ccbbde7e6b91adbfd7e6f90f9305a6",
                  "width": 586,
                  "height": 234
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6l84nwc3gvef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b6ce58e5b6f04a919ca7ebb1329f28ea1812e03",
                    "width": 108,
                    "height": 43
                  },
                  {
                    "url": "https://preview.redd.it/6l84nwc3gvef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b26df86d2091efcaeb2caf8a3ae29e5e74a71365",
                    "width": 216,
                    "height": 86
                  },
                  {
                    "url": "https://preview.redd.it/6l84nwc3gvef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab0e139a7c20c4938872504feeddbf3c6b23197f",
                    "width": 320,
                    "height": 127
                  }
                ],
                "variants": {},
                "id": "21v1ejzHSqCrWCoLUoEo0xlXJGAQHp-JnuM_aAV5s4s"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8dgfu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dr_Karminski",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8dgfu/qwen3235ba22bthinking2507_is_about_to_be_released/",
          "stickied": false,
          "url": "https://i.redd.it/6l84nwc3gvef1.png",
          "subreddit_subscribers": 504253,
          "created_utc": 1753384454,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "First of all, I loved the experience using Qwen Code with Qwen-3-Coder, but I can't stomach the cost of Qwen-3-Coder. While yes, you can use any OpenAI-compatible model out of the box, it's not without limitations.\n\nThat‚Äôs why I forked Qwen CLI Coder (itself derived from Gemini CLI) to create¬†[**Wren Coder CLI**](https://github.com/wren-coder/wren-coder-cli): an open-source, model-agnostic AI agent for coding assistance and terminal workflows.\n\n**Why Fork?**\n\n1. Big players like Google/Qwen have little incentive to support other models. Wren will be fully model-agnostic by design.\n2. I‚Äôm splitting the project into a CLI + SDK (like Claude Code) to enable deeper agent customization.\n3. My priorities as a solo developer probably don't align with respective model companies.\n4. Why not? I just want to experiment and try new things.\n5. I have a lot of time on my hands before I join a new role and want to spend the next month or so heads down building something I will love and use every day.\n\n  \n**What am I shipping?**\n\nOver the next few weeks, I plan to focus on the following:\n\n1. Improving compatibility with a wide range of models\n2. Adding chunking/compression logic to fix token limit errors with models with smaller context windows \\*cough\\* deepseek.\n3. Splitting up the CLI and SDK\n4. Documentation\n5. Multi-model support????\n\n  \nMaybe this is overly ambitious, but again why not? I'll keep y'all posted! Wish me luck!\n\n[https://github.com/wren-coder/wren-coder-cli](https://github.com/wren-coder/wren-coder-cli)",
          "author_fullname": "t2_1sivuwuvea",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why I Forked Qwen Code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8qj9w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 47,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 47,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753420060,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First of all, I loved the experience using Qwen Code with Qwen-3-Coder, but I can&amp;#39;t stomach the cost of Qwen-3-Coder. While yes, you can use any OpenAI-compatible model out of the box, it&amp;#39;s not without limitations.&lt;/p&gt;\n\n&lt;p&gt;That‚Äôs why I forked Qwen CLI Coder (itself derived from Gemini CLI) to create¬†&lt;a href=\"https://github.com/wren-coder/wren-coder-cli\"&gt;&lt;strong&gt;Wren Coder CLI&lt;/strong&gt;&lt;/a&gt;: an open-source, model-agnostic AI agent for coding assistance and terminal workflows.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why Fork?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Big players like Google/Qwen have little incentive to support other models. Wren will be fully model-agnostic by design.&lt;/li&gt;\n&lt;li&gt;I‚Äôm splitting the project into a CLI + SDK (like Claude Code) to enable deeper agent customization.&lt;/li&gt;\n&lt;li&gt;My priorities as a solo developer probably don&amp;#39;t align with respective model companies.&lt;/li&gt;\n&lt;li&gt;Why not? I just want to experiment and try new things.&lt;/li&gt;\n&lt;li&gt;I have a lot of time on my hands before I join a new role and want to spend the next month or so heads down building something I will love and use every day.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;What am I shipping?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Over the next few weeks, I plan to focus on the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Improving compatibility with a wide range of models&lt;/li&gt;\n&lt;li&gt;Adding chunking/compression logic to fix token limit errors with models with smaller context windows *cough* deepseek.&lt;/li&gt;\n&lt;li&gt;Splitting up the CLI and SDK&lt;/li&gt;\n&lt;li&gt;Documentation&lt;/li&gt;\n&lt;li&gt;Multi-model support????&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Maybe this is overly ambitious, but again why not? I&amp;#39;ll keep y&amp;#39;all posted! Wish me luck!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/wren-coder/wren-coder-cli\"&gt;https://github.com/wren-coder/wren-coder-cli&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?auto=webp&amp;s=79d64b1289209fa79a11669d150a850e8d6ce23a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=94b1fa561118622479aef7fd3a0006f928715e0b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=166a7f91b07c437bb627e895db8e0077a2423927",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=075d04567db44f06e5b0bfb432fe8f6f37a04713",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=792082e48bea2b952a5e950a98b815d551f583b3",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ca81ad125907d790e752b70ee99ea801e80dfa3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=64b7d8f9f97d9fd67f00438f1d60441579fa0474",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8qj9w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ryanwang4thepeople",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753420060,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_21qaqh1p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 3 Thinking is coming very soon",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 51,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8dln1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 215,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 215,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1OIHh7R-mfEk892o091BDg-5ivRbaC6jjmct0HHlBxY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753384779,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/61i8pt44hvef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/61i8pt44hvef1.png?auto=webp&amp;s=199ae416540ed35191e6454a349646379029949f",
                  "width": 1654,
                  "height": 606
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f3c31400b9a3cd6f09d39f562bd58e86cdc43cbb",
                    "width": 108,
                    "height": 39
                  },
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a029fe88fcb9fa844f284c0aea7500d00db97c54",
                    "width": 216,
                    "height": 79
                  },
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=178e6c50de0e6ef14b15e781bdcb2a2be7d26232",
                    "width": 320,
                    "height": 117
                  },
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=19b99a58da488472ec93d5842e37998def1cbe76",
                    "width": 640,
                    "height": 234
                  },
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f2495dca0fab016d71398e8436fe7dc30e236a20",
                    "width": 960,
                    "height": 351
                  },
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ea6f2da7e737f0762f01e6b42c9a50cf19a6748d",
                    "width": 1080,
                    "height": 395
                  }
                ],
                "variants": {},
                "id": "Y5bhpE2oH633vsr3feG38JCFuVtcXjLVmrApMikSGkM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8dln1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dulldata",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8dln1/qwen_3_thinking_is_coming_very_soon/",
          "stickied": false,
          "url": "https://i.redd.it/61i8pt44hvef1.png",
          "subreddit_subscribers": 504253,
          "created_utc": 1753384779,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_yjt5w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ByteDance Seed Prover Achieves Silver Medal Score in IMO 2025",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8tmhd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753431610,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "seed.bytedance.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://seed.bytedance.com/en/blog/bytedance-seed-prover-achieves-silver-medal-score-in-imo-2025",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8tmhd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hedgehog0",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8tmhd/bytedance_seed_prover_achieves_silver_medal_score/",
          "stickied": false,
          "url": "https://seed.bytedance.com/en/blog/bytedance-seed-prover-achieves-silver-medal-score-in-imo-2025",
          "subreddit_subscribers": 504253,
          "created_utc": 1753431610,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China‚Äôs First High-End Gaming GPU, the Lisuan G100, Reportedly Outperforms NVIDIA‚Äôs GeForce RTX 4060 &amp; Slightly Behind the RTX 5060 in New Benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 99,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m83644",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 564,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 564,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=140&amp;height=99&amp;crop=140:99,smart&amp;auto=webp&amp;s=e317af62a4b0fea5876b0388a0cd1f5775ef680b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753360416,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/china-first-high-end-gaming-gpu-lisuan-g100-outperforms-nvidia-geforce-rtx-4060/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?auto=webp&amp;s=e789a7dbd0c89c024f728c8e0ac2c066b704ed9a",
                  "width": 728,
                  "height": 516
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff00e39d3c91233a4ad4b2458d203b679086f872",
                    "width": 108,
                    "height": 76
                  },
                  {
                    "url": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=58f702c18129404394dea56cd9d3b6975a719f62",
                    "width": 216,
                    "height": 153
                  },
                  {
                    "url": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8326f56ed914670a98663b55ce61baafb1e3d037",
                    "width": 320,
                    "height": 226
                  },
                  {
                    "url": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6aa69848c81b950052de8eb2024c390e13024272",
                    "width": 640,
                    "height": 453
                  }
                ],
                "variants": {},
                "id": "lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m83644",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 212,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m83644/chinas_first_highend_gaming_gpu_the_lisuan_g100/",
          "stickied": false,
          "url": "https://wccftech.com/china-first-high-end-gaming-gpu-lisuan-g100-outperforms-nvidia-geforce-rtx-4060/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753360416,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Buy the largest GPU that you can really afford to.  Besides the obvious cost of additional electricity, PCI slots, physical space, cooling etc.   Multiple GPUs can be annoying.\n\nFor example, I have some 16gb GPUs, 10 of them when trying to run Kimi, each layer is 7gb.   If I load 2 layers on each GPU, the most context I can put on them is roughly 4k, since one of the layer is odd and ends up taking up 14.7gb. \n\nSo to get more context, 10k, I end up putting 1 layer 7gb on each of them, leaving 9gb free or 90gb of vram free.\n\nIf I had 5 32gb GPUs, at that 7gb, I would be able to place 4 layers \\~ 28gb and still have about 3-4gb each free, which will allow me to have my 10k context.  More context with same sized GPU, and it would be faster too!\n\nGo as big as you can!",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "N + N size GPU != 2N sized GPU, go big if you can",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m8vu80",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753440200,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Buy the largest GPU that you can really afford to.  Besides the obvious cost of additional electricity, PCI slots, physical space, cooling etc.   Multiple GPUs can be annoying.&lt;/p&gt;\n\n&lt;p&gt;For example, I have some 16gb GPUs, 10 of them when trying to run Kimi, each layer is 7gb.   If I load 2 layers on each GPU, the most context I can put on them is roughly 4k, since one of the layer is odd and ends up taking up 14.7gb. &lt;/p&gt;\n\n&lt;p&gt;So to get more context, 10k, I end up putting 1 layer 7gb on each of them, leaving 9gb free or 90gb of vram free.&lt;/p&gt;\n\n&lt;p&gt;If I had 5 32gb GPUs, at that 7gb, I would be able to place 4 layers ~ 28gb and still have about 3-4gb each free, which will allow me to have my 10k context.  More context with same sized GPU, and it would be faster too!&lt;/p&gt;\n\n&lt;p&gt;Go as big as you can!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m8vu80",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753440200,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was just chatting with Claude about my experiments with Aider and qwen2.5-coder (7b &amp; 14b).\n\ni wasn't ready for Claudes response. so good.\n\nFWIW i'm trying codellama:13b next.\n\nAny advice for a local coding model and Aider on RTX3080 10GB?",
          "author_fullname": "t2_8383arktn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do models make fun of other models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m8wi62",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3VfQACGGsrR4huIE2aSbd12G7zEBM2KmWURzyANGXgs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753442447,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was just chatting with Claude about my experiments with Aider and qwen2.5-coder (7b &amp;amp; 14b).&lt;/p&gt;\n\n&lt;p&gt;i wasn&amp;#39;t ready for Claudes response. so good.&lt;/p&gt;\n\n&lt;p&gt;FWIW i&amp;#39;m trying codellama:13b next.&lt;/p&gt;\n\n&lt;p&gt;Any advice for a local coding model and Aider on RTX3080 10GB?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8sdpzbq280ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8sdpzbq280ff1.png?auto=webp&amp;s=916da339fa043930e6f17140452756e404ea0bf2",
                  "width": 718,
                  "height": 371
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8sdpzbq280ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=89b6015ee2a0d88ec0bac662235da5629baf1bbb",
                    "width": 108,
                    "height": 55
                  },
                  {
                    "url": "https://preview.redd.it/8sdpzbq280ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ce1a4289cec1dd8cedead50e63cc09efa5fee80",
                    "width": 216,
                    "height": 111
                  },
                  {
                    "url": "https://preview.redd.it/8sdpzbq280ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3de57e7740d06316720746e9e88263882f7396e9",
                    "width": 320,
                    "height": 165
                  },
                  {
                    "url": "https://preview.redd.it/8sdpzbq280ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb549a0f9db72b902ee2c8fba005b948a7894058",
                    "width": 640,
                    "height": 330
                  }
                ],
                "variants": {},
                "id": "-PX_RQ0KeK0Qo0OiDGvgWia8NQc4pA3mSJFKxM_loHM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m8wi62",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fussy-Fur3608",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8wi62/do_models_make_fun_of_other_models/",
          "stickied": false,
          "url": "https://i.redd.it/8sdpzbq280ff1.png",
          "subreddit_subscribers": 504253,
          "created_utc": 1753442447,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm about to start building my personal AI companion and during my research came across this [awesome list](https://github.com/LongHZ140516/Awesome-GrokAni-VituralMate) of AI companion projects that I wanted to share with the community.\n\n| Companion | Lang | License | Stack | Category |\n| -- | -- | -- | -- | -- |\n| [Êû´‰∫ëAIËôöÊãü‰ºô‰º¥WebÁâà](https://github.com/swordswind/ai_virtual_mate_web) - [Wiki](https://deepwiki.com/swordswind/ai_virtual_mate_web) | zh | gpl-3.0 | python | companion |\n| [Muice-Chatbot](https://github.com/Moemu/Muice-Chatbot) - [Wiki](https://deepwiki.com/Moemu/Muice-Chatbot) | zh, en | mit | python | companion |\n| [MuiceBot](https://github.com/Moemu/MuiceBot) - [Wiki](https://deepwiki.com/Moemu/MuiceBot) | zh | bsd-3-clause | python | companion |\n| [kirara-ai](https://github.com/lss233/kirara-ai) - [Wiki](https://deepwiki.com/lss233/kirara-ai) | zh | agpl-3.0 | python | companion |\n| [my-neuro](https://github.com/morettt/my-neuro) - [Wiki](https://deepwiki.com/morettt/my-neuro) | zh, en | mit | python | companion |\n| [AIAvatarKit](https://github.com/uezo/aiavatarkit) - [Wiki](https://deepwiki.com/uezo/aiavatarkit) | en | apache-2.0 | python | companion |\n| [xinghe-AI](https://github.com/lijiaxing1997/xinghe-AI) - [Wiki](https://deepwiki.com/lijiaxing1997/xinghe-AI) | zh |  | python | companion |\n| [MaiBot](https://github.com/MaiM-with-u/MaiBot) | zh | gpl-3.0 | python | companion |\n| [AI-YinMei](https://github.com/worm128/AI-YinMei) - [Wiki](https://deepwiki.com/worm128/AI-YinMei) | zh | bsd-2-clause | python, web | vtuber |\n| [Open-LLM-VTuber](https://github.com/Open-LLM-VTuber/Open-LLM-VTuber) - [Wiki](https://deepwiki.com/Open-LLM-VTuber/Open-LLM-VTuber) | en | mit | python, web | vtuber, companion |\n| [KouriChat](https://github.com/KouriChat/KouriChat) - [Wiki](https://deepwiki.com/KouriChat/KouriChat) | zh | custom | python, web | companion |\n| [Streamer-Sales](https://github.com/PeterH0323/Streamer-Sales) - [Wiki](https://deepwiki.com/PeterH0323/Streamer-Sales) | zh | agpl-3.0 | python, web | vtuber, professional |\n| [AI-Vtuber](https://github.com/Ikaros-521/AI-Vtuber) - [Wiki](https://deepwiki.com/Ikaros-521/AI-Vtuber) | zh | gpl-3.0 | python, web | vtuber |\n| [SillyTavern](https://github.com/SillyTavern/SillyTavern) - [Wiki](https://deepwiki.com/SillyTavern/SillyTavern) | en | agpl-3.0 | web | companion |\n| [lobe-vidol](https://github.com/lobehub/lobe-vidol) - [Wiki](https://deepwiki.com/lobehub/lobe-vidol) | en | apache-2.0 | web | companion |\n| [Bella](https://github.com/Jackywine/Bella) - [Wiki](https://deepwiki.com/Jackywine/Bella) | zh | mit | web | companion |\n| [AITuberKit](https://github.com/tegnike/aituber-kit) - [Wiki](https://deepwiki.com/tegnike/aituber-kit) | en, ja | custom | web | vtuber, companion |\n| [airi](https://github.com/moeru-ai/airi) - [Wiki](https://deepwiki.com/moeru-ai/airi) | en | mit | tauri | vtuber, companion |\n| [amica](https://github.com/semperai/amica) - [Wiki](https://deepwiki.com/semperai/amica) | en | mit | tauri | companion |\n| [ChatdollKit](https://github.com/uezo/ChatdollKit) - [Wiki](https://deepwiki.com/uezo/ChatdollKit) | en, ja | apache-2.0 | unity | companion |\n| [Unity-AI-Chat-Toolkit](https://github.com/zhangliwei7758/unity-AI-Chat-Toolkit) - [Wiki](https://deepwiki.com/zhangliwei7758/unity-AI-Chat-Toolkit) | zh | mit | unity | companion |\n| [ZcChat](https://github.com/Zao-chen/ZcChat) - [Wiki](https://deepwiki.com/Zao-chen/ZcChat) | zh, en | gpl-3.0 | c++ | galge |\n| [handcrafted-persona-engine](https://github.com/fagenorn/handcrafted-persona-engine) - [Wiki](https://deepwiki.com/fagenorn/handcrafted-persona-engine) | en |  | dotnet | vtuber, companion |\n\n**Notes**:\n\n- I've made some edits, such as adding license info (since I might copy the code) and organizing the list into categories for easier navigation.\n- Not all of these are dedicated companion apps (e.g. SillyTavern), but they can be adapted with some tweaking\n- Several projects only have Chinese READMEs (marked as zh), but I've included DeepWiki links to help with understanding. There's been significant progress in that community so I think it's worth exploring.\n\nI'm starting this thread for two reasons: First, I'd love to hear about your favorite AI companion apps or setups that go beyond basic prompting. For me, a true companion needs a name, avatar, personality, backstory, conversational ability, and most importantly, memory. Second, I'm particularly interested in seeing what alternatives to Grok's Ani this community will build in the future.\n\nIf I've missed anything, please let me know and I'll update the list.",
          "author_fullname": "t2_1s7w9cgxcq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Source Companion Thread",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m8wg2r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753442543,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753442258,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m about to start building my personal AI companion and during my research came across this &lt;a href=\"https://github.com/LongHZ140516/Awesome-GrokAni-VituralMate\"&gt;awesome list&lt;/a&gt; of AI companion projects that I wanted to share with the community.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Companion&lt;/th&gt;\n&lt;th&gt;Lang&lt;/th&gt;\n&lt;th&gt;License&lt;/th&gt;\n&lt;th&gt;Stack&lt;/th&gt;\n&lt;th&gt;Category&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/swordswind/ai_virtual_mate_web\"&gt;Êû´‰∫ëAIËôöÊãü‰ºô‰º¥WebÁâà&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/swordswind/ai_virtual_mate_web\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;gpl-3.0&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Moemu/Muice-Chatbot\"&gt;Muice-Chatbot&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Moemu/Muice-Chatbot\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh, en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Moemu/MuiceBot\"&gt;MuiceBot&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Moemu/MuiceBot\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;bsd-3-clause&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/lss233/kirara-ai\"&gt;kirara-ai&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/lss233/kirara-ai\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;agpl-3.0&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/morettt/my-neuro\"&gt;my-neuro&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/morettt/my-neuro\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh, en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/uezo/aiavatarkit\"&gt;AIAvatarKit&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/uezo/aiavatarkit\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;apache-2.0&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/lijiaxing1997/xinghe-AI\"&gt;xinghe-AI&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/lijiaxing1997/xinghe-AI\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/MaiM-with-u/MaiBot\"&gt;MaiBot&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;gpl-3.0&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/worm128/AI-YinMei\"&gt;AI-YinMei&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/worm128/AI-YinMei\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;bsd-2-clause&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;vtuber&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Open-LLM-VTuber/Open-LLM-VTuber\"&gt;Open-LLM-VTuber&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Open-LLM-VTuber/Open-LLM-VTuber\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;vtuber, companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/KouriChat/KouriChat\"&gt;KouriChat&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/KouriChat/KouriChat\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;custom&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/PeterH0323/Streamer-Sales\"&gt;Streamer-Sales&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/PeterH0323/Streamer-Sales\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;agpl-3.0&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;vtuber, professional&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Ikaros-521/AI-Vtuber\"&gt;AI-Vtuber&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Ikaros-521/AI-Vtuber\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;gpl-3.0&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;vtuber&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/SillyTavern/SillyTavern\"&gt;SillyTavern&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/SillyTavern/SillyTavern\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;agpl-3.0&lt;/td&gt;\n&lt;td&gt;web&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/lobehub/lobe-vidol\"&gt;lobe-vidol&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/lobehub/lobe-vidol\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;apache-2.0&lt;/td&gt;\n&lt;td&gt;web&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Jackywine/Bella\"&gt;Bella&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Jackywine/Bella\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;web&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/tegnike/aituber-kit\"&gt;AITuberKit&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/tegnike/aituber-kit\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en, ja&lt;/td&gt;\n&lt;td&gt;custom&lt;/td&gt;\n&lt;td&gt;web&lt;/td&gt;\n&lt;td&gt;vtuber, companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/moeru-ai/airi\"&gt;airi&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/moeru-ai/airi\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;tauri&lt;/td&gt;\n&lt;td&gt;vtuber, companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/semperai/amica\"&gt;amica&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/semperai/amica\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;tauri&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/uezo/ChatdollKit\"&gt;ChatdollKit&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/uezo/ChatdollKit\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en, ja&lt;/td&gt;\n&lt;td&gt;apache-2.0&lt;/td&gt;\n&lt;td&gt;unity&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/zhangliwei7758/unity-AI-Chat-Toolkit\"&gt;Unity-AI-Chat-Toolkit&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/zhangliwei7758/unity-AI-Chat-Toolkit\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;unity&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Zao-chen/ZcChat\"&gt;ZcChat&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Zao-chen/ZcChat\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh, en&lt;/td&gt;\n&lt;td&gt;gpl-3.0&lt;/td&gt;\n&lt;td&gt;c++&lt;/td&gt;\n&lt;td&gt;galge&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/fagenorn/handcrafted-persona-engine\"&gt;handcrafted-persona-engine&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/fagenorn/handcrafted-persona-engine\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;td&gt;dotnet&lt;/td&gt;\n&lt;td&gt;vtuber, companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;ve made some edits, such as adding license info (since I might copy the code) and organizing the list into categories for easier navigation.&lt;/li&gt;\n&lt;li&gt;Not all of these are dedicated companion apps (e.g. SillyTavern), but they can be adapted with some tweaking&lt;/li&gt;\n&lt;li&gt;Several projects only have Chinese READMEs (marked as zh), but I&amp;#39;ve included DeepWiki links to help with understanding. There&amp;#39;s been significant progress in that community so I think it&amp;#39;s worth exploring.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m starting this thread for two reasons: First, I&amp;#39;d love to hear about your favorite AI companion apps or setups that go beyond basic prompting. For me, a true companion needs a name, avatar, personality, backstory, conversational ability, and most importantly, memory. Second, I&amp;#39;m particularly interested in seeing what alternatives to Grok&amp;#39;s Ani this community will build in the future.&lt;/p&gt;\n\n&lt;p&gt;If I&amp;#39;ve missed anything, please let me know and I&amp;#39;ll update the list.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?auto=webp&amp;s=c5f2d10e0e2bd42132da4d17ae9fd30799dc46d4",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e7fc321ec10284644abea084a0b60656c01283e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0102b60c2d06609a028ba234852f361a842ccba6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9b93ae42c4f64f0fba315ba853f3f199075473f0",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd981813236ec5fa39ab80bda6d206e4844b347a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e5fe64f3be02adea586fa3bbfe4297c790018163",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7078c5e8cfb3be270d93e8bdbd35e203d4e295a3",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m8wg2r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aratahikaru5",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8wg2r/open_source_companion_thread/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8wg2r/open_source_companion_thread/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753442258,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1e1w1ul46b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China's Bytedance releases Seed LiveInterpret simultaneous interpretation model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ozb0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753415015,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "seed.bytedance.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://seed.bytedance.com/en/seed_liveinterpret",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8ozb0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun-Doctor6855",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8ozb0/chinas_bytedance_releases_seed_liveinterpret/",
          "stickied": false,
          "url": "https://seed.bytedance.com/en/seed_liveinterpret",
          "subreddit_subscribers": 504253,
          "created_utc": 1753415015,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Every new model likes to claim it's SOTA, better than DeepSeek, better than whatever OpenAI/Google/Anthropic/xAI put out, and shows some benchmarks making it comparable to or better than everyone else. However, most new models tend to underwhelm me in actual usage. People have spoken of benchmaxxing a lot, and I'm really feeling it from many newer models. World knowledge in particular seems to have stagnated, and most models claiming more world knowledge in a smaller size than some competitor don't really live up to their claims.\n\nI've been experimenting with DeepSeek v3-0324, Kimi K2, Qwen 3 235B-A22B (original), Qwen 3 235B-A22B (2507 non-thinking), Llama 4 Maverick, Llama 3.3 70B, Mistral Large 2411, Cohere Command-A 2503, as well as smaller models like Qwen 3 30B-A3B, Mistral Small 3.2, and Gemma 3 27B. I've also been comparing to mid-size proprietary models like GPT-4.1, Gemini 2.5 Flash, and Claude 4 Sonnet.\n\nIn my experiments asking a broad variety of fresh world knowledge questions I made for a new private eval, they ranked as follows for world knowledge:\n\n1. DeekSeek v3 (0324)\n2. Mistral Large (2411)\n3. Kimi K2\n4. Cohere Command-A (2503)\n5. Qwen 3 235B-A22B (2507, non-thinking)\n6. Llama 4 Maverick\n7. Llama 3.3 70B\n8. Qwen 3 235B-A22B (original hybrid thinking model, with thinking turned off)\n9. Dots.LLM1\n10. Gemma 3 27B\n11. Mistral Small 3.2\n12. Qwen 3 30B-A3B\n\nIn my experiments, the only open model with knowledge comparable to Gemini 2.5 Flash and GPT 4.1 was DeepSeek v3.\n\nOf the open models I tried, the second best for world knowledge was Mistral Large 2411. Kimi K2 was in third place in my tests of world knowledge, not far behind Mistral Large in knowledge, but with more hallucinations, and a more strange, disorganized, and ugly response format.\n\nFourth place was Cohere Command A 2503, and fifth place was Qwen 3 2507. Llama 4 was a substantial step down, and only marginally better than Llama 3.3 70B in knowledge or intelligence. Qwen 3 235B-A22B had really poor knowledge for its size, and Dots.LLM1 was disappointing, hardly any more knowledgeable than Gemma 3 27B and no smarter either. Mistral Small 3.2 gave me good vibes, not too far behind Gemma 3 27B in knowledge, and decent intelligence. Qwen 3 30B-A3B also felt impressive to me; while the worst of the lot in world knowledge, it was very fast and still OK, honestly not that far off in knowledge from the original 235B that's nearly 8x bigger.\n\nAnyway, my point is that knowledge benchmarks like SimpleQA, GPQA, and PopQA need to be taken with a grain of salt. In terms of knowledge density, if you ignore benchmarks and try for yourself, you'll find that the latest and greatest like Qwen 3 235B-A22B-2507 and Kimi K2 are no better than Mistral Large 2407 from one year ago, and a step behind mid-size closed models like Gemini 2.5 Flash. It feels like we're hitting a wall with how much we can compress knowledge, and that improving programming and STEM problem solving capabilities comes at the expense of knowledge unless you increase parameter counts.\n\nThe other thing I noticed is that for Qwen specifically, the giant 235B-A22B models aren't that much more knowledgeable than the small 30B-A3B model. In my own test questions, Gemini 2.5 Flash would get around 90% right, DeepSeek v3 around 85% right, Kimi and Mistral Large around 75% right, Qwen 3 2507 around 70% right, Qwen 3 235B-A22B (original) around 60%, and Qwen 3 30B-A3B around 45%. The step up in knowledge from Qwen 3 30B to the original 235B was very underwhelming for the 8x size increase.",
          "author_fullname": "t2_702zh1r2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Stagnation in Knowledge Density",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8oc9j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 25,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753413019,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every new model likes to claim it&amp;#39;s SOTA, better than DeepSeek, better than whatever OpenAI/Google/Anthropic/xAI put out, and shows some benchmarks making it comparable to or better than everyone else. However, most new models tend to underwhelm me in actual usage. People have spoken of benchmaxxing a lot, and I&amp;#39;m really feeling it from many newer models. World knowledge in particular seems to have stagnated, and most models claiming more world knowledge in a smaller size than some competitor don&amp;#39;t really live up to their claims.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been experimenting with DeepSeek v3-0324, Kimi K2, Qwen 3 235B-A22B (original), Qwen 3 235B-A22B (2507 non-thinking), Llama 4 Maverick, Llama 3.3 70B, Mistral Large 2411, Cohere Command-A 2503, as well as smaller models like Qwen 3 30B-A3B, Mistral Small 3.2, and Gemma 3 27B. I&amp;#39;ve also been comparing to mid-size proprietary models like GPT-4.1, Gemini 2.5 Flash, and Claude 4 Sonnet.&lt;/p&gt;\n\n&lt;p&gt;In my experiments asking a broad variety of fresh world knowledge questions I made for a new private eval, they ranked as follows for world knowledge:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;DeekSeek v3 (0324)&lt;/li&gt;\n&lt;li&gt;Mistral Large (2411)&lt;/li&gt;\n&lt;li&gt;Kimi K2&lt;/li&gt;\n&lt;li&gt;Cohere Command-A (2503)&lt;/li&gt;\n&lt;li&gt;Qwen 3 235B-A22B (2507, non-thinking)&lt;/li&gt;\n&lt;li&gt;Llama 4 Maverick&lt;/li&gt;\n&lt;li&gt;Llama 3.3 70B&lt;/li&gt;\n&lt;li&gt;Qwen 3 235B-A22B (original hybrid thinking model, with thinking turned off)&lt;/li&gt;\n&lt;li&gt;Dots.LLM1&lt;/li&gt;\n&lt;li&gt;Gemma 3 27B&lt;/li&gt;\n&lt;li&gt;Mistral Small 3.2&lt;/li&gt;\n&lt;li&gt;Qwen 3 30B-A3B&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;In my experiments, the only open model with knowledge comparable to Gemini 2.5 Flash and GPT 4.1 was DeepSeek v3.&lt;/p&gt;\n\n&lt;p&gt;Of the open models I tried, the second best for world knowledge was Mistral Large 2411. Kimi K2 was in third place in my tests of world knowledge, not far behind Mistral Large in knowledge, but with more hallucinations, and a more strange, disorganized, and ugly response format.&lt;/p&gt;\n\n&lt;p&gt;Fourth place was Cohere Command A 2503, and fifth place was Qwen 3 2507. Llama 4 was a substantial step down, and only marginally better than Llama 3.3 70B in knowledge or intelligence. Qwen 3 235B-A22B had really poor knowledge for its size, and Dots.LLM1 was disappointing, hardly any more knowledgeable than Gemma 3 27B and no smarter either. Mistral Small 3.2 gave me good vibes, not too far behind Gemma 3 27B in knowledge, and decent intelligence. Qwen 3 30B-A3B also felt impressive to me; while the worst of the lot in world knowledge, it was very fast and still OK, honestly not that far off in knowledge from the original 235B that&amp;#39;s nearly 8x bigger.&lt;/p&gt;\n\n&lt;p&gt;Anyway, my point is that knowledge benchmarks like SimpleQA, GPQA, and PopQA need to be taken with a grain of salt. In terms of knowledge density, if you ignore benchmarks and try for yourself, you&amp;#39;ll find that the latest and greatest like Qwen 3 235B-A22B-2507 and Kimi K2 are no better than Mistral Large 2407 from one year ago, and a step behind mid-size closed models like Gemini 2.5 Flash. It feels like we&amp;#39;re hitting a wall with how much we can compress knowledge, and that improving programming and STEM problem solving capabilities comes at the expense of knowledge unless you increase parameter counts.&lt;/p&gt;\n\n&lt;p&gt;The other thing I noticed is that for Qwen specifically, the giant 235B-A22B models aren&amp;#39;t that much more knowledgeable than the small 30B-A3B model. In my own test questions, Gemini 2.5 Flash would get around 90% right, DeepSeek v3 around 85% right, Kimi and Mistral Large around 75% right, Qwen 3 2507 around 70% right, Qwen 3 235B-A22B (original) around 60%, and Qwen 3 30B-A3B around 45%. The step up in knowledge from Qwen 3 30B to the original 235B was very underwhelming for the 8x size increase.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8oc9j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Federal-Effective879",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8oc9j/stagnation_in_knowledge_density/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8oc9j/stagnation_in_knowledge_density/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753413019,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "  \nIt's a translation model.\n\nKey Features:\n\n* **Multilingual Support for 92 Languages**: Qwen-MT enables high-quality translation across 92 major official languages and prominent dialects, covering over 95% of the global population to meet diverse cross-lingual communication needs.\n* **High Customizability**: The new version provides advanced translation capabilities such as terminology intervention, domain prompts and translation memory. By enabling customizable prompt engineering, it delivers optimized translation performance tailored to complex, domain-specific, and mission-critical application scenarios.\n* **Low Latency &amp; Cost Efficiency**: By leveraging a lightweight Mixture of Experts (MoE) architecture, Qwen-MT achieves high translation performance with faster response times and significantly reduced API costs (as low as¬†$0.5 per million output tokens). This is particularly well-suited for high-concurrency environments and latency-sensitive applications.\n\n[benchmark](https://preview.redd.it/ebw46w8hkuef1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=0652bf1ba1530779185f78006929ce89c53a2aaf)\n\n[https://qwenlm.github.io/blog/qwen-mt/](https://qwenlm.github.io/blog/qwen-mt/)",
          "author_fullname": "t2_xdw24u3am",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen's third bomb: Qwen3-MT",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ebw46w8hkuef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 63,
                  "x": 108,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ac95f2f94be794e56e5173ecf84f81d0e87b4e7"
                },
                {
                  "y": 127,
                  "x": 216,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=879438cc2d25ccf7bbaefa6a935cf40c6d2679f5"
                },
                {
                  "y": 189,
                  "x": 320,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dcf05d1d1e97cf3ae2922a54cfcce722e499a2b7"
                },
                {
                  "y": 378,
                  "x": 640,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d9010db33f3f1a921174e424f03bd7769dd761bf"
                },
                {
                  "y": 567,
                  "x": 960,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=72eef5769345bbf6893557d58d6fbfafd6b58fe9"
                },
                {
                  "y": 638,
                  "x": 1080,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f949bada961df505b0b2d5483c2ddae0bb74ece"
                }
              ],
              "s": {
                "y": 1100,
                "x": 1860,
                "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=0652bf1ba1530779185f78006929ce89c53a2aaf"
              },
              "id": "ebw46w8hkuef1"
            }
          },
          "name": "t3_1m88s09",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 151,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 151,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/LksViWDcxO1eQ0ZQpLUVRXks4wbjVGa9UjqigE3hofA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753373875,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a translation model.&lt;/p&gt;\n\n&lt;p&gt;Key Features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Multilingual Support for 92 Languages&lt;/strong&gt;: Qwen-MT enables high-quality translation across 92 major official languages and prominent dialects, covering over 95% of the global population to meet diverse cross-lingual communication needs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High Customizability&lt;/strong&gt;: The new version provides advanced translation capabilities such as terminology intervention, domain prompts and translation memory. By enabling customizable prompt engineering, it delivers optimized translation performance tailored to complex, domain-specific, and mission-critical application scenarios.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Low Latency &amp;amp; Cost Efficiency&lt;/strong&gt;: By leveraging a lightweight Mixture of Experts (MoE) architecture, Qwen-MT achieves high translation performance with faster response times and significantly reduced API costs (as low as¬†$0.5 per million output tokens). This is particularly well-suited for high-concurrency environments and latency-sensitive applications.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ebw46w8hkuef1.png?width=1860&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0652bf1ba1530779185f78006929ce89c53a2aaf\"&gt;benchmark&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://qwenlm.github.io/blog/qwen-mt/\"&gt;https://qwenlm.github.io/blog/qwen-mt/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m88s09",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BreakfastFriendly728",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m88s09/qwens_third_bomb_qwen3mt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m88s09/qwens_third_bomb_qwen3mt/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753373875,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7g0m6735",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "new mistralai/Magistral-Small-2507 !?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m85vhw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 212,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 212,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=feaece90f7cf4e9ef4f2b2363a06c331aa76c3be",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753367249,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/mistralai/Magistral-Small-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?auto=webp&amp;s=6d8a1d0a5f0cc1e9f8968e79b1869ad8ed3d1a1d",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5f5b9e280105076efaeb7fb4658ea8f168c6e031",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0bb8df5e83f6c1204cc11aed536f322d4ded452a",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=590a3d841dcd1b2437d95fe4576578e3ffad6bab",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1aa4619d7f8ff888c9274c7c014531dcd45ff12e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=74cacf05986588334ed0a0a18df51559c11386b8",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1b24a8930f9c9f8618bbba4dd9619c2b04cb8469",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m85vhw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ApprehensiveAd3629",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m85vhw/new_mistralaimagistralsmall2507/",
          "stickied": false,
          "url": "https://huggingface.co/mistralai/Magistral-Small-2507",
          "subreddit_subscribers": 504253,
          "created_utc": 1753367249,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Boson AI has recently open-sourced the Higgs Audio V2 model.  \n[https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base](https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base)  \n  \nThe model demonstrates strong performance in automatic prosody adjustment and generating natural multi-speaker dialogues across languages . \n\nNotably, it achieved a 75.7% win rate over GPT-4o-mini-tts in emotional expression on the EmergentTTS-Eval benchmark . The total parameter count for this model is approximately 5.8 billion (3.6B for the LLM and 2.2B for the Audio Dual FFN)",
          "author_fullname": "t2_i7c050twt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Higgs Audio V2: A New Open-Source TTS Model with Voice Cloning and SOTA Expressiveness",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8aeh3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 108,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/rcsam20avuef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/rcsam20avuef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/rcsam20avuef1/DASHPlaylist.mpd?a=1756038534%2CY2FjZDQ4NGNjMGY5ZTUwMDczYzFmY2MzZjE5NzQwYThjOGZiNTA0YmRjYjEzYmE1OTc2Y2NlNjNjZGU0N2RiYQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 130,
              "hls_url": "https://v.redd.it/rcsam20avuef1/HLSPlaylist.m3u8?a=1756038534%2CMmIwYzZhZjI5OWQ2MmU3OTFmNWRkNGY2MjBhNDc2NDUxM2Q4MjViMWM5ZGU2NjhlZmFkZDEwYzVkYjBhZGE3Mw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 108,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=2971ada4bf05d18a7be7240c7fb1e72ea3f43d6d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753377531,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Boson AI has recently open-sourced the Higgs Audio V2 model.&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base\"&gt;https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;The model demonstrates strong performance in automatic prosody adjustment and generating natural multi-speaker dialogues across languages . &lt;/p&gt;\n\n&lt;p&gt;Notably, it achieved a 75.7% win rate over GPT-4o-mini-tts in emotional expression on the EmergentTTS-Eval benchmark . The total parameter count for this model is approximately 5.8 billion (3.6B for the LLM and 2.2B for the Audio Dual FFN)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/rcsam20avuef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?format=pjpg&amp;auto=webp&amp;s=99960c10b19dce63b6646f656877f832910c6f5c",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e7d429499980ab3e9e447fab7b848dcad3e259bc",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b7cb31b2460a35289a0eeee70eb956477826c160",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=65d14a7d8fcf6ee96af5785c1ed47a4f2e7f1365",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c5045364c840a4e1f879393708cd7324670a3929",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a3491e419dd1eae0f49e765804d9eb997363cefb",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=342c646526dba4fdd9256e73e3e5cc81956e6b35",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8aeh3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pheonis2",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8aeh3/higgs_audio_v2_a_new_opensource_tts_model_with/",
          "stickied": false,
          "url": "https://v.redd.it/rcsam20avuef1",
          "subreddit_subscribers": 504253,
          "created_utc": 1753377531,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/rcsam20avuef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/rcsam20avuef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/rcsam20avuef1/DASHPlaylist.mpd?a=1756038534%2CY2FjZDQ4NGNjMGY5ZTUwMDczYzFmY2MzZjE5NzQwYThjOGZiNTA0YmRjYjEzYmE1OTc2Y2NlNjNjZGU0N2RiYQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 130,
              "hls_url": "https://v.redd.it/rcsam20avuef1/HLSPlaylist.m3u8?a=1756038534%2CMmIwYzZhZjI5OWQ2MmU3OTFmNWRkNGY2MjBhNDc2NDUxM2Q4MjViMWM5ZGU2NjhlZmFkZDEwYzVkYjBhZGE3Mw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "NeuralAgent lives on your desktop and takes action like a human, it clicks, types, scrolls, and navigates your apps to complete real tasks. Your computer, now working for you. It's now open source.  \n  \nCheck it out on GitHub: [https://github.com/withneural/neuralagent](https://github.com/withneural/neuralagent)\n\nOur website: [https://www.getneuralagent.com](https://www.getneuralagent.com)  \n  \nGive us a star if you like the project!",
          "author_fullname": "t2_11vfewtir0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We just open sourced NeuralAgent: The AI Agent That Lives On Your Desktop and Uses It Like You Do!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8bps2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 85,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 85,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753380472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;NeuralAgent lives on your desktop and takes action like a human, it clicks, types, scrolls, and navigates your apps to complete real tasks. Your computer, now working for you. It&amp;#39;s now open source.  &lt;/p&gt;\n\n&lt;p&gt;Check it out on GitHub: &lt;a href=\"https://github.com/withneural/neuralagent\"&gt;https://github.com/withneural/neuralagent&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Our website: &lt;a href=\"https://www.getneuralagent.com\"&gt;https://www.getneuralagent.com&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Give us a star if you like the project!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/eZKgL90VJLC07PbJlHh8vS4DtlDzLQPNmpPGomAf-0g.png?auto=webp&amp;s=8b994923552bccf9e409beca0d7014a2a20b2506",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/eZKgL90VJLC07PbJlHh8vS4DtlDzLQPNmpPGomAf-0g.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ca1997e6cc45bb0c0dc45882a5df2cb409d4b82",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/eZKgL90VJLC07PbJlHh8vS4DtlDzLQPNmpPGomAf-0g.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=179a1fa0d5787001ed043b3df7514267a8cf7741",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/eZKgL90VJLC07PbJlHh8vS4DtlDzLQPNmpPGomAf-0g.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a31b044f61631d6102d51b819d752f8677f352fe",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/eZKgL90VJLC07PbJlHh8vS4DtlDzLQPNmpPGomAf-0g.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7293fce83bae0780debd9a9670ac61147b2ff7c",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/eZKgL90VJLC07PbJlHh8vS4DtlDzLQPNmpPGomAf-0g.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4d165b0c4fd693e2da5dd6f547b42cd6f2d45d26",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/eZKgL90VJLC07PbJlHh8vS4DtlDzLQPNmpPGomAf-0g.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=12e5abb9decf12f331484d39aa06418654ddb4d2",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "eZKgL90VJLC07PbJlHh8vS4DtlDzLQPNmpPGomAf-0g"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m8bps2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nearby_Tart_9970",
          "discussion_type": null,
          "num_comments": 43,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8bps2/we_just_open_sourced_neuralagent_the_ai_agent/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8bps2/we_just_open_sourced_neuralagent_the_ai_agent/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753380472,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm happy to see this as my experience with these models for image recognition isn't very impressive. They mostly can't even tell when pictures are sideways, for example.",
          "author_fullname": "t2_5b972ieo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.1V-9B-Thinking - claims to \"match or surpass Qwen2.5-72B\" on many tasks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m8xmy9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=ff3e77630a6d9903e8ffc2da84d81c479305e7d8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753445934,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m happy to see this as my experience with these models for image recognition isn&amp;#39;t very impressive. They mostly can&amp;#39;t even tell when pictures are sideways, for example.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/THUDM/GLM-4.1V-Thinking",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?auto=webp&amp;s=f3d8a19a2706316e04aaa9fd5ea8ed3c15e6f304",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f8ce37456b595d44518bc9dbb50bfbbdc4bdd6f",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=613a2c6b97d63cde3e9b6e1957cffa5cfa955644",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9e5860036f1ae510f4d52f708d0e080abc80cd5",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=841806f55387309a64d67cd8a7a49351c70e6ab2",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1f52fb001e2e3c7ce1ba59b80df2da7d7b72a91f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b77075add5d3dfb46a095004810d01976a798dec",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8xmy9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pristine-Woodpecker",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8xmy9/glm41v9bthinking_claims_to_match_or_surpass/",
          "stickied": false,
          "url": "https://github.com/THUDM/GLM-4.1V-Thinking",
          "subreddit_subscribers": 504253,
          "created_utc": 1753445934,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nAm5 9000x3d 128gb ram (2*64) and a 3090\n\nI promised i watch it but I couldn't get what exact quant nor speed.  \nHe said this was \"compressed to 20% of the og model\" so something like a q2.  \nRegarding speed it seems very very descent\n\n",
          "author_fullname": "t2_cj9kap4bx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Level1tech runs deepseek on am5 and it's not that bad!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ewlx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": "#bbbdbf",
          "ups": 62,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T17bpGItqXw?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AI and You Against the Machine: Guide so you can own Big AI and Run Local\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "AI and You Against the Machine: Guide so you can own Big AI and Run Local",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T17bpGItqXw?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AI and You Against the Machine: Guide so you can own Big AI and Run Local\"&gt;&lt;/iframe&gt;",
              "author_name": "Level1Techs",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/T17bpGItqXw/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Level1Techs"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T17bpGItqXw?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AI and You Against the Machine: Guide so you can own Big AI and Run Local\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m8ewlx",
            "height": 200
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 62,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/-htOFPXayCuXw6wsi6x_HzoPwXo6FB_FePd0EceoPtI.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=7ef55c301b8cc1f6c15465ee00f838044f36a7e2",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753387836,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Am5 9000x3d 128gb ram (2*64) and a 3090&lt;/p&gt;\n\n&lt;p&gt;I promised i watch it but I couldn&amp;#39;t get what exact quant nor speed.&lt;br/&gt;\nHe said this was &amp;quot;compressed to 20% of the og model&amp;quot; so something like a q2.&lt;br/&gt;\nRegarding speed it seems very very descent&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/T17bpGItqXw?feature=shared",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/-htOFPXayCuXw6wsi6x_HzoPwXo6FB_FePd0EceoPtI.jpeg?auto=webp&amp;s=d30b35a28848fb554ec800487c21bd3667f7b292",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/-htOFPXayCuXw6wsi6x_HzoPwXo6FB_FePd0EceoPtI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=871b592cff3d042c4a9d07616559fd738802ccec",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/-htOFPXayCuXw6wsi6x_HzoPwXo6FB_FePd0EceoPtI.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3670287e176290bf890eee6ca80fea8b64a76b7",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/-htOFPXayCuXw6wsi6x_HzoPwXo6FB_FePd0EceoPtI.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b8b07ebd60262e21a7df05d698d427003581ec8",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "-htOFPXayCuXw6wsi6x_HzoPwXo6FB_FePd0EceoPtI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m8ewlx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Afternoon_4260",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m8ewlx/level1tech_runs_deepseek_on_am5_and_its_not_that/",
          "stickied": false,
          "url": "https://youtu.be/T17bpGItqXw?feature=shared",
          "subreddit_subscribers": 504253,
          "created_utc": 1753387836,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "AI and You Against the Machine: Guide so you can own Big AI and Run Local",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T17bpGItqXw?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AI and You Against the Machine: Guide so you can own Big AI and Run Local\"&gt;&lt;/iframe&gt;",
              "author_name": "Level1Techs",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/T17bpGItqXw/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Level1Techs"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been writing some AI Agents lately and they work much better than I expected. Here are the 10 learnings for writing AI agents that work:\n\n1. **Tools first.**¬†Design, write and test the tools before connecting to LLMs. Tools are the most deterministic part of your code. Make sure they work 100% before writing actual agents.\n2. **Start with general, low-level tools.**¬†For example,¬†bash¬†is a powerful tool that can cover most needs. You don't need to start with a full suite of 100 tools.\n3. **Start with a single agent.**¬†Once you have all the basic tools, test them with a single react agent. It's extremely easy to write a react agent once you have the tools. All major agent frameworks have a built-in react agent. You just need to plugin your tools.\n4. **Start with the best models.**¬†There will be a lot of problems with your system, so you don't want the model's ability to be one of them. Start with Claude Sonnet or Gemini Pro. You can downgrade later for cost purposes.\n5. **Trace and log your agent.**¬†Writing agents is like doing animal experiments. There will be many unexpected behaviors. You need to monitor it as carefully as possible. There are many logging systems that help, like¬†Langsmith,¬†Langfuse, etc.\n6. **Identify the bottlenecks.**¬†There's a chance that a single agent with general tools already works. But if not, you should read your logs and identify the bottleneck. It could be: context length is too long, tools are not specialized enough, the model doesn't know how to do something, etc.\n7. **Iterate based on the bottleneck.**¬†There are many ways to improve: switch to multi-agents, write better prompts, write more specialized tools, etc. Choose them based on your bottleneck.\n8. **You can combine workflows with agents and it may work better.**¬†If your objective is specialized and there's a unidirectional order in that process, a workflow is better, and each workflow node can be an agent. For example, a deep research agent can be a two-step workflow: first a divergent broad search, then a convergent report writing, with each step being an agentic system by itself.\n9. **Trick: Utilize the filesystem as a hack.**¬†Files are a great way for AI Agents to document, memorize, and communicate. You can save a lot of context length when they simply pass around file URLs instead of full documents.\n10. **Another Trick: Ask Claude Code how to write agents.**¬†Claude Code is the best agent we have out there. Even though it's not open-sourced, CC knows its prompt, architecture, and tools. You can ask its advice for your system.",
          "author_fullname": "t2_ynwvt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I wrote an AI Agent that works better than I expected. Here are 10 learnings.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m8vmoi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753439430,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been writing some AI Agents lately and they work much better than I expected. Here are the 10 learnings for writing AI agents that work:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Tools first.&lt;/strong&gt;¬†Design, write and test the tools before connecting to LLMs. Tools are the most deterministic part of your code. Make sure they work 100% before writing actual agents.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Start with general, low-level tools.&lt;/strong&gt;¬†For example,¬†bash¬†is a powerful tool that can cover most needs. You don&amp;#39;t need to start with a full suite of 100 tools.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Start with a single agent.&lt;/strong&gt;¬†Once you have all the basic tools, test them with a single react agent. It&amp;#39;s extremely easy to write a react agent once you have the tools. All major agent frameworks have a built-in react agent. You just need to plugin your tools.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Start with the best models.&lt;/strong&gt;¬†There will be a lot of problems with your system, so you don&amp;#39;t want the model&amp;#39;s ability to be one of them. Start with Claude Sonnet or Gemini Pro. You can downgrade later for cost purposes.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Trace and log your agent.&lt;/strong&gt;¬†Writing agents is like doing animal experiments. There will be many unexpected behaviors. You need to monitor it as carefully as possible. There are many logging systems that help, like¬†Langsmith,¬†Langfuse, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Identify the bottlenecks.&lt;/strong&gt;¬†There&amp;#39;s a chance that a single agent with general tools already works. But if not, you should read your logs and identify the bottleneck. It could be: context length is too long, tools are not specialized enough, the model doesn&amp;#39;t know how to do something, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Iterate based on the bottleneck.&lt;/strong&gt;¬†There are many ways to improve: switch to multi-agents, write better prompts, write more specialized tools, etc. Choose them based on your bottleneck.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;You can combine workflows with agents and it may work better.&lt;/strong&gt;¬†If your objective is specialized and there&amp;#39;s a unidirectional order in that process, a workflow is better, and each workflow node can be an agent. For example, a deep research agent can be a two-step workflow: first a divergent broad search, then a convergent report writing, with each step being an agentic system by itself.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Trick: Utilize the filesystem as a hack.&lt;/strong&gt;¬†Files are a great way for AI Agents to document, memorize, and communicate. You can save a lot of context length when they simply pass around file URLs instead of full documents.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Another Trick: Ask Claude Code how to write agents.&lt;/strong&gt;¬†Claude Code is the best agent we have out there. Even though it&amp;#39;s not open-sourced, CC knows its prompt, architecture, and tools. You can ask its advice for your system.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8vmoi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Js8544",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vmoi/i_wrote_an_ai_agent_that_works_better_than_i/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8vmoi/i_wrote_an_ai_agent_that_works_better_than_i/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753439430,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I wanted to fine-tune the model so that it performs well with marathi texts in images using unsloth. But I am encountering significant performance degradation with fine-tuning it . The fine-tuned model frequently fails to understand basic prompts and performs worse than the base model for OCR. My dataset is consists of 700 whole pages from hand written notebooks , books etc.  \nHowever, after fine-tuning, the model performs¬†**significantly worse than the base model**¬†‚Äî it struggles with basic OCR prompts and fails to recognize text it previously handled well.\n\nHere‚Äôs how I configured the fine-tuning layers:  \nfinetune\\_vision\\_layers = True\n\nfinetune\\_language\\_layers = True\n\nfinetune\\_attention\\_modules = True\n\nfinetune\\_mlp\\_modules = False\n\nPlease suggest what can I do to improve it.",
          "author_fullname": "t2_pqyq3e9x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fine-tuning qwen2.5 vl for Marathi OCR",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8qtpd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753421050,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanted to fine-tune the model so that it performs well with marathi texts in images using unsloth. But I am encountering significant performance degradation with fine-tuning it . The fine-tuned model frequently fails to understand basic prompts and performs worse than the base model for OCR. My dataset is consists of 700 whole pages from hand written notebooks , books etc.&lt;br/&gt;\nHowever, after fine-tuning, the model performs¬†&lt;strong&gt;significantly worse than the base model&lt;/strong&gt;¬†‚Äî it struggles with basic OCR prompts and fails to recognize text it previously handled well.&lt;/p&gt;\n\n&lt;p&gt;Here‚Äôs how I configured the fine-tuning layers:&lt;br/&gt;\nfinetune_vision_layers = True&lt;/p&gt;\n\n&lt;p&gt;finetune_language_layers = True&lt;/p&gt;\n\n&lt;p&gt;finetune_attention_modules = True&lt;/p&gt;\n\n&lt;p&gt;finetune_mlp_modules = False&lt;/p&gt;\n\n&lt;p&gt;Please suggest what can I do to improve it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8qtpd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rahul_Albus",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8qtpd/finetuning_qwen25_vl_for_marathi_ocr/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8qtpd/finetuning_qwen25_vl_for_marathi_ocr/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753421050,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Greetings, we're a state-owned college, and we want to acquire an IA workstation. We have a strict budget and cannot surpass it, so working with our providers, they gave us two options with our budget\n\n  \n1. One Threadripper PRO 9955WX, with WS WRX90E-SAGE SE, 1 PRO 6000 Blackwell, and 256 GB RAM\n\n2. One AMD Ryzen 9 9950X with a ProArt X870E-CREATOR, 2 PRO 6000 Blackwells and 128 GB RAM\n\n  \nBoth models have a 1600W PSU. The idea on the first model is to try to get another budget the next year in order to buy a second PRO 6000 Blackwell.\n\nWe're not extremely concerned about RAM (we can buy RAM later using a different budget) but we're concerned that the Ryzen 9950X only has enough PCIE lanes to run the blackwell on PCIE x8, instead of x16. Our provider told us that this is not very important unless we want to load and unload models all the time, but we have some reservations about that. So, can you guide us a little on that?\n\nThanks a bunch  \n",
          "author_fullname": "t2_lavl5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How important is to have PRO 6000 Blackwell running on 16 PCIE lanes?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m8wuy7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753443620,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings, we&amp;#39;re a state-owned college, and we want to acquire an IA workstation. We have a strict budget and cannot surpass it, so working with our providers, they gave us two options with our budget&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;One Threadripper PRO 9955WX, with WS WRX90E-SAGE SE, 1 PRO 6000 Blackwell, and 256 GB RAM&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;One AMD Ryzen 9 9950X with a ProArt X870E-CREATOR, 2 PRO 6000 Blackwells and 128 GB RAM&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Both models have a 1600W PSU. The idea on the first model is to try to get another budget the next year in order to buy a second PRO 6000 Blackwell.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re not extremely concerned about RAM (we can buy RAM later using a different budget) but we&amp;#39;re concerned that the Ryzen 9950X only has enough PCIE lanes to run the blackwell on PCIE x8, instead of x16. Our provider told us that this is not very important unless we want to load and unload models all the time, but we have some reservations about that. So, can you guide us a little on that?&lt;/p&gt;\n\n&lt;p&gt;Thanks a bunch  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8wuy7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ferkte",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753443620,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "vLLM commit: [https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29](https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29)\n\n  \nmodelscope/ms-swift commit: [https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7](https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7)\n\nhttps://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;format=png&amp;auto=webp&amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e\n\nWe're going to get a 106B-A12B (Air) model and a 355B-A32B model.  \n",
          "author_fullname": "t2_155sd0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5 Is About to Be Released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "hda2uymxqsef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 48,
                  "x": 108,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=64860b786b5e21e35a840a649825e0180c8cc478"
                },
                {
                  "y": 97,
                  "x": 216,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f06e72fb0c477bec3198b5134c4222990359eb0a"
                },
                {
                  "y": 144,
                  "x": 320,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ed13273132fbf1f9298d35bd285a9a9ab9210519"
                },
                {
                  "y": 288,
                  "x": 640,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9a159ed9a36f9275c9b5a11b3604c7793dde26c0"
                },
                {
                  "y": 432,
                  "x": 960,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5005302c8f7a306e32869b7748202c5cfd5b87fe"
                },
                {
                  "y": 486,
                  "x": 1080,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=058a1510994cdcc7fd2e534901f63f86884f1db2"
                }
              ],
              "s": {
                "y": 586,
                "x": 1300,
                "u": "https://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;format=png&amp;auto=webp&amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e"
              },
              "id": "hda2uymxqsef1"
            }
          },
          "name": "t3_1m80gsn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 325,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 325,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=460c7b2c3bf4d9c06c8551ed35f1d347b924c43a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753351817,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;vLLM commit: &lt;a href=\"https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29\"&gt;https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;modelscope/ms-swift commit: &lt;a href=\"https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7\"&gt;https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e\"&gt;https://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re going to get a 106B-A12B (Air) model and a 355B-A32B model.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?auto=webp&amp;s=c6b87857dd89e2502756d6b53a092e0a220bcbb5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c7a1310eabcbdf43b0d3abda179514f1ac02393",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9bb47d4723d0c3b790dc8d57f5455755b278fc6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d61a12909a1af11f6d9f3cddbf320cfaad72c44",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b9908a35901687f5249e56f8b7bb3e593bf9a82e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b01ed43a5e38bf06f05457b04f802ed86327cd88",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9d88d6adc0d7dbbdf7b26de2a970c2ec9b69a0ce",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m80gsn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NeterOster",
          "discussion_type": null,
          "num_comments": 76,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753351817,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This demo runs Voxtral-Mini-3B, a new audio language model from Mistral, enabling state-of-the-art audio transcription directly in your browser! Everything runs locally, meaning none of your data is sent to a server (and your transcripts are stored on-device).\n\nImportant links:\n- Model: https://huggingface.co/onnx-community/Voxtral-Mini-3B-2507-ONNX\n- Demo: https://huggingface.co/spaces/webml-community/Voxtral-WebGPU",
          "author_fullname": "t2_mizchr3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Voxtral WebGPU: State-of-the-art audio transcription directly in your browser!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m87q21",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 101,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/9p0p7mqnbuef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1008,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/9p0p7mqnbuef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/9p0p7mqnbuef1/DASHPlaylist.mpd?a=1756038534%2CYTBlYjUwNGFkNDQyMDMzMGQ4YWFjNDYwYTQyNTljYTQ3NjUzMmVmNGY2YmU0NDhmNTM2OTM2MWExYmZmOWE4YQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 45,
              "hls_url": "https://v.redd.it/9p0p7mqnbuef1/HLSPlaylist.m3u8?a=1756038534%2CY2UxZWNhNDkzNDViMjQ3OGU3MzkxMjMyZmEyMjg5N2IyZTcyYzcwOWFhMjQ0YTAwOWVlNmNlYmZjYjcwNjc3Nw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 101,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=f1686dbb130dc0803321bf6e7360985752a766f8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753371510,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This demo runs Voxtral-Mini-3B, a new audio language model from Mistral, enabling state-of-the-art audio transcription directly in your browser! Everything runs locally, meaning none of your data is sent to a server (and your transcripts are stored on-device).&lt;/p&gt;\n\n&lt;p&gt;Important links:\n- Model: &lt;a href=\"https://huggingface.co/onnx-community/Voxtral-Mini-3B-2507-ONNX\"&gt;https://huggingface.co/onnx-community/Voxtral-Mini-3B-2507-ONNX&lt;/a&gt;\n- Demo: &lt;a href=\"https://huggingface.co/spaces/webml-community/Voxtral-WebGPU\"&gt;https://huggingface.co/spaces/webml-community/Voxtral-WebGPU&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/9p0p7mqnbuef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?format=pjpg&amp;auto=webp&amp;s=696efe4a003759fef9584a89aa6ae640bc1003d2",
                  "width": 3160,
                  "height": 1658
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1ed93c4a634db259c3e76c429350e309050b88f3",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=44f5105d97067fdedefb4e9a76461c7c8ffc551b",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8232cc34450ab1b876049726f596c02a4d1a3fd7",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7159ca490a26e285cad3449d2030082fe5240c9b",
                    "width": 640,
                    "height": 335
                  },
                  {
                    "url": "https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=aca40ddd0ee86b1a78155181541946e23e3c81b6",
                    "width": 960,
                    "height": 503
                  },
                  {
                    "url": "https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c70476f5a9c13880e2d68d5a19c9925ffc1de8cc",
                    "width": 1080,
                    "height": 566
                  }
                ],
                "variants": {},
                "id": "NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m87q21",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xenovatech",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m87q21/voxtral_webgpu_stateoftheart_audio_transcription/",
          "stickied": false,
          "url": "https://v.redd.it/9p0p7mqnbuef1",
          "subreddit_subscribers": 504253,
          "created_utc": 1753371510,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/9p0p7mqnbuef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1008,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/9p0p7mqnbuef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/9p0p7mqnbuef1/DASHPlaylist.mpd?a=1756038534%2CYTBlYjUwNGFkNDQyMDMzMGQ4YWFjNDYwYTQyNTljYTQ3NjUzMmVmNGY2YmU0NDhmNTM2OTM2MWExYmZmOWE4YQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 45,
              "hls_url": "https://v.redd.it/9p0p7mqnbuef1/HLSPlaylist.m3u8?a=1756038534%2CY2UxZWNhNDkzNDViMjQ3OGU3MzkxMjMyZmEyMjg5N2IyZTcyYzcwOWFhMjQ0YTAwOWVlNmNlYmZjYjcwNjc3Nw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "From what I understand, an MOE model contains many experts, and when you give it a prompt, it chooses one expert to answer your query.\n\nIf I already know that I want to do something like creative writing, why can‚Äôt I just have just the creative writing expert so I only need to load that?\n\nWouldn‚Äôt this help with the required ram/vram amount?",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can you just have one expert from an MOE model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8qmd7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753420347,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From what I understand, an MOE model contains many experts, and when you give it a prompt, it chooses one expert to answer your query.&lt;/p&gt;\n\n&lt;p&gt;If I already know that I want to do something like creative writing, why can‚Äôt I just have just the creative writing expert so I only need to load that?&lt;/p&gt;\n\n&lt;p&gt;Wouldn‚Äôt this help with the required ram/vram amount?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8qmd7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753420347,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,\n\nJust wanted to share some exciting news for anyone here who's into deep, long-form roleplaying. The team behind¬†[Astrsk](https://astrsk.ai), a desktop app for RP that's been in development for about six months, has just announced they are going¬†**fully open source**¬†under the GPL license!\n\nAs a fan of the project, I think this is a huge deal for the community.\n\n**The most important link first:** [https://github.com/astrskai/astrsk](https://github.com/astrskai/astrsk)\n\n[demo](https://reddit.com/link/1m868na/video/zk1ui4ctytef1/player)\n\n**So, what is Astrsk and why is it interesting?**\n\nAt its core, Astrsk is a UI for RP, but its main differentiator is the¬†**agentic workflow**. I've been following it, and the concept is very cool because it moves beyond a simple prompt-response loop.\n\nTo make this concrete, let's look at the default workflow it comes with, called¬†**SAGA**. It's a four-step pipeline that mimics how a human Game Master thinks, breaking down the task of generating a response into logical steps.\n\nHere's how it works:\n\n1. **Step 1: The Analyzer Agent**\n   * **The Job:**¬†This is the GM's logical brain. It looks at what your character just did and analyzes it against the current game state.\n   * **In Practice:**¬†It answers the questions: \"Is the player's action possible? What are the immediate consequences based on game rules or a dice roll?\" It validates the action and determines the outcome.\n2. **Step 2: The Planner Agent**\n   * **The Job:**¬†This is the creative storyteller. It takes the Analyzer's output and designs the narrative response.\n   * **In Practice:**¬†It decides how NPCs will react to the player's action (e.g., with anger, surprise, or a counter-move). It plans the scene, sets the emotional tone, and prepares the key information for the next agent.\n3. **Step 3: The Actor Agent**\n   * **The Job:**¬†This is the performer. It takes the Planner's script and turns it into the actual text you read.\n   * **In Practice:**¬†It writes the scene narration and performs the detailed dialogue for one main NPC, giving them a distinct voice and personality. Other NPCs are handled through the narration, keeping the focus clear.\n4. **Step 4: The Formatter Agent**\n   * **The Job:**¬†This is the final editor.\n   * **In Practice:**¬†It takes the text from the Actor and cleans it up with simple markdown. It automatically wraps actions in¬†italics, dialogue in \"quotes\", and adds¬†**bold**¬†for emphasis, making the final output clean and easy to read without changing the content.\n\nThis pipeline approach allows for incredible consistency and detail. And since you can assign different models to different agents (a key feature!), you could use a large, powerful model for the creative Planner and a faster, smaller model for the structured Analyzer.\n\n**How does it compare to the greats like SillyTavern / Agnaistic?**\n\nFrom what I've seen, while projects like ST/Agnaistic are amazing for chat-based RP, Astrsk seems to aim for a different goal. It feels less like a chat interface and more like a tool for collaborative storytelling, almost like having an AI Dungeon Master powered by a framework of agents.\n\n**Key Features:**\n\n* **Agent-based generation:**¬†The core of Astrsk, designed for more coherent and long-term storytelling.\n* **Sleek, Customizable UI:**¬†A really polished interface where you can tweak settings directly in the app. No more digging through config files to change things.\n* **Per-Agent Model Assignment:**¬†This is a killer feature. You can assign a different LLM endpoint to each agent.\n* **True Cross-Platform Support:**¬†The team provides native builds for Windows, macOS, and Linux. This means you can just download and run it ‚Äî no need to be an engineer or fight with dependencies to get started.\n* **Backend Agnostic:**¬†Connects to any OpenAI-compatible API, so it works with your existing setup (Oobabooga, KoboldCPP, etc.).\n\n**The Open Source Move**\n\nAccording to their announcement, the team wants to build the project out in the open, getting feedback and contributions from the community, which is fantastic news for all of us. The project is still young, but the foundation is solid.\n\nI'm not affiliated with the developers, just a user who is really excited about the project's potential and wanted to share it with a community that might appreciate the tech.\n\nDefinitely worth checking out the [https://github.com/astrskai/astrsk](https://github.com/astrskai/astrsk), especially if the idea of an agentic approach to RP sounds interesting to you. The team is looking for feedback, bug reports, and contributors.\n\nCheers!",
          "author_fullname": "t2_4gwtztjb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The agent-based RP UI 'Astrisk' is now fully open-source under a GPL license.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "zk1ui4ctytef1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1m868na/asset/zk1ui4ctytef1/DASHPlaylist.mpd?a=1756038534%2CNDBiYmI0ZTI2NGI2NWNiMDI4YmE1ZjQzODIyNzQzZTM4NzMzOWI2OWJiMDQwZGJhZmU3N2RmZWRiYzE0YWM5YQ%3D%3D&amp;v=1&amp;f=sd",
              "x": 1920,
              "y": 1080,
              "hlsUrl": "https://v.redd.it/link/1m868na/asset/zk1ui4ctytef1/HLSPlaylist.m3u8?a=1756038534%2CNWFmNzlmYjQyOGRkMDRlZThlNzk0NjlmMTUzOTc0OGFiYjYxYTViOWM5ZGEwY2U5YTU1MzFkMjZiOTIzZjI2MQ%3D%3D&amp;v=1&amp;f=sd",
              "id": "zk1ui4ctytef1",
              "isGif": false
            }
          },
          "name": "t3_1m868na",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 88,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 88,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=d544415f1b12ed38edc0c929b685e2438d394428",
          "edited": 1753391201,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753368120,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;Just wanted to share some exciting news for anyone here who&amp;#39;s into deep, long-form roleplaying. The team behind¬†&lt;a href=\"https://astrsk.ai\"&gt;Astrsk&lt;/a&gt;, a desktop app for RP that&amp;#39;s been in development for about six months, has just announced they are going¬†&lt;strong&gt;fully open source&lt;/strong&gt;¬†under the GPL license!&lt;/p&gt;\n\n&lt;p&gt;As a fan of the project, I think this is a huge deal for the community.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The most important link first:&lt;/strong&gt; &lt;a href=\"https://github.com/astrskai/astrsk\"&gt;https://github.com/astrskai/astrsk&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1m868na/video/zk1ui4ctytef1/player\"&gt;demo&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So, what is Astrsk and why is it interesting?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;At its core, Astrsk is a UI for RP, but its main differentiator is the¬†&lt;strong&gt;agentic workflow&lt;/strong&gt;. I&amp;#39;ve been following it, and the concept is very cool because it moves beyond a simple prompt-response loop.&lt;/p&gt;\n\n&lt;p&gt;To make this concrete, let&amp;#39;s look at the default workflow it comes with, called¬†&lt;strong&gt;SAGA&lt;/strong&gt;. It&amp;#39;s a four-step pipeline that mimics how a human Game Master thinks, breaking down the task of generating a response into logical steps.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how it works:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Step 1: The Analyzer Agent&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Job:&lt;/strong&gt;¬†This is the GM&amp;#39;s logical brain. It looks at what your character just did and analyzes it against the current game state.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;In Practice:&lt;/strong&gt;¬†It answers the questions: &amp;quot;Is the player&amp;#39;s action possible? What are the immediate consequences based on game rules or a dice roll?&amp;quot; It validates the action and determines the outcome.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Step 2: The Planner Agent&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Job:&lt;/strong&gt;¬†This is the creative storyteller. It takes the Analyzer&amp;#39;s output and designs the narrative response.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;In Practice:&lt;/strong&gt;¬†It decides how NPCs will react to the player&amp;#39;s action (e.g., with anger, surprise, or a counter-move). It plans the scene, sets the emotional tone, and prepares the key information for the next agent.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Step 3: The Actor Agent&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Job:&lt;/strong&gt;¬†This is the performer. It takes the Planner&amp;#39;s script and turns it into the actual text you read.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;In Practice:&lt;/strong&gt;¬†It writes the scene narration and performs the detailed dialogue for one main NPC, giving them a distinct voice and personality. Other NPCs are handled through the narration, keeping the focus clear.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Step 4: The Formatter Agent&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Job:&lt;/strong&gt;¬†This is the final editor.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;In Practice:&lt;/strong&gt;¬†It takes the text from the Actor and cleans it up with simple markdown. It automatically wraps actions in¬†italics, dialogue in &amp;quot;quotes&amp;quot;, and adds¬†&lt;strong&gt;bold&lt;/strong&gt;¬†for emphasis, making the final output clean and easy to read without changing the content.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This pipeline approach allows for incredible consistency and detail. And since you can assign different models to different agents (a key feature!), you could use a large, powerful model for the creative Planner and a faster, smaller model for the structured Analyzer.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How does it compare to the greats like SillyTavern / Agnaistic?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;From what I&amp;#39;ve seen, while projects like ST/Agnaistic are amazing for chat-based RP, Astrsk seems to aim for a different goal. It feels less like a chat interface and more like a tool for collaborative storytelling, almost like having an AI Dungeon Master powered by a framework of agents.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Agent-based generation:&lt;/strong&gt;¬†The core of Astrsk, designed for more coherent and long-term storytelling.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Sleek, Customizable UI:&lt;/strong&gt;¬†A really polished interface where you can tweak settings directly in the app. No more digging through config files to change things.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Per-Agent Model Assignment:&lt;/strong&gt;¬†This is a killer feature. You can assign a different LLM endpoint to each agent.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;True Cross-Platform Support:&lt;/strong&gt;¬†The team provides native builds for Windows, macOS, and Linux. This means you can just download and run it ‚Äî no need to be an engineer or fight with dependencies to get started.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Backend Agnostic:&lt;/strong&gt;¬†Connects to any OpenAI-compatible API, so it works with your existing setup (Oobabooga, KoboldCPP, etc.).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;The Open Source Move&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;According to their announcement, the team wants to build the project out in the open, getting feedback and contributions from the community, which is fantastic news for all of us. The project is still young, but the foundation is solid.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not affiliated with the developers, just a user who is really excited about the project&amp;#39;s potential and wanted to share it with a community that might appreciate the tech.&lt;/p&gt;\n\n&lt;p&gt;Definitely worth checking out the &lt;a href=\"https://github.com/astrskai/astrsk\"&gt;https://github.com/astrskai/astrsk&lt;/a&gt;, especially if the idea of an agentic approach to RP sounds interesting to you. The team is looking for feedback, bug reports, and contributors.&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?auto=webp&amp;s=77d4335d7d3e2b6cb38995f2a45cec8122d875db",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c9c3c7e0d2eb3c8db9edce43875022cd7dc1ed41",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9bc7b0d4082a420b0bebbd54d03f6686955d80dd",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b22662cb963afd433d33bf16a8734e437973cdbb",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b7af7edb7da1fe811056d8c05b8f8d8acd8fdb89",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3c4842d8fbabca3b675d7912df2ac0779cfe7c78",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf2e1405a6b96a00c605403c1089162c20dd6308",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m868na",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ru_cyber",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m868na/the_agentbased_rp_ui_astrisk_is_now_fully/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m868na/the_agentbased_rp_ui_astrisk_is_now_fully/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753368120,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just read a fascinating‚Äîand honestly, a bit unsettling‚Äîresearch paper from Anthropic that flips a common assumption in AI on its head: that giving models more time to think (i.e., more compute at test time) leads to better performance.\n\n\nTurns out, that‚Äôs not always true.\n\nTheir paper, ‚ÄúInverse Scaling in Test-Time Compute,‚Äù reveals a surprising phenomenon: in certain tasks, models like Claude and OpenAI's GPT-o series actually perform worse when allowed to \"reason\" for longer. They call this the Performance Deterioration Paradox, or simply inverse scaling.\n\nSo what‚Äôs going wrong?\n\nThe paper breaks it down across several models and tasks. Here's what they found:\n\nüß† More Thinking, More Problems\n\nGiving the models more time (tokens) to reason sometimes hurts accuracy‚Äîespecially on complex reasoning tasks. Instead of refining their answers, models can:\n\nGet Distracted: Claude models, for example, start to veer off course, pulled toward irrelevant details.\n\nOverfit: OpenAI‚Äôs o-series models begin to overfit the framing of the problem instead of generalizing.\n\nFollow Spurious Correlations: Even when the correct approach is available early, models sometimes drift toward wrong patterns with extended reasoning.\n\nFail at Deduction: All models struggled with constraint satisfaction and logical deduction the longer they went on.\n\nAmplify Risky Behaviors: Extended reasoning occasionally made models more likely to express concerning behaviors‚Äîlike self-preservation in Claude Sonnet 4.\n\nTasks Where This Shows Up\n\nThis inverse scaling effect was especially pronounced in:\n\nSimple counting with distractors\n\nRegression with spurious features\n\nConstraint satisfaction logic puzzles\n\nAI risk assessments and alignment probes\n\nüß© Why This Matters\n\nThis isn‚Äôt just a weird performance quirk‚Äîit has deep implications for AI safety, reliability, and interpretability. The paper also points out ‚ÄúChain-of-Thought Faithfulness‚Äù issues: the reasoning steps models output often don‚Äôt reflect what‚Äôs actually driving their answer.\n\nThat‚Äôs a huge deal for alignment and safety. If we can‚Äôt trust the model‚Äôs step-by-step logic, then we can‚Äôt audit or guide their reasoning‚Äîeven if it looks rational on the surface.\n\n\n‚ö†Ô∏è Bottom Line\n\nThis research challenges one of the core assumptions behind features like OpenAI‚Äôs reasoning tokens and Anthropic‚Äôs extended thinking mode in Claude 3.7 Sonnet. It suggests that more test-time compute isn‚Äôt always better‚Äîand can sometimes make things worse\n\n[Research Paper](https://arxiv.org/pdf/2507.14417)",
          "author_fullname": "t2_gsyxhako0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anthropic‚Äôs New Research: Giving AI More \"Thinking Time\" Can Actually Make It Worse",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 55,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7vlpn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 410,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 410,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/kN66IOKheq4z8kTU3sCN0FzDuO-tLQDfmIS6U022Db0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753333763,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just read a fascinating‚Äîand honestly, a bit unsettling‚Äîresearch paper from Anthropic that flips a common assumption in AI on its head: that giving models more time to think (i.e., more compute at test time) leads to better performance.&lt;/p&gt;\n\n&lt;p&gt;Turns out, that‚Äôs not always true.&lt;/p&gt;\n\n&lt;p&gt;Their paper, ‚ÄúInverse Scaling in Test-Time Compute,‚Äù reveals a surprising phenomenon: in certain tasks, models like Claude and OpenAI&amp;#39;s GPT-o series actually perform worse when allowed to &amp;quot;reason&amp;quot; for longer. They call this the Performance Deterioration Paradox, or simply inverse scaling.&lt;/p&gt;\n\n&lt;p&gt;So what‚Äôs going wrong?&lt;/p&gt;\n\n&lt;p&gt;The paper breaks it down across several models and tasks. Here&amp;#39;s what they found:&lt;/p&gt;\n\n&lt;p&gt;üß† More Thinking, More Problems&lt;/p&gt;\n\n&lt;p&gt;Giving the models more time (tokens) to reason sometimes hurts accuracy‚Äîespecially on complex reasoning tasks. Instead of refining their answers, models can:&lt;/p&gt;\n\n&lt;p&gt;Get Distracted: Claude models, for example, start to veer off course, pulled toward irrelevant details.&lt;/p&gt;\n\n&lt;p&gt;Overfit: OpenAI‚Äôs o-series models begin to overfit the framing of the problem instead of generalizing.&lt;/p&gt;\n\n&lt;p&gt;Follow Spurious Correlations: Even when the correct approach is available early, models sometimes drift toward wrong patterns with extended reasoning.&lt;/p&gt;\n\n&lt;p&gt;Fail at Deduction: All models struggled with constraint satisfaction and logical deduction the longer they went on.&lt;/p&gt;\n\n&lt;p&gt;Amplify Risky Behaviors: Extended reasoning occasionally made models more likely to express concerning behaviors‚Äîlike self-preservation in Claude Sonnet 4.&lt;/p&gt;\n\n&lt;p&gt;Tasks Where This Shows Up&lt;/p&gt;\n\n&lt;p&gt;This inverse scaling effect was especially pronounced in:&lt;/p&gt;\n\n&lt;p&gt;Simple counting with distractors&lt;/p&gt;\n\n&lt;p&gt;Regression with spurious features&lt;/p&gt;\n\n&lt;p&gt;Constraint satisfaction logic puzzles&lt;/p&gt;\n\n&lt;p&gt;AI risk assessments and alignment probes&lt;/p&gt;\n\n&lt;p&gt;üß© Why This Matters&lt;/p&gt;\n\n&lt;p&gt;This isn‚Äôt just a weird performance quirk‚Äîit has deep implications for AI safety, reliability, and interpretability. The paper also points out ‚ÄúChain-of-Thought Faithfulness‚Äù issues: the reasoning steps models output often don‚Äôt reflect what‚Äôs actually driving their answer.&lt;/p&gt;\n\n&lt;p&gt;That‚Äôs a huge deal for alignment and safety. If we can‚Äôt trust the model‚Äôs step-by-step logic, then we can‚Äôt audit or guide their reasoning‚Äîeven if it looks rational on the surface.&lt;/p&gt;\n\n&lt;p&gt;‚ö†Ô∏è Bottom Line&lt;/p&gt;\n\n&lt;p&gt;This research challenges one of the core assumptions behind features like OpenAI‚Äôs reasoning tokens and Anthropic‚Äôs extended thinking mode in Claude 3.7 Sonnet. It suggests that more test-time compute isn‚Äôt always better‚Äîand can sometimes make things worse&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/pdf/2507.14417\"&gt;Research Paper&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/srk1p5og9ref1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?auto=webp&amp;s=8c5f17041a7427186a90615947629f7f3b6f5ebe",
                  "width": 1017,
                  "height": 402
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7cc61b1687c4811710598cfd5ca73171183da32e",
                    "width": 108,
                    "height": 42
                  },
                  {
                    "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d48b6667dcc4881b0c36f4c3e8c536a286b9c2c2",
                    "width": 216,
                    "height": 85
                  },
                  {
                    "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c93d22fd22e9f7d4be4d7625b85d2b8344216a1",
                    "width": 320,
                    "height": 126
                  },
                  {
                    "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=69b7dca05f4a287acca18082926d12008127ef3d",
                    "width": 640,
                    "height": 252
                  },
                  {
                    "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2fc4555e2a2c45facaf294b38bf3f5d3af5381e5",
                    "width": 960,
                    "height": 379
                  }
                ],
                "variants": {},
                "id": "MxlZXC1ILxtyvLZA2sratIgRfi8x9R-d2k6wTMUu0yw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7vlpn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Karam1234098",
          "discussion_type": null,
          "num_comments": 103,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7vlpn/anthropics_new_research_giving_ai_more_thinking/",
          "stickied": false,
          "url": "https://i.redd.it/srk1p5og9ref1.jpeg",
          "subreddit_subscribers": 504253,
          "created_utc": 1753333763,
          "num_crossposts": 4,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1eerwvvhpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running an LLM on the Wii",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m85v3a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 72,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/8hvd0nnw0uef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/8hvd0nnw0uef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/8hvd0nnw0uef1/DASHPlaylist.mpd?a=1756038534%2CMDMyYjczZGFmMjQ3YTc5NDY3ODYzZTc2ODgxMWRiODM1NTMyMTE2Mjc4NjIwYzE4NWVhY2U0M2M2NzE5Njc3MQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 47,
              "hls_url": "https://v.redd.it/8hvd0nnw0uef1/HLSPlaylist.m3u8?a=1756038534%2COTE2NDQxZDVhODc0ODg5YWU1NTYxMTY2MDMwOTk5NDdmMjk0ZGQyNjY4MmYzZDM3ZDExMTQyNWY5MmEzNTQ2Zg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 72,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=131894ead3f768772f949c6f62deabe82df53af4",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753367220,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/8hvd0nnw0uef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?format=pjpg&amp;auto=webp&amp;s=9110c7a0373a3aa63886ff744669cbc1c896b250",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e531702d76f3bb29b3c2517cf4dec746b7f76562",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6e8897fc5461d95626c62571c2b725527e1545b6",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fe1f107f7faa28b5d3cf3389fc97a3e1d11bfb8e",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=17f291b77543cd9056e885dbb5af1a025f826d7a",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=eb32353cf4ed834b570506dd362ce35b31ff8e56",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=dc1de4479ba5597bd6d947ea490804f5645ad330",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m85v3a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "leavesandautumn222",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m85v3a/running_an_llm_on_the_wii/",
          "stickied": false,
          "url": "https://v.redd.it/8hvd0nnw0uef1",
          "subreddit_subscribers": 504253,
          "created_utc": 1753367220,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/8hvd0nnw0uef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/8hvd0nnw0uef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/8hvd0nnw0uef1/DASHPlaylist.mpd?a=1756038534%2CMDMyYjczZGFmMjQ3YTc5NDY3ODYzZTc2ODgxMWRiODM1NTMyMTE2Mjc4NjIwYzE4NWVhY2U0M2M2NzE5Njc3MQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 47,
              "hls_url": "https://v.redd.it/8hvd0nnw0uef1/HLSPlaylist.m3u8?a=1756038534%2COTE2NDQxZDVhODc0ODg5YWU1NTYxMTY2MDMwOTk5NDdmMjk0ZGQyNjY4MmYzZDM3ZDExMTQyNWY5MmEzNTQ2Zg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Pretty excited to see what the rest of 2025 holds tbh :)",
          "author_fullname": "t2_fxhl6ngz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Had the Qwen3:1.7B model run on my Mac Mini!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8jy5y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/2af06x4irwef1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 822,
              "width": 720,
              "scrubber_media_url": "https://v.redd.it/2af06x4irwef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/2af06x4irwef1/DASHPlaylist.mpd?a=1756038534%2CNjI5NjJjNzk3Mzc4ZWUxNzNlMmYzMzMxZWFhYjJiNTI3Mjg2OGU3YTFhYmZkZDRjMTI4MjQxYWNjMzg1ZTE5Zg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 18,
              "hls_url": "https://v.redd.it/2af06x4irwef1/HLSPlaylist.m3u8?a=1756038534%2CYTBjYTNkYTBmZGIyMTI0YWMyYzBmOWMyMjNmY2M1ZGZjMjRmZTc1ODg4NTUxMWQzZTY4NmMxNjVjNDA0YjAxOQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": true,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=5602c41420a1c0106ec2d5f91584eb58b83484ed",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753400366,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pretty excited to see what the rest of 2025 holds tbh :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/2af06x4irwef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?format=pjpg&amp;auto=webp&amp;s=b5f327e858f344f28028ea01ba534941e8604684",
                  "width": 736,
                  "height": 840
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=871ff42b5244250e2292f4c525ff011415309acd",
                    "width": 108,
                    "height": 123
                  },
                  {
                    "url": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=58f252a062ae7c154a3d24d14a34c6f81f2523b3",
                    "width": 216,
                    "height": 246
                  },
                  {
                    "url": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=dc92db67a5fc2a99c192e913375784347e76bb2f",
                    "width": 320,
                    "height": 365
                  },
                  {
                    "url": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bbfcb6c2dee59c0b826c64040b3d9f3624446c8c",
                    "width": 640,
                    "height": 730
                  }
                ],
                "variants": {},
                "id": "NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8jy5y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nomadic_Seth",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8jy5y/had_the_qwen317b_model_run_on_my_mac_mini/",
          "stickied": false,
          "url": "https://v.redd.it/2af06x4irwef1",
          "subreddit_subscribers": 504253,
          "created_utc": 1753400366,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/2af06x4irwef1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 822,
              "width": 720,
              "scrubber_media_url": "https://v.redd.it/2af06x4irwef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/2af06x4irwef1/DASHPlaylist.mpd?a=1756038534%2CNjI5NjJjNzk3Mzc4ZWUxNzNlMmYzMzMxZWFhYjJiNTI3Mjg2OGU3YTFhYmZkZDRjMTI4MjQxYWNjMzg1ZTE5Zg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 18,
              "hls_url": "https://v.redd.it/2af06x4irwef1/HLSPlaylist.m3u8?a=1756038534%2CYTBjYTNkYTBmZGIyMTI0YWMyYzBmOWMyMjNmY2M1ZGZjMjRmZTc1ODg4NTUxMWQzZTY4NmMxNjVjNDA0YjAxOQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": true,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI‚Äôm diving deeper into the world of Large Language Models (LLMs) and had a many questions I was hoping to get input on from the community. Feel free to give answer to any of my questions! You don‚Äôt have to answer all!\n\t1.\tLLM Frameworks:\nI‚Äôm currently using LangChain and recently exploring LangGraph. Are there any other LLM orchestration frameworks which companies are actively using?\n\n\t2.\tAgent Evaluation:\nHow do you approach the evaluation of agents in your pipelines? Any best practices or tools you rely on?\n\t3.\tAttention Mechanisms:\nI‚Äôm familiar with multi-head attention, sparse attention, and window attention. Are there other noteworthy attention mechanisms worth checking out?\n\t4.\tFine-Tuning Methods:\nBesides LoRA and QLoRA, are there other commonly used or emerging techniques for LLM fine-tuning?\n\t5.\tUnderstanding the Basics:\nI read a book on attention and LLMs that came out last September. It covered foundational topics well. Has anything crucial come out since then that might not be in the book?\n\t6.\tUsing HuggingFace:\nI mostly use HuggingFace for embedding models, and for local LLMs, I‚Äôve been using OLAMA. Curious how others are using HuggingFace‚Äîespecially beyond embeddings.\n\t7.\tFine-Tuning Datasets:\nWhere do you typically source data for fine-tuning your models? Are there any reliable public datasets or workflows you‚Äôd recommend?\n\nAny book or paper recommendations? (I actively read papers but maybe i see something new)\n\nWould love to hear your approaches or suggestions‚Äîthanks in advance!",
          "author_fullname": "t2_1ocdptcmcg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Guidance on diving deep into LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m8xhxp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753445519,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I‚Äôm diving deeper into the world of Large Language Models (LLMs) and had a many questions I was hoping to get input on from the community. Feel free to give answer to any of my questions! You don‚Äôt have to answer all!\n    1.  LLM Frameworks:\nI‚Äôm currently using LangChain and recently exploring LangGraph. Are there any other LLM orchestration frameworks which companies are actively using?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;2.  Agent Evaluation:\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;How do you approach the evaluation of agents in your pipelines? Any best practices or tools you rely on?\n    3.  Attention Mechanisms:\nI‚Äôm familiar with multi-head attention, sparse attention, and window attention. Are there other noteworthy attention mechanisms worth checking out?\n    4.  Fine-Tuning Methods:\nBesides LoRA and QLoRA, are there other commonly used or emerging techniques for LLM fine-tuning?\n    5.  Understanding the Basics:\nI read a book on attention and LLMs that came out last September. It covered foundational topics well. Has anything crucial come out since then that might not be in the book?\n    6.  Using HuggingFace:\nI mostly use HuggingFace for embedding models, and for local LLMs, I‚Äôve been using OLAMA. Curious how others are using HuggingFace‚Äîespecially beyond embeddings.\n    7.  Fine-Tuning Datasets:\nWhere do you typically source data for fine-tuning your models? Are there any reliable public datasets or workflows you‚Äôd recommend?&lt;/p&gt;\n\n&lt;p&gt;Any book or paper recommendations? (I actively read papers but maybe i see something new)&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your approaches or suggestions‚Äîthanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8xhxp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Far-Run-3778",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8xhxp/guidance_on_diving_deep_into_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8xhxp/guidance_on_diving_deep_into_llms/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753445519,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "bulding paradigm, application for local inference on nvidia gpu, cpu i launched mvp of paradigm , its scrappy , buggy. Finding the right people to help me build this. It changes the models that are compatible to gguf, save the gguf on your system for your use and run inference. \n\nLink - &gt; [https://github.com/NotKshitiz/paradigmai/releases/tag/v1.0.0](https://github.com/NotKshitiz/paradigmai/releases/tag/v1.0.0)\n\nDownload the zip file extract it and then install using the .exe. \n\nMake sure to give the path of the model like this  - C:\\\\\\\\Users\\\\\\\\kshit\\\\\\\\Downloads\\\\\\\\models\\\\\\\\mistral\n\nIf the files are in the mistral folder. \n\nThe application is a little buggy so there might be a chance that you wont get error if the conversion of model. \n\nI am currently working on that. \n\nPlease feel free to be brutally honest and give feedback.",
          "author_fullname": "t2_hu9onfqo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building Paradigm, Looking for right audience and feedbacks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m8xf7n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753445303,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;bulding paradigm, application for local inference on nvidia gpu, cpu i launched mvp of paradigm , its scrappy , buggy. Finding the right people to help me build this. It changes the models that are compatible to gguf, save the gguf on your system for your use and run inference. &lt;/p&gt;\n\n&lt;p&gt;Link - &amp;gt; &lt;a href=\"https://github.com/NotKshitiz/paradigmai/releases/tag/v1.0.0\"&gt;https://github.com/NotKshitiz/paradigmai/releases/tag/v1.0.0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Download the zip file extract it and then install using the .exe. &lt;/p&gt;\n\n&lt;p&gt;Make sure to give the path of the model like this  - C:\\\\Users\\\\kshit\\\\Downloads\\\\models\\\\mistral&lt;/p&gt;\n\n&lt;p&gt;If the files are in the mistral folder. &lt;/p&gt;\n\n&lt;p&gt;The application is a little buggy so there might be a chance that you wont get error if the conversion of model. &lt;/p&gt;\n\n&lt;p&gt;I am currently working on that. &lt;/p&gt;\n\n&lt;p&gt;Please feel free to be brutally honest and give feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?auto=webp&amp;s=4805ae779a2a935ad96c960fd848f1952e665442",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ae171aebdcd8ce9ab4e967566bf659337be51618",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bdc14426e4634519ead473ffb4fd53f9cf5df850",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17092f90bca33a88520e9bc9cf7066901c76f908",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7dd516ceda290a32ded2988a1010aba2ff6ff2c5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=429b065d3ff0dc34aa85876e9765ecb5f4dc45f4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=453975e043ad33717bb42b154fde5d3526ecf0be",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8xf7n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xitizdumb",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8xf7n/building_paradigm_looking_for_right_audience_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8xf7n/building_paradigm_looking_for_right_audience_and/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753445303,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/ppe2udztg0ff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=4706ed21d52a0746d9e2f387aac19de5256098a4\n\n[https://discord.gg/CkNRuS3N](https://discord.gg/CkNRuS3N)",
          "author_fullname": "t2_1kf8lj305b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a Hardware AI Code Editor with real-time profiling and AI optimization. We‚Äôre opening the preview version for free to a few users. If you‚Äôre interested, save your spot on our Discord",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "ppe2udztg0ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/ppe2udztg0ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=efb38509217baf9ef7ddf28eba3a603c38afb324"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/ppe2udztg0ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d09f60d5f63aedc695efcbe66df9cfe87508b090"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/ppe2udztg0ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ada3d5430cee2c9cc46d14d9783d93ebea954269"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/ppe2udztg0ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=43307db8c06753088aaf0af6d1f2f48f62e39f1f"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/ppe2udztg0ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d6f42f4e6019faa91a6cbd33eb5333956f323899"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/ppe2udztg0ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3d2dd24fc1970f34249520d046c305d2dd4e2e8c"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/ppe2udztg0ff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=4706ed21d52a0746d9e2f387aac19de5256098a4"
              },
              "id": "ppe2udztg0ff1"
            }
          },
          "name": "t3_1m8xf6l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/BxeQtBF348lwhBsT4X0Hz_--gZlvny5ugMcVpkF6VH0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753445301,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/ppe2udztg0ff1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4706ed21d52a0746d9e2f387aac19de5256098a4\"&gt;https://preview.redd.it/ppe2udztg0ff1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4706ed21d52a0746d9e2f387aac19de5256098a4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://discord.gg/CkNRuS3N\"&gt;https://discord.gg/CkNRuS3N&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m8xf6l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Firm_Protection4004",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8xf6l/i_built_a_hardware_ai_code_editor_with_realtime/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8xf6l/i_built_a_hardware_ai_code_editor_with_realtime/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753445301,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to fine tune TTS but there are plenty on the market so confused which one to use.\n\nCurrently using chatterbox for voice cloning to TTS, but for some voices the output is not accurate to the reference audio's pace and tone. If the reference audio is normal speech rate, the output audio will be a bit fast, despite lowering the pace.\n\nAnyways, will using RVC improve?\n\nFound these RVCs.. which one to use?  \n  \n[https://github.com/Mangio621/Mangio-RVC-Fork](https://github.com/Mangio621/Mangio-RVC-Fork) \n\n[https://github.com/JackismyShephard/ultimate-rvc](https://github.com/JackismyShephard/ultimate-rvc)\n\n[https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/tree/main](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/tree/main) ",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Good RVC to fine tune TTS?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m8ws0i",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753443360,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to fine tune TTS but there are plenty on the market so confused which one to use.&lt;/p&gt;\n\n&lt;p&gt;Currently using chatterbox for voice cloning to TTS, but for some voices the output is not accurate to the reference audio&amp;#39;s pace and tone. If the reference audio is normal speech rate, the output audio will be a bit fast, despite lowering the pace.&lt;/p&gt;\n\n&lt;p&gt;Anyways, will using RVC improve?&lt;/p&gt;\n\n&lt;p&gt;Found these RVCs.. which one to use?  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Mangio621/Mangio-RVC-Fork\"&gt;https://github.com/Mangio621/Mangio-RVC-Fork&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/JackismyShephard/ultimate-rvc\"&gt;https://github.com/JackismyShephard/ultimate-rvc&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/tree/main\"&gt;https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/tree/main&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?auto=webp&amp;s=54d81ad8121076769edd663c2c98e456a55af2de",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=808c91e6548b11d6746644706e0443a78ab2865d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3200ffd4cc51d1c6da907b3cd186002fb7ec1d33",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a97422065d40cab8e57ba1411dd2cd9f96e04376",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4996aa378dab4bfcb4080ff1c46aecefa2ea1ab6",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5f0446cfc59335aae49f3de9d97cc55e4d4fd5c0",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=91aa321e187fd4eded17d2a9d0deea66fc5ba3b3",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8ws0i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8ws0i/good_rvc_to_fine_tune_tts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8ws0i/good_rvc_to_fine_tune_tts/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753443360,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi! Kinda new to reddit, so I hope I post this to the right community.\n\nI am currently experimenting with 67B model. To run this, getting the quantization model will be really helpful for my system. However, I found myself stuck in llama-cpp-python installation for the last 3 days. I also have tried other file type, like AWQ version, but it's not working. \n\n  \nI notice that many discussions do not use singularity container. If anyone understand how to do it, I would appreciate your help!!!!!!! ",
          "author_fullname": "t2_1qjxvqmvge",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Beginner Here! Anyone knows how to install llama-cpp-python within a Singularity container or use in an HPC?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m8vsge",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753440020,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! Kinda new to reddit, so I hope I post this to the right community.&lt;/p&gt;\n\n&lt;p&gt;I am currently experimenting with 67B model. To run this, getting the quantization model will be really helpful for my system. However, I found myself stuck in llama-cpp-python installation for the last 3 days. I also have tried other file type, like AWQ version, but it&amp;#39;s not working. &lt;/p&gt;\n\n&lt;p&gt;I notice that many discussions do not use singularity container. If anyone understand how to do it, I would appreciate your help!!!!!!! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8vsge",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fluffy-Cress-4356",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vsge/beginner_here_anyone_knows_how_to_install/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8vsge/beginner_here_anyone_knows_how_to_install/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753440020,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,  \nCan anyone say is PCI 4.0 16X going to be bottleneck with tensor parallel inference, lets say with 4090 or 7900 XTX cards 2 or 4?  \nIs there anywhere data how much inference is using PCIE bandwidth, can it be measured during inference?  \nI have currently 2 7900 XTX in 8x pcie 4.0 and both cards uses max 200W during inference. My guess is they would maybe use more and the 8x lane might be bottleneck.  \nOf course it depends of the model.\n\nThen there is PCIE 5.0 cards, where the connection is 64GB/S instead 32GB/s.  \nIs that safe or will that also be bottleneck with 2 - 4 5090 cards? Who knows?  \nHas anyone tested inference in tensor parallel, first with 8X lanes and then 16x lanes? Big difference? I am now talking mainly vLLM and others which can do tensor parallel, not Ollama etc.  \n  \nI guess 4x is for sure too slow.",
          "author_fullname": "t2_1jk2ep8a52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tensor parallel - pcie bandwidth requirement",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m8vqnz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753440020,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753439838,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nCan anyone say is PCI 4.0 16X going to be bottleneck with tensor parallel inference, lets say with 4090 or 7900 XTX cards 2 or 4?&lt;br/&gt;\nIs there anywhere data how much inference is using PCIE bandwidth, can it be measured during inference?&lt;br/&gt;\nI have currently 2 7900 XTX in 8x pcie 4.0 and both cards uses max 200W during inference. My guess is they would maybe use more and the 8x lane might be bottleneck.&lt;br/&gt;\nOf course it depends of the model.&lt;/p&gt;\n\n&lt;p&gt;Then there is PCIE 5.0 cards, where the connection is 64GB/S instead 32GB/s.&lt;br/&gt;\nIs that safe or will that also be bottleneck with 2 - 4 5090 cards? Who knows?&lt;br/&gt;\nHas anyone tested inference in tensor parallel, first with 8X lanes and then 16x lanes? Big difference? I am now talking mainly vLLM and others which can do tensor parallel, not Ollama etc.  &lt;/p&gt;\n\n&lt;p&gt;I guess 4x is for sure too slow.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8vqnz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rich_Artist_8327",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753439838,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "BI obtained an internal list of websites that could and couldn't be used for training Anthropic's latest AI models.  \n  \nAnthropic's contractor Surge AI left the list fully public on Google Docs.  \n  \n'Sites you can use' include Bloomberg, Harvard, &amp; the Mayo Clinic.\n\nMany of the whitelisted sources copyright or otherwise restrict their content.  \n  \nAt least 3 - the Mayo Clinic, Cornell University, &amp; Morningstar - told BI they didn't have any AI training agreements with Anthropic.\n\n  \nThe spreadsheet also includes a blacklist of websites that Surge AI's gig workers were \"now disallowed\" from using.  \n  \nThe blacklist includes companies like the NYT &amp; Reddit which have sued AI startups for scraping without permission.",
          "author_fullname": "t2_3el21u3z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Leaked List Shows Which Websites Contractors Can Use to Train Anthropic's LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m82lwo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 61,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 61,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=21060694fcc62a00fc028087d1d26177aadb8fd8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753358823,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "businessinsider.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;BI obtained an internal list of websites that could and couldn&amp;#39;t be used for training Anthropic&amp;#39;s latest AI models.  &lt;/p&gt;\n\n&lt;p&gt;Anthropic&amp;#39;s contractor Surge AI left the list fully public on Google Docs.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#39;Sites you can use&amp;#39; include Bloomberg, Harvard, &amp;amp; the Mayo Clinic.&lt;/p&gt;\n\n&lt;p&gt;Many of the whitelisted sources copyright or otherwise restrict their content.  &lt;/p&gt;\n\n&lt;p&gt;At least 3 - the Mayo Clinic, Cornell University, &amp;amp; Morningstar - told BI they didn&amp;#39;t have any AI training agreements with Anthropic.&lt;/p&gt;\n\n&lt;p&gt;The spreadsheet also includes a blacklist of websites that Surge AI&amp;#39;s gig workers were &amp;quot;now disallowed&amp;quot; from using.  &lt;/p&gt;\n\n&lt;p&gt;The blacklist includes companies like the NYT &amp;amp; Reddit which have sued AI startups for scraping without permission.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.businessinsider.com/anthropic-surge-ai-leaked-list-sites-2025-7",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?auto=webp&amp;s=bd65cf5480704dc7805fd076e1f24144449bb9a7",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a02d30d66c7029a05158abac8fb3e271b366dbfc",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=86f57b1721691c337431e2352889367aa34f90cd",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6fb6b0c962db6ec04ea54163f60f3315806f90bd",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=42380fe0539f546fdb60963fca95595cf9e80e4c",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7defec9caecbfe1869fcef441c42dddbd5d88f2",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b889902e6adb9632b83e1787082dba4971c91ec9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m82lwo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Amgadoz",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m82lwo/leaked_list_shows_which_websites_contractors_can/",
          "stickied": false,
          "url": "https://www.businessinsider.com/anthropic-surge-ai-leaked-list-sites-2025-7",
          "subreddit_subscribers": 504253,
          "created_utc": 1753358823,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I spent 12 hours testing both models on real development work: Bug fixes, feature implementations, and refactoring tasks across a 38k-line Rust codebase and a 12k-line React frontend. Wanted to see how they perform beyond benchmarks.\n\n**TL;DR:**\n\n* Kimi K2 completed 14/15 tasks successfully with some guidance, Qwen-3 Coder completed 7/15\n* Kimi K2 followed coding guidelines consistently, Qwen-3 often ignored them\n* Kimi K2 cost 39% less\n* Qwen-3 Coder frequently modified tests to pass instead of fixing bugs\n* Both struggled with tool calling as compared to Sonnet 4, but Kimi K2 produced better code\n\n**Limitations:** This is just two code bases with my specific coding style. Your results will vary based on your project structure and requirements.\n\nAnyone else tested these models on real projects? Curious about other experiences.",
          "author_fullname": "t2_9ojglayx7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tested Kimi K2 vs Qwen-3 Coder on 15 Coding tasks - here's what I found",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7ts5g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 261,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 261,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=ef66cea5940bdc18745c99933ccc36a087d15694",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753327849,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "forgecode.dev",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I spent 12 hours testing both models on real development work: Bug fixes, feature implementations, and refactoring tasks across a 38k-line Rust codebase and a 12k-line React frontend. Wanted to see how they perform beyond benchmarks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Kimi K2 completed 14/15 tasks successfully with some guidance, Qwen-3 Coder completed 7/15&lt;/li&gt;\n&lt;li&gt;Kimi K2 followed coding guidelines consistently, Qwen-3 often ignored them&lt;/li&gt;\n&lt;li&gt;Kimi K2 cost 39% less&lt;/li&gt;\n&lt;li&gt;Qwen-3 Coder frequently modified tests to pass instead of fixing bugs&lt;/li&gt;\n&lt;li&gt;Both struggled with tool calling as compared to Sonnet 4, but Kimi K2 produced better code&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt; This is just two code bases with my specific coding style. Your results will vary based on your project structure and requirements.&lt;/p&gt;\n\n&lt;p&gt;Anyone else tested these models on real projects? Curious about other experiences.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://forgecode.dev/blog/kimi-k2-vs-qwen-3-coder-coding-comparison/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?auto=webp&amp;s=b36d593f1f906ab9804f44b4af78d2efcf1649ff",
                  "width": 5120,
                  "height": 2560
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a45fec9933e49c65c0d572dd982201ceeeea911",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5872d71864136e8f532d0f189a06dd40541b8d2",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4a01dce178c5ca9b4d9725309c95c9f6efdeaa30",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2c6be826302bc2f07626447c8d2d5437a5d30688",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2dc80b05cbfb768d8112b5ab17b6b699cdcd1116",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=315616253001017273d93aa470266ed29a9b6065",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m7ts5g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "West-Chocolate2977",
          "discussion_type": null,
          "num_comments": 50,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7ts5g/tested_kimi_k2_vs_qwen3_coder_on_15_coding_tasks/",
          "stickied": false,
          "url": "https://forgecode.dev/blog/kimi-k2-vs-qwen-3-coder-coding-comparison/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753327849,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Another very useful Ai guide from Vendel at Level1 Tech .\n\nI'm soo looking forward to a quantised Qwen3 coder. ",
          "author_fullname": "t2_oy3c84euj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Al and You Against the Machine: Guide so you can own Big Al and Run Local",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m89pk9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T17bpGItqXw?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AI and You Against the Machine: Guide so you can own Big AI and Run Local\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "AI and You Against the Machine: Guide so you can own Big AI and Run Local",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T17bpGItqXw?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AI and You Against the Machine: Guide so you can own Big AI and Run Local\"&gt;&lt;/iframe&gt;",
              "author_name": "Level1Techs",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/T17bpGItqXw/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Level1Techs"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T17bpGItqXw?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AI and You Against the Machine: Guide so you can own Big AI and Run Local\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m89pk9",
            "height": 200
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/m98DBZlqE20q-WCrZIdC6-U5ZZQG9E7NG_eWZskb9cc.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=a053be9222e86b80571eff50afcbd0a0c8db80fc",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753375988,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Another very useful Ai guide from Vendel at Level1 Tech .&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m soo looking forward to a quantised Qwen3 coder. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/T17bpGItqXw?si=P2u2pFLFIaVnhJo-",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/m98DBZlqE20q-WCrZIdC6-U5ZZQG9E7NG_eWZskb9cc.jpeg?auto=webp&amp;s=11e73eeb725b6a9fd0ade4b9c1b5326f638c0579",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/m98DBZlqE20q-WCrZIdC6-U5ZZQG9E7NG_eWZskb9cc.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=14baccbf7df8c8dacf95e3234a0e0cda143e28ac",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/m98DBZlqE20q-WCrZIdC6-U5ZZQG9E7NG_eWZskb9cc.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=62a346b03597783cc1fe94ef48c79c1ce4f720b7",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/m98DBZlqE20q-WCrZIdC6-U5ZZQG9E7NG_eWZskb9cc.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5de5f8b4559128d9a81c41174cc19d1ec0046fb5",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "m98DBZlqE20q-WCrZIdC6-U5ZZQG9E7NG_eWZskb9cc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m89pk9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sub_RedditTor",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m89pk9/al_and_you_against_the_machine_guide_so_you_can/",
          "stickied": false,
          "url": "https://youtu.be/T17bpGItqXw?si=P2u2pFLFIaVnhJo-",
          "subreddit_subscribers": 504253,
          "created_utc": 1753375988,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "AI and You Against the Machine: Guide so you can own Big AI and Run Local",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/T17bpGItqXw?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AI and You Against the Machine: Guide so you can own Big AI and Run Local\"&gt;&lt;/iframe&gt;",
              "author_name": "Level1Techs",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/T17bpGItqXw/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Level1Techs"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've decided to try out LM Studio on my MBP after a few days with ollama/open-webui. However, I can't seem to find any settings to change the Time To Live value in the GUI. Sorry, but can someone enlighten me? TIA.\n\nUpdate: I think I may have found out why‚Äîit is model (format) dependent. I was prioritizing LMX models and the two I have installed don't have the option for TTL. But when I loaded a GGUF (Codestral 22B), there are more options including \"Keep Model in Memory\". That's good enough for me. \n\nUpdate 2: Aside from model-specific settings, there is an inconspicuous \"Settings\" button inside the \"Developer\" tab in the left sidebar. A 'Max idle TTL' is there.",
          "author_fullname": "t2_ley1zpa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "TTL settings in LM Studio (0.3.20)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8tlmk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753439999,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753431513,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve decided to try out LM Studio on my MBP after a few days with ollama/open-webui. However, I can&amp;#39;t seem to find any settings to change the Time To Live value in the GUI. Sorry, but can someone enlighten me? TIA.&lt;/p&gt;\n\n&lt;p&gt;Update: I think I may have found out why‚Äîit is model (format) dependent. I was prioritizing LMX models and the two I have installed don&amp;#39;t have the option for TTL. But when I loaded a GGUF (Codestral 22B), there are more options including &amp;quot;Keep Model in Memory&amp;quot;. That&amp;#39;s good enough for me. &lt;/p&gt;\n\n&lt;p&gt;Update 2: Aside from model-specific settings, there is an inconspicuous &amp;quot;Settings&amp;quot; button inside the &amp;quot;Developer&amp;quot; tab in the left sidebar. A &amp;#39;Max idle TTL&amp;#39; is there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8tlmk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pythoglyphs",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8tlmk/ttl_settings_in_lm_studio_0320/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8tlmk/ttl_settings_in_lm_studio_0320/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753431513,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt;\n\n**tl;dr**¬†\\- Fine-tuned Qwen3 1.7B - called HyprLLM - which outperforms llama 3.2 3B in summarization for user experience because \"vanilla\" models suck at summarization.\n\n**Context**¬†\\- I am building an¬†[open-source](https://github.com/fastrepl/hyprnote)¬†privacy-first AI notetaker for people in compliance-sensitive environments. It uses on-device AI models to process everything locally. Used to use llama 3.2 3B q8 which sucks at summarizing so had to post-train a new model.\n\n**Selection**¬†\\- Juggled between Gemma and Qwen. But found Qwen to show more promising results.\n\n**Preparing**¬†\\- Since I can't get user data, I had to create a pipeline for synthetic data generation.\n\n**Training**¬†\\- Just boring stuff. Used Modal.\n\nPlanning to fine-tune whisper as well. Also trying to create next version for HyprLLM for multi-lingual support; our user base is global.\n\nWould love to get any tips on synthetic dataset generation or suggestions on models!",
          "author_fullname": "t2_j1t6g97wv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "had to fine-tune qwen since llama sucks at summarizing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m86wxa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/37dhjk23dsef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/37dhjk23dsef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/37dhjk23dsef1/DASHPlaylist.mpd?a=1756038534%2CYzBiODYwOWVjNmQxNDY3NDYzOTFhY2U5NTQ3NTllNzRhZjBkNThhMGJmOGE1MzdiZWY0OGUzNDI4N2IyMzc0ZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 16,
              "hls_url": "https://v.redd.it/37dhjk23dsef1/HLSPlaylist.m3u8?a=1756038534%2CYjNjMzc4YjJlODFjNDc4MjMwZmQ3ZGVlMDBkOGM4MTZjNzU5YzA0MmI3ODEwZTRhNmZhMmJhMTlmMjFkNDE3NA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MndnZnFrMjNkc2VmMZ2wNVo-OE2bciGH4sJ2rG79auy1VwP-dEBcS0EBYyDD.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=3e6af3b68012400ae6ecef435390b9e3a79a102e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753369673,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;¬†- Fine-tuned Qwen3 1.7B - called HyprLLM - which outperforms llama 3.2 3B in summarization for user experience because &amp;quot;vanilla&amp;quot; models suck at summarization.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;¬†- I am building an¬†&lt;a href=\"https://github.com/fastrepl/hyprnote\"&gt;open-source&lt;/a&gt;¬†privacy-first AI notetaker for people in compliance-sensitive environments. It uses on-device AI models to process everything locally. Used to use llama 3.2 3B q8 which sucks at summarizing so had to post-train a new model.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Selection&lt;/strong&gt;¬†- Juggled between Gemma and Qwen. But found Qwen to show more promising results.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Preparing&lt;/strong&gt;¬†- Since I can&amp;#39;t get user data, I had to create a pipeline for synthetic data generation.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;¬†- Just boring stuff. Used Modal.&lt;/p&gt;\n\n&lt;p&gt;Planning to fine-tune whisper as well. Also trying to create next version for HyprLLM for multi-lingual support; our user base is global.&lt;/p&gt;\n\n&lt;p&gt;Would love to get any tips on synthetic dataset generation or suggestions on models!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/37dhjk23dsef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MndnZnFrMjNkc2VmMZ2wNVo-OE2bciGH4sJ2rG79auy1VwP-dEBcS0EBYyDD.png?format=pjpg&amp;auto=webp&amp;s=89f5926cec2c81652c49d76ecc42cfdb1cb7706a",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MndnZnFrMjNkc2VmMZ2wNVo-OE2bciGH4sJ2rG79auy1VwP-dEBcS0EBYyDD.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a94bae1199b436d505feb890741b547f65a51597",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/MndnZnFrMjNkc2VmMZ2wNVo-OE2bciGH4sJ2rG79auy1VwP-dEBcS0EBYyDD.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d5202ef158e1eadd2ed0f72e9a9817f42db0c3ac",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/MndnZnFrMjNkc2VmMZ2wNVo-OE2bciGH4sJ2rG79auy1VwP-dEBcS0EBYyDD.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4551d6745f0f2940267f08d6f0ab73b9423cc9cf",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/MndnZnFrMjNkc2VmMZ2wNVo-OE2bciGH4sJ2rG79auy1VwP-dEBcS0EBYyDD.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d855e77bbfd2e7a61b9eac92800e2792145e2bbe",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/MndnZnFrMjNkc2VmMZ2wNVo-OE2bciGH4sJ2rG79auy1VwP-dEBcS0EBYyDD.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bebf992ece623ed79982579bfc608b8a0d5a84eb",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/MndnZnFrMjNkc2VmMZ2wNVo-OE2bciGH4sJ2rG79auy1VwP-dEBcS0EBYyDD.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4aee02859926942c86dbe92c5becb11162f6e59a",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "MndnZnFrMjNkc2VmMZ2wNVo-OE2bciGH4sJ2rG79auy1VwP-dEBcS0EBYyDD"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m86wxa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "beerbellyman4vr",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m86wxa/had_to_finetune_qwen_since_llama_sucks_at/",
          "stickied": false,
          "url": "https://v.redd.it/37dhjk23dsef1",
          "subreddit_subscribers": 504253,
          "created_utc": 1753369673,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/37dhjk23dsef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/37dhjk23dsef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/37dhjk23dsef1/DASHPlaylist.mpd?a=1756038534%2CYzBiODYwOWVjNmQxNDY3NDYzOTFhY2U5NTQ3NTllNzRhZjBkNThhMGJmOGE1MzdiZWY0OGUzNDI4N2IyMzc0ZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 16,
              "hls_url": "https://v.redd.it/37dhjk23dsef1/HLSPlaylist.m3u8?a=1756038534%2CYjNjMzc4YjJlODFjNDc4MjMwZmQ3ZGVlMDBkOGM4MTZjNzU5YzA0MmI3ODEwZTRhNmZhMmJhMTlmMjFkNDE3NA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a scanned form containing a large table with surrounding text. My goal is to extract specific information from certain cells in this table.  \n\nCurrent Approach &amp; Challenges  \n1. OCR Tools (e.g., Tesseract):  \n   - Used to identify the table and extract text.  \n   - Issue: OCR accuracy is inconsistent‚Äîsometimes the table isn‚Äôt recognized or is parsed incorrectly.  \n\n2. Post-OCR Correction (e.g., Mistral):  \n   - A language model refines the extracted text.  \n   - Issue: Poor results due to upstream OCR errors.  \n\nDespite spending hours on this workflow, I haven‚Äôt achieved reliable extraction.  \n\nAlternative Solution (Online Tools Work, but Local Execution is Required)  \n- Observation: Uploading the form to ChatGPT or DeepSeek (online) yields excellent results.  \n- Constraint: The solution must run entirely locally (no internet connection).  \n\nAttempted new Workflow (DINOv2 + Multimodal LLM)  \n1. Step 1: Image Embedding with DINOv2  \n   - Tried converting the image into a vector representation using DINOv2 (Vision Transformer).  \n   - Issue: Did not produce usable results‚Äîpossibly due to incorrect implementation or model limitations. Is this approach even correct?\n\n2. Step 2: Multimodal LLM Processing  \n   - Planned to feed the vector to a local multimodal LLM (e.g., Mistral) for structured output.  \n   - Blocker: Step 2 failed, didn‚Äôt got usable output \n\nQuestion  \nIs there a local, offline-compatible method to replicate the quality of online extraction tools? For example:  \n- Are there better vision models than DINOv2 for this task?  \n- Could a different pipeline (e.g., layout detection + OCR + LLM correction) work?  \n- Any tips for debugging DINOv2 missteps?",
          "author_fullname": "t2_ahnt19ga",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help Needed: Accurate Offline Table Extraction from Scanned Forms",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8n557",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753409440,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a scanned form containing a large table with surrounding text. My goal is to extract specific information from certain cells in this table.  &lt;/p&gt;\n\n&lt;p&gt;Current Approach &amp;amp; Challenges&lt;br/&gt;\n1. OCR Tools (e.g., Tesseract):&lt;br/&gt;\n   - Used to identify the table and extract text.&lt;br/&gt;\n   - Issue: OCR accuracy is inconsistent‚Äîsometimes the table isn‚Äôt recognized or is parsed incorrectly.  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Post-OCR Correction (e.g., Mistral):&lt;br/&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A language model refines the extracted text.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Issue: Poor results due to upstream OCR errors.&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Despite spending hours on this workflow, I haven‚Äôt achieved reliable extraction.  &lt;/p&gt;\n\n&lt;p&gt;Alternative Solution (Online Tools Work, but Local Execution is Required)&lt;br/&gt;\n- Observation: Uploading the form to ChatGPT or DeepSeek (online) yields excellent results.&lt;br/&gt;\n- Constraint: The solution must run entirely locally (no internet connection).  &lt;/p&gt;\n\n&lt;p&gt;Attempted new Workflow (DINOv2 + Multimodal LLM)&lt;br/&gt;\n1. Step 1: Image Embedding with DINOv2&lt;br/&gt;\n   - Tried converting the image into a vector representation using DINOv2 (Vision Transformer).&lt;br/&gt;\n   - Issue: Did not produce usable results‚Äîpossibly due to incorrect implementation or model limitations. Is this approach even correct?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Step 2: Multimodal LLM Processing&lt;br/&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Planned to feed the vector to a local multimodal LLM (e.g., Mistral) for structured output.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Blocker: Step 2 failed, didn‚Äôt got usable output &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Question&lt;br/&gt;\nIs there a local, offline-compatible method to replicate the quality of online extraction tools? For example:&lt;br/&gt;\n- Are there better vision models than DINOv2 for this task?&lt;br/&gt;\n- Could a different pipeline (e.g., layout detection + OCR + LLM correction) work?&lt;br/&gt;\n- Any tips for debugging DINOv2 missteps?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8n557",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Antelito83",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8n557/help_needed_accurate_offline_table_extraction/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8n557/help_needed_accurate_offline_table_extraction/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753409440,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As the title says. I'm writing a book and would like to have it read to me as part of the revision process. Commercial models like ElevenLabs are far too expensive for this sort of iterative process - plus I don't need it sounding that professional anyway.\n\nI have an ROG G14 laptop with an RTX3060 and 32gb RAM. Are there any models I could run on this with reasonable speed? The last few posts I saw here were a year ago, noting AllTalk TTS as a good solution. Is it still the way to go?",
          "author_fullname": "t2_5m90em29",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best local text-to-speech model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8peos",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753416345,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says. I&amp;#39;m writing a book and would like to have it read to me as part of the revision process. Commercial models like ElevenLabs are far too expensive for this sort of iterative process - plus I don&amp;#39;t need it sounding that professional anyway.&lt;/p&gt;\n\n&lt;p&gt;I have an ROG G14 laptop with an RTX3060 and 32gb RAM. Are there any models I could run on this with reasonable speed? The last few posts I saw here were a year ago, noting AllTalk TTS as a good solution. Is it still the way to go?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8peos",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mercurialninja",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8peos/best_local_texttospeech_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8peos/best_local_texttospeech_model/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753416345,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "demo: [https://flappybird.njkumar.com/](https://flappybird.njkumar.com/)\n\nblogpost: [https://njkumar.com/optimizing-flappy-bird-world-model-to-run-in-a-web-browser/](https://njkumar.com/optimizing-flappy-bird-world-model-to-run-in-a-web-browser/)\n\nI finally got some time to put some development into this, but I optimized a flappy bird diffusion model to run around 30FPS on my Macbook, and around 12-15FPS on my iPhone 14 Pro. More details about the optimization experiments in the blog post above, but surprisingly trained this model on a couple hours of flappy bird data and 3-4 days of training on a rented A100. \n\nWorld models are definitely going to be really popular in the future, but I think there should be more accessible ways to distribute and run these models, especially as inference becomes more expensive, which is why I went for an on-device approach.\n\nLet me know what you guys think!",
          "author_fullname": "t2_6xc1kgl4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I optimized a Flappy Bird diffusion world model to run locally on my phone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7p7ek",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 353,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/71l2pz57opef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1920,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/71l2pz57opef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/71l2pz57opef1/DASHPlaylist.mpd?a=1756038534%2CN2Y3MjRmNjkyMGNhN2E1MDUxOTQzM2E4NmYxMzNiNDQxNGRhMWFjNTkyMTc4YzBiZjQ2MzIwMjBlYjkyNjdlNA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 11,
              "hls_url": "https://v.redd.it/71l2pz57opef1/HLSPlaylist.m3u8?a=1756038534%2CMzRlYzA1MmExMTFiYWMzYjUyYzlhNzY4ODgyMDQ1ZTJlZjhjM2M0ZThmNmY4OTdiZjM0YzBmMzhlODFhMWJjMQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 353,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=b962b22bb648b1c6e8f58f793ea34c9c5459c008",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753314632,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;demo: &lt;a href=\"https://flappybird.njkumar.com/\"&gt;https://flappybird.njkumar.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;blogpost: &lt;a href=\"https://njkumar.com/optimizing-flappy-bird-world-model-to-run-in-a-web-browser/\"&gt;https://njkumar.com/optimizing-flappy-bird-world-model-to-run-in-a-web-browser/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I finally got some time to put some development into this, but I optimized a flappy bird diffusion model to run around 30FPS on my Macbook, and around 12-15FPS on my iPhone 14 Pro. More details about the optimization experiments in the blog post above, but surprisingly trained this model on a couple hours of flappy bird data and 3-4 days of training on a rented A100. &lt;/p&gt;\n\n&lt;p&gt;World models are definitely going to be really popular in the future, but I think there should be more accessible ways to distribute and run these models, especially as inference becomes more expensive, which is why I went for an on-device approach.&lt;/p&gt;\n\n&lt;p&gt;Let me know what you guys think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/71l2pz57opef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?format=pjpg&amp;auto=webp&amp;s=981c7dbb770b9f932308688752873c877a45ab76",
                  "width": 1080,
                  "height": 1920
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=82d4dd1d9fe94438a59143a22dda39ec75f1b8d0",
                    "width": 108,
                    "height": 192
                  },
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8dab6463cd659f5b4cf87c83b3eafc0bb67babde",
                    "width": 216,
                    "height": 384
                  },
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bd6e375eb171716a6642ec34d15dbe84a8777e59",
                    "width": 320,
                    "height": 568
                  },
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3b58cbca08da41bc17c7d3a9eb4c34ab9e3a0eab",
                    "width": 640,
                    "height": 1137
                  },
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2d235cc44118aa9d4bb46169ca00de87c5496ed7",
                    "width": 960,
                    "height": 1706
                  },
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9d0408754823b54ec35b393c9897ef1782beaae5",
                    "width": 1080,
                    "height": 1920
                  }
                ],
                "variants": {},
                "id": "amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7p7ek",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fendiwap1234",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7p7ek/i_optimized_a_flappy_bird_diffusion_world_model/",
          "stickied": false,
          "url": "https://v.redd.it/71l2pz57opef1",
          "subreddit_subscribers": 504253,
          "created_utc": 1753314632,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/71l2pz57opef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1920,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/71l2pz57opef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/71l2pz57opef1/DASHPlaylist.mpd?a=1756038534%2CN2Y3MjRmNjkyMGNhN2E1MDUxOTQzM2E4NmYxMzNiNDQxNGRhMWFjNTkyMTc4YzBiZjQ2MzIwMjBlYjkyNjdlNA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 11,
              "hls_url": "https://v.redd.it/71l2pz57opef1/HLSPlaylist.m3u8?a=1756038534%2CMzRlYzA1MmExMTFiYWMzYjUyYzlhNzY4ODgyMDQ1ZTJlZjhjM2M0ZThmNmY4OTdiZjM0YzBmMzhlODFhMWJjMQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello guys I just found out Ollama can't connect to server on Fedora with RX580? ",
          "author_fullname": "t2_1kjopyfn56",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RX580 support",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8t01d",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753429115,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys I just found out Ollama can&amp;#39;t connect to server on Fedora with RX580? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8t01d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Overall_Walrus9871",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8t01d/rx580_support/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8t01d/rx580_support/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753429115,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "AI-generated content is easy to spot these days:  \n  \n‚Äì The em dashes  \n‚Äì The ‚ÄúIt‚Äôs not X, but Y‚Äù   \n‚Äì Snappy one-line sentences  \n‚Äì Lots of emojis  \n...\n\nMany of us use AI to edit text, build chatbots, write reports...  \nWhat technique do you use to make sure the output isn't generic AI slop?\n\nDo you use specific prompts? Few-shot examples? Guardrails? Certain models? Fine-tuning?\n\n",
          "author_fullname": "t2_376cy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you keep AI outputs from sounding AI?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m84s47",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753364580,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AI-generated content is easy to spot these days:  &lt;/p&gt;\n\n&lt;p&gt;‚Äì The em dashes&lt;br/&gt;\n‚Äì The ‚ÄúIt‚Äôs not X, but Y‚Äù&lt;br/&gt;\n‚Äì Snappy one-line sentences&lt;br/&gt;\n‚Äì Lots of emojis&lt;br/&gt;\n...&lt;/p&gt;\n\n&lt;p&gt;Many of us use AI to edit text, build chatbots, write reports...&lt;br/&gt;\nWhat technique do you use to make sure the output isn&amp;#39;t generic AI slop?&lt;/p&gt;\n\n&lt;p&gt;Do you use specific prompts? Few-shot examples? Guardrails? Certain models? Fine-tuning?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m84s47",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "resiros",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m84s47/how_do_you_keep_ai_outputs_from_sounding_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m84s47/how_do_you_keep_ai_outputs_from_sounding_ai/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753364580,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://www.velocitymicro.com/blog/amd-radeon-ai-pro-r9700/\n\nHey y'all. The R9700 was supposedly launched yesterday, but I couldn't find any reviews or listings online for it, outside of one company that had a \"request a quote\" button instead of an actual price. So I kept digging and found Velocity Micro's blog post, which is from yesterday. I've never heard of them before, but they appear to be a well-established System Integrator/boutique PC builder.\n\nIn their blog post, they compared the RTX 5080 and the R9700's AI Inference performance using Phi 3.5 MoE Q4, Mistral Small 3.1 24B Instruct 2503 Q8, Qwen 3 32B Q6, and DeepSeek R1 Distill Qwen 32B Q6. The results are shown in the screenshot above.\n\nNow, I'll freely admit I've been an AMD fan for a long time (RX590 with ROCm 6.3 says hi), but those performance figures are **heavily** biased towards the R9700. There are two big, glaring issues here:\n\n1. No concrete tokens per second performance figures were presented, only relative performance uplift in percentage.\n\n2. ALL of the models used in the benchmark don't fit within the RTX 5080's 16GB VRAM buffer.\n\nThat completely defeats the point of the benchmark lol. None of those models fully fit within the 5080's VRAM, so God knows how many layers were offloaded to the CPU.\n\nThey don't mention the price in their blog post, but I checked the custom build configuration page of their ProMagix HD150 workstation, and the R9700 adds $1500 to the build cost, whereas the 5080 adds $1710. So I suppose there's an argument to be made about comparing the two, considering how close in price they are, but... the models chosen reek of dishonesty.\n\nOh, and as an aside, that's not the only thing the post reeks of. It reeks of LLM-isms, like this one passage right beneath the benchmarks table: \"The takeaway? For professionals running large prompts or full-sized models locally, the Radeon‚Ñ¢ AI PRO R9700 isn‚Äôt just competitive‚Äîit‚Äôs transformative,\" you know, with the classic \"It isn't just X, it's Y!\" But maaaybe I'm being just overly critical in this era of AI slop. idk lol.",
          "author_fullname": "t2_gp8z8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Velocity Micro Published (Faulty?) LLM Benchmarks for the Radeon AI PRO R9700 and Lists it for $1500 in Their Build Configuration Page",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8aw4w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/99Iko3whTqVLFl_kwwdJwnnzIy17oQSxbbdpbYBHMLs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753378626,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.velocitymicro.com/blog/amd-radeon-ai-pro-r9700/\"&gt;https://www.velocitymicro.com/blog/amd-radeon-ai-pro-r9700/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hey y&amp;#39;all. The R9700 was supposedly launched yesterday, but I couldn&amp;#39;t find any reviews or listings online for it, outside of one company that had a &amp;quot;request a quote&amp;quot; button instead of an actual price. So I kept digging and found Velocity Micro&amp;#39;s blog post, which is from yesterday. I&amp;#39;ve never heard of them before, but they appear to be a well-established System Integrator/boutique PC builder.&lt;/p&gt;\n\n&lt;p&gt;In their blog post, they compared the RTX 5080 and the R9700&amp;#39;s AI Inference performance using Phi 3.5 MoE Q4, Mistral Small 3.1 24B Instruct 2503 Q8, Qwen 3 32B Q6, and DeepSeek R1 Distill Qwen 32B Q6. The results are shown in the screenshot above.&lt;/p&gt;\n\n&lt;p&gt;Now, I&amp;#39;ll freely admit I&amp;#39;ve been an AMD fan for a long time (RX590 with ROCm 6.3 says hi), but those performance figures are &lt;strong&gt;heavily&lt;/strong&gt; biased towards the R9700. There are two big, glaring issues here:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;No concrete tokens per second performance figures were presented, only relative performance uplift in percentage.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;ALL of the models used in the benchmark don&amp;#39;t fit within the RTX 5080&amp;#39;s 16GB VRAM buffer.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;That completely defeats the point of the benchmark lol. None of those models fully fit within the 5080&amp;#39;s VRAM, so God knows how many layers were offloaded to the CPU.&lt;/p&gt;\n\n&lt;p&gt;They don&amp;#39;t mention the price in their blog post, but I checked the custom build configuration page of their ProMagix HD150 workstation, and the R9700 adds $1500 to the build cost, whereas the 5080 adds $1710. So I suppose there&amp;#39;s an argument to be made about comparing the two, considering how close in price they are, but... the models chosen reek of dishonesty.&lt;/p&gt;\n\n&lt;p&gt;Oh, and as an aside, that&amp;#39;s not the only thing the post reeks of. It reeks of LLM-isms, like this one passage right beneath the benchmarks table: &amp;quot;The takeaway? For professionals running large prompts or full-sized models locally, the Radeon‚Ñ¢ AI PRO R9700 isn‚Äôt just competitive‚Äîit‚Äôs transformative,&amp;quot; you know, with the classic &amp;quot;It isn&amp;#39;t just X, it&amp;#39;s Y!&amp;quot; But maaaybe I&amp;#39;m being just overly critical in this era of AI slop. idk lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/hb4sc99vyuef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/hb4sc99vyuef1.jpeg?auto=webp&amp;s=96302dd6cdcb2b411601841c937d0121fe96dee2",
                  "width": 1080,
                  "height": 2340
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/hb4sc99vyuef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=25e7447d2948ff3ba829b2fe4b668ffa16126aca",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/hb4sc99vyuef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2225e2c5840b0bf1a9f78c11b1ad8a32c61bd1ed",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://preview.redd.it/hb4sc99vyuef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9b15cc449c73707c71ff19936883e8c41b86d020",
                    "width": 320,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/hb4sc99vyuef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f63a18c6ad86b8d63e1b163fd5dd0ba53844277f",
                    "width": 640,
                    "height": 1280
                  },
                  {
                    "url": "https://preview.redd.it/hb4sc99vyuef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9fc78e96a489a0d8cdec3ba2b56d666ce5f0593e",
                    "width": 960,
                    "height": 1920
                  },
                  {
                    "url": "https://preview.redd.it/hb4sc99vyuef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b575f5d80433824a8e1481ce8eb3f26715a44855",
                    "width": 1080,
                    "height": 2160
                  }
                ],
                "variants": {},
                "id": "ua2gbHgHzn0rn80rVFCnRr-uUof35t7ZglKUGHqi2QE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8aw4w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kamal965",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8aw4w/velocity_micro_published_faulty_llm_benchmarks/",
          "stickied": false,
          "url": "https://i.redd.it/hb4sc99vyuef1.jpeg",
          "subreddit_subscribers": 504253,
          "created_utc": 1753378626,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I built an open-source tool called¬†**Digital Twin Proxy**¬†that uses a local LLM (via Ollama) to analyze my browsing history and create a personal \"digital twin.\" This gives my other AI agents real-time context about what I'm working on.\n\n**GitHub Repo:**¬†[https://github.com/kstonekuan/digital-twin-proxy](https://github.com/kstonekuan/digital-twin-proxy)\n\nIt works by routing traffic through a Squid proxy, and then a Rust app sends the logs to a local model (I'm using Llama 3) for analysis. This way, I can create a more personalized AI experience without my data ever leaving my machine.\n\nThe goal is to enable \"context engineering,\" where agents can anticipate needs or tailor responses based on my current web activity.\n\nI'd love to get feedback, let me know what you think",
          "author_fullname": "t2_2t921gqw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I used a local LLM and http proxy to create a \"Digital Twin\" from my web browsing for my AI agents",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m820ry",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=52a82b76b560b57316460190679454b56136228a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753357044,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I built an open-source tool called¬†&lt;strong&gt;Digital Twin Proxy&lt;/strong&gt;¬†that uses a local LLM (via Ollama) to analyze my browsing history and create a personal &amp;quot;digital twin.&amp;quot; This gives my other AI agents real-time context about what I&amp;#39;m working on.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt;¬†&lt;a href=\"https://github.com/kstonekuan/digital-twin-proxy\"&gt;https://github.com/kstonekuan/digital-twin-proxy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It works by routing traffic through a Squid proxy, and then a Rust app sends the logs to a local model (I&amp;#39;m using Llama 3) for analysis. This way, I can create a more personalized AI experience without my data ever leaving my machine.&lt;/p&gt;\n\n&lt;p&gt;The goal is to enable &amp;quot;context engineering,&amp;quot; where agents can anticipate needs or tailor responses based on my current web activity.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to get feedback, let me know what you think&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/kstonekuan/digital-twin-proxy",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?auto=webp&amp;s=1fe54525cb398fab2f40d0cbb98855f2f863dea4",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ac293c80557525f283d861debdb7eded2285e09",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ae20034bab95e8229244039a9ec6040c11eb7f7",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1e4e8ba27dd6ef1ef3b3b6918f1ce1fb862ecea8",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c930ae840780c055e12d46587ee78dbe04d08779",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a25dd593bc8b471e46e739625dc0d248a23814ae",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f696d14900d9017986d374a07cc8a6138b6b780a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m820ry",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kuaythrone",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m820ry/i_used_a_local_llm_and_http_proxy_to_create_a/",
          "stickied": false,
          "url": "https://github.com/kstonekuan/digital-twin-proxy",
          "subreddit_subscribers": 504253,
          "created_utc": 1753357044,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_pmniwf57y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Google has shared the system prompt that got Gemini 2.5 Pro IMO 2025 Gold Medal üèÖ",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7k4ix",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 411,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 411,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753302182,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "alphaxiv.org",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.alphaxiv.org/abs/2507.15855",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m7k4ix",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "secopsml",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7k4ix/google_has_shared_the_system_prompt_that_got/",
          "stickied": false,
          "url": "https://www.alphaxiv.org/abs/2507.15855",
          "subreddit_subscribers": 504253,
          "created_utc": 1753302182,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/Kwaipilot/KAT-V1-40B](https://huggingface.co/Kwaipilot/KAT-V1-40B)\n\nNote: I am not affiliated with the model creators",
          "author_fullname": "t2_fmd6oq5v6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "KAT-V1-40B: mitigates over-thinking by learning when to produce explicit chain-of-thought and when to answer directly.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 51,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7ufyb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bbbdbf",
          "ups": 100,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 100,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/b7Cpt1an0rQVEqyrYVS52lr_kisl0R4_s5HEZLDdmvY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753329919,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Kwaipilot/KAT-V1-40B\"&gt;https://huggingface.co/Kwaipilot/KAT-V1-40B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Note: I am not affiliated with the model creators&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nylqnllzxqef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nylqnllzxqef1.png?auto=webp&amp;s=2a76625c6790f1b1e80e764391f5c307c370cac0",
                  "width": 4640,
                  "height": 1717
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nylqnllzxqef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6375cdf7b48070d3cc2476ca76a5c824b7cacb4",
                    "width": 108,
                    "height": 39
                  },
                  {
                    "url": "https://preview.redd.it/nylqnllzxqef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c177774e33c2e31dc5b68a0742a2340ece6b0bfd",
                    "width": 216,
                    "height": 79
                  },
                  {
                    "url": "https://preview.redd.it/nylqnllzxqef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d8b0034caf798b019ae8747d6d0f57c58f5c99f",
                    "width": 320,
                    "height": 118
                  },
                  {
                    "url": "https://preview.redd.it/nylqnllzxqef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=10b88450320c1a803baf4cb0625160a4299439c8",
                    "width": 640,
                    "height": 236
                  },
                  {
                    "url": "https://preview.redd.it/nylqnllzxqef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=81b1f737fa5b44c20211f599efaff8fc2d6bda56",
                    "width": 960,
                    "height": 355
                  },
                  {
                    "url": "https://preview.redd.it/nylqnllzxqef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9e969e45713b0734281931d9f2f5f8644a87b5bf",
                    "width": 1080,
                    "height": 399
                  }
                ],
                "variants": {},
                "id": "SMfOoqn7DPSHVtY5DIibrtsaeBB9lXPO6KjdcjAeF8s"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m7ufyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "random-tomato",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m7ufyb/katv140b_mitigates_overthinking_by_learning_when/",
          "stickied": false,
          "url": "https://i.redd.it/nylqnllzxqef1.png",
          "subreddit_subscribers": 504253,
          "created_utc": 1753329919,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Full text: [https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf](https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf)",
          "author_fullname": "t2_1f8trxud0p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Encouragement of \"Open-Source and Open-Weight AI\" is now the official policy of the U.S. government.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 96,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7dmy2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 818,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 818,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/dE8iio5_qxWifLCWxYWX3e_adPkw9bIROwsF8nQf2uk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753287492,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Full text: &lt;a href=\"https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf\"&gt;https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/736cx17efnef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/736cx17efnef1.png?auto=webp&amp;s=1441150f32beb0af75982abe485e922ee54a12ff",
                  "width": 1028,
                  "height": 711
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/736cx17efnef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=251e5e43fe714ddc7032706935cd9fc3d43c1165",
                    "width": 108,
                    "height": 74
                  },
                  {
                    "url": "https://preview.redd.it/736cx17efnef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=53a8eee481ec4de748ebe8209d3ec2aae407dc4c",
                    "width": 216,
                    "height": 149
                  },
                  {
                    "url": "https://preview.redd.it/736cx17efnef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a0f980d4f894d0dc575de06c03023b4c50e561e9",
                    "width": 320,
                    "height": 221
                  },
                  {
                    "url": "https://preview.redd.it/736cx17efnef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b6dc537086eca79402f273c84f9cfeda0bb9e59",
                    "width": 640,
                    "height": 442
                  },
                  {
                    "url": "https://preview.redd.it/736cx17efnef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9f44e16c425c6833fd0d6beb6b440f43e9f77c83",
                    "width": 960,
                    "height": 663
                  }
                ],
                "variants": {},
                "id": "cL7vyiaEvwvylAKGE60rRc6GWzCOBXcgInu5tOXXrMs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m7dmy2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GlowiesEatShitAndDie",
          "discussion_type": null,
          "num_comments": 194,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7dmy2/encouragement_of_opensource_and_openweight_ai_is/",
          "stickied": false,
          "url": "https://i.redd.it/736cx17efnef1.png",
          "subreddit_subscribers": 504253,
          "created_utc": 1753287492,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "# Hey everyone,\n\n**I'm a newcomer to the world of AI and I'm diving into my first big project. I've laid out a plan, but I need the community's wisdom to choose the right tools and navigate the challenges, especially since my goal is to build this completely for free.**\n\n**My project is to build a specific, knowledge-based AI chatbot and host a demo online. Here‚Äôs the breakdown:**\n\n**Objective:**\n\n**An AI chatbot that can answer questions in both English and Bengali.**\n\n**Its knowledge should come only from a 50-page Bengali PDF file.**\n\n**The entire project, from development to hosting, must be 100% free.**\n\n**My Project Plan (The RAG Pipeline):**\n\n**Knowledge Base:**\n\n**Use the 50-page Bengali PDF as the sole data source.**\n\n**Properly pre-process, clean, and chunk the text.**\n\n**Vectorize these chunks and store them.**\n\n**Core RAG Task:**\n\n**The app should accept user queries in English or Bengali.**\n\n**Retrieve the most relevant text chunks from the knowledge base.**\n\n**Generate a coherent answer based only on the retrieved information.**\n\n**Memory:**\n\n**Long-Term Memory: The vectorized PDF content in a vector database.**\n\n**Short-Term Memory: The recent chat history to allow for conversational follow-up questions.**\n\n**My Questions &amp; Where I Need Your Help:**\n\n**I've done some research, but I'm getting lost in the sea of options. Given the \"completely free\" constraint, what is the best tech stack for this? How do I handle the bilingual (Bengali/English) part?**\n\n**Here‚Äôs my thinking, but I would love your feedback and suggestions:**\n\n**1. The Framework: LangChain or LlamaIndex?**\n\n**These seem to be the go-to tools for building RAG applications. Which one is more beginner-friendly for this specific task?**\n\n**2. The \"Brain\" (LLM): How to get a good, free one?**\n\n**The OpenAI API costs money. What's the best free alternative? I've heard about using open-source models from Hugging Face. Can I use their free Inference API for a project like this? If so, any recommendations for a model that's good with both English and Bengali context?**\n\n**3. The \"Translator/Encoder\" (Embeddings): How to handle two languages?**\n\n**This is my biggest confusion. The documents are in Bengali, but the questions can be in English. How does the system find the right Bengali text from an English question?**\n\n**I assume I need a multilingual embedding model. Again, any free recommendations from Hugging Face?**\n\n**4. The \"Long-Term Memory\" (Vector Database): What's a free and easy option?**\n\n**Pinecone has a free tier, but I've heard about self-hosted options like FAISS or ChromaDB. Since my app will be hosted in the cloud, which of these is easier to set up for free?**\n\n**5. The App &amp; Hosting: How to put it online for free?**\n\n**I need to build a simple UI and host the whole Python application. What's the standard, free way to do this for an AI demo? I've seen Streamlit Cloud and Hugging Face Spaces mentioned. Are these good choices?**\n\n**I know this is a lot, but even a small tip on any of these points would be incredibly helpful. My goal is to learn by doing, and your guidance can save me weeks of going down the wrong path.**\n\n**Thank you so much in advance for your help**",
          "author_fullname": "t2_cvi1ys52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Newbie] Seeking Guidance: Building a Free, Bilingual (Bengali/English) RAG Chatbot from a PDF",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8l55o",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753403694,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Hey everyone,&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;I&amp;#39;m a newcomer to the world of AI and I&amp;#39;m diving into my first big project. I&amp;#39;ve laid out a plan, but I need the community&amp;#39;s wisdom to choose the right tools and navigate the challenges, especially since my goal is to build this completely for free.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My project is to build a specific, knowledge-based AI chatbot and host a demo online. Here‚Äôs the breakdown:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;An AI chatbot that can answer questions in both English and Bengali.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Its knowledge should come only from a 50-page Bengali PDF file.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The entire project, from development to hosting, must be 100% free.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Project Plan (The RAG Pipeline):&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Knowledge Base:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Use the 50-page Bengali PDF as the sole data source.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Properly pre-process, clean, and chunk the text.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Vectorize these chunks and store them.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Core RAG Task:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The app should accept user queries in English or Bengali.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Retrieve the most relevant text chunks from the knowledge base.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Generate a coherent answer based only on the retrieved information.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Memory:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Long-Term Memory: The vectorized PDF content in a vector database.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Short-Term Memory: The recent chat history to allow for conversational follow-up questions.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Questions &amp;amp; Where I Need Your Help:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I&amp;#39;ve done some research, but I&amp;#39;m getting lost in the sea of options. Given the &amp;quot;completely free&amp;quot; constraint, what is the best tech stack for this? How do I handle the bilingual (Bengali/English) part?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here‚Äôs my thinking, but I would love your feedback and suggestions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. The Framework: LangChain or LlamaIndex?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;These seem to be the go-to tools for building RAG applications. Which one is more beginner-friendly for this specific task?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. The &amp;quot;Brain&amp;quot; (LLM): How to get a good, free one?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The OpenAI API costs money. What&amp;#39;s the best free alternative? I&amp;#39;ve heard about using open-source models from Hugging Face. Can I use their free Inference API for a project like this? If so, any recommendations for a model that&amp;#39;s good with both English and Bengali context?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3. The &amp;quot;Translator/Encoder&amp;quot; (Embeddings): How to handle two languages?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;This is my biggest confusion. The documents are in Bengali, but the questions can be in English. How does the system find the right Bengali text from an English question?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I assume I need a multilingual embedding model. Again, any free recommendations from Hugging Face?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4. The &amp;quot;Long-Term Memory&amp;quot; (Vector Database): What&amp;#39;s a free and easy option?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Pinecone has a free tier, but I&amp;#39;ve heard about self-hosted options like FAISS or ChromaDB. Since my app will be hosted in the cloud, which of these is easier to set up for free?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5. The App &amp;amp; Hosting: How to put it online for free?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I need to build a simple UI and host the whole Python application. What&amp;#39;s the standard, free way to do this for an AI demo? I&amp;#39;ve seen Streamlit Cloud and Hugging Face Spaces mentioned. Are these good choices?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I know this is a lot, but even a small tip on any of these points would be incredibly helpful. My goal is to learn by doing, and your guidance can save me weeks of going down the wrong path.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Thank you so much in advance for your help&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8l55o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mr_Genius_360",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8l55o/newbie_seeking_guidance_building_a_free_bilingual/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8l55o/newbie_seeking_guidance_building_a_free_bilingual/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753403694,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It should be really easy to make something like: \n\nJust MOE gatting network is  initially loaded into RAM ( or offloaded to the GPU ) and stays there\n\n\n\nActivation Process: When an input is received, the gating network evaluates it and determines which experts should be activated based on the input's characteristics.\n\n\n\nLoading Active Experts: Only the parameters of the selected experts are oflloaded to the GPU (or loaded into RAM, by choice)  for processing.\n\n\n\nFor the next prompt if  gatting network decides different  experts will be activated they are just replaced in RAM ( VRAM) .    \n  \nThere will be a little latency at the start but it is  nothing compared to present clumsiness and huge processing time  if not enough RAM or VRAM and memory swapping..  \n",
          "author_fullname": "t2_1qychuraq9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why there is still no a proper or helpful inference for MOE models ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8oz07",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753414990,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It should be really easy to make something like: &lt;/p&gt;\n\n&lt;p&gt;Just MOE gatting network is  initially loaded into RAM ( or offloaded to the GPU ) and stays there&lt;/p&gt;\n\n&lt;p&gt;Activation Process: When an input is received, the gating network evaluates it and determines which experts should be activated based on the input&amp;#39;s characteristics.&lt;/p&gt;\n\n&lt;p&gt;Loading Active Experts: Only the parameters of the selected experts are oflloaded to the GPU (or loaded into RAM, by choice)  for processing.&lt;/p&gt;\n\n&lt;p&gt;For the next prompt if  gatting network decides different  experts will be activated they are just replaced in RAM ( VRAM) .    &lt;/p&gt;\n\n&lt;p&gt;There will be a little latency at the start but it is  nothing compared to present clumsiness and huge processing time  if not enough RAM or VRAM and memory swapping..  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8oz07",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Highwaytothebeach",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753414990,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm planning to run reinforcement learning experiments using an 8B model (like LLaMA 8B or similar) for academic research. possibly using quantization (e.g., int4/int8) to reduce resource usage.\n\nWhat GPUs and VRAM would be the minimum recommended to make this feasible?\n\nAny advice would be greatly appreciated!",
          "author_fullname": "t2_k1hvhsr9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are the hardware recommendations for reinforcement learning with an 8B model (for research purposes)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8h89j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753393374,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m planning to run reinforcement learning experiments using an 8B model (like LLaMA 8B or similar) for academic research. possibly using quantization (e.g., int4/int8) to reduce resource usage.&lt;/p&gt;\n\n&lt;p&gt;What GPUs and VRAM would be the minimum recommended to make this feasible?&lt;/p&gt;\n\n&lt;p&gt;Any advice would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8h89j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CHLCCGA",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8h89j/what_are_the_hardware_recommendations_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8h89j/what_are_the_hardware_recommendations_for/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753393374,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Took a little bit longer to fix some other bugs and features, but 80-90% of the way in less than an hour is wild. It's not perfect, but it doesn't have to be for my use case.  \n  \nI tried something similar in Cursor a few weeks ago with mixed results. Qwen 3 Coder is really impressive, but still has a ways to go before engineers lose their jobs. IMHO You're losing if you're not using AI for at least prototyping.\n\n",
          "author_fullname": "t2_1sivuwuvea",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Vibe Coded with Qwen 3 Coder in &lt;1 hour",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7u02i",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 81,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/vr5d47x6tqef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1734,
              "scrubber_media_url": "https://v.redd.it/vr5d47x6tqef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/vr5d47x6tqef1/DASHPlaylist.mpd?a=1756038534%2CY2IwZmFjMDMzM2ZlMmYyZjA4Njc2MWVkODY4MGMwMDUzZWRlZTJkYjFiNWVjOTY4YTAwOWMzNzg2YzY2ZmMwNg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 37,
              "hls_url": "https://v.redd.it/vr5d47x6tqef1/HLSPlaylist.m3u8?a=1756038534%2COTllMjNhOWRmOThhNDg5NzBlMDg1ZTZlM2U1N2I2Y2Q0Mzc2MzRkYjVkNjI5ZjAxNGRhM2NiZmZmYWRhOTkxMw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 81,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?width=140&amp;height=87&amp;crop=140:87,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=2c2a44560c13c6dad2e0c58423730cdfe7c2d6e3",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753328546,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Took a little bit longer to fix some other bugs and features, but 80-90% of the way in less than an hour is wild. It&amp;#39;s not perfect, but it doesn&amp;#39;t have to be for my use case.  &lt;/p&gt;\n\n&lt;p&gt;I tried something similar in Cursor a few weeks ago with mixed results. Qwen 3 Coder is really impressive, but still has a ways to go before engineers lose their jobs. IMHO You&amp;#39;re losing if you&amp;#39;re not using AI for at least prototyping.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/vr5d47x6tqef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?format=pjpg&amp;auto=webp&amp;s=65ba3a932bfd0cc8de5fd1b5eee7a99ec9876661",
                  "width": 2210,
                  "height": 1376
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d21958313a951d78751be51d12f37cda5e395dd9",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=883ced8b9e44a182d1dc356e8378f2e082569fb6",
                    "width": 216,
                    "height": 134
                  },
                  {
                    "url": "https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e57661a664a575295708f9f5cae0c691f314bd54",
                    "width": 320,
                    "height": 199
                  },
                  {
                    "url": "https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9771c0303800c5269b81b173510992c5af8f6f2b",
                    "width": 640,
                    "height": 398
                  },
                  {
                    "url": "https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=74d077225696b0732702a39b008654199a3a9983",
                    "width": 960,
                    "height": 597
                  },
                  {
                    "url": "https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ec04de6f0f8744fa4cebb54b3417fdbbba95f411",
                    "width": 1080,
                    "height": 672
                  }
                ],
                "variants": {},
                "id": "Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7u02i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ryanwang4thepeople",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7u02i/vibe_coded_with_qwen_3_coder_in_1_hour/",
          "stickied": false,
          "url": "https://v.redd.it/vr5d47x6tqef1",
          "subreddit_subscribers": 504253,
          "created_utc": 1753328546,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/vr5d47x6tqef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1734,
              "scrubber_media_url": "https://v.redd.it/vr5d47x6tqef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/vr5d47x6tqef1/DASHPlaylist.mpd?a=1756038534%2CY2IwZmFjMDMzM2ZlMmYyZjA4Njc2MWVkODY4MGMwMDUzZWRlZTJkYjFiNWVjOTY4YTAwOWMzNzg2YzY2ZmMwNg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 37,
              "hls_url": "https://v.redd.it/vr5d47x6tqef1/HLSPlaylist.m3u8?a=1756038534%2COTllMjNhOWRmOThhNDg5NzBlMDg1ZTZlM2U1N2I2Y2Q0Mzc2MzRkYjVkNjI5ZjAxNGRhM2NiZmZmYWRhOTkxMw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_58qturpl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Less than two weeks Kimi K2's release, Alibaba Qwen's new Qwen3-Coder surpasses it with half the size and double the context window. Despite a significant initial lead, open source models are catching up to closed source and seem to be reaching escape velocity.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 95,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7kkyn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 270,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 270,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/4F_QBog-_2mH7QRiv8VyzkdamiGlY40D_u3V_zWrFe8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753303228,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/krjfba3oqoef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?auto=webp&amp;s=b6cbfb5587cef2fa66062ecc89fb256764949473",
                  "width": 1512,
                  "height": 1032
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cf6d39fa1fa4f5683732a0b8993daf74e849afa",
                    "width": 108,
                    "height": 73
                  },
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=75db258338b42adc68e3bb0413ff39fa017bc706",
                    "width": 216,
                    "height": 147
                  },
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=24d9a0eca278a1c3b8db5b848d01205a811bb68d",
                    "width": 320,
                    "height": 218
                  },
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c50574d0e0fc9f8e0044c2d18d3618b1d155e4e7",
                    "width": 640,
                    "height": 436
                  },
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a748b89f2ff4d8a9a5d9acb8b0eb069410e02c88",
                    "width": 960,
                    "height": 655
                  },
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9a2fa720b512765ba2dc5a092f21cccc7eac5f8",
                    "width": 1080,
                    "height": 737
                  }
                ],
                "variants": {},
                "id": "SyAH9oAX8vUViOHksDj2yqNlqn4fwNnt93W4G27ThZw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7kkyn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "abdouhlili",
          "discussion_type": null,
          "num_comments": 71,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/",
          "stickied": false,
          "url": "https://i.redd.it/krjfba3oqoef1.jpeg",
          "subreddit_subscribers": 504253,
          "created_utc": 1753303228,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Managed to finally run Gemma3N with a 2 7900 xtx setup.\nBut it fills both cards vram about 90% \nWhy is that?\n\nSo with rocm and 7900 XTX with vLLM can mainly run only non quantized models?\n\nMy goal is to run Gemma3 27b and I am going to add 3rd card, will the model fit in parallel tensor = 3 ? \n\nIs there any Gemma3 27b models which would at least work with VLLM..",
          "author_fullname": "t2_1jk2ep8a52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "about vLLM and rocm.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ja65",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753398618,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Managed to finally run Gemma3N with a 2 7900 xtx setup.\nBut it fills both cards vram about 90% \nWhy is that?&lt;/p&gt;\n\n&lt;p&gt;So with rocm and 7900 XTX with vLLM can mainly run only non quantized models?&lt;/p&gt;\n\n&lt;p&gt;My goal is to run Gemma3 27b and I am going to add 3rd card, will the model fit in parallel tensor = 3 ? &lt;/p&gt;\n\n&lt;p&gt;Is there any Gemma3 27b models which would at least work with VLLM..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8ja65",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rich_Artist_8327",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8ja65/about_vllm_and_rocm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8ja65/about_vllm_and_rocm/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753398618,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "## üöÄ Released: 50k Rows of Tool-Use Reasoning Dataset on Huggingface!\n\nI've just published a **50,000-row dataset compilation** focused on **tool-use reasoning**, now live on Huggingface!\n\n### üß† What‚Äôs Inside?\nThis dataset covers key **BFCL scenarios** for tool-use reasoning:\n- üîß **Single-turn tool-use**\n- üîÅ **Multi-turn tool-use**\n- üß© **Multi-step tool-use**\n- üéØ **Relevance reasoning**\n\nWe've enhanced previous **Hermes function calling datasets** and other **open-source tool-use datasets**, enriching them with **reasoning traces** for deeper learning.\n---\n\n### üìÇ Dataset:\n**Hermes Tool Use Reasoning Dataset**  \nüîó [https://huggingface.co/datasets/interstellarninja/hermes_reasoning_tool_use](https://huggingface.co/datasets/interstellarninja/hermes_reasoning_tool_use)\n\n---\n\n### üõ†Ô∏è How It Was Built:\nWe used [**Nous Research's Atropos**](https://github.com/NousResearch/atropos/pull/160) to create a **multi-turn tool-use RL environment** with:\n- ‚úÖ **Turn-based &amp; trajectory-based rewards**\n- üîÑ **Rejection sampling-based SFT dataset generation**\n\nThis supports better generalization for models needing structured multi-turn reasoning.\n",
          "author_fullname": "t2_rplizde7f",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tool Use Reasoning Dataset Release on Huggingface",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7wqi3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 43,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 43,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/pE91MfApCey997Z8wuBSmaqnZMONdI17zjukCDMwaQs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753337723,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h2&gt;üöÄ Released: 50k Rows of Tool-Use Reasoning Dataset on Huggingface!&lt;/h2&gt;\n\n&lt;p&gt;I&amp;#39;ve just published a &lt;strong&gt;50,000-row dataset compilation&lt;/strong&gt; focused on &lt;strong&gt;tool-use reasoning&lt;/strong&gt;, now live on Huggingface!&lt;/p&gt;\n\n&lt;h3&gt;üß† What‚Äôs Inside?&lt;/h3&gt;\n\n&lt;p&gt;This dataset covers key &lt;strong&gt;BFCL scenarios&lt;/strong&gt; for tool-use reasoning:\n- üîß &lt;strong&gt;Single-turn tool-use&lt;/strong&gt;\n- üîÅ &lt;strong&gt;Multi-turn tool-use&lt;/strong&gt;\n- üß© &lt;strong&gt;Multi-step tool-use&lt;/strong&gt;\n- üéØ &lt;strong&gt;Relevance reasoning&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h2&gt;We&amp;#39;ve enhanced previous &lt;strong&gt;Hermes function calling datasets&lt;/strong&gt; and other &lt;strong&gt;open-source tool-use datasets&lt;/strong&gt;, enriching them with &lt;strong&gt;reasoning traces&lt;/strong&gt; for deeper learning.&lt;/h2&gt;\n\n&lt;h3&gt;üìÇ Dataset:&lt;/h3&gt;\n\n&lt;p&gt;&lt;strong&gt;Hermes Tool Use Reasoning Dataset&lt;/strong&gt;&lt;br/&gt;\nüîó &lt;a href=\"https://huggingface.co/datasets/interstellarninja/hermes_reasoning_tool_use\"&gt;https://huggingface.co/datasets/interstellarninja/hermes_reasoning_tool_use&lt;/a&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;üõ†Ô∏è How It Was Built:&lt;/h3&gt;\n\n&lt;p&gt;We used &lt;a href=\"https://github.com/NousResearch/atropos/pull/160\"&gt;&lt;strong&gt;Nous Research&amp;#39;s Atropos&lt;/strong&gt;&lt;/a&gt; to create a &lt;strong&gt;multi-turn tool-use RL environment&lt;/strong&gt; with:\n- ‚úÖ &lt;strong&gt;Turn-based &amp;amp; trajectory-based rewards&lt;/strong&gt;\n- üîÑ &lt;strong&gt;Rejection sampling-based SFT dataset generation&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;This supports better generalization for models needing structured multi-turn reasoning.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/w54k1k58lref1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/w54k1k58lref1.jpeg?auto=webp&amp;s=be19c78a9f8465852aa210ff914d7e65cc384cc3",
                  "width": 680,
                  "height": 367
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/w54k1k58lref1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7af1143f26fd8a15cb6ac700825cbbb7d15ac493",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://preview.redd.it/w54k1k58lref1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e6b1a4f4de3091cc2dc23251dbf9249125552304",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://preview.redd.it/w54k1k58lref1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=59f0aad35469a9de9f8f59ecfefc9fff526945c1",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://preview.redd.it/w54k1k58lref1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=14ab53a6d727323250320d7b6f742e07264054cb",
                    "width": 640,
                    "height": 345
                  }
                ],
                "variants": {},
                "id": "HE02NFcFk9pxE7SX50TjgIP7VdqWNbNbBKG_imCGjec"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m7wqi3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "interstellar-ninja",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7wqi3/tool_use_reasoning_dataset_release_on_huggingface/",
          "stickied": false,
          "url": "https://i.redd.it/w54k1k58lref1.jpeg",
          "subreddit_subscribers": 504253,
          "created_utc": 1753337723,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI‚Äôm currently experimenting with UnifyAI on Android and trying to get a local LLM (specifically Phi-3.5 Mini) up and running smoothly. I‚Äôve got the app running and I‚Äôm at the stage where I can manually add AI systems (LOCAL_LLM), but I‚Äôm hitting a wall when it comes to:\n\n1. Setting up the local model path and ensuring it connects properly.\n\nI‚Äôve downloaded the Phi-3.5 Mini model files (config, tokenizer, etc.) and placed them in what should be the correct directory. However, I‚Äôm not sure if I‚Äôm referencing the path properly in the app, or if additional config is needed.\n\n2. Understanding how the app routes tasks to each model.\n\nThe UI allows you to define priority, tasks, and endpoints ‚Äî but there‚Äôs limited documentation on what exactly is required or supported for LOCAL_LLM types.\n\n3. Polishing and customizing the UI.\n\nI‚Äôd love to clean up the interface or create a more focused layout for single-model use. Is there a way to tweak the frontend via config or external files?\n\n\nIf anyone has experience with UnifyAI ‚Äî either the Android version or a similar setup ‚Äî I‚Äôd love to hear how you structured your model paths, what config JSON settings (if any) you used, or how you approached task routing. Bonus points if you‚Äôve done any visual or UX customization inside the app.\n\nThanks in advance ‚Äî happy to share more screenshots or logs if helpful!",
          "author_fullname": "t2_1sjif6sg6y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help with UnifyAI ‚Äì Setting Up Local LLMs and UI Integration",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8mdbz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753407213,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I‚Äôm currently experimenting with UnifyAI on Android and trying to get a local LLM (specifically Phi-3.5 Mini) up and running smoothly. I‚Äôve got the app running and I‚Äôm at the stage where I can manually add AI systems (LOCAL_LLM), but I‚Äôm hitting a wall when it comes to:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Setting up the local model path and ensuring it connects properly.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I‚Äôve downloaded the Phi-3.5 Mini model files (config, tokenizer, etc.) and placed them in what should be the correct directory. However, I‚Äôm not sure if I‚Äôm referencing the path properly in the app, or if additional config is needed.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Understanding how the app routes tasks to each model.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The UI allows you to define priority, tasks, and endpoints ‚Äî but there‚Äôs limited documentation on what exactly is required or supported for LOCAL_LLM types.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Polishing and customizing the UI.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I‚Äôd love to clean up the interface or create a more focused layout for single-model use. Is there a way to tweak the frontend via config or external files?&lt;/p&gt;\n\n&lt;p&gt;If anyone has experience with UnifyAI ‚Äî either the Android version or a similar setup ‚Äî I‚Äôd love to hear how you structured your model paths, what config JSON settings (if any) you used, or how you approached task routing. Bonus points if you‚Äôve done any visual or UX customization inside the app.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance ‚Äî happy to share more screenshots or logs if helpful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8mdbz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IgnisIason",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8mdbz/help_with_unifyai_setting_up_local_llms_and_ui/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8mdbz/help_with_unifyai_setting_up_local_llms_and_ui/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753407213,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am using BackyardAI.\n\nWhen I first got into this I grabbed a lot of gguf files from HuggingFace.\n\nI am trying to see if there are updates to all the gguf files I have\n\nIs there an easy way t do this?  Is there a program that can do this for me?\n\nThanks",
          "author_fullname": "t2_lzwh7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Newb] Need help with gguf files",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ltgv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753405607,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using BackyardAI.&lt;/p&gt;\n\n&lt;p&gt;When I first got into this I grabbed a lot of gguf files from HuggingFace.&lt;/p&gt;\n\n&lt;p&gt;I am trying to see if there are updates to all the gguf files I have&lt;/p&gt;\n\n&lt;p&gt;Is there an easy way t do this?  Is there a program that can do this for me?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8ltgv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cmdrmcgarrett",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8ltgv/newb_need_help_with_gguf_files/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8ltgv/newb_need_help_with_gguf_files/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753405607,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "recently built an Al tool called NexNotes Al, this Al tool can generate multiple things just from a single PPT, PDF,DOC, image or even an article- like 5 Al tools combined in a single tool. Here's what it does - Generate TimeTables from content (new) Generate ppts from prompts (customizable)\n\nGenerate mind maps\n\nGenerate flashcards\n\nGenerate Diagrams (customizable, flowcharts, entity relationship, etc.!)\n\nGenerate clear and concise summary\n\nGenerate Ouizzes\n\nAnswer your questions that you provide it\n\nEVEN HUMANIZE AI-WRITTEN CONTENT\n\nYOU CAN EVEN CONVERT TEXT INTO HANDWRITING! FOR LAZY ASSIGNMENTS.\n\nand the twist - ITS COMPLETELY FREE, JUST SIGN IN AND BOOM!\n\nalready 10k+ users are using it, I launched it 3 wks ago.\n\nmake sure to try it out as it increases your productivity 10x.\nHeres the link- [NexNotesAI ](https://nexnotes-ai.pages.dev ) \n\n",
          "author_fullname": "t2_1u0ahr9uc5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New] added a feature for generating study plans and timetables from your content",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8lmby",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753405040,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "nexnotes-ai.pages.dev",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;recently built an Al tool called NexNotes Al, this Al tool can generate multiple things just from a single PPT, PDF,DOC, image or even an article- like 5 Al tools combined in a single tool. Here&amp;#39;s what it does - Generate TimeTables from content (new) Generate ppts from prompts (customizable)&lt;/p&gt;\n\n&lt;p&gt;Generate mind maps&lt;/p&gt;\n\n&lt;p&gt;Generate flashcards&lt;/p&gt;\n\n&lt;p&gt;Generate Diagrams (customizable, flowcharts, entity relationship, etc.!)&lt;/p&gt;\n\n&lt;p&gt;Generate clear and concise summary&lt;/p&gt;\n\n&lt;p&gt;Generate Ouizzes&lt;/p&gt;\n\n&lt;p&gt;Answer your questions that you provide it&lt;/p&gt;\n\n&lt;p&gt;EVEN HUMANIZE AI-WRITTEN CONTENT&lt;/p&gt;\n\n&lt;p&gt;YOU CAN EVEN CONVERT TEXT INTO HANDWRITING! FOR LAZY ASSIGNMENTS.&lt;/p&gt;\n\n&lt;p&gt;and the twist - ITS COMPLETELY FREE, JUST SIGN IN AND BOOM!&lt;/p&gt;\n\n&lt;p&gt;already 10k+ users are using it, I launched it 3 wks ago.&lt;/p&gt;\n\n&lt;p&gt;make sure to try it out as it increases your productivity 10x.\nHeres the link- &lt;a href=\"https://nexnotes-ai.pages.dev\"&gt;NexNotesAI &lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://nexnotes-ai.pages.dev",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m8lmby",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Not_your_average_dev",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8lmby/new_added_a_feature_for_generating_study_plans/",
          "stickied": false,
          "url": "https://nexnotes-ai.pages.dev",
          "subreddit_subscribers": 504253,
          "created_utc": 1753405040,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm seeing a trend in recent advancements in open source models, they're getting big. DeepSeek V3 (670B), Kimi K2 (1T), and now Qwen3 Coder (480B).. I'm starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware. If the scaling laws continue to hold true (which I would bet on) then this problem will just get worse over time. Is there any hope for us?",
          "author_fullname": "t2_e11po",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a future for local models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7o3u8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 117,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 117,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753311706,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m seeing a trend in recent advancements in open source models, they&amp;#39;re getting big. DeepSeek V3 (670B), Kimi K2 (1T), and now Qwen3 Coder (480B).. I&amp;#39;m starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware. If the scaling laws continue to hold true (which I would bet on) then this problem will just get worse over time. Is there any hope for us?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7o3u8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ASTRdeca",
          "discussion_type": null,
          "num_comments": 122,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753311706,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to do work with longer texts using local models (think going through an entire book with each sentence being it's own chat request/response).   \nI've been using LM Studio and Ollama for awhile now.   \nAnd more recently I've been building agents (for working with my Obsidian notes primarily) using PydanticAI.   \nBut I find myself wanting to experiment with long running agents and, knowing that I'm not that original or creative, wanted to hear about what you've been doing to make this work. \n\nWhat is your process?",
          "author_fullname": "t2_1azds5ogui",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do you have a batch/background LLM task processing setup working locally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8cn00",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753382583,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to do work with longer texts using local models (think going through an entire book with each sentence being it&amp;#39;s own chat request/response).&lt;br/&gt;\nI&amp;#39;ve been using LM Studio and Ollama for awhile now.&lt;br/&gt;\nAnd more recently I&amp;#39;ve been building agents (for working with my Obsidian notes primarily) using PydanticAI.&lt;br/&gt;\nBut I find myself wanting to experiment with long running agents and, knowing that I&amp;#39;m not that original or creative, wanted to hear about what you&amp;#39;ve been doing to make this work. &lt;/p&gt;\n\n&lt;p&gt;What is your process?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8cn00",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "This_Conclusion9402",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8cn00/do_you_have_a_batchbackground_llm_task_processing/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8cn00/do_you_have_a_batchbackground_llm_task_processing/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753382583,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Google DeepMind's new paper explore a new advanced Transformers architecture for LLMs called Mixture-of-Recursions which uses recursive Transformers with dynamic recursion per token. Check visual explanation details : https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR",
          "author_fullname": "t2_th2ct5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Google DeepMind release Mixture-of-Recursions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7fwhl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 295,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 295,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753292638,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google DeepMind&amp;#39;s new paper explore a new advanced Transformers architecture for LLMs called Mixture-of-Recursions which uses recursive Transformers with dynamic recursion per token. Check visual explanation details : &lt;a href=\"https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR\"&gt;https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QmbZwjHL_nSls3hlAww-zDS-HWSbRw7J2Tj08JUnDak.jpeg?auto=webp&amp;s=5d63020d7a90f3dd9933e344b8670cb78b0b5165",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QmbZwjHL_nSls3hlAww-zDS-HWSbRw7J2Tj08JUnDak.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=46cf0a3ba4ca4557db533db7facf3345d193ff14",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/QmbZwjHL_nSls3hlAww-zDS-HWSbRw7J2Tj08JUnDak.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e3bcb7fc50ee4b735515cf0df1fa150704579262",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/QmbZwjHL_nSls3hlAww-zDS-HWSbRw7J2Tj08JUnDak.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc83119edf707dba6c55fc32d3a9910075ea589d",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "QmbZwjHL_nSls3hlAww-zDS-HWSbRw7J2Tj08JUnDak"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m7fwhl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Technical-Love-8479",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753292638,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_149me6kcw0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Vibe Coding Anonymous - Satirical take on Vibe Coding",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7yswh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/vui02yr68sef1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/vui02yr68sef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/vui02yr68sef1/DASHPlaylist.mpd?a=1756038534%2CN2I2ZWQ3NmQzZjNhMWViMDc5OWQ4MWMwN2JmNzFiODAyZDlhNDIyMjQxYmJjOGQ0ZmYyYTk0ZDAyMzIyMzA5Ng%3D%3D&amp;v=1&amp;f=sd",
              "duration": 47,
              "hls_url": "https://v.redd.it/vui02yr68sef1/HLSPlaylist.m3u8?a=1756038534%2CMmY0ZDc2ZDU1ZGVjYjU4MzIzMGYzMThjOTkyOWU5MDlhMGMzMzMyNWIwMzZiYTExMmY0MDVkMjM2OThlOTM3Mg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=eb302d10a43e5ed9f3f13f72835828798edcd526",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753345471,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/vui02yr68sef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?format=pjpg&amp;auto=webp&amp;s=0316973f6962793f7e17b99bf8e4d32736419376",
                  "width": 1280,
                  "height": 720
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5a246dbeb7e874ae1950ffd4235e112ddd3bb375",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d3fce9bdc5711838fa1bc432f2b941ee2361760d",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b887178b1a32e12e9d5f8fa2a011e6f833196a95",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=72db31e9fd18c2f4a8282b912ae4912241f7db27",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0e25e095502c3c32920db241d40d72fc50ce4c4b",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=06a27abaf1ee855457a3c671a998e4df15fc172f",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m7yswh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sad_Bandicoot_6925",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7yswh/vibe_coding_anonymous_satirical_take_on_vibe/",
          "stickied": false,
          "url": "https://v.redd.it/vui02yr68sef1",
          "subreddit_subscribers": 504253,
          "created_utc": 1753345471,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/vui02yr68sef1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/vui02yr68sef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/vui02yr68sef1/DASHPlaylist.mpd?a=1756038534%2CN2I2ZWQ3NmQzZjNhMWViMDc5OWQ4MWMwN2JmNzFiODAyZDlhNDIyMjQxYmJjOGQ0ZmYyYTk0ZDAyMzIyMzA5Ng%3D%3D&amp;v=1&amp;f=sd",
              "duration": 47,
              "hls_url": "https://v.redd.it/vui02yr68sef1/HLSPlaylist.m3u8?a=1756038534%2CMmY0ZDc2ZDU1ZGVjYjU4MzIzMGYzMThjOTkyOWU5MDlhMGMzMzMyNWIwMzZiYTExMmY0MDVkMjM2OThlOTM3Mg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm working on a project (multi label ad classification) and I'm trying to finetune a (monolingual) Bert. The problem I face is reproducibility, even though I m using exactly the same hyperparameters , same dataset split , I have over 0.15 accuracy deviation. Any help/insight?\nI have already achieved a pretty good (0.85) accuracy .",
          "author_fullname": "t2_4x1ndhha",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help with Bert fine-tuning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m894mz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753374674,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a project (multi label ad classification) and I&amp;#39;m trying to finetune a (monolingual) Bert. The problem I face is reproducibility, even though I m using exactly the same hyperparameters , same dataset split , I have over 0.15 accuracy deviation. Any help/insight?\nI have already achieved a pretty good (0.85) accuracy .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m894mz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Alanuhoo",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m894mz/help_with_bert_finetuning/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m894mz/help_with_bert_finetuning/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753374674,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If you are like me, you are probably tired of the rote pedaling to the model selector drop down to pick a model, prompt that model and repeat that cycle over and over again. Well I wanted to solve this pesky problem for myself, so I figured i vibe code an extension, make it open source and share it with you all \n\nRouteGPT is a Chrome extension for ChatGPT plus users that automatically selects the right OpenAI model for your prompt based on preferences that you define.\n\nFor example:\n\n1. ‚Äúcreative novel writing, story ideas, imaginative prose‚Äù ‚Üí GPT-4o2.\n2. ‚Äúcritical analysis, deep insights, and market research ‚Äù ‚Üí o3.\n3. etc\n\nInstead of switching models manually, RouteGPT handlesit for you via a¬†[local 1.5B LLM running via ollama](https://huggingface.co/katanemo/Arch-Router-1.5B). The extension is available¬†[here](https://chromewebstore.google.com/detail/routegpt/cbnfoohelfohplngdocidckbnbamghbf)¬†Give it a try, leave me feedback - its absolutely free.\n\nP.S all the code can be found¬†[here](https://github.com/katanemo/archgw/tree/main/demos/use_cases/chatgpt-preference-model-selector), and if you want to build this type of experience for your users who might be interacting with different models in your LLM-based applications, check out this[¬†open source](https://github.com/katanemo/archgw)  \nproject that offers APIs and hooks to make this easy for you.\n\nUpvote2Downvote0Go to comments  \n",
          "author_fullname": "t2_gwq7fd01b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Vibe coding RouteGPT - a chrome extension aligns model routing to my preferences, powered by a small but powerful LLM.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 120,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8k5x0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/4tvn7jztswef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1258,
              "scrubber_media_url": "https://v.redd.it/4tvn7jztswef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/4tvn7jztswef1/DASHPlaylist.mpd?a=1756038534%2CZDY2NTVmM2JmMDdjOWY1MWY1NzUyNWIyMGFjNmIwYmFmZTE2ZGMxOGI1ZjUwNjFlZWJmYTYyOTlhZTJmODdkYw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 43,
              "hls_url": "https://v.redd.it/4tvn7jztswef1/HLSPlaylist.m3u8?a=1756038534%2CNTJiODU3YmQxMmY1ZTg2YmQzYTBkOWU0YmQwMGE1ZTY2ZmIzZDMwMjA5ZDA1ZGU1YmE3NmY0OTk4ZDg2N2QyMQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF.png?width=140&amp;height=120&amp;crop=140:120,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=40b0ca2a161a0ef5ac45f36be7f7a6469f83c605",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753400961,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you are like me, you are probably tired of the rote pedaling to the model selector drop down to pick a model, prompt that model and repeat that cycle over and over again. Well I wanted to solve this pesky problem for myself, so I figured i vibe code an extension, make it open source and share it with you all &lt;/p&gt;\n\n&lt;p&gt;RouteGPT is a Chrome extension for ChatGPT plus users that automatically selects the right OpenAI model for your prompt based on preferences that you define.&lt;/p&gt;\n\n&lt;p&gt;For example:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;‚Äúcreative novel writing, story ideas, imaginative prose‚Äù ‚Üí GPT-4o2.&lt;/li&gt;\n&lt;li&gt;‚Äúcritical analysis, deep insights, and market research ‚Äù ‚Üí o3.&lt;/li&gt;\n&lt;li&gt;etc&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Instead of switching models manually, RouteGPT handlesit for you via a¬†&lt;a href=\"https://huggingface.co/katanemo/Arch-Router-1.5B\"&gt;local 1.5B LLM running via ollama&lt;/a&gt;. The extension is available¬†&lt;a href=\"https://chromewebstore.google.com/detail/routegpt/cbnfoohelfohplngdocidckbnbamghbf\"&gt;here&lt;/a&gt;¬†Give it a try, leave me feedback - its absolutely free.&lt;/p&gt;\n\n&lt;p&gt;P.S all the code can be found¬†&lt;a href=\"https://github.com/katanemo/archgw/tree/main/demos/use_cases/chatgpt-preference-model-selector\"&gt;here&lt;/a&gt;, and if you want to build this type of experience for your users who might be interacting with different models in your LLM-based applications, check out this&lt;a href=\"https://github.com/katanemo/archgw\"&gt;¬†open source&lt;/a&gt;&lt;br/&gt;\nproject that offers APIs and hooks to make this easy for you.&lt;/p&gt;\n\n&lt;p&gt;Upvote2Downvote0Go to comments  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/4tvn7jztswef1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF.png?format=pjpg&amp;auto=webp&amp;s=097054efeaa8b7ba8d1eafd7a4c64364974da93b",
                  "width": 2516,
                  "height": 2160
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=84a67d17e44997f7ff0dbb6dc6ac5ca984d92a60",
                    "width": 108,
                    "height": 92
                  },
                  {
                    "url": "https://external-preview.redd.it/NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=73ef94780a5c381c3ef2025995091ba0aedfb9e6",
                    "width": 216,
                    "height": 185
                  },
                  {
                    "url": "https://external-preview.redd.it/NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e9a705111cfd138bbd97092dbe94273b0a534618",
                    "width": 320,
                    "height": 274
                  },
                  {
                    "url": "https://external-preview.redd.it/NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=29d7494ffb69560fb1cd33c58aadb1af6856472c",
                    "width": 640,
                    "height": 549
                  },
                  {
                    "url": "https://external-preview.redd.it/NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7955dc91c8bf4ed6b142bdf33fdf660086f75647",
                    "width": 960,
                    "height": 824
                  },
                  {
                    "url": "https://external-preview.redd.it/NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=12ce93913e07651650050bc8f528a7042d210cc2",
                    "width": 1080,
                    "height": 927
                  }
                ],
                "variants": {},
                "id": "NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8k5x0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AdditionalWeb107",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8k5x0/vibe_coding_routegpt_a_chrome_extension_aligns/",
          "stickied": false,
          "url": "https://v.redd.it/4tvn7jztswef1",
          "subreddit_subscribers": 504253,
          "created_utc": 1753400961,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/4tvn7jztswef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1258,
              "scrubber_media_url": "https://v.redd.it/4tvn7jztswef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/4tvn7jztswef1/DASHPlaylist.mpd?a=1756038534%2CZDY2NTVmM2JmMDdjOWY1MWY1NzUyNWIyMGFjNmIwYmFmZTE2ZGMxOGI1ZjUwNjFlZWJmYTYyOTlhZTJmODdkYw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 43,
              "hls_url": "https://v.redd.it/4tvn7jztswef1/HLSPlaylist.m3u8?a=1756038534%2CNTJiODU3YmQxMmY1ZTg2YmQzYTBkOWU0YmQwMGE1ZTY2ZmIzZDMwMjA5ZDA1ZGU1YmE3NmY0OTk4ZDg2N2QyMQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nAccording to ArtificialAnalysis data (from their hardware benchmarks, like at¬†[https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1](https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1)), the performance difference between NVIDIA's 8x H200 and 8x B200 systems seems minimal, especially in concurrent load scaling for models like DeepSeek R1 or Llama 3.3 70B. For instance, token processing speeds don't show a huge gap despite B200's superior specs on paper.\n\nIs this due to specific benchmark conditions, like focusing on multi-GPU scaling or model dependencies, or could it be something else like optimization levels? Has anyone seen similar results in other tests, or is this just an artifact of their methodology? I'd love to hear your thoughts or any insights from real-world usage!\n\nThanks!",
          "author_fullname": "t2_93zqvlmj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why is B200 performing similarly to H200? (ArtificialAnalysis)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7ypyb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753345144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;According to ArtificialAnalysis data (from their hardware benchmarks, like at¬†&lt;a href=\"https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1\"&gt;https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1&lt;/a&gt;), the performance difference between NVIDIA&amp;#39;s 8x H200 and 8x B200 systems seems minimal, especially in concurrent load scaling for models like DeepSeek R1 or Llama 3.3 70B. For instance, token processing speeds don&amp;#39;t show a huge gap despite B200&amp;#39;s superior specs on paper.&lt;/p&gt;\n\n&lt;p&gt;Is this due to specific benchmark conditions, like focusing on multi-GPU scaling or model dependencies, or could it be something else like optimization levels? Has anyone seen similar results in other tests, or is this just an artifact of their methodology? I&amp;#39;d love to hear your thoughts or any insights from real-world usage!&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?auto=webp&amp;s=efc17c9f241b4403d22cbacfe5d71900ee1cf85a",
                  "width": 1260,
                  "height": 700
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=700f91dbca11e5a7030b915550ae877ef725a0d4",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b97954336b79c1390848d0e44fa056a85de68672",
                    "width": 216,
                    "height": 120
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65f53b80ab9674ee645013e3e8eeac4f953d657e",
                    "width": 320,
                    "height": 177
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc",
                    "width": 640,
                    "height": 355
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f4359d47b78f5c1aa35de8804dbe36a749fc11a",
                    "width": 960,
                    "height": 533
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62eb4b7216f41af6600fc4df79cfa67425c19442",
                    "width": 1080,
                    "height": 600
                  }
                ],
                "variants": {},
                "id": "RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7ypyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Cyp9715",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753345144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We're working on our open-source game engine plugins over at Aviad, and have been learning a lot and exploring through making games. I'd love to get feedback on our latest game project Bard Battle, which we hope to use as a small platform for testing out new mechanics and interaction ideas with small language models as the backend.\n\nYou can follow our plugin development for LLM usage in Unity here:\n\n\\[aviad-ai/unity: A package to simplify integration of language models into Unity.\\](https://github.com/aviad-ai/unity)",
          "author_fullname": "t2_6dw4rgpn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Check out our game in development for Local LLM mechanics!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8jrzg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/S8Q7S9rtQ_M?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Bard Battle Development Preview - Local LLM with Aviad\"&gt;&lt;/iframe&gt;",
            "width": 267,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Bard Battle Development Preview - Local LLM with Aviad",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 267,
              "html": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/S8Q7S9rtQ_M?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Bard Battle Development Preview - Local LLM with Aviad\"&gt;&lt;/iframe&gt;",
              "author_name": "Alexander James L",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/S8Q7S9rtQ_M/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@alexanderjamesl4868"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/S8Q7S9rtQ_M?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Bard Battle Development Preview - Local LLM with Aviad\"&gt;&lt;/iframe&gt;",
            "width": 267,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m8jrzg",
            "height": 200
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/974AxJgllpMHDozOWYHveTyuVX5-gzTVNkBOkHGWcrk.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=ee11dd6da69f8ef315d9f465cd0b64746bb0971d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753399901,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re working on our open-source game engine plugins over at Aviad, and have been learning a lot and exploring through making games. I&amp;#39;d love to get feedback on our latest game project Bard Battle, which we hope to use as a small platform for testing out new mechanics and interaction ideas with small language models as the backend.&lt;/p&gt;\n\n&lt;p&gt;You can follow our plugin development for LLM usage in Unity here:&lt;/p&gt;\n\n&lt;p&gt;[aviad-ai/unity: A package to simplify integration of language models into Unity.](&lt;a href=\"https://github.com/aviad-ai/unity\"&gt;https://github.com/aviad-ai/unity&lt;/a&gt;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/S8Q7S9rtQ_M?si=kFX9GaSuuO9CUma7",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/974AxJgllpMHDozOWYHveTyuVX5-gzTVNkBOkHGWcrk.jpeg?auto=webp&amp;s=39ca3cecf4539e00a9c60b9ff1c879db18c024e4",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/974AxJgllpMHDozOWYHveTyuVX5-gzTVNkBOkHGWcrk.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2935f84b5cc3714ad88c1b0b612c6feb872ed1fc",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/974AxJgllpMHDozOWYHveTyuVX5-gzTVNkBOkHGWcrk.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b13ea77d74e99c79a7a9bbff7bd04d6de79e41cb",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/974AxJgllpMHDozOWYHveTyuVX5-gzTVNkBOkHGWcrk.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8b33e50417371cdc4df9498fa2cacf3571a5d49e",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "974AxJgllpMHDozOWYHveTyuVX5-gzTVNkBOkHGWcrk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8jrzg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OtherwiseAd4411",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8jrzg/check_out_our_game_in_development_for_local_llm/",
          "stickied": false,
          "url": "https://youtu.be/S8Q7S9rtQ_M?si=kFX9GaSuuO9CUma7",
          "subreddit_subscribers": 504253,
          "created_utc": 1753399901,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Bard Battle Development Preview - Local LLM with Aviad",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 267,
              "html": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/S8Q7S9rtQ_M?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Bard Battle Development Preview - Local LLM with Aviad\"&gt;&lt;/iframe&gt;",
              "author_name": "Alexander James L",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/S8Q7S9rtQ_M/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@alexanderjamesl4868"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So from what it seems like, deepseek r1 0528 is the best large model for completely uncensored, unmoderated chats. With that in mind, I want to understand how or if it even makes sense to \"guide\" the thinking of the model(this could obviously apply to other thinking models)\n\n\"Normally\" one can just ask a user question, and the model usually generates a pretty decent thinking process. This however seems to sometimes (and with specific queries, always) miss key points. \"Guided\" thinking can imo be either both of the following:\n1. A specific persona adopted ie. \"Financial analyst\"\n2. A step by step thinking guide ie. First do this, then do this etc. (Or even branching off depending on earlier reasoning)\n\nThe question I have / discussion I want to start: how do we  make sure deepseek consistently follows these instructions on it's thinking process? Many times I find that if I give a detailed guide in the system prompt, by the 4th round of chat, it already forgets it. When I put the reasoning guide in with the user query, I often get the thinking process repeated outside the thinking process, leading to a higher compute cost and overall response time.\n\nI've tried searching up info, no luck.\n\n\nSo does anyone have any tips? Does anyone think it may actually be detrimental?\n\nMy use-case is a pretty shoddy attempt at a Text Adventure game, but that isn't extremely relevant.",
          "author_fullname": "t2_rlhobztpn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Guiding thinking",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8jgrl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753399089,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So from what it seems like, deepseek r1 0528 is the best large model for completely uncensored, unmoderated chats. With that in mind, I want to understand how or if it even makes sense to &amp;quot;guide&amp;quot; the thinking of the model(this could obviously apply to other thinking models)&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Normally&amp;quot; one can just ask a user question, and the model usually generates a pretty decent thinking process. This however seems to sometimes (and with specific queries, always) miss key points. &amp;quot;Guided&amp;quot; thinking can imo be either both of the following:\n1. A specific persona adopted ie. &amp;quot;Financial analyst&amp;quot;\n2. A step by step thinking guide ie. First do this, then do this etc. (Or even branching off depending on earlier reasoning)&lt;/p&gt;\n\n&lt;p&gt;The question I have / discussion I want to start: how do we  make sure deepseek consistently follows these instructions on it&amp;#39;s thinking process? Many times I find that if I give a detailed guide in the system prompt, by the 4th round of chat, it already forgets it. When I put the reasoning guide in with the user query, I often get the thinking process repeated outside the thinking process, leading to a higher compute cost and overall response time.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried searching up info, no luck.&lt;/p&gt;\n\n&lt;p&gt;So does anyone have any tips? Does anyone think it may actually be detrimental?&lt;/p&gt;\n\n&lt;p&gt;My use-case is a pretty shoddy attempt at a Text Adventure game, but that isn&amp;#39;t extremely relevant.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8jgrl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Federal_Order4324",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8jgrl/guiding_thinking/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8jgrl/guiding_thinking/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753399089,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Currently running with 20GB VRAM in my current build (RTX 4000 Ada SFF) and it's not feasible to upgrade since it's my travel setup (3L in volume).\n\nI've been wanting to run larger models, but I'm intimidated by these massive systems people post here, but now with my recent bonus, I can finally afford a better build.\n\nMostly interested in image/video gen and RAG.\n\nI'm split between the RTX Pro 6000 and Mac 512GB, are there other options aside from those? Multiple Frameworks?\n\nAdditionally, I have a spare RTX 4000 Ada that I'm not currently using.\n\nAny advice would be welcome and appreciated.\n\nEDIT: Thanks all for the recommendations, for the sake of simplicity and flexibility, I decided to snag a RTX Pro 6000. Between my use case, upgradability, and power usage, it makes sense to go with a single GPU where I can branch out from there. Appreciate the help.",
          "author_fullname": "t2_c2n1h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "$10000 budget, what's the right route?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8j842",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "03eba0e8-72f2-11ee-96eb-9a14648159ce",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753428959,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "koboldcpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753398470,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently running with 20GB VRAM in my current build (RTX 4000 Ada SFF) and it&amp;#39;s not feasible to upgrade since it&amp;#39;s my travel setup (3L in volume).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been wanting to run larger models, but I&amp;#39;m intimidated by these massive systems people post here, but now with my recent bonus, I can finally afford a better build.&lt;/p&gt;\n\n&lt;p&gt;Mostly interested in image/video gen and RAG.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m split between the RTX Pro 6000 and Mac 512GB, are there other options aside from those? Multiple Frameworks?&lt;/p&gt;\n\n&lt;p&gt;Additionally, I have a spare RTX 4000 Ada that I&amp;#39;m not currently using.&lt;/p&gt;\n\n&lt;p&gt;Any advice would be welcome and appreciated.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Thanks all for the recommendations, for the sake of simplicity and flexibility, I decided to snag a RTX Pro 6000. Between my use case, upgradability, and power usage, it makes sense to go with a single GPU where I can branch out from there. Appreciate the help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "koboldcpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8j842",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Commander_",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m8j842/10000_budget_whats_the_right_route/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8j842/10000_budget_whats_the_right_route/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753398470,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "aka MythoMax-L2-13B-Unfiltered-ErebusBlend-v2.gguf",
          "author_fullname": "t2_1sspl6gqmt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I want the ErebusBlend v2. The one that doesn‚Äôt blink. The one that whispers back.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8t17l",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.14,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753429249,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;aka MythoMax-L2-13B-Unfiltered-ErebusBlend-v2.gguf&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8t17l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dazzling_Tailor_891",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8t17l/i_want_the_erebusblend_v2_the_one_that_doesnt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8t17l/i_want_the_erebusblend_v2_the_one_that_doesnt/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753429249,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I guess you could say I'm a fan of Local Llama. I decide I've had it writing code, time to use one of the new CLI Coding Agents. \n\nDownload anon-kode, it throws a ton of errors- you gotta hit xyz API you're out of tokens - and that's not something I can fix. So I install Claude Code, point it at anon-kode, and tell it to fix it so that I can run it off Ollama. Two hours later, Claude tells me it's good to go and I'm able to successfully use a locally hosted AI model to talk to in the CLI.\n\nDuring that two hours, bored, pressing \"approve\" whenever Claude Code asked me without even reading what it was asking permission to do, I see that Qwen 3 Coder has released and it's basically just Gemini CLI but \"qwen\" replacing the words \"gemini\" in a good 60% of all the places it's supposed to. \n\nDownload that, point it at my Ollama server. 5 minutes later I'm able to talk to the AI and ask it to do some basic setup stuff.\n\n\"I'm sorry Dave, I can't do that\". \n\nSame exact thing with Anon-Kode. These CLI agents that exist specifically to write code because I'm not smart enough to do it apparently can't do the one thing they exist to do.\n\nAnon-Code is literally just Claude Code. They didn't even bother replacing mentions of Claude Code in the UI or in the backend. Qwen is just Gemini, if you ask it what tools it has access to, it just shows \"Gemini Tools\". These things are supposed to work and are based off things that do work. What am I doing wrong? It won't execute code no matter what I try, and I have tried a ton of things:\n\n\\- Tell it to check what tools it has, tell it to use those specific tools  \n\\- YOLO mode in Qwen  \n\\- Start off demanding it actually do code  \n\\- ALL CAPS  \n\\- Switching out model after model after model, all listed to support coding tools  \n\\- Looked around for config files to turn it from \"off\" to \"on\"  \n\\- With Aider and Continue, I was using LM Studio instead of Ollama and I couldn't get those to work either\n\nI got Claude Code running in maybe 30 seconds this is not a general inability to use a product intended for the mass market. What am I missing that hundreds of thousands of people easily figured out?\n\nhttps://preview.redd.it/5674jetvquef1.png?width=1799&amp;format=png&amp;auto=webp&amp;s=23f1623e8b0b00867b12dbda611ba3aaa1c8ca6a\n\nhttps://preview.redd.it/cvnydetvquef1.png?width=1326&amp;format=png&amp;auto=webp&amp;s=47ddb460596e0b1fd18155d736eb976683b3f420\n\n",
          "author_fullname": "t2_jkslu7in5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Seriously, how do you get CLI Coding Agents etc to work?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "5674jetvquef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 63,
                  "x": 108,
                  "u": "https://preview.redd.it/5674jetvquef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ce3cf6cc70042e10ac7382a8bc64917e1e86369"
                },
                {
                  "y": 127,
                  "x": 216,
                  "u": "https://preview.redd.it/5674jetvquef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6bcfb8104b79dc972ecb56a90c5f8c5c3b1558ed"
                },
                {
                  "y": 188,
                  "x": 320,
                  "u": "https://preview.redd.it/5674jetvquef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=38b67e788cab58c2267e903b78ea43f146564b76"
                },
                {
                  "y": 377,
                  "x": 640,
                  "u": "https://preview.redd.it/5674jetvquef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e80b05fee373a3831c35c1654b2e63898a8c84c"
                },
                {
                  "y": 565,
                  "x": 960,
                  "u": "https://preview.redd.it/5674jetvquef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e45c1d4c930a3726f6fd922aef1797299d0fb7d3"
                },
                {
                  "y": 636,
                  "x": 1080,
                  "u": "https://preview.redd.it/5674jetvquef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=22661057707d99e919f698fe6c1eeaafa0271b26"
                }
              ],
              "s": {
                "y": 1060,
                "x": 1799,
                "u": "https://preview.redd.it/5674jetvquef1.png?width=1799&amp;format=png&amp;auto=webp&amp;s=23f1623e8b0b00867b12dbda611ba3aaa1c8ca6a"
              },
              "id": "5674jetvquef1"
            },
            "cvnydetvquef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 122,
                  "x": 108,
                  "u": "https://preview.redd.it/cvnydetvquef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cf57aa36bd7b1f3ea27a1ac89230859ec5f85f1"
                },
                {
                  "y": 244,
                  "x": 216,
                  "u": "https://preview.redd.it/cvnydetvquef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d59dfe89b51130bb9a95533e39b22877595a361"
                },
                {
                  "y": 362,
                  "x": 320,
                  "u": "https://preview.redd.it/cvnydetvquef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a2c3bf935de0eaba37f875137f56070a94a04ec9"
                },
                {
                  "y": 725,
                  "x": 640,
                  "u": "https://preview.redd.it/cvnydetvquef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=aa93b4e7b8cd266ff2f35ee4a863fffc0ef3f17e"
                },
                {
                  "y": 1088,
                  "x": 960,
                  "u": "https://preview.redd.it/cvnydetvquef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=300a5c88b6de390b1ffb146128dea0133d204392"
                },
                {
                  "y": 1224,
                  "x": 1080,
                  "u": "https://preview.redd.it/cvnydetvquef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=744a228521cd0a9e987d7231f29dc5c318693433"
                }
              ],
              "s": {
                "y": 1503,
                "x": 1326,
                "u": "https://preview.redd.it/cvnydetvquef1.png?width=1326&amp;format=png&amp;auto=webp&amp;s=47ddb460596e0b1fd18155d736eb976683b3f420"
              },
              "id": "cvnydetvquef1"
            }
          },
          "name": "t3_1m89s6y",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/LR97FcIY5mrnVPnzjPQyDpmV0_J8RkPuP80rm0c_fGc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753376158,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I guess you could say I&amp;#39;m a fan of Local Llama. I decide I&amp;#39;ve had it writing code, time to use one of the new CLI Coding Agents. &lt;/p&gt;\n\n&lt;p&gt;Download anon-kode, it throws a ton of errors- you gotta hit xyz API you&amp;#39;re out of tokens - and that&amp;#39;s not something I can fix. So I install Claude Code, point it at anon-kode, and tell it to fix it so that I can run it off Ollama. Two hours later, Claude tells me it&amp;#39;s good to go and I&amp;#39;m able to successfully use a locally hosted AI model to talk to in the CLI.&lt;/p&gt;\n\n&lt;p&gt;During that two hours, bored, pressing &amp;quot;approve&amp;quot; whenever Claude Code asked me without even reading what it was asking permission to do, I see that Qwen 3 Coder has released and it&amp;#39;s basically just Gemini CLI but &amp;quot;qwen&amp;quot; replacing the words &amp;quot;gemini&amp;quot; in a good 60% of all the places it&amp;#39;s supposed to. &lt;/p&gt;\n\n&lt;p&gt;Download that, point it at my Ollama server. 5 minutes later I&amp;#39;m able to talk to the AI and ask it to do some basic setup stuff.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;I&amp;#39;m sorry Dave, I can&amp;#39;t do that&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;Same exact thing with Anon-Kode. These CLI agents that exist specifically to write code because I&amp;#39;m not smart enough to do it apparently can&amp;#39;t do the one thing they exist to do.&lt;/p&gt;\n\n&lt;p&gt;Anon-Code is literally just Claude Code. They didn&amp;#39;t even bother replacing mentions of Claude Code in the UI or in the backend. Qwen is just Gemini, if you ask it what tools it has access to, it just shows &amp;quot;Gemini Tools&amp;quot;. These things are supposed to work and are based off things that do work. What am I doing wrong? It won&amp;#39;t execute code no matter what I try, and I have tried a ton of things:&lt;/p&gt;\n\n&lt;p&gt;- Tell it to check what tools it has, tell it to use those specific tools&lt;br/&gt;\n- YOLO mode in Qwen&lt;br/&gt;\n- Start off demanding it actually do code&lt;br/&gt;\n- ALL CAPS&lt;br/&gt;\n- Switching out model after model after model, all listed to support coding tools&lt;br/&gt;\n- Looked around for config files to turn it from &amp;quot;off&amp;quot; to &amp;quot;on&amp;quot;&lt;br/&gt;\n- With Aider and Continue, I was using LM Studio instead of Ollama and I couldn&amp;#39;t get those to work either&lt;/p&gt;\n\n&lt;p&gt;I got Claude Code running in maybe 30 seconds this is not a general inability to use a product intended for the mass market. What am I missing that hundreds of thousands of people easily figured out?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5674jetvquef1.png?width=1799&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23f1623e8b0b00867b12dbda611ba3aaa1c8ca6a\"&gt;https://preview.redd.it/5674jetvquef1.png?width=1799&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23f1623e8b0b00867b12dbda611ba3aaa1c8ca6a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cvnydetvquef1.png?width=1326&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47ddb460596e0b1fd18155d736eb976683b3f420\"&gt;https://preview.redd.it/cvnydetvquef1.png?width=1326&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47ddb460596e0b1fd18155d736eb976683b3f420&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m89s6y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "KingofRheinwg",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m89s6y/seriously_how_do_you_get_cli_coding_agents_etc_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m89s6y/seriously_how_do_you_get_cli_coding_agents_etc_to/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753376158,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just tested the `unsloth/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL.gguf` model using `llama.cpp` on a Threadripper machine equiped with 128 GB RAM + 72 GB VRAM. \n\nBy selectively offloading MoE tensors to the CPU - aiming to maximize the VRAM usage - I managed to run the model at generation rate of 15 tokens/s and a context window of 32k tokens. This token generation speed is really great for a non-reasoning model. \n  \nHere is the full execution command I used:\n\n```\n./llama-server \\\n--model downloaded_models/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL-00001-of-00003.gguf \\\n--port 11433 \\\n--host \"0.0.0.0\" \\\n--verbose \\\n--flash-attn \\\n--cache-type-k q8_0 \\\n--cache-type-v q8_0 \\\n--n-gpu-layers 999 \\\n-ot \"blk\\.(?:[1-8]?[1379])\\.ffn_.*_exps\\.weight=CPU\" \\\n--prio 3 \\\n--threads 32 \\\n--ctx-size 32768 \\\n--temp 0.6 \\\n--min-p 0.0 \\\n--top-p 0.95 \\\n--top-k 20 \\\n--repeat-penalty 1\n```\n\nI'm still new to `llama.cpp` and quantization, so any advice is welcome. I think Q4_K_XL might be too heavy for this machine, so I wonder how much quality I would lose by using Q3_K_XL instead.\n\n",
          "author_fullname": "t2_14u3g9s5kx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running Qwen3 235B-A22B 2507 on a Threadripper 3970X + 3x RTX 3090 Machine at 15 tok/s",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7pqln",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 61,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7HXCQ-4F_oQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7HXCQ-4F_oQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine\"&gt;&lt;/iframe&gt;",
              "author_name": "Septerium",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/7HXCQ-4F_oQ/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@JohnnyGomezSn"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7HXCQ-4F_oQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m7pqln",
            "height": 200
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 61,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=1b8996779707c4a5f85298d6cf4e8395ec809c0d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753316083,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtube.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just tested the &lt;code&gt;unsloth/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL.gguf&lt;/code&gt; model using &lt;code&gt;llama.cpp&lt;/code&gt; on a Threadripper machine equiped with 128 GB RAM + 72 GB VRAM. &lt;/p&gt;\n\n&lt;p&gt;By selectively offloading MoE tensors to the CPU - aiming to maximize the VRAM usage - I managed to run the model at generation rate of 15 tokens/s and a context window of 32k tokens. This token generation speed is really great for a non-reasoning model. &lt;/p&gt;\n\n&lt;p&gt;Here is the full execution command I used:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n./llama-server \\\n--model downloaded_models/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL-00001-of-00003.gguf \\\n--port 11433 \\\n--host &amp;quot;0.0.0.0&amp;quot; \\\n--verbose \\\n--flash-attn \\\n--cache-type-k q8_0 \\\n--cache-type-v q8_0 \\\n--n-gpu-layers 999 \\\n-ot &amp;quot;blk\\.(?:[1-8]?[1379])\\.ffn_.*_exps\\.weight=CPU&amp;quot; \\\n--prio 3 \\\n--threads 32 \\\n--ctx-size 32768 \\\n--temp 0.6 \\\n--min-p 0.0 \\\n--top-p 0.95 \\\n--top-k 20 \\\n--repeat-penalty 1\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m still new to &lt;code&gt;llama.cpp&lt;/code&gt; and quantization, so any advice is welcome. I think Q4_K_XL might be too heavy for this machine, so I wonder how much quality I would lose by using Q3_K_XL instead.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.youtube.com/watch?v=7HXCQ-4F_oQ",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?auto=webp&amp;s=fb78672ddcf654bd2c828f30bcdaede2ae00db46",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b68e4415698a411ba429105637449852662e35d9",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7a94209a8c4dae66ae50d2f66698b6671ae7897",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b8cd0c77917208f92bbcf8528d34b5d0cb74b361",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7pqln",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FalseMap1582",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/",
          "stickied": false,
          "url": "https://www.youtube.com/watch?v=7HXCQ-4F_oQ",
          "subreddit_subscribers": 504253,
          "created_utc": 1753316083,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7HXCQ-4F_oQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine\"&gt;&lt;/iframe&gt;",
              "author_name": "Septerium",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/7HXCQ-4F_oQ/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@JohnnyGomezSn"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Still taking a few cables out doing management but just built this beast! ",
          "author_fullname": "t2_1lvyip3xqa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Local llm build, 144gb vram monster",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "azb7bsq4hnef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=adf186a078b154422310a8ab85dc4132d62a884d"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c4e3360ec9a223a2e1993e68b89239bea8fab5d"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9dd1df27f9a6dde12d214b9f664c2a6d6becad8c"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=992b723a50e9d89ba6dcf55d25b4a32b0903800a"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=058f88502941d5e4b9b4a9b6d971f05145512c21"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8e1de07a0090f10302ba99275055a1af4da7ed70"
                }
              ],
              "s": {
                "y": 4284,
                "x": 5712,
                "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=5712&amp;format=pjpg&amp;auto=webp&amp;s=e5a817b70709275d6498ac676dabcc5a07ed4165"
              },
              "id": "azb7bsq4hnef1"
            },
            "nxp6tyq4hnef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=33662b7541293a8c293d31753b58f64f8bbda4a7"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d757ced2e85613e3198790f7afb3ae6bac7b3d6"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a8779de52430894d54ebad319add5ed70ce03b64"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eaa15f34cc480d047699e1a8d5b71f60d5495d61"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=01d519c0039f3e130011732eab99230101d47b7c"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=440c40a1c74d5d5f9d87e940413381fc01da53fc"
                }
              ],
              "s": {
                "y": 4284,
                "x": 5712,
                "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=5712&amp;format=pjpg&amp;auto=webp&amp;s=9d8c4f1e52a3beb39d8661bbf0151cb972d2bfa6"
              },
              "id": "nxp6tyq4hnef1"
            }
          },
          "name": "t3_1m7dtpm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 252,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "azb7bsq4hnef1",
                "id": 712359206
              },
              {
                "media_id": "nxp6tyq4hnef1",
                "id": 712359207
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 252,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/VGM2yiS76HMEN0da0De5H87rkjtR_9prbewrkSRRamQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753287916,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Still taking a few cables out doing management but just built this beast! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m7dtpm",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7dtpm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EasyConference4177",
          "discussion_type": null,
          "num_comments": 61,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7dtpm/local_llm_build_144gb_vram_monster/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m7dtpm",
          "subreddit_subscribers": 504253,
          "created_utc": 1753287916,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI have gotten my work to onboard some AI solutions which I find incredibly exciting.\n\n  \nFor some legacy reasons, I am allowed to use this quantized llama model: [https://ollama.com/library/llama3.1:8b](https://ollama.com/library/llama3.1:8b)\n\nNow, the only challenge is I need to discover which is the identical model on huggingface (the bloke..unsloth...etc).\n\nDoes anyone know of a way to figure that out?  \nThank you so much for any guidance",
          "author_fullname": "t2_2jhr5wgg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Finding the equivalent ollama model on huggingface hub",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8n3ry",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753409329,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I have gotten my work to onboard some AI solutions which I find incredibly exciting.&lt;/p&gt;\n\n&lt;p&gt;For some legacy reasons, I am allowed to use this quantized llama model: &lt;a href=\"https://ollama.com/library/llama3.1:8b\"&gt;https://ollama.com/library/llama3.1:8b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Now, the only challenge is I need to discover which is the identical model on huggingface (the bloke..unsloth...etc).&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of a way to figure that out?&lt;br/&gt;\nThank you so much for any guidance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?auto=webp&amp;s=a080c4707584d3aa14134960cda9ba2d339b93a3",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc759de0e8fa36d241c5728d41ee3cf022cab96",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ccf136f5d3091254a0067a3bc5d6c7df9d62d89",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2530aa4ecbcf7899ec0d023e217fe24af15fe0a6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=750a6d42fd91c5a6e9a9c069e74247c877644e97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eab390b865b031211658564ad5fe5241c9661c5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8n3ry",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blackandscholes1978",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8n3ry/finding_the_equivalent_ollama_model_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8n3ry/finding_the_equivalent_ollama_model_on/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753409329,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a 3090, So I downloaded koboldcpp, installed SillyTavern and got it to work well. The problem seems to be the responses for MythoMax are very bland, only 1 or 2 sentences long even with the character cards from chub AI.\n\nOn Chub.Ai, I love the responses, haven't tried the paid versions but the free version is so good, lengthy, descriptive, goes along. So I had downloaded MythoMax Q5_K_M since I saw that one was used for the paid tier and like I mentioned, just bland answers. Even downloading the the exact same character card and providing the same sentences to each gave me wildly different answers.\n\nI did also download and install Gemma 3 27band the answers got way better, but not quite like on chub.ai.\n\nIs it maybe settings I have to mess with? Because I did try changing from default, to novel.ai, and others. Or if there's a better one to download I can also give that a shot.",
          "author_fullname": "t2_61lju",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the best gguf file for roleplay?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8cha8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753382223,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 3090, So I downloaded koboldcpp, installed SillyTavern and got it to work well. The problem seems to be the responses for MythoMax are very bland, only 1 or 2 sentences long even with the character cards from chub AI.&lt;/p&gt;\n\n&lt;p&gt;On Chub.Ai, I love the responses, haven&amp;#39;t tried the paid versions but the free version is so good, lengthy, descriptive, goes along. So I had downloaded MythoMax Q5_K_M since I saw that one was used for the paid tier and like I mentioned, just bland answers. Even downloading the the exact same character card and providing the same sentences to each gave me wildly different answers.&lt;/p&gt;\n\n&lt;p&gt;I did also download and install Gemma 3 27band the answers got way better, but not quite like on chub.ai.&lt;/p&gt;\n\n&lt;p&gt;Is it maybe settings I have to mess with? Because I did try changing from default, to novel.ai, and others. Or if there&amp;#39;s a better one to download I can also give that a shot.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8cha8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Alchy919",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8cha8/whats_the_best_gguf_file_for_roleplay/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8cha8/whats_the_best_gguf_file_for_roleplay/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753382223,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôve been experimenting with open-source LLMs to see how far they can go in maintaining tone and emotional continuity over longer chats. Most of the use cases I‚Äôve seen are either task-based or productivity-focused, but I‚Äôm more interested in conversational flow, especially personality consistency, memory simulation, and emotional nuance.\n\nHas anyone here tried using LLaMA-based models as the backbone for character-driven or relationship-style interactions? I‚Äôm not talking about full-on RP scripts, but more like companion-style chats that adapt to your long-term mood and behavior. What models or local setups have worked best for that?",
          "author_fullname": "t2_1q9k6ch1ni",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Curious if anyone‚Äôs used fine-tuned LLaMA models for emotional or character-based responses?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8i53g",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753395669,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôve been experimenting with open-source LLMs to see how far they can go in maintaining tone and emotional continuity over longer chats. Most of the use cases I‚Äôve seen are either task-based or productivity-focused, but I‚Äôm more interested in conversational flow, especially personality consistency, memory simulation, and emotional nuance.&lt;/p&gt;\n\n&lt;p&gt;Has anyone here tried using LLaMA-based models as the backbone for character-driven or relationship-style interactions? I‚Äôm not talking about full-on RP scripts, but more like companion-style chats that adapt to your long-term mood and behavior. What models or local setups have worked best for that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8i53g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Roll_5714",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8i53g/curious_if_anyones_used_finetuned_llama_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8i53g/curious_if_anyones_used_finetuned_llama_models/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753395669,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Sooooo‚Ä¶ When Qwen3-Coder üá∫üá∏ Freedom üá∫üá∏ edition GGUF?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m84ked",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.54,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753364043,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/\"&gt;https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?auto=webp&amp;s=ecf43e8e82602652ec95e06f13b6ce18da205b9c",
                  "width": 1200,
                  "height": 628
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c1e4661cbba0b6e1e232602fbabfa0384ba0123",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b84255c302c8464ea76b251e4d4ab64cac0ec723",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7c4bae3b4c97261af353a9ec64d3ef027f6deac",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb89e898879eb7adef969749433776a6f6a543ad",
                    "width": 640,
                    "height": 334
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f16221a57c07b16c8cef11acfc0eeb15f6f1254e",
                    "width": 960,
                    "height": 502
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=db29c2e5309166fabf6283791735d6762adf4b55",
                    "width": 1080,
                    "height": 565
                  }
                ],
                "variants": {},
                "id": "4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m84ked",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m84ked/sooooo_when_qwen3coder_freedom_edition_gguf/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m84ked/sooooo_when_qwen3coder_freedom_edition_gguf/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753364043,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Apart from purpose of learning llm or for your job/work, I like to understand thoughts and purpose behind why many of you run models locally for inference or training/fine tuning. What is your objective and what problems have you solved by doing that.\n\nAlso which models have you used and on what hardware",
          "author_fullname": "t2_7psxegnc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why do you run or train in local system",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8mwme",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753408750,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apart from purpose of learning llm or for your job/work, I like to understand thoughts and purpose behind why many of you run models locally for inference or training/fine tuning. What is your objective and what problems have you solved by doing that.&lt;/p&gt;\n\n&lt;p&gt;Also which models have you used and on what hardware&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8mwme",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Psychological-Tie304",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8mwme/why_do_you_run_or_train_in_local_system/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8mwme/why_do_you_run_or_train_in_local_system/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753408750,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi LocalLLaMa !\n\nI'm looking for something that from what I see looks like Graphiti or Cognee or some of those tools. But that could support a lot of users or run on top of PostGRES.\n\nDo you have any suggestions that I could checkout ?",
          "author_fullname": "t2_n10ib",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for a GraphRAG type of backend that supports multiple users",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8c77v",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753381576,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi LocalLLaMa !&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for something that from what I see looks like Graphiti or Cognee or some of those tools. But that could support a lot of users or run on top of PostGRES.&lt;/p&gt;\n\n&lt;p&gt;Do you have any suggestions that I could checkout ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8c77v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BraceletGrolf",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8c77v/looking_for_a_graphrag_type_of_backend_that/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8c77v/looking_for_a_graphrag_type_of_backend_that/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753381576,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What's an open source alternative to LM studio that uses GitHub and can be freely accessible, is generally very feature-rich, and can feasibly stand up to LM studio for people who want a free open source solution?",
          "author_fullname": "t2_1sznzjx7fy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source alternative to LM studio?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m81whq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753356654,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s an open source alternative to LM studio that uses GitHub and can be freely accessible, is generally very feature-rich, and can feasibly stand up to LM studio for people who want a free open source solution?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m81whq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "datascientist2964",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m81whq/open_source_alternative_to_lm_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m81whq/open_source_alternative_to_lm_studio/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753356654,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\nI‚Äôm experimenting with self-hosted LLM agents for software development tasks ‚Äî think writing code, submitting PRs, etc. My current stack is OpenHands + LM Studio, which I‚Äôve tested on an M4 Pro Mac Mini and a Windows machine with a 3080 Ti.\n\nThe Mac Mini actually held up better than expected for 7B/13B models (quantized), but anything larger is slow. The 3080 Ti felt underutilized ‚Äî even at 100% GPU setting, performance wasn‚Äôt impressive.\n\nI‚Äôm now considering a dedicated GPU for my homelab server. The top candidates:\n\t‚Ä¢\tRTX 4000 Blackwell (24GB ECC) ‚Äì ¬£1400\n\t‚Ä¢\tRTX 4500 Blackwell (32GB ECC) ‚Äì ¬£2400\n\nUse case is primarily local coding agents, possibly running 13B‚Äì32B models, with a future goal of supporting multi-agent sessions. Power efficiency and stability matter ‚Äî this will run 24/7.\n\nQuestions:\n\t‚Ä¢\tIs the 4000 Blackwell enough for local 32B models (quantized), or is 32GB VRAM realistically required?\n\t‚Ä¢\tAny caveats with Blackwell cards for LLMs (driver maturity, inference compatibility)?\n\t‚Ä¢\tWould a used 3090 or A6000 be more practical in terms of cost vs performance, despite higher power usage?\n\t‚Ä¢\tAnyone running OpenHands locally or in K8s ‚Äî any advice around GPU utilization or deployment?\n\nLooking for input from people already running LLMs or agents locally. Thanks in advanced. ",
          "author_fullname": "t2_hnm8h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Considering RTX 4000 Blackwell for Local Agentic AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8hbnn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753393605,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm experimenting with self-hosted LLM agents for software development tasks ‚Äî think writing code, submitting PRs, etc. My current stack is OpenHands + LM Studio, which I‚Äôve tested on an M4 Pro Mac Mini and a Windows machine with a 3080 Ti.&lt;/p&gt;\n\n&lt;p&gt;The Mac Mini actually held up better than expected for 7B/13B models (quantized), but anything larger is slow. The 3080 Ti felt underutilized ‚Äî even at 100% GPU setting, performance wasn‚Äôt impressive.&lt;/p&gt;\n\n&lt;p&gt;I‚Äôm now considering a dedicated GPU for my homelab server. The top candidates:\n    ‚Ä¢ RTX 4000 Blackwell (24GB ECC) ‚Äì ¬£1400\n    ‚Ä¢ RTX 4500 Blackwell (32GB ECC) ‚Äì ¬£2400&lt;/p&gt;\n\n&lt;p&gt;Use case is primarily local coding agents, possibly running 13B‚Äì32B models, with a future goal of supporting multi-agent sessions. Power efficiency and stability matter ‚Äî this will run 24/7.&lt;/p&gt;\n\n&lt;p&gt;Questions:\n    ‚Ä¢ Is the 4000 Blackwell enough for local 32B models (quantized), or is 32GB VRAM realistically required?\n    ‚Ä¢ Any caveats with Blackwell cards for LLMs (driver maturity, inference compatibility)?\n    ‚Ä¢ Would a used 3090 or A6000 be more practical in terms of cost vs performance, despite higher power usage?\n    ‚Ä¢ Anyone running OpenHands locally or in K8s ‚Äî any advice around GPU utilization or deployment?&lt;/p&gt;\n\n&lt;p&gt;Looking for input from people already running LLMs or agents locally. Thanks in advanced. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8hbnn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "b1uedust",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8hbnn/considering_rtx_4000_blackwell_for_local_agentic/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8hbnn/considering_rtx_4000_blackwell_for_local_agentic/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753393605,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm building a tool to automatically detect and flag animal abuse and exploitation in social media videos using Gemini 2.5 Pro. I've been pretty impressed with its capabilities, but I was hoping to eventually find tune a model that I could self host for free (I have a lot of GPUs). Is there anything open source that even comes close, that I could potentially fine tune with multimodal data that I'm generating with Gemini?",
          "author_fullname": "t2_1a0oiggi8d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best open source vision model fine tuneable for animal abuse detection?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8b72y",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753379319,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building a tool to automatically detect and flag animal abuse and exploitation in social media videos using Gemini 2.5 Pro. I&amp;#39;ve been pretty impressed with its capabilities, but I was hoping to eventually find tune a model that I could self host for free (I have a lot of GPUs). Is there anything open source that even comes close, that I could potentially fine tune with multimodal data that I&amp;#39;m generating with Gemini?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8b72y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Scam_Altman",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8b72y/best_open_source_vision_model_fine_tuneable_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8b72y/best_open_source_vision_model_fine_tuneable_for/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753379319,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been experimenting with using AI to generate a Bash script for me. The script's purpose is to follow a specific task logic while downloading items. Despite giving detailed feedback, the AI repeatedly failed to get it right. I thought maybe the problem was complexity, so I tried simplifying it ‚Äî starting with just the task logic, planning to add downloading and other functions later.\n\nI used this new prompt as in image with several models, including Gemini 2.5, ChatGPT-4o, OpenAI GPT-4.1, o4, and o4-mini. None of them could generate a correct solution, even after I provided detailed outputs and feedback. Surprisingly, DeepSeek R1 got it right on the first try, though it took nearly 10 minutes to process. I haven't tried o1 o3 or other premium models yet, but they might be capable too.\n\nHere are my main questions:\n\n* For a medium-to-light scripting task like this (about 100‚Äì500 lines, single file), is it better to break the task into smaller pieces, and ask AI to build it bit by bit, or to write a detailed, complete prompt up front?\n* Is this type of logic too complex for non-flagship models? If I want to avoid using expensive flagship models, how can I structure prompts to still get reliable results? Currently, only R1 seems to handle it.\n* When using models like o4mini, I‚Äôve tried breaking the problem down, but they often fix one part and break another. How should I prompt non-flagship models to handle complex logic like this more effectively?\n\nHere‚Äôs the prompt I used:\n\n    write a bash script write to a log file\n    Requirements\n    Prints one ‚Äò+‚Äô per second\n    New line after every 5 ‚Äò+‚Äô\n    Starts a new ‚ÄúTask N‚Äù at every real-time 10-second boundary (when seconds end in 0, 10, 20, ...)\n    Each task has a running ‚ÄúTotal N: X‚Äù line at the end of its block, which is always updated in place (never duplicated).\n    All previous tasks remain in the log (each with their own Task/Total block).\n    Script can be stopped and resumed at any time, continuing current task‚Äôs count and log format perfectly.\n    \n    Sample Log Format\n    Task 1\n    +++++\n    ++++\n    Total 1: 9\n    \n    Task 2\n    +++++\n    ++\n    Total 2: 7\n    \n    If you stop and restart during Task 2, it continues like:\n    Task 1\n    +++++\n    ++++\n    Total 1: 9\n    \n    Task 2\n    +++++\n    ++++\n    Total 2: 9\n    ",
          "author_fullname": "t2_w0frc97",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is this too much logic for AI? should I break it smaller to prompt?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8qr9q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.22,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/iiq4jlppORjVU9AD-yD0CbOb_lhtMf8QRo34T4quEiM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753420811,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been experimenting with using AI to generate a Bash script for me. The script&amp;#39;s purpose is to follow a specific task logic while downloading items. Despite giving detailed feedback, the AI repeatedly failed to get it right. I thought maybe the problem was complexity, so I tried simplifying it ‚Äî starting with just the task logic, planning to add downloading and other functions later.&lt;/p&gt;\n\n&lt;p&gt;I used this new prompt as in image with several models, including Gemini 2.5, ChatGPT-4o, OpenAI GPT-4.1, o4, and o4-mini. None of them could generate a correct solution, even after I provided detailed outputs and feedback. Surprisingly, DeepSeek R1 got it right on the first try, though it took nearly 10 minutes to process. I haven&amp;#39;t tried o1 o3 or other premium models yet, but they might be capable too.&lt;/p&gt;\n\n&lt;p&gt;Here are my main questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;For a medium-to-light scripting task like this (about 100‚Äì500 lines, single file), is it better to break the task into smaller pieces, and ask AI to build it bit by bit, or to write a detailed, complete prompt up front?&lt;/li&gt;\n&lt;li&gt;Is this type of logic too complex for non-flagship models? If I want to avoid using expensive flagship models, how can I structure prompts to still get reliable results? Currently, only R1 seems to handle it.&lt;/li&gt;\n&lt;li&gt;When using models like o4mini, I‚Äôve tried breaking the problem down, but they often fix one part and break another. How should I prompt non-flagship models to handle complex logic like this more effectively?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here‚Äôs the prompt I used:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;write a bash script write to a log file\nRequirements\nPrints one ‚Äò+‚Äô per second\nNew line after every 5 ‚Äò+‚Äô\nStarts a new ‚ÄúTask N‚Äù at every real-time 10-second boundary (when seconds end in 0, 10, 20, ...)\nEach task has a running ‚ÄúTotal N: X‚Äù line at the end of its block, which is always updated in place (never duplicated).\nAll previous tasks remain in the log (each with their own Task/Total block).\nScript can be stopped and resumed at any time, continuing current task‚Äôs count and log format perfectly.\n\nSample Log Format\nTask 1\n+++++\n++++\nTotal 1: 9\n\nTask 2\n+++++\n++\nTotal 2: 7\n\nIf you stop and restart during Task 2, it continues like:\nTask 1\n+++++\n++++\nTotal 1: 9\n\nTask 2\n+++++\n++++\nTotal 2: 9\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/87gik9pocyef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/87gik9pocyef1.png?auto=webp&amp;s=882040b1524e85d1e541c67292268579de6da0c5",
                  "width": 724,
                  "height": 799
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/87gik9pocyef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2c84b0bebcb0355231aa023e96ed7f79041f45c",
                    "width": 108,
                    "height": 119
                  },
                  {
                    "url": "https://preview.redd.it/87gik9pocyef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=272c1b889ee5e602d6979d5fe7de4e4fc6bb99b0",
                    "width": 216,
                    "height": 238
                  },
                  {
                    "url": "https://preview.redd.it/87gik9pocyef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9fc5cacee906f6d0b24bc0d0d69836864bacf73e",
                    "width": 320,
                    "height": 353
                  },
                  {
                    "url": "https://preview.redd.it/87gik9pocyef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf518d01131f779b03efca1031bb5d2e32a952d2",
                    "width": 640,
                    "height": 706
                  }
                ],
                "variants": {},
                "id": "SZrbj05Ymb4g4TsIRAsqq5SAHLPZi-6WK0OeRzd30QE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8qr9q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CJCCJJ",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8qr9q/is_this_too_much_logic_for_ai_should_i_break_it/",
          "stickied": false,
          "url": "https://i.redd.it/87gik9pocyef1.png",
          "subreddit_subscribers": 504253,
          "created_utc": 1753420811,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "About a year ago deepseek-coder-v2:236b performed pretty well in my tests.   \nI used it serveral times in non-coding tasks and it always outperformed llama3.1:70b or qwen2.5:72b then.  \nSince my local deepseek-coder-v2:236b can only run on CPU, the speed made it unusefull for any production use. \n\nSo my question aims: Had anyone already tested qwen3-coder:480b with tasks apart from coding?  \n  \nMy high-end favorites at the moment are:  \nqwen3:235b and Kimi2\n\n  \nMaybe qwen3-coder:480b can fill in the gap in between those two models?\n\n",
          "author_fullname": "t2_1chawnfp64",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "qwen3-coder:480b - usability for non-coding tasks?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m83mu1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753361681,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;About a year ago deepseek-coder-v2:236b performed pretty well in my tests.&lt;br/&gt;\nI used it serveral times in non-coding tasks and it always outperformed llama3.1:70b or qwen2.5:72b then.&lt;br/&gt;\nSince my local deepseek-coder-v2:236b can only run on CPU, the speed made it unusefull for any production use. &lt;/p&gt;\n\n&lt;p&gt;So my question aims: Had anyone already tested qwen3-coder:480b with tasks apart from coding?  &lt;/p&gt;\n\n&lt;p&gt;My high-end favorites at the moment are:&lt;br/&gt;\nqwen3:235b and Kimi2&lt;/p&gt;\n\n&lt;p&gt;Maybe qwen3-coder:480b can fill in the gap in between those two models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m83mu1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Impossible_Art9151",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m83mu1/qwen3coder480b_usability_for_noncoding_tasks/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m83mu1/qwen3coder480b_usability_for_noncoding_tasks/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753361681,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is probably a very uninspiring question for most people here, but I am looking to replace my current AMD RX 6600 (8GB) for both UWQHD gaming and experimentation with Local LLMs.\n\nI've been running various models in the 4-15GB range, so ocasionally VRAM only, sometimes VRAM+RAM (of which I also only have 32GB, DDR4, decent timings.) CPU is a 5800X3D on a MSI B550 Pro (so PCI 4.0)\n\nObviously, that's very meh, but my budget is quite constrained.\n\nI've mostly done text generation (creative writing, not RP; code). I am interest in pushing context windows and making more use of RAG). I want to also look into image and audio generation in the future. \n\nI'd also love to run some hobbyist expirements with *training* midi or score based composition networks (obviously being quite limited in ressources... this is more for my education/edification than getting any kind of competetive results).\n\nSo... what's the most generally useful kind of purchase I might be looking at?\n\nCurrently my research indicates the following candidates:   \n\n* Radeon RX 9060 XT 16GB ~380‚Ç¨ (gaming, price+, not CUDA is limiting for some things)  \n* RTX 5060 Ti 16GB, ~440‚Ç¨ (similar performance, for 60‚Ç¨ more, but maybe an NVIDIA bonus)\n* last generation used, 16GB, seem to be about 100‚Ç¨ cheaper?, so in the 300-360‚Ç¨ range (7600XT-4060Ti16)?\n* Arc A770, ~ 250-280‚Ç¨ (cheapest ? 16GB option that isn't incredibly old, I assume?)\n\n\nI haven't really looked into a dual setup or two generations old, so if I should do that (2xused RX 6800 or some such), chime up. I guess biggest downside of using two cards now is I can't just extend one of the above with a duplicate in the future.\n\nRadeon RX 7900 XT 20GB (680‚Ç¨) or XTX 24 GB (880‚Ç¨) seem like the cheapest options beyond 16GB and that's probably beyond what I should spend, as tempting as they seem.\n\nAs you all seem way more knowledgeable, I'd love some advice. Thanks in advance.",
          "author_fullname": "t2_k9k5w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "lowish/midrange budget general purpose GPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8dufz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753385354,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is probably a very uninspiring question for most people here, but I am looking to replace my current AMD RX 6600 (8GB) for both UWQHD gaming and experimentation with Local LLMs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been running various models in the 4-15GB range, so ocasionally VRAM only, sometimes VRAM+RAM (of which I also only have 32GB, DDR4, decent timings.) CPU is a 5800X3D on a MSI B550 Pro (so PCI 4.0)&lt;/p&gt;\n\n&lt;p&gt;Obviously, that&amp;#39;s very meh, but my budget is quite constrained.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve mostly done text generation (creative writing, not RP; code). I am interest in pushing context windows and making more use of RAG). I want to also look into image and audio generation in the future. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d also love to run some hobbyist expirements with &lt;em&gt;training&lt;/em&gt; midi or score based composition networks (obviously being quite limited in ressources... this is more for my education/edification than getting any kind of competetive results).&lt;/p&gt;\n\n&lt;p&gt;So... what&amp;#39;s the most generally useful kind of purchase I might be looking at?&lt;/p&gt;\n\n&lt;p&gt;Currently my research indicates the following candidates:   &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Radeon RX 9060 XT 16GB ~380‚Ç¨ (gaming, price+, not CUDA is limiting for some things)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;RTX 5060 Ti 16GB, ~440‚Ç¨ (similar performance, for 60‚Ç¨ more, but maybe an NVIDIA bonus)&lt;/li&gt;\n&lt;li&gt;last generation used, 16GB, seem to be about 100‚Ç¨ cheaper?, so in the 300-360‚Ç¨ range (7600XT-4060Ti16)?&lt;/li&gt;\n&lt;li&gt;Arc A770, ~ 250-280‚Ç¨ (cheapest ? 16GB option that isn&amp;#39;t incredibly old, I assume?)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I haven&amp;#39;t really looked into a dual setup or two generations old, so if I should do that (2xused RX 6800 or some such), chime up. I guess biggest downside of using two cards now is I can&amp;#39;t just extend one of the above with a duplicate in the future.&lt;/p&gt;\n\n&lt;p&gt;Radeon RX 7900 XT 20GB (680‚Ç¨) or XTX 24 GB (880‚Ç¨) seem like the cheapest options beyond 16GB and that&amp;#39;s probably beyond what I should spend, as tempting as they seem.&lt;/p&gt;\n\n&lt;p&gt;As you all seem way more knowledgeable, I&amp;#39;d love some advice. Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8dufz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BrainOnLoan",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8dufz/lowishmidrange_budget_general_purpose_gpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8dufz/lowishmidrange_budget_general_purpose_gpu/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753385354,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I‚Äôve spent 2 weeks fighting to get a local Scottish voice clone running for my work, and I‚Äôm totally blocked because these old wheels are missing everywhere. If anyone has backups of fairseq-0.12.0, omegaconf-2.0.5, and hydra-core-1.0.6 for Python 3.9 (Ubuntu), I‚Äôd be so grateful. Please DM me with a link if you can help. Thank you!",
          "author_fullname": "t2_1qe29mhs5h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for fairseq-0.12.0, omegaconf-2.0.5, hydra-core-1.0.6 .whl files for Python 3.9/Ubuntu‚ÄîRVC project stuck!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m86v60",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753369563,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I‚Äôve spent 2 weeks fighting to get a local Scottish voice clone running for my work, and I‚Äôm totally blocked because these old wheels are missing everywhere. If anyone has backups of fairseq-0.12.0, omegaconf-2.0.5, and hydra-core-1.0.6 for Python 3.9 (Ubuntu), I‚Äôd be so grateful. Please DM me with a link if you can help. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m86v60",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Foreign-Demand-9815",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m86v60/looking_for_fairseq0120_omegaconf205_hydracore106/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m86v60/looking_for_fairseq0120_omegaconf205_hydracore106/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753369563,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI have gotten my work to onboard some AI solutions which I find incredibly exciting.\n\n  \nFor some legacy reasons, I am allowed to use this quantized llama model: [https://ollama.com/library/llama3.1:8b](https://ollama.com/library/llama3.1:8b)\n\nNow, the only challenge is I need to discover which is the identical model on huggingface (the bloke..unsloth...etc).\n\nDoes anyone know of a way to figure that out?  \nThank you so much for any guidance",
          "author_fullname": "t2_2jhr5wgg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Discovering the huggingface hub equivalent of an ollama model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8myv9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753408928,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I have gotten my work to onboard some AI solutions which I find incredibly exciting.&lt;/p&gt;\n\n&lt;p&gt;For some legacy reasons, I am allowed to use this quantized llama model: &lt;a href=\"https://ollama.com/library/llama3.1:8b\"&gt;https://ollama.com/library/llama3.1:8b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Now, the only challenge is I need to discover which is the identical model on huggingface (the bloke..unsloth...etc).&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of a way to figure that out?&lt;br/&gt;\nThank you so much for any guidance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?auto=webp&amp;s=a080c4707584d3aa14134960cda9ba2d339b93a3",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc759de0e8fa36d241c5728d41ee3cf022cab96",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ccf136f5d3091254a0067a3bc5d6c7df9d62d89",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2530aa4ecbcf7899ec0d023e217fe24af15fe0a6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=750a6d42fd91c5a6e9a9c069e74247c877644e97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eab390b865b031211658564ad5fe5241c9661c5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8myv9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blackandscholes1978",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8myv9/discovering_the_huggingface_hub_equivalent_of_an/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8myv9/discovering_the_huggingface_hub_equivalent_of_an/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753408928,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "XTC: I haven‚Äôt seen these settings in the UI but I have seen in the documentation that there should be a couple fields for this. Am I just blind or is there something I have to do outside of the UI to enable XTC?\n\nDRY: I have no clue how to go about trying to get DRY in LMStudio. I‚Äôm aware that there are other LM software that have DRY implemented, but I‚Äôd really like to avoid having 5 different applications for LLM inference and just use 1 for everything if possible. ",
          "author_fullname": "t2_1loou9xu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to get DRY and XTC in LMStudio?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8c7ku",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753381600,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;XTC: I haven‚Äôt seen these settings in the UI but I have seen in the documentation that there should be a couple fields for this. Am I just blind or is there something I have to do outside of the UI to enable XTC?&lt;/p&gt;\n\n&lt;p&gt;DRY: I have no clue how to go about trying to get DRY in LMStudio. I‚Äôm aware that there are other LM software that have DRY implemented, but I‚Äôd really like to avoid having 5 different applications for LLM inference and just use 1 for everything if possible. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8c7ku",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Shadow-Amulet-Ambush",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8c7ku/how_to_get_dry_and_xtc_in_lmstudio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8c7ku/how_to_get_dry_and_xtc_in_lmstudio/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753381600,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "After all the buzz, Moonshot AI dropped Kimi K2 with 1T parameters, and it‚Äôs being pitched as the open-source Claude Sonnet 4 alternative. Naturally, I had to run the ultimate coding face-off.\n\nI‚Äôve mostly compared them on the following factors:\n\n* Pricing and Speed\n* Frontend Coding\n* Agentic Coding (MCP integration) and how well it works with recent libraries\n\n# Pricing and Speed\n\nYou might already know Sonnet 4 comes with $3/M input tokens and $15/M output tokens. K2, on the other hand, costs about $0.15/M input tokens and $2.50/M output tokens.\n\nWe can already see a massive price gap between these two models. In the test, we ran two code-heavy prompts for both models, roughly totaling 300k tokens each. Sonnet 4 cost around $5 for the entire test, whereas K2 cost just $0.53 - straight up, K2 is around 10x cheaper.\n\n**Speed:** Claude Sonnet 4 clocks around 91 output tokens per second, while K2 manages just 34.1. That‚Äôs painfully slow in comparison.\n\n# Frontend Coding\n\n* **Kimi K2:** Took ages to implement it, but nailed the entire thing in one go.\n* **Claude Sonnet 4:** Super quick with the implementation, but broke the voice support and even ghosted parts of what was asked in the prompt.\n\n# Agentic Coding\n\n* Neither of them wrote a fully working implementation‚Ä¶ which was completely unexpected.\n* Sonnet 4 was worse: it took over 10 minutes and spent most of that time stuck on TypeScript type errors. After all that, it returned false positives in the implementation.\n\n* K2 came close but still couldn‚Äôt figure it out completely.\n\n# Final Take\n\n* On a budget? K2 is a no‚Äëbrainer - almost the same (or better) code quality, at a tenth of the cost.\n* Need speed and can swallow the cost? Stick with Sonnet 4 - you won‚Äôt get much performance gain with K2.\n* Minor edge? K2 might have the upper hand in prompt-following and agentic fluency, despite being slower.\n\nYou can find the entire blog post with a demo for each here: [Kimi K2 vs. Claude 4 Sonnet: what you should pick for agentic coding](https://composio.dev/blog/kimi-k2-vs-claude-4-sonnet-what-you-should-pick-for-agentic-coding)\n\nAlso, I would love to know your preference between the two models. I'm still unsure whether to stick with my go-to Sonnet 4 or switch to Kimi K2. What's your experience with Kimi's response?",
          "author_fullname": "t2_1jl5023gxv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 vs Sonnet 4 for Agentic Coding (Tested on Claude Code)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7c2gr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 148,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 148,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753284823,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753283941,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After all the buzz, Moonshot AI dropped Kimi K2 with 1T parameters, and it‚Äôs being pitched as the open-source Claude Sonnet 4 alternative. Naturally, I had to run the ultimate coding face-off.&lt;/p&gt;\n\n&lt;p&gt;I‚Äôve mostly compared them on the following factors:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pricing and Speed&lt;/li&gt;\n&lt;li&gt;Frontend Coding&lt;/li&gt;\n&lt;li&gt;Agentic Coding (MCP integration) and how well it works with recent libraries&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Pricing and Speed&lt;/h1&gt;\n\n&lt;p&gt;You might already know Sonnet 4 comes with $3/M input tokens and $15/M output tokens. K2, on the other hand, costs about $0.15/M input tokens and $2.50/M output tokens.&lt;/p&gt;\n\n&lt;p&gt;We can already see a massive price gap between these two models. In the test, we ran two code-heavy prompts for both models, roughly totaling 300k tokens each. Sonnet 4 cost around $5 for the entire test, whereas K2 cost just $0.53 - straight up, K2 is around 10x cheaper.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Speed:&lt;/strong&gt; Claude Sonnet 4 clocks around 91 output tokens per second, while K2 manages just 34.1. That‚Äôs painfully slow in comparison.&lt;/p&gt;\n\n&lt;h1&gt;Frontend Coding&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Kimi K2:&lt;/strong&gt; Took ages to implement it, but nailed the entire thing in one go.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Claude Sonnet 4:&lt;/strong&gt; Super quick with the implementation, but broke the voice support and even ghosted parts of what was asked in the prompt.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Agentic Coding&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Neither of them wrote a fully working implementation‚Ä¶ which was completely unexpected.&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Sonnet 4 was worse: it took over 10 minutes and spent most of that time stuck on TypeScript type errors. After all that, it returned false positives in the implementation.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;K2 came close but still couldn‚Äôt figure it out completely.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Final Take&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;On a budget? K2 is a no‚Äëbrainer - almost the same (or better) code quality, at a tenth of the cost.&lt;/li&gt;\n&lt;li&gt;Need speed and can swallow the cost? Stick with Sonnet 4 - you won‚Äôt get much performance gain with K2.&lt;/li&gt;\n&lt;li&gt;Minor edge? K2 might have the upper hand in prompt-following and agentic fluency, despite being slower.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can find the entire blog post with a demo for each here: &lt;a href=\"https://composio.dev/blog/kimi-k2-vs-claude-4-sonnet-what-you-should-pick-for-agentic-coding\"&gt;Kimi K2 vs. Claude 4 Sonnet: what you should pick for agentic coding&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Also, I would love to know your preference between the two models. I&amp;#39;m still unsure whether to stick with my go-to Sonnet 4 or switch to Kimi K2. What&amp;#39;s your experience with Kimi&amp;#39;s response?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?auto=webp&amp;s=06517b450b86f9c3e33b83c23366d9b9246259a9",
                  "width": 1058,
                  "height": 705
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b664894969871c1c911d4ca3de0afe330df8b82c",
                    "width": 108,
                    "height": 71
                  },
                  {
                    "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bfcb06298216e380b6e35365b82b0eb2c6f8ed93",
                    "width": 216,
                    "height": 143
                  },
                  {
                    "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bab6cda0d392796fb48a8642a28f9e5c8195c10c",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=09088db67cbc9fe6a1cebb98a6169ee77b106553",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3ab7f9649c1c5bffa55dd4c99fa7f1804f61119d",
                    "width": 960,
                    "height": 639
                  }
                ],
                "variants": {},
                "id": "89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7c2gr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shricodev",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7c2gr/kimi_k2_vs_sonnet_4_for_agentic_coding_tested_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7c2gr/kimi_k2_vs_sonnet_4_for_agentic_coding_tested_on/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753283941,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Audio Flamingo 3 (AF3) is a fully open, state-of-the-art Large Audio-Language Model (LALM) that advances reasoning and understanding across speech, sounds, and music. AF3 builds on previous work with innovations in:\n\n- Unified audio representation learning (speech, sound, music)  \n- Flexible, on-demand chain-of-thought reasoning  \n- Long-context audio comprehension (up to 10 minutes)\n- Multi-turn, multi-audio conversational dialogue (AF3-Chat)    \n- Voice-to-voice interaction (AF3-Chat)    \n\nExtensive evaluations confirm AF3‚Äôs effectiveness, setting new benchmarks on over 20 public audio understanding and reasoning tasks.\n\n**This model is for non-commercial research purposes only.**\n\n### Model Architecture:\nAudio Flamingo 3 uses AF-Whisper unified audio encoder, MLP-based audio adaptor, Decoder-only LLM backbone (Qwen2.5-7B), and Streaming TTS module (AF3-Chat). Audio Flamingo 3 can take up to 10 minutes of audio inputs.\n\nPaper: https://arxiv.org/abs/2507.08128\nVoice-chat finetune: https://huggingface.co/nvidia/audio-flamingo-3-chat",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "nvidia/audio-flamingo-3",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7fb78",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 93,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 93,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=b68dc8be3ef9abb2b3521ac5287ddf288a2a5bb9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753291299,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Audio Flamingo 3 (AF3) is a fully open, state-of-the-art Large Audio-Language Model (LALM) that advances reasoning and understanding across speech, sounds, and music. AF3 builds on previous work with innovations in:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Unified audio representation learning (speech, sound, music)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Flexible, on-demand chain-of-thought reasoning&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Long-context audio comprehension (up to 10 minutes)&lt;/li&gt;\n&lt;li&gt;Multi-turn, multi-audio conversational dialogue (AF3-Chat)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Voice-to-voice interaction (AF3-Chat)&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Extensive evaluations confirm AF3‚Äôs effectiveness, setting new benchmarks on over 20 public audio understanding and reasoning tasks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;This model is for non-commercial research purposes only.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h3&gt;Model Architecture:&lt;/h3&gt;\n\n&lt;p&gt;Audio Flamingo 3 uses AF-Whisper unified audio encoder, MLP-based audio adaptor, Decoder-only LLM backbone (Qwen2.5-7B), and Streaming TTS module (AF3-Chat). Audio Flamingo 3 can take up to 10 minutes of audio inputs.&lt;/p&gt;\n\n&lt;p&gt;Paper: &lt;a href=\"https://arxiv.org/abs/2507.08128\"&gt;https://arxiv.org/abs/2507.08128&lt;/a&gt;\nVoice-chat finetune: &lt;a href=\"https://huggingface.co/nvidia/audio-flamingo-3-chat\"&gt;https://huggingface.co/nvidia/audio-flamingo-3-chat&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/nvidia/audio-flamingo-3",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?auto=webp&amp;s=b761cba7c3002de5cc09bc2aa3e367a07fde1f1e",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f082162e7876351e6a01bc3afa7b6cd69a0c79e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=86a2cf2589774fb2ca8180c2e526be9d4cd4bd04",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8e4544fbcf82c91a69b0577016983a6985b755c8",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d904bc28461c7ba9d24fbdf4cac5832b8e4b862",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d94c311df14dd7728d4e405ada02529c1f99e4ac",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=878903c73b68d742900684e628d41f580d6f9735",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m7fb78",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7fb78/nvidiaaudioflamingo3/",
          "stickied": false,
          "url": "https://huggingface.co/nvidia/audio-flamingo-3",
          "subreddit_subscribers": 504253,
          "created_utc": 1753291299,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Why they be slacking on local llama and LLM generally? They big nation, clever, work hard. Many robots. No LLM? Why?",
          "author_fullname": "t2_16rs3mlp2i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Where is Japan?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7d9d9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 120,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 120,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753286646,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why they be slacking on local llama and LLM generally? They big nation, clever, work hard. Many robots. No LLM? Why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7d9d9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ethereel1",
          "discussion_type": null,
          "num_comments": 177,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7d9d9/where_is_japan/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7d9d9/where_is_japan/",
          "subreddit_subscribers": 504253,
          "created_utc": 1753286646,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}