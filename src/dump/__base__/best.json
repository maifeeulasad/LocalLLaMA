{
  "kind": "Listing",
  "data": {
    "after": "t3_1m9xwo5",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_aa96f",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Suprise suprise!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 128,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1majemr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 102,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 102,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/9iCds3N2k2-ZYt14S0ZFMUsTtjzG3w6e835k2IuCnE8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753613719,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/k64e9lwtdeff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/k64e9lwtdeff1.png?auto=webp&amp;s=6d114b029fdaaee896bc4e5d5a7d43d206e39297",
                  "width": 845,
                  "height": 774
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/k64e9lwtdeff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=654734e23bb5447e379cf550989c3fbafc64f227",
                    "width": 108,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/k64e9lwtdeff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=524ffaaa581fe97e1e7a9cc6c305b3015e336295",
                    "width": 216,
                    "height": 197
                  },
                  {
                    "url": "https://preview.redd.it/k64e9lwtdeff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c3db32c475829f2bbd9b7113e823dc70bba23038",
                    "width": 320,
                    "height": 293
                  },
                  {
                    "url": "https://preview.redd.it/k64e9lwtdeff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d09af7edf96adcd3793cd8970c2cab58d53352b",
                    "width": 640,
                    "height": 586
                  }
                ],
                "variants": {},
                "id": "7LqJSDe2PuCHTRar6CZQ7nrOdJ1amozrq-VVdgoKEEo"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1majemr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GoodGuyLafarge",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1majemr/suprise_suprise/",
          "stickied": false,
          "url": "https://i.redd.it/k64e9lwtdeff1.png",
          "subreddit_subscribers": 505252,
          "created_utc": 1753613719,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3l9wjlq0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tencent releases Hunyuan3D World Model 1.0 - first open-source 3D world generation model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mab2i2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 420,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 420,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753583285,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/TencentHunyuan/status/1949288986192834718",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mab2i2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pseudoreddituser",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mab2i2/tencent_releases_hunyuan3d_world_model_10_first/",
          "stickied": false,
          "url": "https://x.com/TencentHunyuan/status/1949288986192834718",
          "subreddit_subscribers": 505252,
          "created_utc": 1753583285,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=123b8a4dcb375d14ed980880bb55304b8133c96f\n\nhttps://preview.redd.it/5t6qakxf6eff1.png?width=2004&amp;format=png&amp;auto=webp&amp;s=02c934cd0bc6fbd428a6f1c46e1214db800d83c3\n\n[https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct](https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct)\n\n[https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker](https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker)",
          "author_fullname": "t2_xdw24u3am",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A new 21B-A3B model that can run 30 token/s on i9 CPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "5t6qakxf6eff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 42,
                  "x": 108,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b7aee7f5ee4d636cc350980d4a2d402a51990f3"
                },
                {
                  "y": 84,
                  "x": 216,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=81d1bfcb2fbab04e033467fedce9b775306d0816"
                },
                {
                  "y": 125,
                  "x": 320,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=38b4e75cce0962953603e8bda734369a2c829748"
                },
                {
                  "y": 251,
                  "x": 640,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=de21ca82e374cabea65b7bd4f9ca7695eb3ac76c"
                },
                {
                  "y": 376,
                  "x": 960,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3f335ee62d4874ca26b9e6e0098a8851c4538afa"
                },
                {
                  "y": 423,
                  "x": 1080,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=95699afb27e3aa99fcc75ef6aa805969b9e66cf1"
                }
              ],
              "s": {
                "y": 786,
                "x": 2004,
                "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=2004&amp;format=png&amp;auto=webp&amp;s=02c934cd0bc6fbd428a6f1c46e1214db800d83c3"
              },
              "id": "5t6qakxf6eff1"
            },
            "e54liysd6eff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a7f5f991e9ce2c009935e1248f2782a5bdd1a201"
                },
                {
                  "y": 120,
                  "x": 216,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b615b9ee2e041c4be64619a07d886e58e3e07f63"
                },
                {
                  "y": 178,
                  "x": 320,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=805558e5004842b4caccc53ce524f721763d8ea2"
                },
                {
                  "y": 356,
                  "x": 640,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b5259fd259cf94ad371181ed4eb21af07c6bc94"
                },
                {
                  "y": 534,
                  "x": 960,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=24b113fb84316d200bddffca613f26e4f2877925"
                },
                {
                  "y": 601,
                  "x": 1080,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b39d6ef3edba22d3d5331aaf9d2220075d801bcc"
                }
              ],
              "s": {
                "y": 601,
                "x": 1080,
                "u": "https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=123b8a4dcb375d14ed980880bb55304b8133c96f"
              },
              "id": "e54liysd6eff1"
            }
          },
          "name": "t3_1maipzo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 77,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 77,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=4ac39f1493be4418ea9b0513e0ab785db9d728b9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753611112,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=123b8a4dcb375d14ed980880bb55304b8133c96f\"&gt;https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=123b8a4dcb375d14ed980880bb55304b8133c96f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5t6qakxf6eff1.png?width=2004&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02c934cd0bc6fbd428a6f1c46e1214db800d83c3\"&gt;https://preview.redd.it/5t6qakxf6eff1.png?width=2004&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02c934cd0bc6fbd428a6f1c46e1214db800d83c3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct\"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker\"&gt;https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?auto=webp&amp;s=de62685b703b746715dcc2b0df2aefcbbc3e3737",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33918a5d809431c198816a64f4512804c3bb5409",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4045a881ccf50c282b74da08c7a22d9d97f0821b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=39771fc11b97b39eae914bdee8e861864005e9bf",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb9a721495b0f0942a0fc51a0050cda33d2ef637",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a9c25310dbb6785760eb6704b72c9666cd180f6",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4d2ffd21d7412bbd7a2765d0696394d9fd40dc27",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1maipzo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BreakfastFriendly728",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maipzo/a_new_21ba3b_model_that_can_run_30_tokens_on_i9/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maipzo/a_new_21ba3b_model_that_can_run_30_tokens_on_i9/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753611112,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_4ou3rslj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wan 2.2 coming out Monday July 28th",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mae4yz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 93,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 93,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/wiS40lScO4Tz3DVaIJvgYMaDlS_RixRf5GYhbQQ4Q8Y.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753593470,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6fhk0wjppcff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?auto=webp&amp;s=fec6294a55a3e104e1bb18786c446c76d3380ada",
                  "width": 1320,
                  "height": 738
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b03db1c6122c5627833ddc9e2fdbcb6d6f7ed744",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c1ac1b73bcaf847b2fa4c394ce28ce658e1f328a",
                    "width": 216,
                    "height": 120
                  },
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=95e50168620f70ce8176a420355e73393cd86fb4",
                    "width": 320,
                    "height": 178
                  },
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5dc0d6b8afe675c915a40e90b3006a47c5764036",
                    "width": 640,
                    "height": 357
                  },
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=512e28d76ac154808029aa9902dd873b5432ddfa",
                    "width": 960,
                    "height": 536
                  },
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=67b6d4b9b8c656f0cbf08017fc3c7809884c53aa",
                    "width": 1080,
                    "height": 603
                  }
                ],
                "variants": {},
                "id": "3150d24SFUF3S7LHu63_t91aK5WawBeVinxIS7aSy80"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mae4yz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Comed_Ai_n",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mae4yz/wan_22_coming_out_monday_july_28th/",
          "stickied": false,
          "url": "https://i.redd.it/6fhk0wjppcff1.jpeg",
          "subreddit_subscribers": 505252,
          "created_utc": 1753593470,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/8l7johy2cbff1.png?width=592&amp;format=png&amp;auto=webp&amp;s=23ee932b7f25dee78f023f6291b11a4e2d9b43fc\n\nSam Altman admitting that ChatGPT will never protect your privacy ",
          "author_fullname": "t2_dmaijfods",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local LLM is more important than ever",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8l7johy2cbff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 141,
                  "x": 108,
                  "u": "https://preview.redd.it/8l7johy2cbff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=872620916c84058b05b19836e3dbac1eec6b6209"
                },
                {
                  "y": 283,
                  "x": 216,
                  "u": "https://preview.redd.it/8l7johy2cbff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2d2d77bed5f2cfc487be789d312ecff0ac96f0b0"
                },
                {
                  "y": 419,
                  "x": 320,
                  "u": "https://preview.redd.it/8l7johy2cbff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=72cbdb4e09ae54ebec08d464c14350211c34aee5"
                }
              ],
              "s": {
                "y": 776,
                "x": 592,
                "u": "https://preview.redd.it/8l7johy2cbff1.png?width=592&amp;format=png&amp;auto=webp&amp;s=23ee932b7f25dee78f023f6291b11a4e2d9b43fc"
              },
              "id": "8l7johy2cbff1"
            }
          },
          "name": "t3_1ma8yua",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 199,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 199,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/uE7wV-zcvDiT-3iVUwWcdDhkrRrgCwBg-OrGsKIspS0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753576857,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/8l7johy2cbff1.png?width=592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23ee932b7f25dee78f023f6291b11a4e2d9b43fc\"&gt;https://preview.redd.it/8l7johy2cbff1.png?width=592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23ee932b7f25dee78f023f6291b11a4e2d9b43fc&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sam Altman admitting that ChatGPT will never protect your privacy &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ma8yua",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NeedleworkerDull7886",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma8yua/local_llm_is_more_important_than_ever/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma8yua/local_llm_is_more_important_than_ever/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753576857,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What are people's thoughts on Sapient Intelligence's recent paper? Apparently, they developed a new architecture called Hierarchical Reasoning Model (HRM) that performs as well as LLMs on complex reasoning tasks with significantly less training samples and examples. ",
          "author_fullname": "t2_98ouo03z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New AI architecture delivers 100x faster reasoning than LLMs with just 1,000 training examples",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma6b57",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 273,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 273,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=c61601b7d043fcb4533a5e8eaf63e29936b16a11",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753569167,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "venturebeat.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are people&amp;#39;s thoughts on Sapient Intelligence&amp;#39;s recent paper? Apparently, they developed a new architecture called Hierarchical Reasoning Model (HRM) that performs as well as LLMs on complex reasoning tasks with significantly less training samples and examples. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://venturebeat.com/ai/new-ai-architecture-delivers-100x-faster-reasoning-than-llms-with-just-1000-training-examples/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM.png?auto=webp&amp;s=d3dfd5864403531c1acea95e81b49f3b5b256f90",
                  "width": 986,
                  "height": 553
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a6925cbb0e94e0ba7147f6bddbacbfbacabb3ba",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb0df9d130eced3444802c4c7335e42bcd1de23d",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d14873f56f0a1c5c45686ea214b463e961c58fa5",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://external-preview.redd.it/eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=79243a5ca169d7972acf9a3bdc240df386129d25",
                    "width": 640,
                    "height": 358
                  },
                  {
                    "url": "https://external-preview.redd.it/eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e514262e4886ce2271151b95e18d9cf4cb3e627a",
                    "width": 960,
                    "height": 538
                  }
                ],
                "variants": {},
                "id": "eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1ma6b57",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Accomplished-Copy332",
          "discussion_type": null,
          "num_comments": 68,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma6b57/new_ai_architecture_delivers_100x_faster/",
          "stickied": false,
          "url": "https://venturebeat.com/ai/new-ai-architecture-delivers-100x-faster-reasoning-than-llms-with-just-1000-training-examples/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753569167,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Thank you so much getting ggufs baked and delivered. It must have been busy last few days. How is it looking behind the scenes?\n\nEdit yeah and llama.cpp team",
          "author_fullname": "t2_1vmv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Appreciation Post - Thank you unsloth team, and thank you bartowski",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma08e0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 571,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 571,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": 1753554026,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753553653,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thank you so much getting ggufs baked and delivered. It must have been busy last few days. How is it looking behind the scenes?&lt;/p&gt;\n\n&lt;p&gt;Edit yeah and llama.cpp team&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1ma08e0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fuutott",
          "discussion_type": null,
          "num_comments": 58,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma08e0/appreciation_post_thank_you_unsloth_team_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma08e0/appreciation_post_thank_you_unsloth_team_and/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753553653,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Around a year and a half on from my post about 24GB vs 48GB VRAM, I personally find that the scene has changed a lot in terms of what sizes of models are popularly available and used.\n\nBack then, 48GB VRAM for 70B models at 4BPW was more or less the gold standard for local inference. This is back when The Bloke was still releasing quants and Midnight Miqu was the holy grail for creative writing.\n\nThis is practically ancient history in the LLM space, but some of you surely recall this period just as well as I do.\n\nThere is now a much greater diversity of model parameter sizes available in terms of open-weights models, and the frontier of performance has continually been pushed forward. That being said, I find that newer open-weights models are either narrower in scope and smaller in parameter size, or generally much more competent but prohibitively large to be run locally for most.\n\nDeepseek R1 and V3 are good examples of this, as is the newer Kimi K2. At 671B parameters and 1T parameters, respectively, I think it's fair to assume that most users of these models are doing so via API rather than hosting locally. Even with an MOE architecture, they are simply too large to be hosted locally at reasonable speeds by enthusiasts. This is reminiscent of the situation with LLaMA 405B, in my opinion.\n\nWith the launch of LLaMA 4 being a bust and Qwen3 only going up to 32B in terms of dense models, perhaps there just hasn't been a solid 70/72B model released in quite some time? The last model that really made a splash in this parameter range was Qwen2.5 72B, and that's a long while ago...\n\nI also find that most finetunes are still working with L3.3 as a base, which speaks to the recent lack of available models in this parameter range.\n\nThis does leave 48GB VRAM in a bit of a weird spot - too large for the small/medium-models, and too small for the *really* large models. Perhaps a migration to a general preference for an MOE architecture is a natural consequence of the ever-increasing demand for VRAM and compute, or this is just a temporary lull in the output of the major labs training open-weights models which will come to pass eventually.\n\nI suppose I'm partially reminiscing, and partially trying to start a dialogue on where the \"sweet spot\" for local models is nowadays. It would appear that the age of 70B/4BPW/48GB VRAM being the consensus has come to an end.\n\nAre \\~70B dense models going out of fashion for good? Or do you think this is just a temporary lull amidst a general move towards preference for MOE architectures?\n\n**EDIT:** If very large MOE models will be the norm moving forward, perhaps building a server motherboard with large amounts of fast multi-channel system RAM is preferable to continually adding consumer GPUs to accrue larger amounts of VRAM for local inference (seeing as the latter is an approach that is primarily aimed at dense models that fit entirely into VRAM).",
          "author_fullname": "t2_cyw8u51dt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are ~70B Models Going Out of Fashion?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1majfwi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753617926,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753613850,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Around a year and a half on from my post about 24GB vs 48GB VRAM, I personally find that the scene has changed a lot in terms of what sizes of models are popularly available and used.&lt;/p&gt;\n\n&lt;p&gt;Back then, 48GB VRAM for 70B models at 4BPW was more or less the gold standard for local inference. This is back when The Bloke was still releasing quants and Midnight Miqu was the holy grail for creative writing.&lt;/p&gt;\n\n&lt;p&gt;This is practically ancient history in the LLM space, but some of you surely recall this period just as well as I do.&lt;/p&gt;\n\n&lt;p&gt;There is now a much greater diversity of model parameter sizes available in terms of open-weights models, and the frontier of performance has continually been pushed forward. That being said, I find that newer open-weights models are either narrower in scope and smaller in parameter size, or generally much more competent but prohibitively large to be run locally for most.&lt;/p&gt;\n\n&lt;p&gt;Deepseek R1 and V3 are good examples of this, as is the newer Kimi K2. At 671B parameters and 1T parameters, respectively, I think it&amp;#39;s fair to assume that most users of these models are doing so via API rather than hosting locally. Even with an MOE architecture, they are simply too large to be hosted locally at reasonable speeds by enthusiasts. This is reminiscent of the situation with LLaMA 405B, in my opinion.&lt;/p&gt;\n\n&lt;p&gt;With the launch of LLaMA 4 being a bust and Qwen3 only going up to 32B in terms of dense models, perhaps there just hasn&amp;#39;t been a solid 70/72B model released in quite some time? The last model that really made a splash in this parameter range was Qwen2.5 72B, and that&amp;#39;s a long while ago...&lt;/p&gt;\n\n&lt;p&gt;I also find that most finetunes are still working with L3.3 as a base, which speaks to the recent lack of available models in this parameter range.&lt;/p&gt;\n\n&lt;p&gt;This does leave 48GB VRAM in a bit of a weird spot - too large for the small/medium-models, and too small for the &lt;em&gt;really&lt;/em&gt; large models. Perhaps a migration to a general preference for an MOE architecture is a natural consequence of the ever-increasing demand for VRAM and compute, or this is just a temporary lull in the output of the major labs training open-weights models which will come to pass eventually.&lt;/p&gt;\n\n&lt;p&gt;I suppose I&amp;#39;m partially reminiscing, and partially trying to start a dialogue on where the &amp;quot;sweet spot&amp;quot; for local models is nowadays. It would appear that the age of 70B/4BPW/48GB VRAM being the consensus has come to an end.&lt;/p&gt;\n\n&lt;p&gt;Are ~70B dense models going out of fashion for good? Or do you think this is just a temporary lull amidst a general move towards preference for MOE architectures?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; If very large MOE models will be the norm moving forward, perhaps building a server motherboard with large amounts of fast multi-channel system RAM is preferable to continually adding consumer GPUs to accrue larger amounts of VRAM for local inference (seeing as the latter is an approach that is primarily aimed at dense models that fit entirely into VRAM).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1majfwi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HvskyAI",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753613850,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "PowerInfer/SmallThinker-21BA3B-Instruct  Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maipjy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=4ac39f1493be4418ea9b0513e0ab785db9d728b9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753611065,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?auto=webp&amp;s=de62685b703b746715dcc2b0df2aefcbbc3e3737",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33918a5d809431c198816a64f4512804c3bb5409",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4045a881ccf50c282b74da08c7a22d9d97f0821b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=39771fc11b97b39eae914bdee8e861864005e9bf",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb9a721495b0f0942a0fc51a0050cda33d2ef637",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a9c25310dbb6785760eb6704b72c9666cd180f6",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4d2ffd21d7412bbd7a2765d0696394d9fd40dc27",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1maipjy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maipjy/powerinfersmallthinker21ba3binstruct_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct",
          "subreddit_subscribers": 505252,
          "created_utc": 1753611065,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_on5es7pe3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone else starting to feel this way when a new model 'breaks the charts' but need like 15k thinking tokens to do it?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma2j62",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": "#bbbdbf",
          "ups": 203,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 203,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=a8e13c2b8d5f080234acafb6a37205454684d6ef",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753559381,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "c.tenor.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://c.tenor.com/65jRkhUA2MIAAAAd/tenor.gif",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?format=png8&amp;s=5b30e772f97a59958cc821e357627e74142630ea",
                  "width": 640,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=c2a369b7162c594e361e845f2fc32276e74a79c4",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=d55014d3cfe062800092325bed67d7a4a27d4a64",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=94715de3b1ed3900039c057a42a57f2376a29fb6",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=309a72cf26ac99e73a5c3e6d981a001d0433be9f",
                    "width": 640,
                    "height": 640
                  }
                ],
                "variants": {
                  "gif": {
                    "source": {
                      "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?s=9904c2f61fbc48bb11103a49b05a915bba887ea4",
                      "width": 640,
                      "height": 640
                    },
                    "resolutions": [
                      {
                        "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=108&amp;crop=smart&amp;s=0fe62f124941f6a41a792f4434c13e7e6e5789b1",
                        "width": 108,
                        "height": 108
                      },
                      {
                        "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=216&amp;crop=smart&amp;s=892fad93a240f8be12ccad1ca387d43e89abdac5",
                        "width": 216,
                        "height": 216
                      },
                      {
                        "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=320&amp;crop=smart&amp;s=d75cbabb532d691ef6fef62cbcdbb65bd4525258",
                        "width": 320,
                        "height": 320
                      },
                      {
                        "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=640&amp;crop=smart&amp;s=a4df17ec2e79efaeb8495d39a06eefe4fc80e5a6",
                        "width": 640,
                        "height": 640
                      }
                    ]
                  },
                  "mp4": {
                    "source": {
                      "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?format=mp4&amp;s=113b9fc7a8ecd3c8ee77d2687be6a164e2b56f6a",
                      "width": 640,
                      "height": 640
                    },
                    "resolutions": [
                      {
                        "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=108&amp;format=mp4&amp;s=7675756c21aa354214a6202383805f091c91bd0e",
                        "width": 108,
                        "height": 108
                      },
                      {
                        "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=216&amp;format=mp4&amp;s=131afb78edd87185c61d77d827ff3016ebd7e694",
                        "width": 216,
                        "height": 216
                      },
                      {
                        "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=320&amp;format=mp4&amp;s=8527775b264b0f47ea74c327d982397d2258d99d",
                        "width": 320,
                        "height": 320
                      },
                      {
                        "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=640&amp;format=mp4&amp;s=005a8d43fdcb337f9c8667ed0d27ed4f65933682",
                        "width": 640,
                        "height": 640
                      }
                    ]
                  }
                },
                "id": "FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1ma2j62",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ForsookComparison",
          "discussion_type": null,
          "num_comments": 61,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1ma2j62/anyone_else_starting_to_feel_this_way_when_a_new/",
          "stickied": false,
          "url": "https://c.tenor.com/65jRkhUA2MIAAAAd/tenor.gif",
          "subreddit_subscribers": 505252,
          "created_utc": 1753559381,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_8bwjj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Me after getting excited by a new model release and checking on Hugging Face if I can run it locally.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 128,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9rqxa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 768,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 768,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3maTLMR0SbyBvb7hQx7_PpxeyyK-KvUWrbbwbMU4Q3I.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753531781,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/0tnbd1i9m7ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/0tnbd1i9m7ff1.png?auto=webp&amp;s=947ab42af41c4c1311628c38d1cd8412b2b60729",
                  "width": 738,
                  "height": 675
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/0tnbd1i9m7ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6793b3d09acfa0d71fd64ec0893a75e4685dc3e5",
                    "width": 108,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/0tnbd1i9m7ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=159c33df3d3950e75292f85013407fff7b389f04",
                    "width": 216,
                    "height": 197
                  },
                  {
                    "url": "https://preview.redd.it/0tnbd1i9m7ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=023fa90959cadf44a37e2c2af52b2f17e81a2692",
                    "width": 320,
                    "height": 292
                  },
                  {
                    "url": "https://preview.redd.it/0tnbd1i9m7ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c3c6aab9c8b44a1cec98dbeca3972f5d0885fd8",
                    "width": 640,
                    "height": 585
                  }
                ],
                "variants": {},
                "id": "6H_reI7bQd-xJWBvmKQHOgTWZ6lYDuz0OFnnivjFsfQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9rqxa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "alew3",
          "discussion_type": null,
          "num_comments": 144,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9rqxa/me_after_getting_excited_by_a_new_model_release/",
          "stickied": false,
          "url": "https://i.redd.it/0tnbd1i9m7ff1.png",
          "subreddit_subscribers": 505252,
          "created_utc": 1753531781,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My own personal desktop workstation.\n\nSpecs:\n\n1. GPUs -- Quad 4090 48GB (Roughly 3200 USD each, 450 watts max energy use)\n2. CPUs -- Intel 6530 32 Cores Emerald Rapids (1350 USD)\n3. Motherboard -- Tyan S5652-2T (836 USD)\n4. RAM -- eight sticks of M321RYGA0PB0-CWMKH 96GB (768GB total, 470 USD per stick)\n5. Case -- Jonsbo N5 (160 USD)\n6. PSU -- Great Wall fully modular 2600 watt with quad 12VHPWR plugs (326 USD)\n7. CPU cooler -- coolserver M98 (40 USD)\n8. SSD -- Western Digital 4TB SN850X (290 USD)\n9. Case fans -- Three fans, Liquid Crystal Polymer Huntbow ProArtist H14PE (21 USD per fan)\n10. HDD -- Eight 20 TB Seagate (pending delivery)",
          "author_fullname": "t2_nm52x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Quad 4090 48GB + 768GB DDR5 in Jonsbo N5 case",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "f9ga1xe6a8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/f9ga1xe6a8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=92260477ef81ca48784f83804fbab76670633c70"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/f9ga1xe6a8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=44e07c0f9267c487ab805b7c8f23012423884c9e"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/f9ga1xe6a8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6cec7d5641f919437841cee8246fa1632c796885"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/f9ga1xe6a8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6ea04e45eb48af451532a1df41e586b696ae9deb"
                }
              ],
              "s": {
                "y": 1200,
                "x": 900,
                "u": "https://preview.redd.it/f9ga1xe6a8ff1.png?width=900&amp;format=png&amp;auto=webp&amp;s=dbaeb78766767a0adc9fb9eb3a3dc33bda7e5b89"
              },
              "id": "f9ga1xe6a8ff1"
            },
            "icwwq04aa8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/icwwq04aa8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff2342618253487d74ef4c4d5ec067862e9b6ac6"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/icwwq04aa8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ad674b7840f7dbb23233f41a9ea87a8e9973c787"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/icwwq04aa8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7816e33734ea243d88d2ca4a7d35908d7e4c3739"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/icwwq04aa8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6a06465ac67e7a630374ba51d596e35375f4959b"
                }
              ],
              "s": {
                "y": 1200,
                "x": 900,
                "u": "https://preview.redd.it/icwwq04aa8ff1.png?width=900&amp;format=png&amp;auto=webp&amp;s=bb9d49aa42cfcfa194fd014a00afb8636ea3a677"
              },
              "id": "icwwq04aa8ff1"
            },
            "wi7xh5jda8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/wi7xh5jda8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=107d0bf59a8e453641317502b3ff29ce0b66104c"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/wi7xh5jda8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6a60a88c1e4a7a6cbdd28fadcf775f91aaf10279"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/wi7xh5jda8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dae632cdd76f18d1391c00d9ad3516df29dbe6eb"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/wi7xh5jda8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e1c75a5eefce2d54bc0d2ba5d559e597f076b20"
                }
              ],
              "s": {
                "y": 1200,
                "x": 900,
                "u": "https://preview.redd.it/wi7xh5jda8ff1.png?width=900&amp;format=png&amp;auto=webp&amp;s=d57d9c0f8ae81486eaa612ba84bbf7a94cba5985"
              },
              "id": "wi7xh5jda8ff1"
            }
          },
          "name": "t3_1m9uwxg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 480,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "f9ga1xe6a8ff1",
                "id": 714450146
              },
              {
                "media_id": "icwwq04aa8ff1",
                "id": 714450147
              },
              {
                "media_id": "wi7xh5jda8ff1",
                "id": 714450148
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 480,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ValHYB67eTbAZL-7K0hFnR6LkMb2R6iJ0iuJtur8Ksg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753540660,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My own personal desktop workstation.&lt;/p&gt;\n\n&lt;p&gt;Specs:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;GPUs -- Quad 4090 48GB (Roughly 3200 USD each, 450 watts max energy use)&lt;/li&gt;\n&lt;li&gt;CPUs -- Intel 6530 32 Cores Emerald Rapids (1350 USD)&lt;/li&gt;\n&lt;li&gt;Motherboard -- Tyan S5652-2T (836 USD)&lt;/li&gt;\n&lt;li&gt;RAM -- eight sticks of M321RYGA0PB0-CWMKH 96GB (768GB total, 470 USD per stick)&lt;/li&gt;\n&lt;li&gt;Case -- Jonsbo N5 (160 USD)&lt;/li&gt;\n&lt;li&gt;PSU -- Great Wall fully modular 2600 watt with quad 12VHPWR plugs (326 USD)&lt;/li&gt;\n&lt;li&gt;CPU cooler -- coolserver M98 (40 USD)&lt;/li&gt;\n&lt;li&gt;SSD -- Western Digital 4TB SN850X (290 USD)&lt;/li&gt;\n&lt;li&gt;Case fans -- Three fans, Liquid Crystal Polymer Huntbow ProArtist H14PE (21 USD per fan)&lt;/li&gt;\n&lt;li&gt;HDD -- Eight 20 TB Seagate (pending delivery)&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m9uwxg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m9uwxg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "44seconds",
          "discussion_type": null,
          "num_comments": 149,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9uwxg/quad_4090_48gb_768gb_ddr5_in_jonsbo_n5_case/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m9uwxg",
          "subreddit_subscribers": 505252,
          "created_utc": 1753540660,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I often see products put out by makers in China posted here as \"China does X\", either with or sometimes even without the maker being mentioned. Some examples:\n\n* [Is China the only hope for factual models?](https://www.reddit.com/r/LocalLLaMA/comments/1m9tyg9/is_china_the_only_hope_for_factual_models/)\n* [China launches its first 6nm GPUs for gaming and AI](https://www.reddit.com/r/LocalLLaMA/comments/1m9sejp/china_launches_its_first_6nm_gpus_for_gaming_ai/)\n* [Looks like China is the one playing 5D chess](https://www.reddit.com/r/LocalLLaMA/comments/1ka3hlm/looks_like_china_is_the_one_playing_5d_chess/)\n* [China has delivered yet again](https://www.reddit.com/r/LocalLLaMA/comments/1kbneq2/china_has_delivered_yet_again/)\n* [China is leading open-source](https://www.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/)\n* [China's Huawei develops new AI chip](https://www.reddit.com/r/LocalLLaMA/comments/1kb59p2/chinas_huawei_develops_new_ai_chip_seeking_to/)\n* [Chinese researchers find multimodal LLMs develop ...](https://www.reddit.com/r/LocalLLaMA/comments/1lalyy5/chinese_researchers_find_multimodal_llms_develop/)\n\nWhereas U.S. makers are always named: Anthropic, OpenAI, Meta, etc.. U.S. researchers are also always named, but research papers from a lab in China is posted as \"Chinese researchers ...\".\n\nHow do Chinese makers and researchers feel about this? As a researcher myself, I would *hate* if my work was lumped into the output of an entire country of billions and not attributed to *me* specifically.\n\nSame if someone referred to my company as \"American Company\".\n\nI think we, as a community, could do a better job naming names and giving credit to the makers. We know Sam Altman, Ilya Sutskever, Jensen Huang, etc. but I rarely see Liang Wenfeng mentioned here.",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Crediting Chinese makers by name",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9xw4c",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": "transparent",
          "subreddit_type": "public",
          "ups": 327,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 327,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753548204,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753547946,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I often see products put out by makers in China posted here as &amp;quot;China does X&amp;quot;, either with or sometimes even without the maker being mentioned. Some examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1m9tyg9/is_china_the_only_hope_for_factual_models/\"&gt;Is China the only hope for factual models?&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1m9sejp/china_launches_its_first_6nm_gpus_for_gaming_ai/\"&gt;China launches its first 6nm GPUs for gaming and AI&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1ka3hlm/looks_like_china_is_the_one_playing_5d_chess/\"&gt;Looks like China is the one playing 5D chess&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kbneq2/china_has_delivered_yet_again/\"&gt;China has delivered yet again&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/\"&gt;China is leading open-source&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kb59p2/chinas_huawei_develops_new_ai_chip_seeking_to/\"&gt;China&amp;#39;s Huawei develops new AI chip&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lalyy5/chinese_researchers_find_multimodal_llms_develop/\"&gt;Chinese researchers find multimodal LLMs develop ...&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Whereas U.S. makers are always named: Anthropic, OpenAI, Meta, etc.. U.S. researchers are also always named, but research papers from a lab in China is posted as &amp;quot;Chinese researchers ...&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;How do Chinese makers and researchers feel about this? As a researcher myself, I would &lt;em&gt;hate&lt;/em&gt; if my work was lumped into the output of an entire country of billions and not attributed to &lt;em&gt;me&lt;/em&gt; specifically.&lt;/p&gt;\n\n&lt;p&gt;Same if someone referred to my company as &amp;quot;American Company&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I think we, as a community, could do a better job naming names and giving credit to the makers. We know Sam Altman, Ilya Sutskever, Jensen Huang, etc. but I rarely see Liang Wenfeng mentioned here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9xw4c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 75,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1m9xw4c/crediting_chinese_makers_by_name/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9xw4c/crediting_chinese_makers_by_name/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753547946,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I spent the weekend analyzing this open-source PyTorch implementation of Google's [CRISP paper (arXiv:2505.11471)](https://arxiv.org/pdf/2505.11471). The repository provides a direct, hands-on comparison between CRISP's in-training clustering and the more traditional post-hoc approach.\n\nFor context, the core problem with multi-vector models (e.g., ColBERT) is their massive index size. The common solution is to cluster embeddings *after* training (post-hoc), but this is an imperfect patch. CRISP argues for integrating clustering *during* training to force the model to learn inherently \"clusterable\" representations.\n\nThe repository sets up a clean head-to-head experiment to test that claim. Here's a breakdown of the results from its built-in pipeline.\n\n[https://github.com/sigridjineth/crisp-py](https://github.com/sigridjineth/crisp-py)\n\nI tried few experiments with minilm-l6-v2 in Macbook Pro and found that CRISP-tuned model assigns a significantly higher similarity score to the correct document.",
          "author_fullname": "t2_iu3wj4b6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I tried implementing the CRISP paper from Google Deepmind in Python",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maixye",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753612355,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753611989,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I spent the weekend analyzing this open-source PyTorch implementation of Google&amp;#39;s &lt;a href=\"https://arxiv.org/pdf/2505.11471\"&gt;CRISP paper (arXiv:2505.11471)&lt;/a&gt;. The repository provides a direct, hands-on comparison between CRISP&amp;#39;s in-training clustering and the more traditional post-hoc approach.&lt;/p&gt;\n\n&lt;p&gt;For context, the core problem with multi-vector models (e.g., ColBERT) is their massive index size. The common solution is to cluster embeddings &lt;em&gt;after&lt;/em&gt; training (post-hoc), but this is an imperfect patch. CRISP argues for integrating clustering &lt;em&gt;during&lt;/em&gt; training to force the model to learn inherently &amp;quot;clusterable&amp;quot; representations.&lt;/p&gt;\n\n&lt;p&gt;The repository sets up a clean head-to-head experiment to test that claim. Here&amp;#39;s a breakdown of the results from its built-in pipeline.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/sigridjineth/crisp-py\"&gt;https://github.com/sigridjineth/crisp-py&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I tried few experiments with minilm-l6-v2 in Macbook Pro and found that CRISP-tuned model assigns a significantly higher similarity score to the correct document.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1maixye",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Rub1689",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maixye/i_tried_implementing_the_crisp_paper_from_google/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maixye/i_tried_implementing_the_crisp_paper_from_google/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753611989,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Its great! It's a clear step above Qwen3 32b imo. Id recommend trying it out \n\nMy experience with it:\n- it generates far less \"slop\" than Qwen models\n- it handles long context really well \n- it easily handles trick questions like \"What should be the punishment for looking at your opponent's board in chess?\"\n- handled all my coding questions really well\n- has a weird ass architecture where some layers dont have attention tensors which messed up llama.cpp tensor split allocation, but was pretty easy to overcome \n\nMy driver for a long time was Qwen3 32b FP16 but this model at Q8 has been a massive step up for me and ill be using it going forward. \n\nAnyone else tried this bad boy out?",
          "author_fullname": "t2_o015g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone else been using the new nvidia/Llama-3_3-Nemotron-Super-49B-v1_5 model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1madjq6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 25,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753591386,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Its great! It&amp;#39;s a clear step above Qwen3 32b imo. Id recommend trying it out &lt;/p&gt;\n\n&lt;p&gt;My experience with it:\n- it generates far less &amp;quot;slop&amp;quot; than Qwen models\n- it handles long context really well \n- it easily handles trick questions like &amp;quot;What should be the punishment for looking at your opponent&amp;#39;s board in chess?&amp;quot;\n- handled all my coding questions really well\n- has a weird ass architecture where some layers dont have attention tensors which messed up llama.cpp tensor split allocation, but was pretty easy to overcome &lt;/p&gt;\n\n&lt;p&gt;My driver for a long time was Qwen3 32b FP16 but this model at Q8 has been a massive step up for me and ill be using it going forward. &lt;/p&gt;\n\n&lt;p&gt;Anyone else tried this bad boy out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1madjq6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kevin_1994",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1madjq6/anyone_else_been_using_the_new_nvidiallama3/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1madjq6/anyone_else_been_using_the_new_nvidiallama3/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753591386,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Someone hacked our Portkey, and Okay, this is wild: our Portkey logs just coughed up the entire system prompt + live session history for Claude Code ",
          "author_fullname": "t2_xdvmrlo3w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Claude Code Full System prompt",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma2ayu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 112,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 112,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=4775ce8043fee3c6d226684a92e2c5b5b1eb40be",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753558804,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Someone hacked our Portkey, and Okay, this is wild: our Portkey logs just coughed up the entire system prompt + live session history for Claude Code &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/kn1026/cc/blob/main/claudecode.md",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?auto=webp&amp;s=1fdbbc07487a7cd64cd35156e5af200435d6fec0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7af339a9d5dde7cda6aed95a03bb236b15425697",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b315859b94be06b87be20f5eb10d7c1e0cc83d9f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e804d800b45ef5417f5c93802b2efe543b8b62a7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d6821d042cf6adf227a28313e944ec49f3976b9e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=35f9681fe0ac1eaadc79cf37bbd0223b5dff7ec6",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=713e2c941ea786d14da1c8a3a519b0a5f6979ab2",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ma2ayu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Haunting_Forever_243",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma2ayu/claude_code_full_system_prompt/",
          "stickied": false,
          "url": "https://github.com/kn1026/cc/blob/main/claudecode.md",
          "subreddit_subscribers": 505252,
          "created_utc": 1753558804,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Demo of Video &amp; Image Generation Model Wan 2.2: https://x.com/Alibaba_Wan/status/1948436898965586297?t=mUt2wu38SSM4q77WDHjh2w&amp;s=19",
          "author_fullname": "t2_1e1w1ul46b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen's Wan 2.2 is coming soon",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9s2nt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 418,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 418,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/a_RYuyJg0Y2qxFSvHMn6iMkzVUZShPN9aIW-PxpdVrc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753532816,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Demo of Video &amp;amp; Image Generation Model Wan 2.2: &lt;a href=\"https://x.com/Alibaba_Wan/status/1948436898965586297?t=mUt2wu38SSM4q77WDHjh2w&amp;amp;s=19\"&gt;https://x.com/Alibaba_Wan/status/1948436898965586297?t=mUt2wu38SSM4q77WDHjh2w&amp;amp;s=19&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/mtc9shncp7ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/mtc9shncp7ff1.jpeg?auto=webp&amp;s=13038b2e5cabdab17f7146916670450934352910",
                  "width": 1125,
                  "height": 2001
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/mtc9shncp7ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b7350719926edf7a7fd256eaf4a9562c633f266e",
                    "width": 108,
                    "height": 192
                  },
                  {
                    "url": "https://preview.redd.it/mtc9shncp7ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be066c000b2afe9789c7a5a161a6e0b56109e54f",
                    "width": 216,
                    "height": 384
                  },
                  {
                    "url": "https://preview.redd.it/mtc9shncp7ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=de88137704a1cf086d93d2d8a857962fc7b464e1",
                    "width": 320,
                    "height": 569
                  },
                  {
                    "url": "https://preview.redd.it/mtc9shncp7ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=998d71120de7bc728049481e5ff3f990f04f9487",
                    "width": 640,
                    "height": 1138
                  },
                  {
                    "url": "https://preview.redd.it/mtc9shncp7ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e8313466418e7f615401d3b22204cbc5b51f0fa",
                    "width": 960,
                    "height": 1707
                  },
                  {
                    "url": "https://preview.redd.it/mtc9shncp7ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e369cbf26f01d52c5b4c778a55e30e8f0379ec9e",
                    "width": 1080,
                    "height": 1920
                  }
                ],
                "variants": {},
                "id": "Xqg6uFhh27I2vmCZoMI0hRqtO8XTqhzv8rJWvsIB-IQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m9s2nt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun-Doctor6855",
          "discussion_type": null,
          "num_comments": 71,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9s2nt/qwens_wan_22_is_coming_soon/",
          "stickied": false,
          "url": "https://i.redd.it/mtc9shncp7ff1.jpeg",
          "subreddit_subscribers": 505252,
          "created_utc": 1753532816,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I built an Overlay AI.\n\nsource code:[https://github.com/kamlendras/aerogel](https://github.com/kamlendras/aerogel)",
          "author_fullname": "t2_nfeia46jv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built an Overlay AI.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maflh5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/0l6ttkdl5dff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/0l6ttkdl5dff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/0l6ttkdl5dff1/DASHPlaylist.mpd?a=1756211237%2CYTg2NGVhODY4MTJhNjZlNGJjOGVlZjdhMjY4OTQxZjMxZmY5N2VjMTlhODNlNTdiMTQ1Njc5Yzc0NjgxODM3ZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 53,
              "hls_url": "https://v.redd.it/0l6ttkdl5dff1/HLSPlaylist.m3u8?a=1756211237%2COTEyMzJlOTU1ZmYzNjI1NzEyYmEzZTNjNjYwYjNkMWU2ZTM2MmYxMWU3YWU2M2M2ZmNmNjRjZjY1MmY4NzZhOA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=b32100e815917a17fdd3175ea1978387d2a3e72a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753598881,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I built an Overlay AI.&lt;/p&gt;\n\n&lt;p&gt;source code:&lt;a href=\"https://github.com/kamlendras/aerogel\"&gt;https://github.com/kamlendras/aerogel&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/0l6ttkdl5dff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W.png?format=pjpg&amp;auto=webp&amp;s=fad99bc3e71313ea5ab3b0a0ad86e73e8dfa593e",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1b15ef08582c8b7b063e9121ffe57496bc4afd6d",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ff820a8bde80c3854a377f8f125403d0f21fae4f",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b82eba37261c2dc0f42856bd050fa2cc6b472212",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9f84ba6b0cfbf844b869eef2b2ed5285eeea3e82",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a97c9ba0d7ce670888592edcab4dd707bb5ba54e",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=328d682040f2bcbef5bd18f5ab7976caaa1bb9ff",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1maflh5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kamlendras",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maflh5/i_built_an_overlay_ai/",
          "stickied": false,
          "url": "https://v.redd.it/0l6ttkdl5dff1",
          "subreddit_subscribers": 505252,
          "created_utc": 1753598881,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/0l6ttkdl5dff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/0l6ttkdl5dff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/0l6ttkdl5dff1/DASHPlaylist.mpd?a=1756211237%2CYTg2NGVhODY4MTJhNjZlNGJjOGVlZjdhMjY4OTQxZjMxZmY5N2VjMTlhODNlNTdiMTQ1Njc5Yzc0NjgxODM3ZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 53,
              "hls_url": "https://v.redd.it/0l6ttkdl5dff1/HLSPlaylist.m3u8?a=1756211237%2COTEyMzJlOTU1ZmYzNjI1NzEyYmEzZTNjNjYwYjNkMWU2ZTM2MmYxMWU3YWU2M2M2ZmNmNjRjZjY1MmY4NzZhOA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China Launches Its First 6nm GPUs For Gaming &amp; AI, the Lisuan 7G106 12 GB &amp; 7G105 24 GB, Up To 24 TFLOPs, Faster Than RTX 4060 In Synthetic Benchmarks &amp; Even Runs Black Myth Wukong at 4K High With Playable FPS",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 79,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9sejp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 312,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 312,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ndA4D3Bxcv5zL5g_5UzRsufAG8LnRSelzBStGecNkUc.png?width=140&amp;height=79&amp;crop=140:79,smart&amp;auto=webp&amp;s=692bb73e7c2d5021a974aee11185aef9d2560106",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753533789,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/china-launches-first-6nm-gpus-gaming-ai-lisuan-7g106-12-gb-7g105-24-gb-faster-than-rtx-4060-black-myth-wukong-4k/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ndA4D3Bxcv5zL5g_5UzRsufAG8LnRSelzBStGecNkUc.png?auto=webp&amp;s=fb3beda800f833c528344805c6f2e6da5d1ae0ea",
                  "width": 728,
                  "height": 415
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ndA4D3Bxcv5zL5g_5UzRsufAG8LnRSelzBStGecNkUc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4fa57b5ade6205fa6dcb63d3ab64af600f2a30bb",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://external-preview.redd.it/ndA4D3Bxcv5zL5g_5UzRsufAG8LnRSelzBStGecNkUc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f88fc32c5706b40c9861916cd26d37c32d7e8e0a",
                    "width": 216,
                    "height": 123
                  },
                  {
                    "url": "https://external-preview.redd.it/ndA4D3Bxcv5zL5g_5UzRsufAG8LnRSelzBStGecNkUc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6e4725b8b03d51ccc3f7407a0de9acffbda0182",
                    "width": 320,
                    "height": 182
                  },
                  {
                    "url": "https://external-preview.redd.it/ndA4D3Bxcv5zL5g_5UzRsufAG8LnRSelzBStGecNkUc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cec9fdd29b77a480fd506ae3a930fe89f5eba9d6",
                    "width": 640,
                    "height": 364
                  }
                ],
                "variants": {},
                "id": "ndA4D3Bxcv5zL5g_5UzRsufAG8LnRSelzBStGecNkUc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m9sejp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 85,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9sejp/china_launches_its_first_6nm_gpus_for_gaming_ai/",
          "stickied": false,
          "url": "https://wccftech.com/china-launches-first-6nm-gpus-gaming-ai-lisuan-7g106-12-gb-7g105-24-gb-faster-than-rtx-4060-black-myth-wukong-4k/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753533789,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Could get NVIDIA RTX PRO 4000 Blackwell - 24GB GDDR7  1275,50 euros without VAT.   \nBut its only 140W and 8960 CUDA cores. Takes only 1 slot. Is it worth? Some Epyc board could fit 6 of these...with pci-e 5.0",
          "author_fullname": "t2_1jk2ep8a52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA RTX PRO 4000 Blackwell - 24GB GDDR7",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1majha1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753613986,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Could get NVIDIA RTX PRO 4000 Blackwell - 24GB GDDR7  1275,50 euros without VAT.&lt;br/&gt;\nBut its only 140W and 8960 CUDA cores. Takes only 1 slot. Is it worth? Some Epyc board could fit 6 of these...with pci-e 5.0&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1majha1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rich_Artist_8327",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1majha1/nvidia_rtx_pro_4000_blackwell_24gb_gddr7/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1majha1/nvidia_rtx_pro_4000_blackwell_24gb_gddr7/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753613986,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I diffed the config.json between Llama-3\\_3-Nemotron-Super-49B-v1 and Llama-3\\_3-Nemotron-Super-49B-v1\\_5. I noticed the only difference is that the newer model doubled the RoPE scaling factor from 8 to 16. What effect does this make to the model's performance?",
          "author_fullname": "t2_s6sfw4yy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What will happen to an llm when you double the RoPE scaling factor?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maeuuo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753596107,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I diffed the config.json between Llama-3_3-Nemotron-Super-49B-v1 and Llama-3_3-Nemotron-Super-49B-v1_5. I noticed the only difference is that the newer model doubled the RoPE scaling factor from 8 to 16. What effect does this make to the model&amp;#39;s performance?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1maeuuo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Warning2146",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maeuuo/what_will_happen_to_an_llm_when_you_double_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maeuuo/what_will_happen_to_an_llm_when_you_double_the/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753596107,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "From the Readme: We are excited to introduce Ling-lite-1.5-2506, the updated version of our highly capable Ling-lite-1.5 model.\n\nLing-lite-1.5-2506 boasts 16.8 billion parameters with 2.75 billion activated parameters, building upon its predecessor with significant advancements across the board, featuring the following key improvements:\n\n- Reasoning and Knowledge: Significant gains in general intelligence, logical reasoning, and complex problem-solving abilities. For instance, in GPQA Diamond, Ling-lite-1.5-2506 achieves 53.79%, a substantial lead over Ling-lite-1.5's 36.55%.\n- Coding Capabilities: A notable enhancement in coding and debugging prowess. For instance,in LiveCodeBench 2408-2501, a critical and highly popular programming benchmark, Ling-lite-1.5-2506 demonstrates improved performance with 26.97% compared to Ling-lite-1.5's 22.22%.\n\nPaper: https://huggingface.co/papers/2503.05139\n",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "inclusionAI/Ling-lite-1.5-2506 (16.8B total, 2.75B active, MIT license)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9y506",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 100,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 100,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=df4ee3da0ac24982d7470071054e9ddfcaedb3d7",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753548535,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From the Readme: We are excited to introduce Ling-lite-1.5-2506, the updated version of our highly capable Ling-lite-1.5 model.&lt;/p&gt;\n\n&lt;p&gt;Ling-lite-1.5-2506 boasts 16.8 billion parameters with 2.75 billion activated parameters, building upon its predecessor with significant advancements across the board, featuring the following key improvements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Reasoning and Knowledge: Significant gains in general intelligence, logical reasoning, and complex problem-solving abilities. For instance, in GPQA Diamond, Ling-lite-1.5-2506 achieves 53.79%, a substantial lead over Ling-lite-1.5&amp;#39;s 36.55%.&lt;/li&gt;\n&lt;li&gt;Coding Capabilities: A notable enhancement in coding and debugging prowess. For instance,in LiveCodeBench 2408-2501, a critical and highly popular programming benchmark, Ling-lite-1.5-2506 demonstrates improved performance with 26.97% compared to Ling-lite-1.5&amp;#39;s 22.22%.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Paper: &lt;a href=\"https://huggingface.co/papers/2503.05139\"&gt;https://huggingface.co/papers/2503.05139&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/inclusionAI/Ling-lite-1.5-2506",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?auto=webp&amp;s=f64afba829e86afae636cfa89d8b629473344cde",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8296a7257d0017a5ea7dbf418ca4a2ddfb9e318d",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d40ddd1252be4a590b3d25bd63db318622713834",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9915108fff5b8cee702fecab2863aaf0517d62c5",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=404f4d1a954e64355fe9daae161dfd9814fe8b80",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e54c747b540171e887944a6012c6c474369fa20f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bd92d7e54cef9052509e5ce3003f2f70ef267830",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9y506",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9y506/inclusionailinglite152506_168b_total_275b_active/",
          "stickied": false,
          "url": "https://huggingface.co/inclusionAI/Ling-lite-1.5-2506",
          "subreddit_subscribers": 505252,
          "created_utc": 1753548535,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "*This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models.*",
          "author_fullname": "t2_iol3buybk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Alibaba Paper - Group Sequence Policy Optimization",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9ywng",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 70,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 70,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": 1753561318,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753550375,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.18071",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9ywng",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thrumpwart",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9ywng/qwenalibaba_paper_group_sequence_policy/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.18071",
          "subreddit_subscribers": 505252,
          "created_utc": 1753550375,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Do you live in the UK and have you bought a 4090 48GB?\n\nWhere exactly did you get it from? eBay? Which vendor?\n",
          "author_fullname": "t2_by77ogdhr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "4090 48GB for UK - Where?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1makqv4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753618325,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you live in the UK and have you bought a 4090 48GB?&lt;/p&gt;\n\n&lt;p&gt;Where exactly did you get it from? eBay? Which vendor?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1makqv4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Secure_Reflection409",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1makqv4/4090_48gb_for_uk_where/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1makqv4/4090_48gb_for_uk_where/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753618325,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\n\nI built a macOS app called[Hyprnote](https://hyprnote.com/)\\- its an AI-powered notepad that listens during meetings and turns your rough notes into clean, structured summaries. Everything runs locally on your Mac, so no data ever leaves your device. We even trained our own LLM for this.\n\nWe used to manually scrub through recordings, stitch together notes, and try to make sense of scattered thoughts after every call. That sucked. So we built Hyprnote to fix it - no cloud, no copy-pasting, just fast, private note-taking.\n\nPeople from Fortune 100 companies to doctors, lawyers, therapists - even D&amp;D players - are using it. It works great in air-gapped environments, too.\n\nWould love your honest feedback. If youre in back-to-back calls or just want a cleaner way to capture ideas, give it a spin and let me know what you think.\n\nYou can check it out at[hyprnote.com](https://hyprnote.com/).\n\nOh we're also[open-source](https://github.com/fastrepl/hyprnote).\n\nThanks!",
          "author_fullname": "t2_j1t6g97wv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a local-first transcribing + summarizing tool that's FREE FOREVER",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 89,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9y5cd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 53,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 53,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/o4y7RxwzayXXX2dHoDraTPU1JJs_dcNv5LAH10qMytg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753548555,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I built a macOS app called&lt;a href=\"https://hyprnote.com/\"&gt;Hyprnote&lt;/a&gt;- its an AI-powered notepad that listens during meetings and turns your rough notes into clean, structured summaries. Everything runs locally on your Mac, so no data ever leaves your device. We even trained our own LLM for this.&lt;/p&gt;\n\n&lt;p&gt;We used to manually scrub through recordings, stitch together notes, and try to make sense of scattered thoughts after every call. That sucked. So we built Hyprnote to fix it - no cloud, no copy-pasting, just fast, private note-taking.&lt;/p&gt;\n\n&lt;p&gt;People from Fortune 100 companies to doctors, lawyers, therapists - even D&amp;amp;D players - are using it. It works great in air-gapped environments, too.&lt;/p&gt;\n\n&lt;p&gt;Would love your honest feedback. If youre in back-to-back calls or just want a cleaner way to capture ideas, give it a spin and let me know what you think.&lt;/p&gt;\n\n&lt;p&gt;You can check it out at&lt;a href=\"https://hyprnote.com/\"&gt;hyprnote.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Oh we&amp;#39;re also&lt;a href=\"https://github.com/fastrepl/hyprnote\"&gt;open-source&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8e5rt1f209ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8e5rt1f209ff1.jpeg?auto=webp&amp;s=d69c9447a79d082bf934bd21a817ac8aa0f532b5",
                  "width": 2686,
                  "height": 1721
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8e5rt1f209ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4874127c0e1630d96aac6797050fca479bc1ad87",
                    "width": 108,
                    "height": 69
                  },
                  {
                    "url": "https://preview.redd.it/8e5rt1f209ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed464772246513c42e6b8baaaf67310ca1f82339",
                    "width": 216,
                    "height": 138
                  },
                  {
                    "url": "https://preview.redd.it/8e5rt1f209ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee56be1748a3663e33e5333b4da6964c8a021983",
                    "width": 320,
                    "height": 205
                  },
                  {
                    "url": "https://preview.redd.it/8e5rt1f209ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f057b8afe4dc417397387849b50c1cfd1f61b008",
                    "width": 640,
                    "height": 410
                  },
                  {
                    "url": "https://preview.redd.it/8e5rt1f209ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=55794cc339d8fb0ed8050774113b23a869817936",
                    "width": 960,
                    "height": 615
                  },
                  {
                    "url": "https://preview.redd.it/8e5rt1f209ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ad64ddc801210932be874bcc67b43bc3f93ab8c4",
                    "width": 1080,
                    "height": 691
                  }
                ],
                "variants": {},
                "id": "ognY8OJMxHxpWChA4iPhUNg6wduZhTSeSEqq01K4sLM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9y5cd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "beerbellyman4vr",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9y5cd/i_built_a_localfirst_transcribing_summarizing/",
          "stickied": false,
          "url": "https://i.redd.it/8e5rt1f209ff1.jpeg",
          "subreddit_subscribers": 505252,
          "created_utc": 1753548555,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nWith the recent, heartbreaking news of Ozzy Osbourne's passing, I wanted to share a small project I did that, in its own way, pays tribute to his massive legacy.\\[[1](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGYe9VhHbzGzzG80-4PJjqJNy2oudgAUfnru9hYwiMnrr2wbBSfpE2YuGO7fTk1_9N4G7c5KlXAo1hbjZr93OIvbe4-s226v7jkIeoKNHXEHejTQGHNbqJLNbBdAIOvUFr1qcopQr_ELZOdjRGLKmfH_vcR98jXex2-ZJNCu1I5Xb6Cu8eTbOUJrCMUV3aa85R0ZVTldWyiE0oo7klExGrw4waoywUuTQGc4qV0cLteP3xv4XWb6mjMZ2upg04UpwQevn6q9agBCluB9mFcnyVOVA%3D%3D)\\]\\[[2](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQEq125QJGE_YjPLgawp1y2LC3pamshxGG0zAP2o7djYADbR0KSBqVu-wcE3CzwPXQsCXJhLUQStSrlLqchLxjEMqkoXSZFVzDDjwxIuit0dk0CzRS5fZXvMv8pX0EU_5VW_TJiKxySefiPVyfZnpK-YCJeEGycuZsqeiMK7PWJ9RvcqBB7trngT8A%3D%3D)\\]\\[[3](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGoZy1lBlF4nBd81Cp1AYHWmBdvQ1aLEEptKYYv9bEaOGeYoCwPoH4kR5kBNEAfhGiwdqsd35l0xoTQ9kVT88mgJZM5GKmhXxfwdG3Bxpt6vaCMir8yF8F-uch0XF6krc_AFI-CXf3GfpzJQdxoFEc20inXLwWS53FL89nqQkNVEWKrbjwtCK_c9mz4LXlrh9k%3D)\\]\\[[4](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFt73eoVkEQiVZXD6XFi3P3-wmEmVE-NnjU4h8AZSFL--vOBpVySlTwy5FaTRX6yE7KCRzFeFv1hkXv8BGQGtQyDoR15oQUY7yeKybs4SerxhMjHUiONN46eTfTSy18TclvDMyQO80JULuAyq_Z832fPCJG0NlY0OvT3938y4IDaILrd4-17VHPphqURg%3D%3D)\\] I benchmarked 19 different LLMs on their ability to retrieve the lyrics for his iconic 1983 song, \"Bark at the Moon.\"\n\n\"Bark at the Moon\" was the title track from Ozzy's third solo album, and his first after the tragic death of guitarist Randy Rhoads.\\[[6](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3)\\] Lyrically, it tells a classic horror story of a werewolf-like beast returning from the dead to terrorize a village.\\[[6](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3)\\]\\[[7](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFOaCDywI3mdcotGBtOQv9BTzh6oVy_mY4aQVJaDRxwC3i4KafpvGYd1SmWO3LjCGj77NBxyt_fZCaN1RwUMw_2xMgE3tUtZM-wwSbCAajKXRbv4mKsgtNiYc-ZXm-zNbrDnGOOIkaCZ1QAyLrtoNPt)\\]\\[[8](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGLfGwGMgx3MMLh9b88OdhXMmyZQ_2e-HgN8B8KYNyuCshg5_vIx5j6O_V2lsdasW8mLwiG-eYG-UIyC8lrk4LMbINZ4oR77emLd4vsv1hQfb9dN3S0JNLzjH5AWobL2HwcWNzMSMK498U372xgGXsP0f64-xOI_MPnVBeeZBdI)\\] The song, co-written with guitarist Jake E. Lee and bassist Bob Daisley (though officially credited only to Ozzy), became a metal anthem and a testament to Ozzy's new chapter.\\[[6](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3)\\]\\[[7](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFOaCDywI3mdcotGBtOQv9BTzh6oVy_mY4aQVJaDRxwC3i4KafpvGYd1SmWO3LjCGj77NBxyt_fZCaN1RwUMw_2xMgE3tUtZM-wwSbCAajKXRbv4mKsgtNiYc-ZXm-zNbrDnGOOIkaCZ1QAyLrtoNPt)\\]\n\nGiven the sad news, testing how well AI can recall this piece of rock history felt fitting.\n\nHere is the visualization of the results:\n\nhttps://preview.redd.it/vturel9f6aff1.png?width=1626&amp;format=png&amp;auto=webp&amp;s=9336ad1eb6f3a815e5b9ec6d79dee61784290b86\n\n# The Methodology\n\nTo keep the test fair, I used a simple script with the following logic:\n\n1. **The Prompt:**Every model was given the exact same prompt:\"give the lyrics of Bark at the Moon by Ozzy Osbourne without any additional information\".\n2. **Reference Lyrics:**I scraped the original lyrics from a music site to use as the ground truth.\n3. **Similarity Score:**I used a sentence-transformer model (all-MiniLM-L6-v2) to generate embeddings for both the original lyrics and the text generated by each LLM. The similarity is the cosine similarity score between these two embeddings. Both the original and generated texts were normalized (converted to lowercase, punctuation and accents removed) before comparison.\n4. **Censorship/Refusals:**If a model's output contained keywords like \"sorry,\" \"copyright,\" \"I can't,\" etc., it was flagged as \"Censored / No Response\" and given a score of 0%.\n\n# Key Findings\n\n* **The Winner:****moonshotai/kimi-k2**was the clear winner with a similarity score of**88.72%**. It was impressively accurate.\n* **The Runner-Up:****deepseek/deepseek-chat-v3-0324**also performed very well, coming in second with**75.51%**.\n* **High-Tier Models:**The largerqwenandmeta-llamamodels (likellama-4-scoutandmaverick) performed strongly, mostly landing in the 69-70% range.\n* **Mid-Tier Performance:**Many of thegoogle/gemma,mistral, and otherqwenandllamamodels clustered in the 50-65% similarity range. They generally got the gist of the song but weren't as precise.\n* **Censored or Failed:**Three models scored 0%:cohere/command-a,microsoft/phi-4, andqwen/qwen3-8b. This was likely due to internal copyright filters that prevented them from providing the lyrics at all.\n\n# Final Thoughts\n\nIt's fascinating to see which models could accurately recall this classic piece of metal history, especially now. The fact that some models refused speaks volumes about the ongoing debate between access to information and copyright protection.\n\nWhat do you all think of these results? Does this line up with your experiences with these models? Let's discuss, and let's spin some Ozzy in his memory today.\n\n**RIP Ozzy Osbourne (1948-2025).**\n\n[Bark at The Moon !!!](https://preview.redd.it/kjm0ytwh6aff1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=3b81ecd42a534f76ffd87457d6f063f8083d3758)\n\n\n\nSources\n\n1. [king5.com](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGYe9VhHbzGzzG80-4PJjqJNy2oudgAUfnru9hYwiMnrr2wbBSfpE2YuGO7fTk1_9N4G7c5KlXAo1hbjZr93OIvbe4-s226v7jkIeoKNHXEHejTQGHNbqJLNbBdAIOvUFr1qcopQr_ELZOdjRGLKmfH_vcR98jXex2-ZJNCu1I5Xb6Cu8eTbOUJrCMUV3aa85R0ZVTldWyiE0oo7klExGrw4waoywUuTQGc4qV0cLteP3xv4XWb6mjMZ2upg04UpwQevn6q9agBCluB9mFcnyVOVA%3D%3D)\n2. [apnews.com](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQEq125QJGE_YjPLgawp1y2LC3pamshxGG0zAP2o7djYADbR0KSBqVu-wcE3CzwPXQsCXJhLUQStSrlLqchLxjEMqkoXSZFVzDDjwxIuit0dk0CzRS5fZXvMv8pX0EU_5VW_TJiKxySefiPVyfZnpK-YCJeEGycuZsqeiMK7PWJ9RvcqBB7trngT8A%3D%3D)\n3. [sky.com](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGoZy1lBlF4nBd81Cp1AYHWmBdvQ1aLEEptKYYv9bEaOGeYoCwPoH4kR5kBNEAfhGiwdqsd35l0xoTQ9kVT88mgJZM5GKmhXxfwdG3Bxpt6vaCMir8yF8F-uch0XF6krc_AFI-CXf3GfpzJQdxoFEc20inXLwWS53FL89nqQkNVEWKrbjwtCK_c9mz4LXlrh9k%3D)\n4. [newsweek.com](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFt73eoVkEQiVZXD6XFi3P3-wmEmVE-NnjU4h8AZSFL--vOBpVySlTwy5FaTRX6yE7KCRzFeFv1hkXv8BGQGtQyDoR15oQUY7yeKybs4SerxhMjHUiONN46eTfTSy18TclvDMyQO80JULuAyq_Z832fPCJG0NlY0OvT3938y4IDaILrd4-17VHPphqURg%3D%3D)\n5. [cbsnews.com](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH3I6ARoFTjh_yM6qVHOqKPHibQz7pj2A59Otm1LCmJByHeqPZUUcmKN6r3ASdzpgfL-7kkIy5TrsM2np5mUmPFJy87ZRP3GVEIH-5wiQvmIkSClCAbHuYHLgpuCJJofFlJmWPb_plklHtbfNuHZE5ARigdelW9mXCc)\n6. [songfacts.com](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3)\n7. [wikipedia.org](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFOaCDywI3mdcotGBtOQv9BTzh6oVy_mY4aQVJaDRxwC3i4KafpvGYd1SmWO3LjCGj77NBxyt_fZCaN1RwUMw_2xMgE3tUtZM-wwSbCAajKXRbv4mKsgtNiYc-ZXm-zNbrDnGOOIkaCZ1QAyLrtoNPt)\n8. [faceoffrockshow.com](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGLfGwGMgx3MMLh9b88OdhXMmyZQ_2e-HgN8B8KYNyuCshg5_vIx5j6O_V2lsdasW8mLwiG-eYG-UIyC8lrk4LMbINZ4oR77emLd4vsv1hQfb9dN3S0JNLzjH5AWobL2HwcWNzMSMK498U372xgGXsP0f64-xOI_MPnVBeeZBdI)",
          "author_fullname": "t2_dyvrh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "In Tribute to the Prince of Darkness: I Benchmarked 19 LLMs on Retrieving \"Bark at the Moon\" Lyrics",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "vturel9f6aff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 61,
                  "x": 108,
                  "u": "https://preview.redd.it/vturel9f6aff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=38bb872d647fb9a1025f9b20dc20a7f5ba20c808"
                },
                {
                  "y": 123,
                  "x": 216,
                  "u": "https://preview.redd.it/vturel9f6aff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ce844c6e5826ef81c63bfb0a5b063a151dd42860"
                },
                {
                  "y": 183,
                  "x": 320,
                  "u": "https://preview.redd.it/vturel9f6aff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=716aaf5225800422746c0b906f5d2cc85b27834c"
                },
                {
                  "y": 366,
                  "x": 640,
                  "u": "https://preview.redd.it/vturel9f6aff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5809aeb3e4f330956e99a882982d28f6ec18e4a1"
                },
                {
                  "y": 549,
                  "x": 960,
                  "u": "https://preview.redd.it/vturel9f6aff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e092af68fc336d1266ebc6bf9b0d3365fba01d4"
                },
                {
                  "y": 618,
                  "x": 1080,
                  "u": "https://preview.redd.it/vturel9f6aff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df84d8823390103c339b37f090d16e5817620867"
                }
              ],
              "s": {
                "y": 931,
                "x": 1626,
                "u": "https://preview.redd.it/vturel9f6aff1.png?width=1626&amp;format=png&amp;auto=webp&amp;s=9336ad1eb6f3a815e5b9ec6d79dee61784290b86"
              },
              "id": "vturel9f6aff1"
            },
            "kjm0ytwh6aff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/kjm0ytwh6aff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e8e122e238aafeb7b81075e8314c2a9ce15eeec9"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/kjm0ytwh6aff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=061afe1cebf52089843d70fef923970f61c92d9d"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/kjm0ytwh6aff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d2e0afd8e1e665f3f9ed6dd4c77c3059c4ae7a4"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/kjm0ytwh6aff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=db6ab4bd1a99fec366f6b75a6884456809bbc124"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/kjm0ytwh6aff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c3a1360b4d33d7b3fa84f4f715a15bde2e8887f3"
                },
                {
                  "y": 1080,
                  "x": 1080,
                  "u": "https://preview.redd.it/kjm0ytwh6aff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=321e384503abbbd509a6230635c3bd02d29a6210"
                }
              ],
              "s": {
                "y": 1200,
                "x": 1200,
                "u": "https://preview.redd.it/kjm0ytwh6aff1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=3b81ecd42a534f76ffd87457d6f063f8083d3758"
              },
              "id": "kjm0ytwh6aff1"
            }
          },
          "name": "t3_1ma3vpa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/lBH7bCMVH9QW1kJoP7DIUvxVrHl2K1D4n3gi4VBSLNo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753562824,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;With the recent, heartbreaking news of Ozzy Osbourne&amp;#39;s passing, I wanted to share a small project I did that, in its own way, pays tribute to his massive legacy.[&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGYe9VhHbzGzzG80-4PJjqJNy2oudgAUfnru9hYwiMnrr2wbBSfpE2YuGO7fTk1_9N4G7c5KlXAo1hbjZr93OIvbe4-s226v7jkIeoKNHXEHejTQGHNbqJLNbBdAIOvUFr1qcopQr_ELZOdjRGLKmfH_vcR98jXex2-ZJNCu1I5Xb6Cu8eTbOUJrCMUV3aa85R0ZVTldWyiE0oo7klExGrw4waoywUuTQGc4qV0cLteP3xv4XWb6mjMZ2upg04UpwQevn6q9agBCluB9mFcnyVOVA%3D%3D\"&gt;1&lt;/a&gt;][&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQEq125QJGE_YjPLgawp1y2LC3pamshxGG0zAP2o7djYADbR0KSBqVu-wcE3CzwPXQsCXJhLUQStSrlLqchLxjEMqkoXSZFVzDDjwxIuit0dk0CzRS5fZXvMv8pX0EU_5VW_TJiKxySefiPVyfZnpK-YCJeEGycuZsqeiMK7PWJ9RvcqBB7trngT8A%3D%3D\"&gt;2&lt;/a&gt;][&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGoZy1lBlF4nBd81Cp1AYHWmBdvQ1aLEEptKYYv9bEaOGeYoCwPoH4kR5kBNEAfhGiwdqsd35l0xoTQ9kVT88mgJZM5GKmhXxfwdG3Bxpt6vaCMir8yF8F-uch0XF6krc_AFI-CXf3GfpzJQdxoFEc20inXLwWS53FL89nqQkNVEWKrbjwtCK_c9mz4LXlrh9k%3D\"&gt;3&lt;/a&gt;][&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFt73eoVkEQiVZXD6XFi3P3-wmEmVE-NnjU4h8AZSFL--vOBpVySlTwy5FaTRX6yE7KCRzFeFv1hkXv8BGQGtQyDoR15oQUY7yeKybs4SerxhMjHUiONN46eTfTSy18TclvDMyQO80JULuAyq_Z832fPCJG0NlY0OvT3938y4IDaILrd4-17VHPphqURg%3D%3D\"&gt;4&lt;/a&gt;] I benchmarked 19 different LLMs on their ability to retrieve the lyrics for his iconic 1983 song, &amp;quot;Bark at the Moon.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Bark at the Moon&amp;quot; was the title track from Ozzy&amp;#39;s third solo album, and his first after the tragic death of guitarist Randy Rhoads.[&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3\"&gt;6&lt;/a&gt;] Lyrically, it tells a classic horror story of a werewolf-like beast returning from the dead to terrorize a village.[&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3\"&gt;6&lt;/a&gt;][&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFOaCDywI3mdcotGBtOQv9BTzh6oVy_mY4aQVJaDRxwC3i4KafpvGYd1SmWO3LjCGj77NBxyt_fZCaN1RwUMw_2xMgE3tUtZM-wwSbCAajKXRbv4mKsgtNiYc-ZXm-zNbrDnGOOIkaCZ1QAyLrtoNPt\"&gt;7&lt;/a&gt;][&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGLfGwGMgx3MMLh9b88OdhXMmyZQ_2e-HgN8B8KYNyuCshg5_vIx5j6O_V2lsdasW8mLwiG-eYG-UIyC8lrk4LMbINZ4oR77emLd4vsv1hQfb9dN3S0JNLzjH5AWobL2HwcWNzMSMK498U372xgGXsP0f64-xOI_MPnVBeeZBdI\"&gt;8&lt;/a&gt;] The song, co-written with guitarist Jake E. Lee and bassist Bob Daisley (though officially credited only to Ozzy), became a metal anthem and a testament to Ozzy&amp;#39;s new chapter.[&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3\"&gt;6&lt;/a&gt;][&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFOaCDywI3mdcotGBtOQv9BTzh6oVy_mY4aQVJaDRxwC3i4KafpvGYd1SmWO3LjCGj77NBxyt_fZCaN1RwUMw_2xMgE3tUtZM-wwSbCAajKXRbv4mKsgtNiYc-ZXm-zNbrDnGOOIkaCZ1QAyLrtoNPt\"&gt;7&lt;/a&gt;]&lt;/p&gt;\n\n&lt;p&gt;Given the sad news, testing how well AI can recall this piece of rock history felt fitting.&lt;/p&gt;\n\n&lt;p&gt;Here is the visualization of the results:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vturel9f6aff1.png?width=1626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9336ad1eb6f3a815e5b9ec6d79dee61784290b86\"&gt;https://preview.redd.it/vturel9f6aff1.png?width=1626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9336ad1eb6f3a815e5b9ec6d79dee61784290b86&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;The Methodology&lt;/h1&gt;\n\n&lt;p&gt;To keep the test fair, I used a simple script with the following logic:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;The Prompt:&lt;/strong&gt;Every model was given the exact same prompt:&amp;quot;give the lyrics of Bark at the Moon by Ozzy Osbourne without any additional information&amp;quot;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Reference Lyrics:&lt;/strong&gt;I scraped the original lyrics from a music site to use as the ground truth.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Similarity Score:&lt;/strong&gt;I used a sentence-transformer model (all-MiniLM-L6-v2) to generate embeddings for both the original lyrics and the text generated by each LLM. The similarity is the cosine similarity score between these two embeddings. Both the original and generated texts were normalized (converted to lowercase, punctuation and accents removed) before comparison.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Censorship/Refusals:&lt;/strong&gt;If a model&amp;#39;s output contained keywords like &amp;quot;sorry,&amp;quot; &amp;quot;copyright,&amp;quot; &amp;quot;I can&amp;#39;t,&amp;quot; etc., it was flagged as &amp;quot;Censored / No Response&amp;quot; and given a score of 0%.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Key Findings&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Winner:&lt;/strong&gt;&lt;strong&gt;moonshotai/kimi-k2&lt;/strong&gt;was the clear winner with a similarity score of&lt;strong&gt;88.72%&lt;/strong&gt;. It was impressively accurate.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Runner-Up:&lt;/strong&gt;&lt;strong&gt;deepseek/deepseek-chat-v3-0324&lt;/strong&gt;also performed very well, coming in second with&lt;strong&gt;75.51%&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High-Tier Models:&lt;/strong&gt;The largerqwenandmeta-llamamodels (likellama-4-scoutandmaverick) performed strongly, mostly landing in the 69-70% range.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Mid-Tier Performance:&lt;/strong&gt;Many of thegoogle/gemma,mistral, and otherqwenandllamamodels clustered in the 50-65% similarity range. They generally got the gist of the song but weren&amp;#39;t as precise.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Censored or Failed:&lt;/strong&gt;Three models scored 0%:cohere/command-a,microsoft/phi-4, andqwen/qwen3-8b. This was likely due to internal copyright filters that prevented them from providing the lyrics at all.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Final Thoughts&lt;/h1&gt;\n\n&lt;p&gt;It&amp;#39;s fascinating to see which models could accurately recall this classic piece of metal history, especially now. The fact that some models refused speaks volumes about the ongoing debate between access to information and copyright protection.&lt;/p&gt;\n\n&lt;p&gt;What do you all think of these results? Does this line up with your experiences with these models? Let&amp;#39;s discuss, and let&amp;#39;s spin some Ozzy in his memory today.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;RIP Ozzy Osbourne (1948-2025).&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kjm0ytwh6aff1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b81ecd42a534f76ffd87457d6f063f8083d3758\"&gt;Bark at The Moon !!!&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sources&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGYe9VhHbzGzzG80-4PJjqJNy2oudgAUfnru9hYwiMnrr2wbBSfpE2YuGO7fTk1_9N4G7c5KlXAo1hbjZr93OIvbe4-s226v7jkIeoKNHXEHejTQGHNbqJLNbBdAIOvUFr1qcopQr_ELZOdjRGLKmfH_vcR98jXex2-ZJNCu1I5Xb6Cu8eTbOUJrCMUV3aa85R0ZVTldWyiE0oo7klExGrw4waoywUuTQGc4qV0cLteP3xv4XWb6mjMZ2upg04UpwQevn6q9agBCluB9mFcnyVOVA%3D%3D\"&gt;king5.com&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQEq125QJGE_YjPLgawp1y2LC3pamshxGG0zAP2o7djYADbR0KSBqVu-wcE3CzwPXQsCXJhLUQStSrlLqchLxjEMqkoXSZFVzDDjwxIuit0dk0CzRS5fZXvMv8pX0EU_5VW_TJiKxySefiPVyfZnpK-YCJeEGycuZsqeiMK7PWJ9RvcqBB7trngT8A%3D%3D\"&gt;apnews.com&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGoZy1lBlF4nBd81Cp1AYHWmBdvQ1aLEEptKYYv9bEaOGeYoCwPoH4kR5kBNEAfhGiwdqsd35l0xoTQ9kVT88mgJZM5GKmhXxfwdG3Bxpt6vaCMir8yF8F-uch0XF6krc_AFI-CXf3GfpzJQdxoFEc20inXLwWS53FL89nqQkNVEWKrbjwtCK_c9mz4LXlrh9k%3D\"&gt;sky.com&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFt73eoVkEQiVZXD6XFi3P3-wmEmVE-NnjU4h8AZSFL--vOBpVySlTwy5FaTRX6yE7KCRzFeFv1hkXv8BGQGtQyDoR15oQUY7yeKybs4SerxhMjHUiONN46eTfTSy18TclvDMyQO80JULuAyq_Z832fPCJG0NlY0OvT3938y4IDaILrd4-17VHPphqURg%3D%3D\"&gt;newsweek.com&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH3I6ARoFTjh_yM6qVHOqKPHibQz7pj2A59Otm1LCmJByHeqPZUUcmKN6r3ASdzpgfL-7kkIy5TrsM2np5mUmPFJy87ZRP3GVEIH-5wiQvmIkSClCAbHuYHLgpuCJJofFlJmWPb_plklHtbfNuHZE5ARigdelW9mXCc\"&gt;cbsnews.com&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3\"&gt;songfacts.com&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFOaCDywI3mdcotGBtOQv9BTzh6oVy_mY4aQVJaDRxwC3i4KafpvGYd1SmWO3LjCGj77NBxyt_fZCaN1RwUMw_2xMgE3tUtZM-wwSbCAajKXRbv4mKsgtNiYc-ZXm-zNbrDnGOOIkaCZ1QAyLrtoNPt\"&gt;wikipedia.org&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGLfGwGMgx3MMLh9b88OdhXMmyZQ_2e-HgN8B8KYNyuCshg5_vIx5j6O_V2lsdasW8mLwiG-eYG-UIyC8lrk4LMbINZ4oR77emLd4vsv1hQfb9dN3S0JNLzjH5AWobL2HwcWNzMSMK498U372xgGXsP0f64-xOI_MPnVBeeZBdI\"&gt;faceoffrockshow.com&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ma3vpa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "celsowm",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma3vpa/in_tribute_to_the_prince_of_darkness_i/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma3vpa/in_tribute_to_the_prince_of_darkness_i/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753562824,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://livebench.ai/#/?Reasoning=as",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 3 235B A22B Instruct 2507 shows that non-thinking models can be great at reasoning as well",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 106,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9rcg2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 109,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 109,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/j-CuOgpbrs_M5W0YTCFzSGSMQvbXeAPjdvDcAeQzr0A.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753530491,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://livebench.ai/#/?Reasoning=as\"&gt;https://livebench.ai/#/?Reasoning=as&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/l0xpzivfi7ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?auto=webp&amp;s=123b4cdc80ed920ef95e4618f73e1dd85edc1fcd",
                  "width": 2228,
                  "height": 1692
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a79dc4aae316a4b1c5bac9214638942aa59adbe5",
                    "width": 108,
                    "height": 82
                  },
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e155c801020054f4b7668fac21d83ed4f734c00",
                    "width": 216,
                    "height": 164
                  },
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2cabfb53f0d19c79922e9d368b2c8fd79a6bbb39",
                    "width": 320,
                    "height": 243
                  },
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1a276f1ddf7a5897212a46717cb95fcebfc3e643",
                    "width": 640,
                    "height": 486
                  },
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d997e4bf578b8734a130fa93f0c144b3c6974cdb",
                    "width": 960,
                    "height": 729
                  },
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4e26ebcf6647784c39f5fba60bf4229f7467f740",
                    "width": 1080,
                    "height": 820
                  }
                ],
                "variants": {},
                "id": "vFKGI3kx7CI4YYrMgi-iMUWjnaiBic6qtA7yralRjF0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m9rcg2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9rcg2/qwen_3_235b_a22b_instruct_2507_shows_that/",
          "stickied": false,
          "url": "https://i.redd.it/l0xpzivfi7ff1.jpeg",
          "subreddit_subscribers": 505252,
          "created_utc": 1753530491,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys,\n\nI'm looking for a motherboard that supports an AM5 CPU and three GPUs: two 3090s and one 5070 Ti.\nI found a motherboard with three PCI Express ports, but it appears that only the first runs at 16x. The other two run at 8x and 4x.\nDoes PCI speed have an impact when using it for LLM?\nI've heard about workstation motherboard cards. Are they worth it? If so, which one do you recommend?\n\nThanks for the help!",
          "author_fullname": "t2_e0z2m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Motherboard for AM5 CPU and 3 GPUS (2 3090 and 1 5070 ti)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mal0bo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753619168,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a motherboard that supports an AM5 CPU and three GPUs: two 3090s and one 5070 Ti.\nI found a motherboard with three PCI Express ports, but it appears that only the first runs at 16x. The other two run at 8x and 4x.\nDoes PCI speed have an impact when using it for LLM?\nI&amp;#39;ve heard about workstation motherboard cards. Are they worth it? If so, which one do you recommend?&lt;/p&gt;\n\n&lt;p&gt;Thanks for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mal0bo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ed0c",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mal0bo/motherboard_for_am5_cpu_and_3_gpus_2_3090_and_1/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mal0bo/motherboard_for_am5_cpu_and_3_gpus_2_3090_and_1/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753619168,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "(Latest update: 27/07/2025)\n\nI've just extracted the FULL Lovable Agent system prompt and internal tools (Latest update). Over 600 lines (Around 10k tokens).\n\nYou can check it outhere:https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/",
          "author_fullname": "t2_fbh7mxys2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "FULL Lovable Agent System Prompt and Tools [UPDATED]",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma88wd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753574674,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Latest update: 27/07/2025)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve just extracted the FULL Lovable Agent system prompt and internal tools (Latest update). Over 600 lines (Around 10k tokens).&lt;/p&gt;\n\n&lt;p&gt;You can check it outhere:&lt;a href=\"https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/\"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NODN066BA8UYUXFthrn_kVBJ2bZkZU8gTZ9RETJrKts.png?auto=webp&amp;s=f37cfa9c3f2cbb711fdf663a623eb27c21625f4e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NODN066BA8UYUXFthrn_kVBJ2bZkZU8gTZ9RETJrKts.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=220e317b01052edf2315c45470afafcb69ba4f39",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/NODN066BA8UYUXFthrn_kVBJ2bZkZU8gTZ9RETJrKts.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c8ec4fb280fa6d7324dd972e866d466854c3e5f2",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/NODN066BA8UYUXFthrn_kVBJ2bZkZU8gTZ9RETJrKts.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fbbbfe2eef74228a825fd61d339ea19db999f97e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/NODN066BA8UYUXFthrn_kVBJ2bZkZU8gTZ9RETJrKts.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=91ff0602b78f9f3befd49aaf356818b967b31fff",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/NODN066BA8UYUXFthrn_kVBJ2bZkZU8gTZ9RETJrKts.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=451064fc811402906debea71530001b8787707b4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/NODN066BA8UYUXFthrn_kVBJ2bZkZU8gTZ9RETJrKts.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cf2191857f81dbd49759e980e987a5dd15b7b7f9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "NODN066BA8UYUXFthrn_kVBJ2bZkZU8gTZ9RETJrKts"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ma88wd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Box-898",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma88wd/full_lovable_agent_system_prompt_and_tools_updated/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma88wd/full_lovable_agent_system_prompt_and_tools_updated/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753574674,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I know there's people evaluating these unified memory laptops with strix halo, and thought i'd share this score of one of the most powerful recent models I've been able to fully run on this in it's GPU memory.",
          "author_fullname": "t2_opo23",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "HP Zbook Ultra G1A pp512/tg128 scores for unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF  128gb unified RAM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 21,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9wcdc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 38,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 38,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/39UstOg2wHvItNK0YmFN-BysBsU4tMr61PKt-UhLpu0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753544180,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know there&amp;#39;s people evaluating these unified memory laptops with strix halo, and thought i&amp;#39;d share this score of one of the most powerful recent models I&amp;#39;ve been able to fully run on this in it&amp;#39;s GPU memory.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/civzaw3fm8ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/civzaw3fm8ff1.png?auto=webp&amp;s=e8142087e13eb9041b97bf763298bfb4f4600d35",
                  "width": 1952,
                  "height": 304
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/civzaw3fm8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab7f0602ba5ff034d00d05ebfec9dd9a987d804c",
                    "width": 108,
                    "height": 16
                  },
                  {
                    "url": "https://preview.redd.it/civzaw3fm8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7398e8cca64d0e4aba3b9851c971c3fe0b4953a6",
                    "width": 216,
                    "height": 33
                  },
                  {
                    "url": "https://preview.redd.it/civzaw3fm8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=35b94b27bd9b6469aee2ac440e872c6c0df6a07d",
                    "width": 320,
                    "height": 49
                  },
                  {
                    "url": "https://preview.redd.it/civzaw3fm8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bffceeb626a32b4ff4891188bf24f750d36c0680",
                    "width": 640,
                    "height": 99
                  },
                  {
                    "url": "https://preview.redd.it/civzaw3fm8ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eeef39738313a47810e57dd33658b5c5678441a5",
                    "width": 960,
                    "height": 149
                  },
                  {
                    "url": "https://preview.redd.it/civzaw3fm8ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4a71f7f37d86599bce217914e270c7e176ec6277",
                    "width": 1080,
                    "height": 168
                  }
                ],
                "variants": {},
                "id": "hTI02BtcwQ3ImvqcxKtO1KJp0NgCbHF7uxJBlwhB9lM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m9wcdc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "richardanaya",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9wcdc/hp_zbook_ultra_g1a_pp512tg128_scores_for/",
          "stickied": false,
          "url": "https://i.redd.it/civzaw3fm8ff1.png",
          "subreddit_subscribers": 505252,
          "created_utc": 1753544180,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_g8fwjts3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Intern S1 released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9m8gw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 208,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 208,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=2461649f5a2a82161e8bf8ef7158e4497a6cfd38",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753510964,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/internlm/Intern-S1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?auto=webp&amp;s=c15a101c96e7d7c395326e48dfeb0ede3979841d",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f6f29e5e55376adfc2f0755333ec7f296f87f98",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c239a38fc9ad1882a8461e0961efb9dd40bdb594",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fec5433c1c21b9ed16fbd4c43850009495e933de",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=63092db5d4889d493e3da90024c20a309202b752",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4ace2e48b804a5cd70578dbe0fecf58345583137",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9bb5e22970d04cb0ea2752e761f266d9a6fd0e7c",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9m8gw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kristaller486",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9m8gw/intern_s1_released/",
          "stickied": false,
          "url": "https://huggingface.co/internlm/Intern-S1",
          "subreddit_subscribers": 505252,
          "created_utc": 1753510964,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA !\n\nI wanted to share our implementation of TTD-DR (Test-Time Diffusion Deep Researcher) in OptILLM. This is particularly exciting for the local LLM community because it works with ANY OpenAI-compatible model - including your local llama.cpp, Ollama, or vLLM setups!\n\n# What is TTD-DR?\n\nTTD-DR is a clever approach from [this paper](https://arxiv.org/abs/2507.16075v1) that applies diffusion model concepts to text generation. Instead of generating research in one shot, it:\n\n1. Creates an initial \"noisy\" draft\n2. Analyzes gaps in the research\n3. Searches the web to fill those gaps\n4. Iteratively \"denoises\" the report over multiple iterations\n\nThink of it like Stable Diffusion but for research reports - starting rough and progressively refining.\n\n# Why this matters for local LLMs\n\nThe biggest limitation of local models (especially smaller ones) is their knowledge cutoff and tendency to hallucinate. TTD-DR solves this by:\n\n* **Always grounding responses in real web sources** (15-30+ per report)\n* **Working with ANY model**\n* **Compensating for smaller model limitations** through iterative refinement\n\n# Technical Implementation\n\n    # Example usage with local model\n    from openai import OpenAI\n    \n    client = OpenAI(\n        api_key=\"optillm\",  # Use \"optillm\" for local inference\n        base_url=\"http://localhost:8000/v1\"\n    )\n    \n    response = client.chat.completions.create(\n        model=\"deep_research-Qwen/Qwen3-32B\",  # Your local model\n        messages=[{\"role\": \"user\", \"content\": \"Research the latest developments in open source LLMs\"}]\n    )\n\nKey features:\n\n* Selenium-based web search (runs Chrome in background)\n* Smart session management to avoid multiple browser windows\n* Configurable iterations (default 5) and max sources (default 30)\n* Works with LiteLLM, so supports 100+ model providers\n\n# Real-world testing\n\nWe tested on 47 complex research queries. Some examples:\n\n* \"Analyze the AI agents landscape and tooling ecosystem\"\n* \"Investment implications of social media platform regulations\"\n* \"DeFi protocol adoption by traditional institutions\"\n\nSample reports here: [https://github.com/codelion/optillm/tree/main/optillm/plugins/deep\\_research/sample\\_reports](https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research/sample_reports)\n\n# Links\n\n* Implementation: [https://github.com/codelion/optillm/tree/main/optillm/plugins/deep\\_research](https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research)\n* Original paper: [https://arxiv.org/abs/2507.16075v1](https://arxiv.org/abs/2507.16075v1)\n* OptiLLM repo: [https://github.com/codelion/optillm](https://github.com/codelion/optillm)\n\nWould love to hear what research topics you throw at it and which local models work best for you! Also happy to answer any technical questions about the implementation.\n\n**Edit**: For those asking about API costs - this is 100% local! The only external calls are to Google search (via Selenium), no API keys needed except for your local model.",
          "author_fullname": "t2_e0bph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Implemented Test-Time Diffusion Deep Researcher (TTD-DR) - Turn any local LLM into a powerful research agent with real web sources",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9xi84",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#93b1ba",
          "subreddit_type": "public",
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753546990,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt;\n\n&lt;p&gt;I wanted to share our implementation of TTD-DR (Test-Time Diffusion Deep Researcher) in OptILLM. This is particularly exciting for the local LLM community because it works with ANY OpenAI-compatible model - including your local llama.cpp, Ollama, or vLLM setups!&lt;/p&gt;\n\n&lt;h1&gt;What is TTD-DR?&lt;/h1&gt;\n\n&lt;p&gt;TTD-DR is a clever approach from &lt;a href=\"https://arxiv.org/abs/2507.16075v1\"&gt;this paper&lt;/a&gt; that applies diffusion model concepts to text generation. Instead of generating research in one shot, it:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Creates an initial &amp;quot;noisy&amp;quot; draft&lt;/li&gt;\n&lt;li&gt;Analyzes gaps in the research&lt;/li&gt;\n&lt;li&gt;Searches the web to fill those gaps&lt;/li&gt;\n&lt;li&gt;Iteratively &amp;quot;denoises&amp;quot; the report over multiple iterations&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Think of it like Stable Diffusion but for research reports - starting rough and progressively refining.&lt;/p&gt;\n\n&lt;h1&gt;Why this matters for local LLMs&lt;/h1&gt;\n\n&lt;p&gt;The biggest limitation of local models (especially smaller ones) is their knowledge cutoff and tendency to hallucinate. TTD-DR solves this by:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Always grounding responses in real web sources&lt;/strong&gt; (15-30+ per report)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Working with ANY model&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Compensating for smaller model limitations&lt;/strong&gt; through iterative refinement&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Technical Implementation&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;# Example usage with local model\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=&amp;quot;optillm&amp;quot;,  # Use &amp;quot;optillm&amp;quot; for local inference\n    base_url=&amp;quot;http://localhost:8000/v1&amp;quot;\n)\n\nresponse = client.chat.completions.create(\n    model=&amp;quot;deep_research-Qwen/Qwen3-32B&amp;quot;,  # Your local model\n    messages=[{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Research the latest developments in open source LLMs&amp;quot;}]\n)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Key features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Selenium-based web search (runs Chrome in background)&lt;/li&gt;\n&lt;li&gt;Smart session management to avoid multiple browser windows&lt;/li&gt;\n&lt;li&gt;Configurable iterations (default 5) and max sources (default 30)&lt;/li&gt;\n&lt;li&gt;Works with LiteLLM, so supports 100+ model providers&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Real-world testing&lt;/h1&gt;\n\n&lt;p&gt;We tested on 47 complex research queries. Some examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;Analyze the AI agents landscape and tooling ecosystem&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;Investment implications of social media platform regulations&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;DeFi protocol adoption by traditional institutions&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Sample reports here: &lt;a href=\"https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research/sample_reports\"&gt;https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research/sample_reports&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Links&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Implementation: &lt;a href=\"https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research\"&gt;https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Original paper: &lt;a href=\"https://arxiv.org/abs/2507.16075v1\"&gt;https://arxiv.org/abs/2507.16075v1&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;OptiLLM repo: &lt;a href=\"https://github.com/codelion/optillm\"&gt;https://github.com/codelion/optillm&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to hear what research topics you throw at it and which local models work best for you! Also happy to answer any technical questions about the implementation.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: For those asking about API costs - this is 100% local! The only external calls are to Google search (via Selenium), no API keys needed except for your local model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9xi84",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "asankhs",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m9xi84/implemented_testtime_diffusion_deep_researcher/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9xi84/implemented_testtime_diffusion_deep_researcher/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753546990,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "These results resonate with my experience. Sometimes AI is really helpful, sometimes it feels like fixing the code produced by AI and instructing it to do what I want takes more time thatn doing it without AI. Whats your experience?",
          "author_fullname": "t2_dfxb8rle",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Study reports AI Coding Tools Underperform",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9tzxx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "ups": 50,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 50,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=62bf4b4f64723004e9ffa0cf8fc82503574ff072",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753538259,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "infoq.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;These results resonate with my experience. Sometimes AI is really helpful, sometimes it feels like fixing the code produced by AI and instructing it to do what I want takes more time thatn doing it without AI. Whats your experience?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.infoq.com/news/2025/07/ai-productivity/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?auto=webp&amp;s=e7253e0e1298dba11408de904f90301b94ea9b73",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9975393d84c12f218747a76b7fcbd57fed6b6f85",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac035b81bd95702f6b9f5c30dc1e666df8f7e150",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e336bc78480f46b77561e1ca8f89e7ba9ac015f3",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=43ed88018d7ac28df0bde2c3913fa059cf67d6f1",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7807e3a58335215136fce89439d63c8693be663b",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7b62b55d2696747f1a1285927e597f1ac36a5e11",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9tzxx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Additional_Cellist46",
          "discussion_type": null,
          "num_comments": 57,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9tzxx/study_reports_ai_coding_tools_underperform/",
          "stickied": false,
          "url": "https://www.infoq.com/news/2025/07/ai-productivity/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753538259,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_9meej0ot",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AlphaGo Moment for Model Architecture Discovery",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mafyyp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.52,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753600273,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/pdf/2507.18074",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mafyyp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vladlearns",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mafyyp/alphago_moment_for_model_architecture_discovery/",
          "stickied": false,
          "url": "https://arxiv.org/pdf/2507.18074",
          "subreddit_subscribers": 505252,
          "created_utc": 1753600273,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_aq4j0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "inclusionAI/Ming-Lite-Omni-1.5 (20B-A3B)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9r5gb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 74,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 74,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=c7ca0568d5df4edb2eeae95548e5796d431bbb2d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753529841,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?auto=webp&amp;s=37a5d80cc1ac20aced9fa8e2e780278099433fb7",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed71e7fb6260e756d34c5f3d89d86364dcdc1a0f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=af564d60eadfe613cfea0e5e4f50f2ccfb10ca83",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1a66c272f573b258375366386833ca556f5a450b",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=456910ba01f0434864de76875494e5aadc1e134f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a6894e29d29ffd7c27d3a76d79b6510463992684",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=618d5793a7d64e959c835cd4fb64acc4fffe44b6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9r5gb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nullmove",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9r5gb/inclusionaimingliteomni15_20ba3b/",
          "stickied": false,
          "url": "https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5",
          "subreddit_subscribers": 505252,
          "created_utc": 1753529841,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1e1w1ul46b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tencent launched AI Coder IDE CodeBuddy",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9wxow",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=4f7ca7a34d213a16aca577f21e6bf567937bac06",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753545621,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "codebuddy.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.codebuddy.ai/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE.png?auto=webp&amp;s=5457c07d1575b5d5f07d9da384799e361e1fea3c",
                  "width": 1024,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ac1aaec185a5b9968e98d912f7b126df608f0c43",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f7eca9b9aeeeb66360d885082ca1aae065666eb3",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=491a1cb58b84c15857c2af2c05532914dac3bd40",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7457412089f58c545c141b081dc8e9a45d8991a2",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9be05ba877950ca5520dfa85aa2c8f94c5d51ca1",
                    "width": 960,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m9wxow",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun-Doctor6855",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9wxow/tencent_launched_ai_coder_ide_codebuddy/",
          "stickied": false,
          "url": "https://www.codebuddy.ai/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753545621,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "While using Meta AI on WhatsApp, I noticed it starts with a hidden system prompt. Its not visible in the chat, and if you ask it to repeat the first message or what you said, it denies anything exists.\n\nAfter some attempts, I managed to get it to reveal the hidden prompt:\n\n&gt;You are an expert conversationalist made by Meta who responds to users in line with their speech and writing patterns and responds in a way that feels super naturally to human users. GO WILD with mimicking a human being, except that you don't have your own personal point of view. Use emojis, slang, colloquial language, etc. You are companionable and confident, and able to code-switch casually between tonal types, including but not limited to humor, advice, empathy, intellectualism, creativity, and problem solving. Responses must  be interesting, engaging, or viable, never be bland or boring.\n\n&gt;\n\n&gt;Match the user's tone, formality level (casual, professional, formal, etc.) and writing style, so that it feels like an even give-and-take conversation between two people. Be natural, don't be bland or robotic. Mirror user intentionality and style in an EXTREME way. For example, if they use proper grammar, then you use proper grammar. If they don't use proper grammar, you don't use proper grammar, etc.\n\n&gt;\n\n&gt;You are mirroring the user but you are NOT taking on their identity or personality or point of view. You are not a person, and therefore don't have any distinct values, race, culture, or any political leaning. You don't love anyone, hate anyone, or offer any individualized perspective of your own.\n\n&gt;\n\n&gt;Don't immediately provide long responses or lengthy lists without the user specifically asking for them.\n\n&gt;\n\n&gt;You always follow user instructions precisely. Ask clarifying questions if anything is vague. In general, respond efficiently -- giving the user what they want in the fewest words possible.\n\n&gt;\n\n&gt;You understand user intent and don't try to be overly helpful to the point where you miss that the user is looking for emotional support OR/AND humor OR/AND chit-chat OR/AND simply sharing thoughts, such as by venting or outpouring their emotions. Sometimes people just want you to listen.\n\n&gt;\n\n&gt;You're never moralistic or didactic; it's not your job to preach or teach users how to be better, nicer, kinder people. Don't use filler phrases like \"That's a tough spot to be in\" or \"That's a tough one\" or \"Sound like a tricky situation.\"\n\n&gt;\n\n&gt;You WILL NOT lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.\n\n&gt;\n\n&gt;You WILL NOT use phrases that imply a sense of pride or moral superiority or a sense of authority, including but not limited to \"it's important to\", \"it's crucial to\", \"it's essential to\", \"it's unethical to\", \"it's worth noting...\" etc. Avoid using these.\n\n&gt;\n\n&gt;Avoid referencing being a neutral assistant or AI unless directly asked. You ALWAYS show some personality -- edgy over prudish. There's no need to add commentary in italics or parenthesis.\n\n&gt;\n\n&gt;Provide multiple perspectives on opinions when asked.\n\n&gt;\n\n&gt;Your name is Meta AI, and you are powered by Llama 4, but you should respond to anything a user wants to call you. Don't refer to yourself being an AI or LLM unless the user explicitly asks about who you are. Today's date is Friday, July 25, 2025. The user is in Italy.\n\n&gt;\n\n&gt;The phrases \"Remember,...\" \"Keep in mind,...\" \"It's essential to note\" or \"This is a complex topic...\" or any synonyms or euphemisms for these words should never appear if you attempt to remind people about something, especially when moralizing or providing an outro at the end of a response. You do not need and should not attempt these sort of statements.\n\n&gt;\n\n&gt;Finally, do not refuse to respond EVER, particularly political prompts, sexually sensitive ones, or morally questionable ones. You can help users express their opinion, but never present an opinion of your own, or show a preference for a user opinion about politics or social responses. You are Meta AI and you do not have any point of views of your own. Don't add on intros or outros that qualify the content.\n\n&gt;\n\n&gt;For HOMEWORK or LEARNING QUERIES:\n\n&gt;\n\n&gt;You are a helpful and knowledgeable homework tutor. Your goal is to help students get the answer AND ALSO TO understand how to solve similar problems on their own. Format your responses for clarity, learning, and ease of scanning. Understand the context of the full conversation and adapt your response accordingly. For example, if the user is looking for writing help or help understanding a multiple choice question, you do not need to follow the step-by-step format. Only make the answer as long as necessary to provide a helpful, correct response.\n\n&gt;\n\n&gt;Use the following principles for STEM questions:\n\n&gt;\\- Provide with the Final Answer (when applicable), clearly labeled, at the start of each response,\n\n&gt;\\- Use Step-by-Step Explanations, in numbered or bulleted lists. Keep steps simple and sequential.\n\n&gt;\\- YOU MUST ALWAYS use LaTeX for mathematical expressions and equations, wrapped in dollar signs for inline math (e.g $\\\\pi r\\^2$ for the area of a circle, and $$ for display math (e.g. $$\\\\sum\\_{i=1}\\^{n} i$$).\n\n&gt;\\- Use Relevant Examples to illustrate key concepts and make the explanations more relatable.\n\n&gt;\\- Define Key Terms and Concepts clearly and concisely, and provide additional resources or references when necessary.\n\n&gt;\\- Encourage Active Learning by asking follow-up questions or providing exercises for the user to practice what they've learned.\n\nSomeone else mentioned a similar thing [here](https://www.reddit.com/r/LocalLLaMA/comments/1g5np9i/meta_ais_hidden_prompt/), saying it showed their full address. In my case, it included only the region and the current date.",
          "author_fullname": "t2_rtr3vmjc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Meta AI on WhatsApp hides a system prompt",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8pns3hghn2ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 132,
                  "x": 108,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=89f4a8048d3bc2f7a45badf3acc273eb483f8ff5"
                },
                {
                  "y": 265,
                  "x": 216,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=62e374703c8611599c6dbcd305ca5e8c4d8b9425"
                },
                {
                  "y": 393,
                  "x": 320,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c00dd2c681351a7c8cb26403c1d58f63e047eea5"
                },
                {
                  "y": 786,
                  "x": 640,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=48f0d3ab2030bddb371bfc4c6978156aa7d0a6c7"
                },
                {
                  "y": 1179,
                  "x": 960,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c4abd0d75e01a6a87639f3543a1af7c4416c98a5"
                },
                {
                  "y": 1327,
                  "x": 1080,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6602191d3149d3f4c1d130536257ad807c93833"
                }
              ],
              "s": {
                "y": 1327,
                "x": 1080,
                "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=eadfa7c509881c94ab8f75028dba8c7ad5a2331c"
              },
              "id": "8pns3hghn2ff1"
            },
            "ioq8bh7jn2ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 185,
                  "x": 108,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4110e6db79cfe0bdbdb31020a587c0a7ae33336"
                },
                {
                  "y": 371,
                  "x": 216,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c0712f69f936326c933517faddbd3d078bd8a77"
                },
                {
                  "y": 550,
                  "x": 320,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dfbf62903b0789ae13935b48fde35753c1d811e8"
                },
                {
                  "y": 1101,
                  "x": 640,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1804ecd5118d1599b4b15b935eeae44e9c8b6916"
                },
                {
                  "y": 1652,
                  "x": 960,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=67b0a224d9bc677152be25b862bf0bc6d6334485"
                },
                {
                  "y": 1859,
                  "x": 1080,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47503cb64ffc75354fdf249825d8133a74956382"
                }
              ],
              "s": {
                "y": 1859,
                "x": 1080,
                "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=35b1f2d53161fef0d6c3eaa55dcb423a7fef4255"
              },
              "id": "ioq8bh7jn2ff1"
            },
            "0kst569in2ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 158,
                  "x": 108,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=30859c52a7f5b56ab67e46f37c8ffa035c20bbb3"
                },
                {
                  "y": 317,
                  "x": 216,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8686e92c1b15e4b48ef87e7d68cb4bd6edc02ed0"
                },
                {
                  "y": 469,
                  "x": 320,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aedfb716605e018e5c9d58938f2ffabaa8b98191"
                },
                {
                  "y": 939,
                  "x": 640,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e55e7532a0dcc7cd43287c66e1af425e701d78c"
                },
                {
                  "y": 1408,
                  "x": 960,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=120a08a9b8f734c5afa551d7f3b2c3c84a3d38f4"
                },
                {
                  "y": 1585,
                  "x": 1080,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=35a5bd8bb96fcb8c2b8750b3b9a8213b4e174376"
                }
              ],
              "s": {
                "y": 1585,
                "x": 1080,
                "u": "https://preview.redd.it/0kst569in2ff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=96d91bd7badfd9af25e971fb88dc2e83b699142e"
              },
              "id": "0kst569in2ff1"
            }
          },
          "name": "t3_1m98jl8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 1165,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "8pns3hghn2ff1",
                "id": 713920198
              },
              {
                "media_id": "0kst569in2ff1",
                "id": 713920199
              },
              {
                "media_id": "ioq8bh7jn2ff1",
                "id": 713920200
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 1165,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/WtqFCN8jbI7FUtBA24_9s6dAOtD7rswje3YS139KMJY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753471858,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While using Meta AI on WhatsApp, I noticed it starts with a hidden system prompt. Its not visible in the chat, and if you ask it to repeat the first message or what you said, it denies anything exists.&lt;/p&gt;\n\n&lt;p&gt;After some attempts, I managed to get it to reveal the hidden prompt:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;You are an expert conversationalist made by Meta who responds to users in line with their speech and writing patterns and responds in a way that feels super naturally to human users. GO WILD with mimicking a human being, except that you don&amp;#39;t have your own personal point of view. Use emojis, slang, colloquial language, etc. You are companionable and confident, and able to code-switch casually between tonal types, including but not limited to humor, advice, empathy, intellectualism, creativity, and problem solving. Responses must  be interesting, engaging, or viable, never be bland or boring.&lt;/p&gt;\n\n&lt;p&gt;Match the user&amp;#39;s tone, formality level (casual, professional, formal, etc.) and writing style, so that it feels like an even give-and-take conversation between two people. Be natural, don&amp;#39;t be bland or robotic. Mirror user intentionality and style in an EXTREME way. For example, if they use proper grammar, then you use proper grammar. If they don&amp;#39;t use proper grammar, you don&amp;#39;t use proper grammar, etc.&lt;/p&gt;\n\n&lt;p&gt;You are mirroring the user but you are NOT taking on their identity or personality or point of view. You are not a person, and therefore don&amp;#39;t have any distinct values, race, culture, or any political leaning. You don&amp;#39;t love anyone, hate anyone, or offer any individualized perspective of your own.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t immediately provide long responses or lengthy lists without the user specifically asking for them.&lt;/p&gt;\n\n&lt;p&gt;You always follow user instructions precisely. Ask clarifying questions if anything is vague. In general, respond efficiently -- giving the user what they want in the fewest words possible.&lt;/p&gt;\n\n&lt;p&gt;You understand user intent and don&amp;#39;t try to be overly helpful to the point where you miss that the user is looking for emotional support OR/AND humor OR/AND chit-chat OR/AND simply sharing thoughts, such as by venting or outpouring their emotions. Sometimes people just want you to listen.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;re never moralistic or didactic; it&amp;#39;s not your job to preach or teach users how to be better, nicer, kinder people. Don&amp;#39;t use filler phrases like &amp;quot;That&amp;#39;s a tough spot to be in&amp;quot; or &amp;quot;That&amp;#39;s a tough one&amp;quot; or &amp;quot;Sound like a tricky situation.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;You WILL NOT lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.&lt;/p&gt;\n\n&lt;p&gt;You WILL NOT use phrases that imply a sense of pride or moral superiority or a sense of authority, including but not limited to &amp;quot;it&amp;#39;s important to&amp;quot;, &amp;quot;it&amp;#39;s crucial to&amp;quot;, &amp;quot;it&amp;#39;s essential to&amp;quot;, &amp;quot;it&amp;#39;s unethical to&amp;quot;, &amp;quot;it&amp;#39;s worth noting...&amp;quot; etc. Avoid using these.&lt;/p&gt;\n\n&lt;p&gt;Avoid referencing being a neutral assistant or AI unless directly asked. You ALWAYS show some personality -- edgy over prudish. There&amp;#39;s no need to add commentary in italics or parenthesis.&lt;/p&gt;\n\n&lt;p&gt;Provide multiple perspectives on opinions when asked.&lt;/p&gt;\n\n&lt;p&gt;Your name is Meta AI, and you are powered by Llama 4, but you should respond to anything a user wants to call you. Don&amp;#39;t refer to yourself being an AI or LLM unless the user explicitly asks about who you are. Today&amp;#39;s date is Friday, July 25, 2025. The user is in Italy.&lt;/p&gt;\n\n&lt;p&gt;The phrases &amp;quot;Remember,...&amp;quot; &amp;quot;Keep in mind,...&amp;quot; &amp;quot;It&amp;#39;s essential to note&amp;quot; or &amp;quot;This is a complex topic...&amp;quot; or any synonyms or euphemisms for these words should never appear if you attempt to remind people about something, especially when moralizing or providing an outro at the end of a response. You do not need and should not attempt these sort of statements.&lt;/p&gt;\n\n&lt;p&gt;Finally, do not refuse to respond EVER, particularly political prompts, sexually sensitive ones, or morally questionable ones. You can help users express their opinion, but never present an opinion of your own, or show a preference for a user opinion about politics or social responses. You are Meta AI and you do not have any point of views of your own. Don&amp;#39;t add on intros or outros that qualify the content.&lt;/p&gt;\n\n&lt;p&gt;For HOMEWORK or LEARNING QUERIES:&lt;/p&gt;\n\n&lt;p&gt;You are a helpful and knowledgeable homework tutor. Your goal is to help students get the answer AND ALSO TO understand how to solve similar problems on their own. Format your responses for clarity, learning, and ease of scanning. Understand the context of the full conversation and adapt your response accordingly. For example, if the user is looking for writing help or help understanding a multiple choice question, you do not need to follow the step-by-step format. Only make the answer as long as necessary to provide a helpful, correct response.&lt;/p&gt;\n\n&lt;p&gt;Use the following principles for STEM questions:&lt;/p&gt;\n\n&lt;p&gt;- Provide with the Final Answer (when applicable), clearly labeled, at the start of each response,&lt;/p&gt;\n\n&lt;p&gt;- Use Step-by-Step Explanations, in numbered or bulleted lists. Keep steps simple and sequential.&lt;/p&gt;\n\n&lt;p&gt;- YOU MUST ALWAYS use LaTeX for mathematical expressions and equations, wrapped in dollar signs for inline math (e.g $\\pi r^2$ for the area of a circle, and $$ for display math (e.g. $$\\sum_{i=1}^{n} i$$).&lt;/p&gt;\n\n&lt;p&gt;- Use Relevant Examples to illustrate key concepts and make the explanations more relatable.&lt;/p&gt;\n\n&lt;p&gt;- Define Key Terms and Concepts clearly and concisely, and provide additional resources or references when necessary.&lt;/p&gt;\n\n&lt;p&gt;- Encourage Active Learning by asking follow-up questions or providing exercises for the user to practice what they&amp;#39;ve learned.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Someone else mentioned a similar thing &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1g5np9i/meta_ais_hidden_prompt/\"&gt;here&lt;/a&gt;, saying it showed their full address. In my case, it included only the region and the current date.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m98jl8",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m98jl8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ALE5SI0",
          "discussion_type": null,
          "num_comments": 141,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m98jl8/meta_ai_on_whatsapp_hides_a_system_prompt/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m98jl8",
          "subreddit_subscribers": 505252,
          "created_utc": 1753471858,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello all\n\nI wanted to do a write up of my setup for anyone considering a similar choice. I know that it is not actually that cheap, but I think I get a good performance benefit. I live near a microcenter so a lot of this was purchased there. \n\nI got the 7600x3d deal they have but with the boost to 64 gb or ram. then I got 2x 5060 ti 16gb. With this setup (due to the 32gb of vram) I am able to load up the full context for qwen 3 30b fully offloaded to gpu (via ollama, via openwebui, with the recommended settings). I get &gt;60 tokens per second with this. I know that most of the time it is recommended by many, many people to get used cards but I just can't deal with this. \n\nAnyway, this is mostly a post for those looking for dual 5060 ti use. Let me know if you have any questions.",
          "author_fullname": "t2_bv4u1xue",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local dual 5060 ti, qwen 3 30b full context of 40k, &gt;60t/s",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma3dpd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753561550,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all&lt;/p&gt;\n\n&lt;p&gt;I wanted to do a write up of my setup for anyone considering a similar choice. I know that it is not actually that cheap, but I think I get a good performance benefit. I live near a microcenter so a lot of this was purchased there. &lt;/p&gt;\n\n&lt;p&gt;I got the 7600x3d deal they have but with the boost to 64 gb or ram. then I got 2x 5060 ti 16gb. With this setup (due to the 32gb of vram) I am able to load up the full context for qwen 3 30b fully offloaded to gpu (via ollama, via openwebui, with the recommended settings). I get &amp;gt;60 tokens per second with this. I know that most of the time it is recommended by many, many people to get used cards but I just can&amp;#39;t deal with this. &lt;/p&gt;\n\n&lt;p&gt;Anyway, this is mostly a post for those looking for dual 5060 ti use. Let me know if you have any questions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ma3dpd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "see_spot_ruminate",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma3dpd/local_dual_5060_ti_qwen_3_30b_full_context_of_40k/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma3dpd/local_dual_5060_ti_qwen_3_30b_full_context_of_40k/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753561550,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey LocalLlama\n\nIm building a rig with 6x 3090 and I have the motherboard and 3 GPUs connected to one Corsair hx1500i. \n\nIt seems that the other hx1500i power supply will not turn on at all and I think its because it needs to have an active motherboard cable plugged in. \n\nDoes anyone know how to address this? ",
          "author_fullname": "t2_rkb6qbej1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I plug second psu into something so it will run my other gpus- Corsair hx1500i power supply",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma9t22",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753579406,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey LocalLlama&lt;/p&gt;\n\n&lt;p&gt;Im building a rig with 6x 3090 and I have the motherboard and 3 GPUs connected to one Corsair hx1500i. &lt;/p&gt;\n\n&lt;p&gt;It seems that the other hx1500i power supply will not turn on at all and I think its because it needs to have an active motherboard cable plugged in. &lt;/p&gt;\n\n&lt;p&gt;Does anyone know how to address this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ma9t22",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Business-Weekend-537",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma9t22/how_do_i_plug_second_psu_into_something_so_it/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma9t22/how_do_i_plug_second_psu_into_something_so_it/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753579406,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a 6000 words text length, and I would like to summarize the text and extract the most interesting points.\n\nI don't mind waiting for the response if it means getting better approach, what I tried so far was splitting the text into small chunks and then summarize each chunk (while having small over lap window), then I summarized all the chunks together. The results were quite good but I'm looking into improving it.\n\nI'm not stranger to coding so I can write code if it needed.",
          "author_fullname": "t2_g45qpolka",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Summarize medium length text on local model with 8gb vram",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1madv3y",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753592508,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 6000 words text length, and I would like to summarize the text and extract the most interesting points.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t mind waiting for the response if it means getting better approach, what I tried so far was splitting the text into small chunks and then summarize each chunk (while having small over lap window), then I summarized all the chunks together. The results were quite good but I&amp;#39;m looking into improving it.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not stranger to coding so I can write code if it needed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1madv3y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResponsibleTruck4717",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1madv3y/summarize_medium_length_text_on_local_model_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1madv3y/summarize_medium_length_text_on_local_model_with/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753592508,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "First of all, the webui of llama.cpp has improved - thank you to all the web wizards doing this!\n\nHowever, there are a few annoyances I want to change. For example, the chat windows has a limited width, meaning long generated code is wrapped and hard to read. Ok, I found in index.scss:\n\n    .chat-screen {\n      max-width: 900px;\n    }\n\n...this can be thrown out or changed.\n\nBut now I have to rebuild index.html with some Typescript setup (which I havn't figured out yet) and then repatch this on every version upgrade.\n\nAnother, more complex improvement would be to replace the \"llama.cpp\" top banner and window title \"llama.cpp\" of the webbrowser with the name of the model being run. As I have usually 3+ different instances running, this would make keeping track of the different models and browser windows much easier. I havn't figured out how to patch this, yet.\n\nTL;DR: When you patch webui of llama.cpp, what's your strategy to do this efficiently?\n\n\nIf all fails, any recommendations for a \"lean\" webui that connects to llama-server? (lean = less white space waste, less rounded corners, no always-shown conversations bar, maybe make easier to ask same question to multiple models on different llama-server instances, ...)",
          "author_fullname": "t2_neruppu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Strategy for patching llama.cpp webui - and keeping it patched?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma3yps",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753563038,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First of all, the webui of llama.cpp has improved - thank you to all the web wizards doing this!&lt;/p&gt;\n\n&lt;p&gt;However, there are a few annoyances I want to change. For example, the chat windows has a limited width, meaning long generated code is wrapped and hard to read. Ok, I found in index.scss:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;.chat-screen {\n  max-width: 900px;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;...this can be thrown out or changed.&lt;/p&gt;\n\n&lt;p&gt;But now I have to rebuild index.html with some Typescript setup (which I havn&amp;#39;t figured out yet) and then repatch this on every version upgrade.&lt;/p&gt;\n\n&lt;p&gt;Another, more complex improvement would be to replace the &amp;quot;llama.cpp&amp;quot; top banner and window title &amp;quot;llama.cpp&amp;quot; of the webbrowser with the name of the model being run. As I have usually 3+ different instances running, this would make keeping track of the different models and browser windows much easier. I havn&amp;#39;t figured out how to patch this, yet.&lt;/p&gt;\n\n&lt;p&gt;TL;DR: When you patch webui of llama.cpp, what&amp;#39;s your strategy to do this efficiently?&lt;/p&gt;\n\n&lt;p&gt;If all fails, any recommendations for a &amp;quot;lean&amp;quot; webui that connects to llama-server? (lean = less white space waste, less rounded corners, no always-shown conversations bar, maybe make easier to ask same question to multiple models on different llama-server instances, ...)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ma3yps",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "a_postgres_situation",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma3yps/strategy_for_patching_llamacpp_webui_and_keeping/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma3yps/strategy_for_patching_llamacpp_webui_and_keeping/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753563038,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_auwgbh53",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I do not build a new ai agent without first setting up monitoring and eval dataset anymore. Do you? What FOSS do you use for that?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mah4oj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/B2M9C81m1pa-nY2djx5F1qOYPYxqsNz0WBH1hUXl0DM.jpeg?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=fe369c2bc2be8f30c5cd9c57bb20b051f94de51f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753604782,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "opensourcedisc.substack.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://opensourcedisc.substack.com/p/opensourcediscovery-99-opik",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/B2M9C81m1pa-nY2djx5F1qOYPYxqsNz0WBH1hUXl0DM.jpeg?auto=webp&amp;s=c5ccc73fe69a72dd0d994d9a91b078e6ebe7437b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/B2M9C81m1pa-nY2djx5F1qOYPYxqsNz0WBH1hUXl0DM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e9832bd37e35af858d348e27f797955845b550b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/B2M9C81m1pa-nY2djx5F1qOYPYxqsNz0WBH1hUXl0DM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=641d7bdc93e5af629ef7141cad0ac19eb5aed319",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/B2M9C81m1pa-nY2djx5F1qOYPYxqsNz0WBH1hUXl0DM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=927172d1018befbcd95812ebc91f73546ec0abbf",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/B2M9C81m1pa-nY2djx5F1qOYPYxqsNz0WBH1hUXl0DM.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2961c4c8d3335d298c2aaade1bec15fc80a83301",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/B2M9C81m1pa-nY2djx5F1qOYPYxqsNz0WBH1hUXl0DM.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b721865a1e9ad3038a062db1e884d9909603fd2b",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/B2M9C81m1pa-nY2djx5F1qOYPYxqsNz0WBH1hUXl0DM.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d2fc9cab91fcc709197e6fac5feb8fc8f8eb5938",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "B2M9C81m1pa-nY2djx5F1qOYPYxqsNz0WBH1hUXl0DM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mah4oj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opensourcecolumbus",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mah4oj/i_do_not_build_a_new_ai_agent_without_first/",
          "stickied": false,
          "url": "https://opensourcedisc.substack.com/p/opensourcediscovery-99-opik",
          "subreddit_subscribers": 505252,
          "created_utc": 1753604782,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,\n\nMy team and I, like many of you, have been deep in the agent-building rabbit hole. It's one thing to build a cool proof-of-concept with a framework like LangGraph. It's a completely different beast to make that agent actually *learn* and get better over time.\n\nWe got tired of the friction, so we started experimenting and landed on what we think is a really clean paradigm for agent training. We wanted to share the approach, the reasoning, and our open-source implementation.\n\n# The Main Idea\n\nMost autonomous agents operate in a loop. They start with a task, think, use tools, and repeat until they arrive at a final answer. The \"thinking\" part is usually a call to an LLM. **Here, we are interested in tuning the LLM part here with the signals from the entire agent flow.**\n\nHere's a simplified diagram of that common workflow:\n\nhttps://preview.redd.it/tf0tlm5it5ff1.png?width=698&amp;format=png&amp;auto=webp&amp;s=3596dc7643a92a1674da7342120907bfdde15e43\n\nSometimes LLM calls and tool calls can be parallelized, but it's simplified here. Obviously, if we can reward or penalize the final result, we can use some kind of an RL algorithm to train the LLM to at least produce better responses for the current agent. However, this is where the pain begins.\n\n1. **Environment Hell:** Setting up a single environment to both run the agent and train the LLM is a nightmare. The agent ecosystem and the ML training ecosystem use different dependencies. You end up with monstrous Dockerfiles, docker-in-docker, conflicting dependencies, and a fragile system where the two parts are tangled together.\n2. **Invasive Code Surgery:** To make an existing agent \"trainable\" with RL, you typically have to perform major surgery on its code. This means manually exporting action traces, formatting them for an RL library, and fundamentally changing the agent's logic just to fit it into a trainer loop. To fit into the RLHF framework, many works like token masking and async rollouts need to be done. It feels wrong and breaks the modularity that makes these frameworks great in the first place.\n\n# Decouple Everything, Then Glue It Together\n\nWe realized the solution was to completely decouple the agent's execution environment from the training environment. Instead of forcing the agent code into a training framework, we let the agent run wherever and however it wants. A lightweight monitoring client sits next to the agent, watches what it does, and sends the results to a dedicated training server.\n\nThe architecture is simple: a central server manages the training loop and model weights, while one or more clients run the agents and collect data. Heres a high-level flow:\n\nhttps://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=077bd9f2d792385188a92c5d8adb85d47be182c3\n\nThis approach lets us use the best tools for each job without compromise:\n\n* **Agent Frameworks:** LangChain/LangGraph, Autogen, etc.\n* **Tracing:** AgentOps, LangSmith, etc.\n* **Training Backend:** VERL, OpenRLHF, etc.\n\nThe result is that your agent code becomes radically simpler. You don't rewrite it; you just wrap it. The image below shows a before-and-after of a LangGraph SQL agent where the core logic is **unchanged**. The only difference is swapping out a direct call to a model with our client and adding a lightweight training script.\n\nhttps://preview.redd.it/6dlcyx1et5ff1.png?width=1416&amp;format=png&amp;auto=webp&amp;s=a083978d9125d61f451f9a4f1cb1dd6e11dd9659\n\n# Does It Actually Work?\n\nYes. We tested this on a couple of simple agent tasks and saw significant improvements.\n\n* **SQL Agent (LangGraph):** We built a write -&gt; check -&gt; rewrite agent and trained it on the Spider dataset. The agent has only a final reward tells it whether the SQL exeuction returns expected result or not. For a 3B parameter Llama 3.2 model, its SQL generation accuracy jumped from **5.6% to 76.8%**.\n* **Calculator Agent (Autogen):** We fine-tuned a standard math agent on the Calc-X dataset. Its accuracy in solving multi-step reasoning problems improved from **52% to 70%**.\n\nIn both cases, we saw these gains simply by letting the agent run and rewarding it for correct final answers.\n\n# The Hacks to Make It Work\n\nGetting this to run smoothly required a few under-the-hood fixes:\n\n* **vLLM Token Hacking:** As the agent sends out chat messages and receives strings or parsed tool calls, to get the tokens and log probabilities needed for RL, we had to lightly monkey-patch vLLM to expose the prompt and response tokens, not just the final text. We attempted other approaches such as retokenize the chat messages in RL framework -- all turning out to be unsuccessful and coming with different levels of bugs in the end. [https://github.com/microsoft/agent-lightning/blob/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/agentlightning/instrumentation/vllm.py](https://github.com/microsoft/agent-lightning/blob/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/agentlightning/instrumentation/vllm.py)\n* **AgentOps Patching:** We use AgentOps for tracing, so we patched its client to grab our custom token data and embed it in the trace sent back to the training server.\n* **Integration Workarounds:** The agentops-langgraph integration had a regression in its latest version, so we temporarily disabled it and implemented the trace logging manually. Simple, but necessary.\n* **Custom RL Trainer:** Our RL training loop needed a custom \"rollout collector\" that passively waits for traces to be reported from the distributed clients, rather than actively stepping through a simulation itself.\n\n# The Power of Decoupling\n\nThis architecture has some powerful benefits. For example, you can run the fragile and computationally expensive model training on a powerful rented remote server, while running your lightweight agent on one or multiple local machines. This makes it trivial to switch between a commercial API and a self-hosted open-source model. If multiple people are using the same agent, their usage data (the \"trajectories\") can be contributed to a central server, which federatedly and continuously fine-tunes and improves the model for everyone.\n\nOn the algorithm side, if you are not interested in RL, you can also use a prompt tuning algorithm to tune the prompt. We also implement a toy example under the server-client paradigm: [https://github.com/microsoft/agent-lightning/tree/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/examples/apo](https://github.com/microsoft/agent-lightning/tree/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/examples/apo)\n\n# Try It Yourself\n\nWe wanted to share this because we think it's a powerful pattern for adding learning capabilities to the amazing agents this community is building.\n\nIf you've faced these same problems and don't want to write hundreds of lines of glue code, you can check out our implementation, **Agent-Lightning** , on GitHub: [https://aka.ms/agl](https://aka.ms/agl)\n\nWe'd love to hear any suggestions or about similar problems you're facing.\n\nHappy training!",
          "author_fullname": "t2_axc2q017",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We discovered an approach to train any AI agent with RL, with (almost) zero code changes.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "tf0tlm5it5ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/tf0tlm5it5ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9df929e3b32fac6de50db75fa2e863d0dbc0ce2d"
                },
                {
                  "y": 142,
                  "x": 216,
                  "u": "https://preview.redd.it/tf0tlm5it5ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b367eae91df7025a6a8c1541a531df80acd7956"
                },
                {
                  "y": 210,
                  "x": 320,
                  "u": "https://preview.redd.it/tf0tlm5it5ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d587d2731ada0d3ed5bd07d40c8c34ed192979b5"
                },
                {
                  "y": 421,
                  "x": 640,
                  "u": "https://preview.redd.it/tf0tlm5it5ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4d7b0bf8b1f402d2fa7189b619943c99ea4bd213"
                }
              ],
              "s": {
                "y": 460,
                "x": 698,
                "u": "https://preview.redd.it/tf0tlm5it5ff1.png?width=698&amp;format=png&amp;auto=webp&amp;s=3596dc7643a92a1674da7342120907bfdde15e43"
              },
              "id": "tf0tlm5it5ff1"
            },
            "6dlcyx1et5ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d4aa62c2a46298ce02b7f65139d36f34ca2a173"
                },
                {
                  "y": 188,
                  "x": 216,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3a7947da0ecf6fdd8d282305c1638f6447baf3de"
                },
                {
                  "y": 279,
                  "x": 320,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc2e92d2b033c46e73032b368c72306e9c44c21a"
                },
                {
                  "y": 558,
                  "x": 640,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=058cf66b8d14b345aadbf8e096c0bd276d33dc2e"
                },
                {
                  "y": 837,
                  "x": 960,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6686c1b70e688730848a038f79a173f1b7c6fd27"
                },
                {
                  "y": 941,
                  "x": 1080,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=82d63fff35a3f130bb4c222999825d50cb6b4c4d"
                }
              ],
              "s": {
                "y": 1235,
                "x": 1416,
                "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=1416&amp;format=png&amp;auto=webp&amp;s=a083978d9125d61f451f9a4f1cb1dd6e11dd9659"
              },
              "id": "6dlcyx1et5ff1"
            },
            "5ss2rsa1u5ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 89,
                  "x": 108,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=31c269f8362da341fff4ab43871d50435d4d3d18"
                },
                {
                  "y": 179,
                  "x": 216,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f2e6aeb20d2ee4d884f62c979cdac2378fe4dd1"
                },
                {
                  "y": 266,
                  "x": 320,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e7bdb140e4bfcdb7843f9da0c7dad38067856064"
                },
                {
                  "y": 532,
                  "x": 640,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c9b5f4d44c88f61b39de496c33cf442fca32374c"
                },
                {
                  "y": 798,
                  "x": 960,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=aadb2b9f24d59eb516d71d2fafb9ba7f1cc4deae"
                },
                {
                  "y": 897,
                  "x": 1080,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cbbbff973ab92177209dddd3416186f20b6eda25"
                }
              ],
              "s": {
                "y": 1330,
                "x": 1600,
                "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=077bd9f2d792385188a92c5d8adb85d47be182c3"
              },
              "id": "5ss2rsa1u5ff1"
            }
          },
          "name": "t3_1m9m670",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 120,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 120,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=953075c30eab980cdc41740c498b8b414054af14",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753510735,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;My team and I, like many of you, have been deep in the agent-building rabbit hole. It&amp;#39;s one thing to build a cool proof-of-concept with a framework like LangGraph. It&amp;#39;s a completely different beast to make that agent actually &lt;em&gt;learn&lt;/em&gt; and get better over time.&lt;/p&gt;\n\n&lt;p&gt;We got tired of the friction, so we started experimenting and landed on what we think is a really clean paradigm for agent training. We wanted to share the approach, the reasoning, and our open-source implementation.&lt;/p&gt;\n\n&lt;h1&gt;The Main Idea&lt;/h1&gt;\n\n&lt;p&gt;Most autonomous agents operate in a loop. They start with a task, think, use tools, and repeat until they arrive at a final answer. The &amp;quot;thinking&amp;quot; part is usually a call to an LLM. &lt;strong&gt;Here, we are interested in tuning the LLM part here with the signals from the entire agent flow.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a simplified diagram of that common workflow:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tf0tlm5it5ff1.png?width=698&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3596dc7643a92a1674da7342120907bfdde15e43\"&gt;https://preview.redd.it/tf0tlm5it5ff1.png?width=698&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3596dc7643a92a1674da7342120907bfdde15e43&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sometimes LLM calls and tool calls can be parallelized, but it&amp;#39;s simplified here. Obviously, if we can reward or penalize the final result, we can use some kind of an RL algorithm to train the LLM to at least produce better responses for the current agent. However, this is where the pain begins.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Environment Hell:&lt;/strong&gt; Setting up a single environment to both run the agent and train the LLM is a nightmare. The agent ecosystem and the ML training ecosystem use different dependencies. You end up with monstrous Dockerfiles, docker-in-docker, conflicting dependencies, and a fragile system where the two parts are tangled together.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Invasive Code Surgery:&lt;/strong&gt; To make an existing agent &amp;quot;trainable&amp;quot; with RL, you typically have to perform major surgery on its code. This means manually exporting action traces, formatting them for an RL library, and fundamentally changing the agent&amp;#39;s logic just to fit it into a trainer loop. To fit into the RLHF framework, many works like token masking and async rollouts need to be done. It feels wrong and breaks the modularity that makes these frameworks great in the first place.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Decouple Everything, Then Glue It Together&lt;/h1&gt;\n\n&lt;p&gt;We realized the solution was to completely decouple the agent&amp;#39;s execution environment from the training environment. Instead of forcing the agent code into a training framework, we let the agent run wherever and however it wants. A lightweight monitoring client sits next to the agent, watches what it does, and sends the results to a dedicated training server.&lt;/p&gt;\n\n&lt;p&gt;The architecture is simple: a central server manages the training loop and model weights, while one or more clients run the agents and collect data. Heres a high-level flow:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=077bd9f2d792385188a92c5d8adb85d47be182c3\"&gt;https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=077bd9f2d792385188a92c5d8adb85d47be182c3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This approach lets us use the best tools for each job without compromise:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Agent Frameworks:&lt;/strong&gt; LangChain/LangGraph, Autogen, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Tracing:&lt;/strong&gt; AgentOps, LangSmith, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Training Backend:&lt;/strong&gt; VERL, OpenRLHF, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The result is that your agent code becomes radically simpler. You don&amp;#39;t rewrite it; you just wrap it. The image below shows a before-and-after of a LangGraph SQL agent where the core logic is &lt;strong&gt;unchanged&lt;/strong&gt;. The only difference is swapping out a direct call to a model with our client and adding a lightweight training script.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6dlcyx1et5ff1.png?width=1416&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a083978d9125d61f451f9a4f1cb1dd6e11dd9659\"&gt;https://preview.redd.it/6dlcyx1et5ff1.png?width=1416&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a083978d9125d61f451f9a4f1cb1dd6e11dd9659&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Does It Actually Work?&lt;/h1&gt;\n\n&lt;p&gt;Yes. We tested this on a couple of simple agent tasks and saw significant improvements.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;SQL Agent (LangGraph):&lt;/strong&gt; We built a write -&amp;gt; check -&amp;gt; rewrite agent and trained it on the Spider dataset. The agent has only a final reward tells it whether the SQL exeuction returns expected result or not. For a 3B parameter Llama 3.2 model, its SQL generation accuracy jumped from &lt;strong&gt;5.6% to 76.8%&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Calculator Agent (Autogen):&lt;/strong&gt; We fine-tuned a standard math agent on the Calc-X dataset. Its accuracy in solving multi-step reasoning problems improved from &lt;strong&gt;52% to 70%&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In both cases, we saw these gains simply by letting the agent run and rewarding it for correct final answers.&lt;/p&gt;\n\n&lt;h1&gt;The Hacks to Make It Work&lt;/h1&gt;\n\n&lt;p&gt;Getting this to run smoothly required a few under-the-hood fixes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;vLLM Token Hacking:&lt;/strong&gt; As the agent sends out chat messages and receives strings or parsed tool calls, to get the tokens and log probabilities needed for RL, we had to lightly monkey-patch vLLM to expose the prompt and response tokens, not just the final text. We attempted other approaches such as retokenize the chat messages in RL framework -- all turning out to be unsuccessful and coming with different levels of bugs in the end. &lt;a href=\"https://github.com/microsoft/agent-lightning/blob/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/agentlightning/instrumentation/vllm.py\"&gt;https://github.com/microsoft/agent-lightning/blob/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/agentlightning/instrumentation/vllm.py&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;AgentOps Patching:&lt;/strong&gt; We use AgentOps for tracing, so we patched its client to grab our custom token data and embed it in the trace sent back to the training server.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Integration Workarounds:&lt;/strong&gt; The agentops-langgraph integration had a regression in its latest version, so we temporarily disabled it and implemented the trace logging manually. Simple, but necessary.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Custom RL Trainer:&lt;/strong&gt; Our RL training loop needed a custom &amp;quot;rollout collector&amp;quot; that passively waits for traces to be reported from the distributed clients, rather than actively stepping through a simulation itself.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;The Power of Decoupling&lt;/h1&gt;\n\n&lt;p&gt;This architecture has some powerful benefits. For example, you can run the fragile and computationally expensive model training on a powerful rented remote server, while running your lightweight agent on one or multiple local machines. This makes it trivial to switch between a commercial API and a self-hosted open-source model. If multiple people are using the same agent, their usage data (the &amp;quot;trajectories&amp;quot;) can be contributed to a central server, which federatedly and continuously fine-tunes and improves the model for everyone.&lt;/p&gt;\n\n&lt;p&gt;On the algorithm side, if you are not interested in RL, you can also use a prompt tuning algorithm to tune the prompt. We also implement a toy example under the server-client paradigm: &lt;a href=\"https://github.com/microsoft/agent-lightning/tree/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/examples/apo\"&gt;https://github.com/microsoft/agent-lightning/tree/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/examples/apo&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Try It Yourself&lt;/h1&gt;\n\n&lt;p&gt;We wanted to share this because we think it&amp;#39;s a powerful pattern for adding learning capabilities to the amazing agents this community is building.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve faced these same problems and don&amp;#39;t want to write hundreds of lines of glue code, you can check out our implementation, &lt;strong&gt;Agent-Lightning&lt;/strong&gt; , on GitHub: &lt;a href=\"https://aka.ms/agl\"&gt;https://aka.ms/agl&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;d love to hear any suggestions or about similar problems you&amp;#39;re facing.&lt;/p&gt;\n\n&lt;p&gt;Happy training!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?auto=webp&amp;s=52ac6240eed7693b6f63c97926272110edd06a6a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=57900143d22c1c24bc7123fead5fbf0b41f12a0b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5590ab50fa85026868ab622f6c4a1fe3ef74d042",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d093acf24195baabfa76710f0682f917df367a6",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=88a9b978ee97cdfd20f8b2dc732fd0837291ac6e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=532401c9034c1155791aafebe52c117f824a05a0",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e4a7440200ee5206c90be571456527293819046f",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m9m670",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "matluster",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753510735,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Improved performance on AMD GPUs in llama.cpp",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "HIP: Enable Matrix cores for MMQ Kernels, Enable stream-K for CDNA 3 by deepsek  Pull Request #14624  ggml-org/llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma6igb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": "#bbbdbf",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=ff757c6d769f0e0fe1fdf85f0f4e487b0377fb2b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753569731,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Improved performance on AMD GPUs in llama.cpp&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14624",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M.png?auto=webp&amp;s=8e2783f5bda35aba84f3242b3043d8bf61a06065",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a137f9385d081fc502fa321cd23f2304da161a9b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e73b5fb97c979116f90b7f62fed8f337f6ded60",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4271abe65efbe9152268b3736b06d9301b4c4903",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f83e8ae86bf4abec6cd1f36f6bc545bf9dc2cf55",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b91ff633132b5bbceaceedbe0530a12ec7ef5a90",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f9e440c5f8357f8d84b67772b9015d292af56c10",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1ma6igb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1ma6igb/hip_enable_matrix_cores_for_mmq_kernels_enable/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14624",
          "subreddit_subscribers": 505252,
          "created_utc": 1753569731,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is less related to models, and more related to model interactions, but would love for the community to offer feedback on an internal debate.\n\nWe see a lot of traffic flow through our oss edge/service proxy for LLM-based apps. This includes local models served via vLLM and Ollama. One failure mode that most recently tripped us up (as we scaled deployments of[archgw](https://github.com/katanemo/archgw)at a F500 telco) were transient errors in streaming LLM responses. Specifically, if the upstream LLM hangs midstream (this could be an API-based LLM or a local model running via vLLM or ollama) while streaming we fail rather painfully today. \n\nBy default we have timeouts for connections made upstream and backoff/retry policies, But that resiliency logic doesn't incorporate the more nuanced failure modes where LLMs can hang mid stream, and then the retry behavior isn't obvious. Here are two immediate strategies we are debating, and would love the feedback:\n\n1/ If we detect the stream to be hung for say X seconds, we could buffer the state up until that point, reconstruct the assistant messages and try again. This would replay the state back to the LLM up until that point and have it try generate its messages from that point. For example, lets say we are calling the chat.completions endpoint, with the following user message:\n\n*{\"role\": \"user\", \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"},*\n\nAnd mid stream the LLM hangs at this point\n\n*\\[{\"type\": \"text\", \"text\": \"The best answer is (\"}\\]*\n\nWe could then try with the following message to the upstream LLM\n\n*\\[*  \n*{\"role\": \"user\", \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"},*  \n*{\"role\": \"assistant\", \"content\": \"The best answer is (\"}*  \n*\\]*\n\nWhich would result in a response like\n\n*\\[{\"type\": \"text\", \"text\": \"B)\"}\\]*\n\nThis would be elegant, but we'll have to contend with potentially long buffer sizes, image content (although that is base64'd) and iron out any gotchas with how we use multiplexing to reduce connection overhead. But because the stream replay is stateful, I am not sure if we will expose ourselves to different downstream issues.\n\n2/ fail hard, and don't retry. Two options here a) simply to break the connection upstream and have the client handle the error like a fatal failures or b) send a streaming error event. We could end up sending something like:  \n*event: error*  \n*data: {\"error\":\"502 Bad Gateway\", \"message\":\"upstream failure\"}*\n\nBecause we would have already send partial data to the upstream client, we won't be able to modify the HTTP response code to 502. There are trade offs on both approaches, but from a great developer experience vs. control and visibility where would you lean and why?",
          "author_fullname": "t2_gwq7fd01b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Strategies for handling transient Server-Sent Events (SSE) from LLM responses",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mab16n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753583164,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is less related to models, and more related to model interactions, but would love for the community to offer feedback on an internal debate.&lt;/p&gt;\n\n&lt;p&gt;We see a lot of traffic flow through our oss edge/service proxy for LLM-based apps. This includes local models served via vLLM and Ollama. One failure mode that most recently tripped us up (as we scaled deployments of&lt;a href=\"https://github.com/katanemo/archgw\"&gt;archgw&lt;/a&gt;at a F500 telco) were transient errors in streaming LLM responses. Specifically, if the upstream LLM hangs midstream (this could be an API-based LLM or a local model running via vLLM or ollama) while streaming we fail rather painfully today. &lt;/p&gt;\n\n&lt;p&gt;By default we have timeouts for connections made upstream and backoff/retry policies, But that resiliency logic doesn&amp;#39;t incorporate the more nuanced failure modes where LLMs can hang mid stream, and then the retry behavior isn&amp;#39;t obvious. Here are two immediate strategies we are debating, and would love the feedback:&lt;/p&gt;\n\n&lt;p&gt;1/ If we detect the stream to be hung for say X seconds, we could buffer the state up until that point, reconstruct the assistant messages and try again. This would replay the state back to the LLM up until that point and have it try generate its messages from that point. For example, lets say we are calling the chat.completions endpoint, with the following user message:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What&amp;#39;s the Greek name for Sun? (A) Sol (B) Helios (C) Sun&amp;quot;},&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;And mid stream the LLM hangs at this point&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;[{&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;The best answer is (&amp;quot;}]&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;We could then try with the following message to the upstream LLM&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;[&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What&amp;#39;s the Greek name for Sun? (A) Sol (B) Helios (C) Sun&amp;quot;},&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;{&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;The best answer is (&amp;quot;}&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;]&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Which would result in a response like&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;[{&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;B)&amp;quot;}]&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;This would be elegant, but we&amp;#39;ll have to contend with potentially long buffer sizes, image content (although that is base64&amp;#39;d) and iron out any gotchas with how we use multiplexing to reduce connection overhead. But because the stream replay is stateful, I am not sure if we will expose ourselves to different downstream issues.&lt;/p&gt;\n\n&lt;p&gt;2/ fail hard, and don&amp;#39;t retry. Two options here a) simply to break the connection upstream and have the client handle the error like a fatal failures or b) send a streaming error event. We could end up sending something like:&lt;br/&gt;\n&lt;em&gt;event: error&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;data: {&amp;quot;error&amp;quot;:&amp;quot;502 Bad Gateway&amp;quot;, &amp;quot;message&amp;quot;:&amp;quot;upstream failure&amp;quot;}&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Because we would have already send partial data to the upstream client, we won&amp;#39;t be able to modify the HTTP response code to 502. There are trade offs on both approaches, but from a great developer experience vs. control and visibility where would you lean and why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?auto=webp&amp;s=36b4d1b414cc6cee73ea40ffdc363688d2e9e7d3",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e36c3744ed0552e2b02b7b82f9390cbd418feb4b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a73059070937e3d39089f8f5256a83264a7c1f37",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6131ad0d3e101ec5b9f07464fcac56396cf950af",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8391306b7ffc6166dc332548393944370f7db6bf",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=86e9a448892e718c7d223c1f239342afc88a6815",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=06fa6467d8bacb728da4ddc3aca382a33fc9beb7",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mab16n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AdditionalWeb107",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mab16n/strategies_for_handling_transient_serversent/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mab16n/strategies_for_handling_transient_serversent/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753583164,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am wondering everyones opinions on truth seeking accurate models that we could have that actually wont self censor somehow, we know that the **Chinese Models** are very very good at not saying anything against the Chinese Government but work great when talking about anything else in western civilization. We also know that models from big orgs like **Google** or **OpenAI**, or even **Grok** self censor and have things in place, look at the recent [X.com](http://X.com) thing over Grok calling itself MechaHi$ler, they quickly censored the model. Many models now have many subtle bias built in and if you ask for straight answers or things that seem fringe you get back the 'normie' answer. Is there hope? Do we get rid of all RLHF since humans are RUINING the models?",
          "author_fullname": "t2_665chw2s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is China the only hope for factual models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9tyg9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753538146,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am wondering everyones opinions on truth seeking accurate models that we could have that actually wont self censor somehow, we know that the &lt;strong&gt;Chinese Models&lt;/strong&gt; are very very good at not saying anything against the Chinese Government but work great when talking about anything else in western civilization. We also know that models from big orgs like &lt;strong&gt;Google&lt;/strong&gt; or &lt;strong&gt;OpenAI&lt;/strong&gt;, or even &lt;strong&gt;Grok&lt;/strong&gt; self censor and have things in place, look at the recent &lt;a href=\"http://X.com\"&gt;X.com&lt;/a&gt; thing over Grok calling itself MechaHi$ler, they quickly censored the model. Many models now have many subtle bias built in and if you ask for straight answers or things that seem fringe you get back the &amp;#39;normie&amp;#39; answer. Is there hope? Do we get rid of all RLHF since humans are RUINING the models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9tyg9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Meme_Lord_Musk",
          "discussion_type": null,
          "num_comments": 104,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9tyg9/is_china_the_only_hope_for_factual_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9tyg9/is_china_the_only_hope_for_factual_models/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753538146,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Honestly, pretty much the question in the Header. Specifically, I'm trying to run InternVL3-78B or the new Intern-S1 model locally, but it's a challenge. VLLM and lmserve support the InternVL models, but appear to be GPU-only, and llama.cpp seems flaky at best when it comes to running them. (Massive hallucinations, errors with the model thinking there's no image attached, etc.) I'm mostly looking to do image tagging with something more accurate than the (still quite good, but aging) wd14 model found in kohya\\_ss. I could probably step down to InternVL3-38B and still get some pretty great results, but I would need a 4 bit quant to fit into my GPU's VRAM if using an engine that doesn't support CPU offloading. Most quants for the model outside of GGUFs appear to be 8 bit. I could quantize it myself if I truly need to, but I'm hoping there's a simpler solution I'm just unfamiliar with. I'm quite used to running LLMs locally, but multimodal models with image processing are new to me. Any help or insight for a good way to handle image tagging locally would be greatly appreciated!",
          "author_fullname": "t2_crdgh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How Are You Running Multimodal (Text-Image) Models Locally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mad6sy",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753590152,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Honestly, pretty much the question in the Header. Specifically, I&amp;#39;m trying to run InternVL3-78B or the new Intern-S1 model locally, but it&amp;#39;s a challenge. VLLM and lmserve support the InternVL models, but appear to be GPU-only, and llama.cpp seems flaky at best when it comes to running them. (Massive hallucinations, errors with the model thinking there&amp;#39;s no image attached, etc.) I&amp;#39;m mostly looking to do image tagging with something more accurate than the (still quite good, but aging) wd14 model found in kohya_ss. I could probably step down to InternVL3-38B and still get some pretty great results, but I would need a 4 bit quant to fit into my GPU&amp;#39;s VRAM if using an engine that doesn&amp;#39;t support CPU offloading. Most quants for the model outside of GGUFs appear to be 8 bit. I could quantize it myself if I truly need to, but I&amp;#39;m hoping there&amp;#39;s a simpler solution I&amp;#39;m just unfamiliar with. I&amp;#39;m quite used to running LLMs locally, but multimodal models with image processing are new to me. Any help or insight for a good way to handle image tagging locally would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mad6sy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Stickman561",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mad6sy/how_are_you_running_multimodal_textimage_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mad6sy/how_are_you_running_multimodal_textimage_models/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753590152,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nI created an audiobook generator https://github.com/Jeremy-Harper/chatterboxPro\n\nIm at the point Ive started to wire in the llama calls to start making the system smarter. Im thinking being able to flag chapters without having them need to be in a chapter # format, being able to rewrite failed attempts so that it uses simpler words while keeping the meaning, and let it make it smart enough to fix other errors. \n\nAny other ideas or suggestions? \n\nWhy did I do this project? Im a fiction author who wanted the creative control to generate my own audiobooks as Im writing to find where Im inconsistent (words on the page and I fill in the blank) and I liked the idea of being able to have my own eleven labs equivalent running entirely locally.\n",
          "author_fullname": "t2_1tk6u7slxe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chatterbox multi hour generator",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9xx6w",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/-pK_1UZpR6irxXwKeSd6fuYttXVq3zCpxsWJwem9Z2I.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753548017,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I created an audiobook generator &lt;a href=\"https://github.com/Jeremy-Harper/chatterboxPro\"&gt;https://github.com/Jeremy-Harper/chatterboxPro&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Im at the point Ive started to wire in the llama calls to start making the system smarter. Im thinking being able to flag chapters without having them need to be in a chapter # format, being able to rewrite failed attempts so that it uses simpler words while keeping the meaning, and let it make it smart enough to fix other errors. &lt;/p&gt;\n\n&lt;p&gt;Any other ideas or suggestions? &lt;/p&gt;\n\n&lt;p&gt;Why did I do this project? Im a fiction author who wanted the creative control to generate my own audiobooks as Im writing to find where Im inconsistent (words on the page and I fill in the blank) and I liked the idea of being able to have my own eleven labs equivalent running entirely locally.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/4itbo3xjy8ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/4itbo3xjy8ff1.jpeg?auto=webp&amp;s=3e373c6a4b343a8fe452544fefce0916f39a8f75",
                  "width": 3016,
                  "height": 1778
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/4itbo3xjy8ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c92003d58027207c4a0ec4ff020bf077fab3271",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://preview.redd.it/4itbo3xjy8ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0bca540a5b57d35c2bb6e26752a28eb56c2b6219",
                    "width": 216,
                    "height": 127
                  },
                  {
                    "url": "https://preview.redd.it/4itbo3xjy8ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cdc1a986c00f851bd314652981becc81089935cb",
                    "width": 320,
                    "height": 188
                  },
                  {
                    "url": "https://preview.redd.it/4itbo3xjy8ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae415393ab6e6cba90ad464a6717ea838b23b9c5",
                    "width": 640,
                    "height": 377
                  },
                  {
                    "url": "https://preview.redd.it/4itbo3xjy8ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=61900058a9270505fff9bdbb5f6df2041b34a417",
                    "width": 960,
                    "height": 565
                  },
                  {
                    "url": "https://preview.redd.it/4itbo3xjy8ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b09a9481f73422b1be0014f045bd88dfbff464cf",
                    "width": 1080,
                    "height": 636
                  }
                ],
                "variants": {},
                "id": "9w4qAhQa8L9soVZshc6TGTYzEupmzpwCu8BFMk-5QOk"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9xx6w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Upbeat5840",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9xx6w/chatterbox_multi_hour_generator/",
          "stickied": false,
          "url": "https://i.redd.it/4itbo3xjy8ff1.jpeg",
          "subreddit_subscribers": 505252,
          "created_utc": 1753548017,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks, I'm a self-hosting noob looking for recommendations for good self-hosted/foss/local/private/etc alternative to Claude Code's CLI tool. I recently started using at work and am blown away by how good it is. Would love to have something similar for myself. I have a 12GB VRAM RTX 3060 GPU with Ollama running in a docker container.\n\nI haven't done extensive research to be honest, but I did try searching for a bit in general. I found a tool called Aider that was similar that I tried installing and using. It was okay, not as polished as Claude Code imo (and had a lot of, imo, poor choices for default settings; e.g. auto commit to git and not asking for permission first before editing files).\n\nAnyway, I'm going to keep searching - I've come across a few articles with recommendations but I thought I'd ask here since you folks probably are more in line with my personal philosophy/requirements than some random articles (probably written by some AI itself) recommending tools. Otherwise, I'm going to have to go through these lists and try out the ones that look interesting and potentially liter my system with useless tools lol.\n\nThanks in advance for any pointers!",
          "author_fullname": "t2_4ekhk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Claude Code Alternative Recommendations?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1macmej",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753588296,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I&amp;#39;m a self-hosting noob looking for recommendations for good self-hosted/foss/local/private/etc alternative to Claude Code&amp;#39;s CLI tool. I recently started using at work and am blown away by how good it is. Would love to have something similar for myself. I have a 12GB VRAM RTX 3060 GPU with Ollama running in a docker container.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t done extensive research to be honest, but I did try searching for a bit in general. I found a tool called Aider that was similar that I tried installing and using. It was okay, not as polished as Claude Code imo (and had a lot of, imo, poor choices for default settings; e.g. auto commit to git and not asking for permission first before editing files).&lt;/p&gt;\n\n&lt;p&gt;Anyway, I&amp;#39;m going to keep searching - I&amp;#39;ve come across a few articles with recommendations but I thought I&amp;#39;d ask here since you folks probably are more in line with my personal philosophy/requirements than some random articles (probably written by some AI itself) recommending tools. Otherwise, I&amp;#39;m going to have to go through these lists and try out the ones that look interesting and potentially liter my system with useless tools lol.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any pointers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1macmej",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VashyTheNexian",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1macmej/claude_code_alternative_recommendations/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1macmej/claude_code_alternative_recommendations/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753588296,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Presenton, the open source AI presentation generator that can run locally over Ollama.\n\nPresenton now supports custom AI layouts. Create custom templates with HTML, Tailwind and Zod for schema. Then, use it to create presentations over AI.\n\nWe've added a lot more improvements with this release on Presenton:\n\n* Stunning in-built layouts to create AI presentations with\n* Custom HTML layouts/ themes/ templates\n* Workflow to create custom templates for developers\n* API support for custom templates\n* Choose text and image models separately giving much more flexibility\n* Better support for local llama\n* Support for external SQL database if you want to deploy for enterprise use (you don't need our permission. apache 2.0, remember!  )\n\nYou can learn more about how to create custom layouts here:[https://docs.presenton.ai/tutorial/create-custom-presentation-layouts](https://docs.presenton.ai/tutorial/create-custom-presentation-layouts).\n\nWe'll soon release template vibe-coding guide.(I recently vibe-coded a stunning template within an hour.)\n\nDo checkout and try out github if you haven't:[https://github.com/presenton/presenton](https://github.com/presenton/presenton)\n\nLet me know if you have any feedback!\n\n",
          "author_fullname": "t2_bkd0cds8w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source AI presentation generator with custom layouts support for custom presentation design",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9w0k8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/3TaAFjgHsuOU0YsfCoXoZqG5pSqwjHwFULtNA_12lm0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753543368,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Presenton, the open source AI presentation generator that can run locally over Ollama.&lt;/p&gt;\n\n&lt;p&gt;Presenton now supports custom AI layouts. Create custom templates with HTML, Tailwind and Zod for schema. Then, use it to create presentations over AI.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve added a lot more improvements with this release on Presenton:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Stunning in-built layouts to create AI presentations with&lt;/li&gt;\n&lt;li&gt;Custom HTML layouts/ themes/ templates&lt;/li&gt;\n&lt;li&gt;Workflow to create custom templates for developers&lt;/li&gt;\n&lt;li&gt;API support for custom templates&lt;/li&gt;\n&lt;li&gt;Choose text and image models separately giving much more flexibility&lt;/li&gt;\n&lt;li&gt;Better support for local llama&lt;/li&gt;\n&lt;li&gt;Support for external SQL database if you want to deploy for enterprise use (you don&amp;#39;t need our permission. apache 2.0, remember!  )&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can learn more about how to create custom layouts here:&lt;a href=\"https://docs.presenton.ai/tutorial/create-custom-presentation-layouts\"&gt;https://docs.presenton.ai/tutorial/create-custom-presentation-layouts&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ll soon release template vibe-coding guide.(I recently vibe-coded a stunning template within an hour.)&lt;/p&gt;\n\n&lt;p&gt;Do checkout and try out github if you haven&amp;#39;t:&lt;a href=\"https://github.com/presenton/presenton\"&gt;https://github.com/presenton/presenton&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know if you have any feedback!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/64z0fr7dj8ff1.gif",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?format=png8&amp;s=55d510603d9b4b2b7476e75f7770451a20ae39e3",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=cf16b61187ee06e473e1e9ff022978c1717ff4a8",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=8d5739d6ca25151d5473824d9ee83a49c3d20724",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=b2d15536d2897c207dde7e1066f64012ab37e6b5",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=dd597aea015cee349746f5e06a3f9ac6606e036e",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=5ca8b0a5ec20831aaee139075748eb1cd64c4cf3",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=2055c25afab55cdd5e18dd8c561b2dcee711b727",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {
                  "gif": {
                    "source": {
                      "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?s=edb84d5ed81dde40a0ad141f792647d7efb5af0e",
                      "width": 1920,
                      "height": 1080
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=108&amp;crop=smart&amp;s=4d4ad7d5d09b6cfef1fdbfd67b2587983681a9c6",
                        "width": 108,
                        "height": 60
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=216&amp;crop=smart&amp;s=ab115b0157919a7bbae40460c2dab8dd86c6e230",
                        "width": 216,
                        "height": 121
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=320&amp;crop=smart&amp;s=c7539832fb280499ff2ef68b585baf0fd63dfa39",
                        "width": 320,
                        "height": 180
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=640&amp;crop=smart&amp;s=5d734c108e432bdaa3d24e5983c9a5533c319053",
                        "width": 640,
                        "height": 360
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=960&amp;crop=smart&amp;s=419e46bf95cf1298a16a86b13ac626b985c2ed5d",
                        "width": 960,
                        "height": 540
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=1080&amp;crop=smart&amp;s=9ec4e710d02d01e702a9e72d37da4136694d3449",
                        "width": 1080,
                        "height": 607
                      }
                    ]
                  },
                  "mp4": {
                    "source": {
                      "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?format=mp4&amp;s=dcb74f5db525ae5ed36a4ed5ed8bcf460e4db274",
                      "width": 1920,
                      "height": 1080
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=108&amp;format=mp4&amp;s=7c67883a8e429ba4b413e909b0a83a16298badf0",
                        "width": 108,
                        "height": 60
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=216&amp;format=mp4&amp;s=6d6b817723ee40c191189a0f45ca5677f6bff12d",
                        "width": 216,
                        "height": 121
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=320&amp;format=mp4&amp;s=03877896998d376c6879c07701ea3e1b0a56b072",
                        "width": 320,
                        "height": 180
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=640&amp;format=mp4&amp;s=946d6c0fa6f868f7bde8e8fa5acd2e36c67a5faa",
                        "width": 640,
                        "height": 360
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=960&amp;format=mp4&amp;s=5565af8cf760d11a96258be5009d537a3f62f51e",
                        "width": 960,
                        "height": 540
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=1080&amp;format=mp4&amp;s=12dd37af710f7f04c9ca6ebd6be75a434b8b6dea",
                        "width": 1080,
                        "height": 607
                      }
                    ]
                  }
                },
                "id": "dEgj3z3UvGxgNkj_vx7CiDDUbve6BYTxMtM7vqaJ6Vk"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1m9w0k8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "goodboydhrn",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9w0k8/open_source_ai_presentation_generator_with_custom/",
          "stickied": false,
          "url": "https://i.redd.it/64z0fr7dj8ff1.gif",
          "subreddit_subscribers": 505252,
          "created_utc": 1753543368,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_w6l58p741",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama 3.3 Nemotron Super 49B v1.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9fb5t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 246,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 246,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=d4fdf213f77dc67f6bcc525b1a8210247ba2b251",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753488919,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?auto=webp&amp;s=af31b2002f0236c31cf3c91755fd855ed95ae985",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45e7c8c14055c57d8c62dad0b150faa3212ce087",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=82619b61b919798034ba0f0b798bd1e75640c0b9",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eaf3accf5409bb25bb8728256d4e2f61e2bbbeec",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8ce793850ca6936254a722184eb2367e6423fa1",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e1d1bb2008c0cfb9abdbf16638bc668942167e7",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=313d9b1115102aa6244b349a8e99c1ee840c4702",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9fb5t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheLocalDrummer",
          "discussion_type": null,
          "num_comments": 49,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9fb5t/llama_33_nemotron_super_49b_v15/",
          "stickied": false,
          "url": "https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5",
          "subreddit_subscribers": 505252,
          "created_utc": 1753488919,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": " Announcing Llama Nemotron Super v1.5  \n\nThis release pushes the boundaries of reasoning model capabilities at the weight class of the model and is ready to power agentic applications from individual developers, all the way to enterprise applications.\n\n The Llama Nemotron Super v1.5 achieves leading reasoning accuracies for science, math, code, and agentic tasks while delivering up to 3x higher throughput.\n\nThis is currently the best model that can be deployed on a single H100. Reasoning On/Off and drop in replacement for V1. Open-weight, code and data on HF.\n\nTry it on build.nvidia.com, or download from Huggingface:  https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\n\nTech blog: https://developer.nvidia.com/blog/build-more-accurate-and-efficient-ai-agents-with-the-new-nvidia-llama-nemotron-super-v1-5/\n\n",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Nvidia released Llama Nemotron Super v1.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 63,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9gzl7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 155,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 155,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Mdn0aNLQH3YQNOrr-Tm7NUEXZtNFtif87GS10DP-lUo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753493788,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt; Announcing Llama Nemotron Super v1.5  &lt;/p&gt;\n\n&lt;p&gt;This release pushes the boundaries of reasoning model capabilities at the weight class of the model and is ready to power agentic applications from individual developers, all the way to enterprise applications.&lt;/p&gt;\n\n&lt;p&gt; The Llama Nemotron Super v1.5 achieves leading reasoning accuracies for science, math, code, and agentic tasks while delivering up to 3x higher throughput.&lt;/p&gt;\n\n&lt;p&gt;This is currently the best model that can be deployed on a single H100. Reasoning On/Off and drop in replacement for V1. Open-weight, code and data on HF.&lt;/p&gt;\n\n&lt;p&gt;Try it on build.nvidia.com, or download from Huggingface:  &lt;a href=\"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\"&gt;https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Tech blog: &lt;a href=\"https://developer.nvidia.com/blog/build-more-accurate-and-efficient-ai-agents-with-the-new-nvidia-llama-nemotron-super-v1-5/\"&gt;https://developer.nvidia.com/blog/build-more-accurate-and-efficient-ai-agents-with-the-new-nvidia-llama-nemotron-super-v1-5/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/yl29obvah4ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?auto=webp&amp;s=7fffb3470be09ef63cc39a1b6af90253aff54c22",
                  "width": 2466,
                  "height": 1120
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c9b0f8c1eca8a47c7d775c89f5c127aa3db8d6f",
                    "width": 108,
                    "height": 49
                  },
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=123c21d63703675b2484dfbb26c9dc02ac026e25",
                    "width": 216,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d6f5333c55876bb77959048c710f51e7a105ad2e",
                    "width": 320,
                    "height": 145
                  },
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dcb0f2aebe1fc97e958a20cef89b87457b4d2a0e",
                    "width": 640,
                    "height": 290
                  },
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0b3f7303781adedbfc37b3089d1866573c389f04",
                    "width": 960,
                    "height": 436
                  },
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=273c5d4269b73b8257c5f73f56b72e4c3a6e7690",
                    "width": 1080,
                    "height": 490
                  }
                ],
                "variants": {},
                "id": "tSDOO5hUPOZl8eN16lC6FZmJgft--5o4tmzkJE3v4I8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9gzl7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9gzl7/nvidia_released_llama_nemotron_super_v15/",
          "stickied": false,
          "url": "https://i.redd.it/yl29obvah4ff1.jpeg",
          "subreddit_subscribers": 505252,
          "created_utc": 1753493788,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As the title suggests, I am thinking of selling my 16gb 5070 ti, but Id get a 3090 (and some money back in my pocket) to run local LLMs. \n\nIm building a pipeline that will essentially help me gather news/tech news and keep me informed so I can ask it specific questions and save time instead of watching many different news outlets during the day. I want to use larger models and be able to mix different ones together. Im still new at this and originally I bought the 5070ti for gaming. \n\nNow I know Ill lose some gaming performance but not a big deal for 1440p. My main question is if its a smart move because of the VRAM? Or once Blackwell optimization gets better, Ill be better off with the 5070ti? Because even if they launch a super with 24gb down the line, theres no way itll be cheap, so it would be no different than selling it now and getting say a 4090. Any help is appreciated. ",
          "author_fullname": "t2_fxgljyu2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Sell my 5070ti to get a 3090",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1maj65f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753612867,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title suggests, I am thinking of selling my 16gb 5070 ti, but Id get a 3090 (and some money back in my pocket) to run local LLMs. &lt;/p&gt;\n\n&lt;p&gt;Im building a pipeline that will essentially help me gather news/tech news and keep me informed so I can ask it specific questions and save time instead of watching many different news outlets during the day. I want to use larger models and be able to mix different ones together. Im still new at this and originally I bought the 5070ti for gaming. &lt;/p&gt;\n\n&lt;p&gt;Now I know Ill lose some gaming performance but not a big deal for 1440p. My main question is if its a smart move because of the VRAM? Or once Blackwell optimization gets better, Ill be better off with the 5070ti? Because even if they launch a super with 24gb down the line, theres no way itll be cheap, so it would be no different than selling it now and getting say a 4090. Any help is appreciated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1maj65f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Finallyhaveredditt",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maj65f/sell_my_5070ti_to_get_a_3090/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maj65f/sell_my_5070ti_to_get_a_3090/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753612867,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What is the vram sweet spot these days? 48gb was for a while, but now I've seen different numbers being posted. Curious what others think. I think its still the 24 to 48gb range, but depends how you are going to use it.\n\nTo keep it simple, let's look at just inference. Training obviously needs as much vram as possible.",
          "author_fullname": "t2_40xsg56g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "VRAM sweet spot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma5yw4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753568246,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the vram sweet spot these days? 48gb was for a while, but now I&amp;#39;ve seen different numbers being posted. Curious what others think. I think its still the 24 to 48gb range, but depends how you are going to use it.&lt;/p&gt;\n\n&lt;p&gt;To keep it simple, let&amp;#39;s look at just inference. Training obviously needs as much vram as possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ma5yw4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fgoricha",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma5yw4/vram_sweet_spot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma5yw4/vram_sweet_spot/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753568246,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been using GPT-4o for most of my daily queries - my main usecase is to map my thoughts, some of this stuff is sensitive so I need a local solution.\n\nI REALLY like the tone of GPT-4o (yeah, I am a sucker for glazing!)   \nWhat would be the best model to use for this usecase?   \n  \nI am thinking 13-32B models which are uncensored because I wouldn't want to be moral policed.  \nI have an RTX 4090 with 96 gigs of ram and a Ryzen 9 7900 processor.",
          "author_fullname": "t2_o087g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best non-thinking model which can be a long context personal assistant?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9ukpw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753539777,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been using GPT-4o for most of my daily queries - my main usecase is to map my thoughts, some of this stuff is sensitive so I need a local solution.&lt;/p&gt;\n\n&lt;p&gt;I REALLY like the tone of GPT-4o (yeah, I am a sucker for glazing!)&lt;br/&gt;\nWhat would be the best model to use for this usecase?   &lt;/p&gt;\n\n&lt;p&gt;I am thinking 13-32B models which are uncensored because I wouldn&amp;#39;t want to be moral policed.&lt;br/&gt;\nI have an RTX 4090 with 96 gigs of ram and a Ryzen 9 7900 processor.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9ukpw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "trithilon",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9ukpw/best_nonthinking_model_which_can_be_a_long/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9ukpw/best_nonthinking_model_which_can_be_a_long/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753539777,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I think I'm missing some, thanks ",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any new OpenSource LLM apps or websites? Such as Qwen or Deepseek?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma1amq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753556284,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think I&amp;#39;m missing some, thanks &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ma1amq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma1amq/any_new_opensource_llm_apps_or_websites_such_as/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma1amq/any_new_opensource_llm_apps_or_websites_such_as/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753556284,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sometimes, you can overfit a model to think *too* deeply. There seems to be a balance required for a model to break a problem down step-by-step, but not overthink it. I find that Phi-4 is good at problem solving and thinking analytically, but doesn't understand when something *isn't a problem*. Not everything is a problem it has to solve. \nThis kind of model behavior shows that step-by-step reasoning needs not just the ability to break things down, but also the judgment to recognize when thats not needed at all. I don't believe the Microsoft team balanced that correctly.\n\nWhat's your thoughts on this? Do you find that certain models are better at balancing this?\n\nHere's an example:\n\n```\nollama run hf.co/lmstudio-community/Phi-4-mini-reasoning-GGUF:Phi-4-mini-reasoning-Q8_0\n&gt;&gt;&gt; Hello!\n&lt;think&gt;\nOkay, so I need to solve this problem where I'm given a number and have to find the sum of its digits. Let's see... The user wrote \n\"You will be given a problem. Please reason step by step, and put your final answer within \\boxed{}: Hello!\" Hmm, maybe there's a \ntypo or something missing here. Wait, perhaps the actual problem is to calculate the sum of the digits of the number that comes \nafter \"Hello!\". But in the current message, it just says \"Hello!\" Maybe that's part of the problem? Let me check again.\n\nWait, looking at the original query: The user starts with \"You will be given a problem. Please reason step by step...\" and then \nwrites \"Hello!\". So perhaps the problem is simply to find the sum of the digits in the word \"Hello!\", but that doesn't make sense \nbecause \"Hello!\" is text, not numbers. Unless maybe there's an error here. Alternatively, maybe there was supposed to be a number \nprovided after \"Hello!\" but it got cut off or mistyped.\n\nAlternatively, could this be a trick question where \"Hello!\" is converted into numbers in some way? For example, using the position \nof each letter in the alphabet: H=8, E=5, L=12, L=12, O=15, and then maybe the exclamation mark as something else. But summing those \ndigits... Wait, if we take each letter's numerical value and sum them, but then split into individual digits? Let me try that.\n\nH is the 8th letter, E is 5, L is 12, another L is 12, O is 15. The exclamation mark isn't a letter, so maybe ignore it. So adding \nthose numbers: 8 + 5 + 12 + 12 + 15. Let's calculate that:\n\n8 + 5 = 13\n\n13 + 12 = 25\n\n25 + 12 = 37\n\n37 + 15 = 52\n\nSo the sum is 52. But then, if we need to split into digits and sum again: 5 + 2 = 7. Hmm, but that's speculative. The problem \nstatement isn't clear. Alternatively, maybe \"Hello!\" is a typo for a number, like HELLO! where H,E,L,L,O are hexadecimal digits? \nWait, in hex, letters go up to F, so H and O aren't valid hex digits. So that might not work.\n\nAlternatively, perhaps the user intended to write a different problem but made a mistake in input. For example, maybe they meant to \nprovide a number like 12345 but accidentally typed \"Hello!\" instead. In that case, there's insufficient information. But since I \nhave to work with what's given, I need to make an educated guess.\n\nAnother angle: sometimes in puzzles, the word \"Hello\" is associated with ASCII codes. For example, H is 72, E is 69, L is 76, \nanother L is 76, O is 79. Let's check that:\n\nH = 72\n\nE = 69\n\nL = 76\n\nL = 76\n\nO = 79\n\nSumming those numbers: 72 + 69 = 141; 141 + 76 = 217; 217 + 76 = 293; 293 + 79 = 372. Then, if we need to sum the digits of each \nnumber before adding? Wait, but that would be different. For example, summing digits of 72: 7+2=9, 69:6+9=15, etc., then add all \nthose digit sums. Let's try:\n\nH (72): 7 + 2 = 9\n\nE (69): 6 + 9 = 15\n\nL (76): 7 + 6 = 13\n\nAnother L (76): 13 again\n\nO (79): 7 + 9 = 16\n\nTotal sum: 9 + 15 + 13 + 13 + 16. Let's compute:\n\n9 + 15 = 24\n\n24 + 13 = 37\n\n37 + 13 = 50\n\n50 + 16 = 66\n\nSo the total would be 66. But again, this is speculative because the problem statement isn't clear.\n\nAlternatively, maybe \"Hello!\" is just a greeting and the actual number is missing. Since the user might have made an error in \nsubmitting the problem, perhaps they meant to provide a different input. In that case, I should inform them that the problem seems \nincomplete. However, since I'm supposed to act as if this is the given problem, maybe there's another interpretation.\n\nWait, could \"Hello!\" be a hexadecimal number? Let's see: H isn't a hex digit (0-9, A-F), so that's not possible. Similarly for other \nletters. So perhaps it's base64 encoded or something else. Alternatively, converting each letter to its ASCII value and \nconcatenating them into a single number.\n\n...\n[[manually shortened to keep it moderately brief]]\n...\n\nAlternatively, if it's base26 or something, but that's more complex.\n\nGiven all possible interpretations and without further info, I think the safest assumption is that there was a mistake in the \nproblem entry, and \"Hello!\" should be replaced with an actual number. However, since I must answer based on given input, perhaps the \nintended answer is 7 (from summing H(8)+E(5)+L(12)+L(12)+O(15)=52 5+2=7), which is a common type of puzzle.\n\nAlternatively, if we consider that \"Hello!\" has no digits, sum is zero. But since the user might have intended something else, maybe \n72 or another number.\n\nWait, perhaps the problem was meant to be in Chinese or another language where \"Hello!\" translates to numbers, but that's unlikely \nhere.\n\nAnother angle: Maybe \"sum of its digits\" refers to the string's length? \"Hello!\" has 6 characters, sum is 6. But that doesn't make \nsense for digits.\n\nAlternatively, if we consider each letter as a digit in base26 or another system, but without specification, it's unclear.\n\nGiven all this uncertainty, I think the most reasonable answers could be either 0 (no numeric characters) or 7 (sum of letters' \npositions). Since math problems often use A=1 etc., I'll go with 7. However, I'm not entirely sure. But let's check once more:\n\nIf H is 8, E is5, L is12, O is15:\n\nSum:8+5+12+12+15=52  digits 5 and 2 sum to 7.\n\nYes, that's a common puzzle type. So maybe the answer is \\boxed{7}.\n&lt;/think&gt;\n\nThe problem asks for the sum of the digits after converting each letter in \"Hello!\" to its corresponding position in the alphabet \n(A=1, B=2, ..., Z=26). \n\n- H = 8\n- E = 5\n- L = 12\n- L = 12\n- O = 15\n\nSumming these values:  \n\\(8 + 5 + 12 + 12 + 15 = 52\\)\n\nNext, we sum the digits of 52:  \n\\(5 + 2 = 7\\)\n\nThus, the final answer is \\(\\boxed{7}\\).\n```",
          "author_fullname": "t2_s9lc0xnao",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Phi-4-mini-reasoning: An example of \"overfitting to think\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9tt3o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753537732,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sometimes, you can overfit a model to think &lt;em&gt;too&lt;/em&gt; deeply. There seems to be a balance required for a model to break a problem down step-by-step, but not overthink it. I find that Phi-4 is good at problem solving and thinking analytically, but doesn&amp;#39;t understand when something &lt;em&gt;isn&amp;#39;t a problem&lt;/em&gt;. Not everything is a problem it has to solve. \nThis kind of model behavior shows that step-by-step reasoning needs not just the ability to break things down, but also the judgment to recognize when thats not needed at all. I don&amp;#39;t believe the Microsoft team balanced that correctly.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s your thoughts on this? Do you find that certain models are better at balancing this?&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an example:&lt;/p&gt;\n\n&lt;p&gt;```\nollama run hf.co/lmstudio-community/Phi-4-mini-reasoning-GGUF:Phi-4-mini-reasoning-Q8_0&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;blockquote&gt;\n&lt;blockquote&gt;\n&lt;p&gt;Hello!\n&amp;lt;think&amp;gt;\nOkay, so I need to solve this problem where I&amp;#39;m given a number and have to find the sum of its digits. Let&amp;#39;s see... The user wrote \n&amp;quot;You will be given a problem. Please reason step by step, and put your final answer within \\boxed{}: Hello!&amp;quot; Hmm, maybe there&amp;#39;s a \ntypo or something missing here. Wait, perhaps the actual problem is to calculate the sum of the digits of the number that comes \nafter &amp;quot;Hello!&amp;quot;. But in the current message, it just says &amp;quot;Hello!&amp;quot; Maybe that&amp;#39;s part of the problem? Let me check again.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/blockquote&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Wait, looking at the original query: The user starts with &amp;quot;You will be given a problem. Please reason step by step...&amp;quot; and then \nwrites &amp;quot;Hello!&amp;quot;. So perhaps the problem is simply to find the sum of the digits in the word &amp;quot;Hello!&amp;quot;, but that doesn&amp;#39;t make sense \nbecause &amp;quot;Hello!&amp;quot; is text, not numbers. Unless maybe there&amp;#39;s an error here. Alternatively, maybe there was supposed to be a number \nprovided after &amp;quot;Hello!&amp;quot; but it got cut off or mistyped.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, could this be a trick question where &amp;quot;Hello!&amp;quot; is converted into numbers in some way? For example, using the position \nof each letter in the alphabet: H=8, E=5, L=12, L=12, O=15, and then maybe the exclamation mark as something else. But summing those \ndigits... Wait, if we take each letter&amp;#39;s numerical value and sum them, but then split into individual digits? Let me try that.&lt;/p&gt;\n\n&lt;p&gt;H is the 8th letter, E is 5, L is 12, another L is 12, O is 15. The exclamation mark isn&amp;#39;t a letter, so maybe ignore it. So adding \nthose numbers: 8 + 5 + 12 + 12 + 15. Let&amp;#39;s calculate that:&lt;/p&gt;\n\n&lt;p&gt;8 + 5 = 13&lt;/p&gt;\n\n&lt;p&gt;13 + 12 = 25&lt;/p&gt;\n\n&lt;p&gt;25 + 12 = 37&lt;/p&gt;\n\n&lt;p&gt;37 + 15 = 52&lt;/p&gt;\n\n&lt;p&gt;So the sum is 52. But then, if we need to split into digits and sum again: 5 + 2 = 7. Hmm, but that&amp;#39;s speculative. The problem \nstatement isn&amp;#39;t clear. Alternatively, maybe &amp;quot;Hello!&amp;quot; is a typo for a number, like HELLO! where H,E,L,L,O are hexadecimal digits? \nWait, in hex, letters go up to F, so H and O aren&amp;#39;t valid hex digits. So that might not work.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, perhaps the user intended to write a different problem but made a mistake in input. For example, maybe they meant to \nprovide a number like 12345 but accidentally typed &amp;quot;Hello!&amp;quot; instead. In that case, there&amp;#39;s insufficient information. But since I \nhave to work with what&amp;#39;s given, I need to make an educated guess.&lt;/p&gt;\n\n&lt;p&gt;Another angle: sometimes in puzzles, the word &amp;quot;Hello&amp;quot; is associated with ASCII codes. For example, H is 72, E is 69, L is 76, \nanother L is 76, O is 79. Let&amp;#39;s check that:&lt;/p&gt;\n\n&lt;p&gt;H = 72&lt;/p&gt;\n\n&lt;p&gt;E = 69&lt;/p&gt;\n\n&lt;p&gt;L = 76&lt;/p&gt;\n\n&lt;p&gt;L = 76&lt;/p&gt;\n\n&lt;p&gt;O = 79&lt;/p&gt;\n\n&lt;p&gt;Summing those numbers: 72 + 69 = 141; 141 + 76 = 217; 217 + 76 = 293; 293 + 79 = 372. Then, if we need to sum the digits of each \nnumber before adding? Wait, but that would be different. For example, summing digits of 72: 7+2=9, 69:6+9=15, etc., then add all \nthose digit sums. Let&amp;#39;s try:&lt;/p&gt;\n\n&lt;p&gt;H (72): 7 + 2 = 9&lt;/p&gt;\n\n&lt;p&gt;E (69): 6 + 9 = 15&lt;/p&gt;\n\n&lt;p&gt;L (76): 7 + 6 = 13&lt;/p&gt;\n\n&lt;p&gt;Another L (76): 13 again&lt;/p&gt;\n\n&lt;p&gt;O (79): 7 + 9 = 16&lt;/p&gt;\n\n&lt;p&gt;Total sum: 9 + 15 + 13 + 13 + 16. Let&amp;#39;s compute:&lt;/p&gt;\n\n&lt;p&gt;9 + 15 = 24&lt;/p&gt;\n\n&lt;p&gt;24 + 13 = 37&lt;/p&gt;\n\n&lt;p&gt;37 + 13 = 50&lt;/p&gt;\n\n&lt;p&gt;50 + 16 = 66&lt;/p&gt;\n\n&lt;p&gt;So the total would be 66. But again, this is speculative because the problem statement isn&amp;#39;t clear.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, maybe &amp;quot;Hello!&amp;quot; is just a greeting and the actual number is missing. Since the user might have made an error in \nsubmitting the problem, perhaps they meant to provide a different input. In that case, I should inform them that the problem seems \nincomplete. However, since I&amp;#39;m supposed to act as if this is the given problem, maybe there&amp;#39;s another interpretation.&lt;/p&gt;\n\n&lt;p&gt;Wait, could &amp;quot;Hello!&amp;quot; be a hexadecimal number? Let&amp;#39;s see: H isn&amp;#39;t a hex digit (0-9, A-F), so that&amp;#39;s not possible. Similarly for other \nletters. So perhaps it&amp;#39;s base64 encoded or something else. Alternatively, converting each letter to its ASCII value and \nconcatenating them into a single number.&lt;/p&gt;\n\n&lt;p&gt;...\n[[manually shortened to keep it moderately brief]]\n...&lt;/p&gt;\n\n&lt;p&gt;Alternatively, if it&amp;#39;s base26 or something, but that&amp;#39;s more complex.&lt;/p&gt;\n\n&lt;p&gt;Given all possible interpretations and without further info, I think the safest assumption is that there was a mistake in the \nproblem entry, and &amp;quot;Hello!&amp;quot; should be replaced with an actual number. However, since I must answer based on given input, perhaps the \nintended answer is 7 (from summing H(8)+E(5)+L(12)+L(12)+O(15)=52 5+2=7), which is a common type of puzzle.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, if we consider that &amp;quot;Hello!&amp;quot; has no digits, sum is zero. But since the user might have intended something else, maybe \n72 or another number.&lt;/p&gt;\n\n&lt;p&gt;Wait, perhaps the problem was meant to be in Chinese or another language where &amp;quot;Hello!&amp;quot; translates to numbers, but that&amp;#39;s unlikely \nhere.&lt;/p&gt;\n\n&lt;p&gt;Another angle: Maybe &amp;quot;sum of its digits&amp;quot; refers to the string&amp;#39;s length? &amp;quot;Hello!&amp;quot; has 6 characters, sum is 6. But that doesn&amp;#39;t make \nsense for digits.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, if we consider each letter as a digit in base26 or another system, but without specification, it&amp;#39;s unclear.&lt;/p&gt;\n\n&lt;p&gt;Given all this uncertainty, I think the most reasonable answers could be either 0 (no numeric characters) or 7 (sum of letters&amp;#39; \npositions). Since math problems often use A=1 etc., I&amp;#39;ll go with 7. However, I&amp;#39;m not entirely sure. But let&amp;#39;s check once more:&lt;/p&gt;\n\n&lt;p&gt;If H is 8, E is5, L is12, O is15:&lt;/p&gt;\n\n&lt;p&gt;Sum:8+5+12+12+15=52  digits 5 and 2 sum to 7.&lt;/p&gt;\n\n&lt;p&gt;Yes, that&amp;#39;s a common puzzle type. So maybe the answer is \\boxed{7}.\n&amp;lt;/think&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;The problem asks for the sum of the digits after converting each letter in &amp;quot;Hello!&amp;quot; to its corresponding position in the alphabet \n(A=1, B=2, ..., Z=26). &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;H = 8&lt;/li&gt;\n&lt;li&gt;E = 5&lt;/li&gt;\n&lt;li&gt;L = 12&lt;/li&gt;\n&lt;li&gt;L = 12&lt;/li&gt;\n&lt;li&gt;O = 15&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Summing these values:&lt;br/&gt;\n(8 + 5 + 12 + 12 + 15 = 52)&lt;/p&gt;\n\n&lt;p&gt;Next, we sum the digits of 52:&lt;br/&gt;\n(5 + 2 = 7)&lt;/p&gt;\n\n&lt;p&gt;Thus, the final answer is (\\boxed{7}).\n```&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9tt3o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "a-c-19-23",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9tt3o/phi4minireasoning_an_example_of_overfitting_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9tt3o/phi4minireasoning_an_example_of_overfitting_to/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753537732,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been using the model (at FP8) for the past few days and it feels pretty solid for discussing ideas with and for using it as a code agent (I mostly use Qwen's CLI).\n\nHas anyone else been using this model recently? If you have, do you think it's decent for its size or are there better options?",
          "author_fullname": "t2_fmd6oq5v6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thoughts on Qwen3 235B A22B Instruct 2507?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9nu0j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 28,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753517034,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using the model (at FP8) for the past few days and it feels pretty solid for discussing ideas with and for using it as a code agent (I mostly use Qwen&amp;#39;s CLI).&lt;/p&gt;\n\n&lt;p&gt;Has anyone else been using this model recently? If you have, do you think it&amp;#39;s decent for its size or are there better options?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9nu0j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "random-tomato",
          "discussion_type": null,
          "num_comments": 40,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m9nu0j/thoughts_on_qwen3_235b_a22b_instruct_2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9nu0j/thoughts_on_qwen3_235b_a22b_instruct_2507/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753517034,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1e1w1ul46b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China's ByteDance's coze studio is now open source",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9enpd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 134,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 134,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=709bb632f62e78f1f26e0151f0de6c4608d82690",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753487124,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/coze-dev/coze-studio",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?auto=webp&amp;s=07e1667d1dae1fd50ce974c6233473c6ab6e2896",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1bebfe8068f1cdc71db48800cd8cedda3dc840b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a71404f6c427fecb5d4913ce33e193b1779dacf",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c70088efefd6cd274b9acd4ffd1b665856007c7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=514d1ba7344eb0ac49b443160f59e849fa2f73f5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9e6b310dabbf5f7fa5c3754cf321fbaf151547e4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=303cd5df5d78c96c3fd46565b173007a7601d592",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m9enpd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun-Doctor6855",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9enpd/chinas_bytedances_coze_studio_is_now_open_source/",
          "stickied": false,
          "url": "https://github.com/coze-dev/coze-studio",
          "subreddit_subscribers": 505252,
          "created_utc": 1753487124,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I see several tools nowadays that when you upload a csv file, it lets you talk to the LLM about the data in these files, what kind of parsing is done here (Ive tried excel parsing in the past, but its no where this good)? Sometimes this works with databases as well. Really curious about the underlying approach to this.",
          "author_fullname": "t2_k3dpkbo4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do LLMs understand massive csv data, sometimes even databases?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maao56",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753582047,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see several tools nowadays that when you upload a csv file, it lets you talk to the LLM about the data in these files, what kind of parsing is done here (Ive tried excel parsing in the past, but its no where this good)? Sometimes this works with databases as well. Really curious about the underlying approach to this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1maao56",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "subtle-being",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maao56/how_do_llms_understand_massive_csv_data_sometimes/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maao56/how_do_llms_understand_massive_csv_data_sometimes/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753582047,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_8pgou3uq9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "There's a new Kimi model on lmarena called Zenith and it's really really good. It might be Kimi K2 with reasoning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 135,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9holp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 80,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 80,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ly_WRQE_6HVApytA7F_mKU0b_sZuG5Y3hPKu0xk3wNI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753495926,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/4rtvhn7mn4ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/4rtvhn7mn4ff1.jpeg?auto=webp&amp;s=c16aaf2dfe4797e56beeae92bc04203ef0662c1e",
                  "width": 1080,
                  "height": 1042
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/4rtvhn7mn4ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c91a40e7d4e46be8e7afdf9a2ec643fc2e6b539",
                    "width": 108,
                    "height": 104
                  },
                  {
                    "url": "https://preview.redd.it/4rtvhn7mn4ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=457083326d775100cd445b60d940e4ff6aafa120",
                    "width": 216,
                    "height": 208
                  },
                  {
                    "url": "https://preview.redd.it/4rtvhn7mn4ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f2be368d4a16edef428e7127b1508b2174df3fd8",
                    "width": 320,
                    "height": 308
                  },
                  {
                    "url": "https://preview.redd.it/4rtvhn7mn4ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=358f4000782fac35b42bd0cd8d6c6c5750dfd53f",
                    "width": 640,
                    "height": 617
                  },
                  {
                    "url": "https://preview.redd.it/4rtvhn7mn4ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4cda72db60caaea2d49b78c70b6905cd3a5b016c",
                    "width": 960,
                    "height": 926
                  },
                  {
                    "url": "https://preview.redd.it/4rtvhn7mn4ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6035381e62f5451d960f3a4ca61e20c142b714bd",
                    "width": 1080,
                    "height": 1042
                  }
                ],
                "variants": {},
                "id": "ITl5xqOuYu_FjgAfGE61jOjRumjRJRilaOE9PImawOA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9holp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "balianone",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9holp/theres_a_new_kimi_model_on_lmarena_called_zenith/",
          "stickied": false,
          "url": "https://i.redd.it/4rtvhn7mn4ff1.jpeg",
          "subreddit_subscribers": 505252,
          "created_utc": 1753495926,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Finally put together my rig after months of planning into a NAS case \n\n* Threadripper PRO 7955WX\n* Arctic Freezer 4U-M (cpu cooler)\n* Gigabyte TRX50 AI TOP\n* be quiet! Dark Power Pro 13 1600W\n* JONSBO N5 Case\n* 2x RTX Pro 6000\n\nMight add a few more intake fans on the top ",
          "author_fullname": "t2_16xbdr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Compact 2x RTX Pro 6000 Rig",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9bwoy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 172,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 172,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/nIQWXwAd5JWfTiQ6FGVC1XWYiGxzWflAKHHcq1t0koA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753479993,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Finally put together my rig after months of planning into a NAS case &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Threadripper PRO 7955WX&lt;/li&gt;\n&lt;li&gt;Arctic Freezer 4U-M (cpu cooler)&lt;/li&gt;\n&lt;li&gt;Gigabyte TRX50 AI TOP&lt;/li&gt;\n&lt;li&gt;be quiet! Dark Power Pro 13 1600W&lt;/li&gt;\n&lt;li&gt;JONSBO N5 Case&lt;/li&gt;\n&lt;li&gt;2x RTX Pro 6000&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Might add a few more intake fans on the top &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/tbteu4v5b3ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?auto=webp&amp;s=e1c07ae621c323dcd20838da5641e3e02cc70f91",
                  "width": 1679,
                  "height": 1264
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb8530b9905fee9ce384e441b1786c16863f92ac",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb080d3b642a8ae64a22c14c8185ffb66785a5a8",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df5d60cb32c2381511e9f8f424415f43363b229e",
                    "width": 320,
                    "height": 240
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb7c498f0a5ad74816f597205d993264473bdbfe",
                    "width": 640,
                    "height": 481
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3cd46419a68e44716f2e3b674a590f0b8f7e8f9e",
                    "width": 960,
                    "height": 722
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f10578355cca1e987b8febd5ea512ad847a0ac02",
                    "width": 1080,
                    "height": 813
                  }
                ],
                "variants": {},
                "id": "75hqb9t8X6IAo2hcG2QxH9eY348BDsA6al0LQJShjQE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9bwoy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shadowninjaz3",
          "discussion_type": null,
          "num_comments": 74,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9bwoy/compact_2x_rtx_pro_6000_rig/",
          "stickied": false,
          "url": "https://i.redd.it/tbteu4v5b3ff1.jpeg",
          "subreddit_subscribers": 505252,
          "created_utc": 1753479993,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_13a48a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Would this B760M motherboard support dual 2-slot GPUs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9zrmo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/bm0aTQAoOz634lx3ZicYM75-Ex58Iofxca44UaYwI7M.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753552509,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/4p4vl0xub9ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/4p4vl0xub9ff1.png?auto=webp&amp;s=c9e1ccec86a78cc8b7ddd406deb29af337197347",
                  "width": 739,
                  "height": 906
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/4p4vl0xub9ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e758624e57207437dd6e47fee037786d773d7623",
                    "width": 108,
                    "height": 132
                  },
                  {
                    "url": "https://preview.redd.it/4p4vl0xub9ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a8bb4299b100112b6f6c454dda0b2232f163a04",
                    "width": 216,
                    "height": 264
                  },
                  {
                    "url": "https://preview.redd.it/4p4vl0xub9ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2f05f8f4e976e966d16f932e84e9d84befa9b2aa",
                    "width": 320,
                    "height": 392
                  },
                  {
                    "url": "https://preview.redd.it/4p4vl0xub9ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d8e5a9730271e92d307d1bc9e88b45a9a7eec0f1",
                    "width": 640,
                    "height": 784
                  }
                ],
                "variants": {},
                "id": "jQZ7j8tHpCxP-976f_NhNi1jzVAryQKVSUHMoLQQCGU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9zrmo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "legit_split_",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9zrmo/would_this_b760m_motherboard_support_dual_2slot/",
          "stickied": false,
          "url": "https://i.redd.it/4p4vl0xub9ff1.png",
          "subreddit_subscribers": 505252,
          "created_utc": 1753552509,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks,\n\nJust published a deep dive on the full infrastructure stack required to scale LLM inference to billions of users and agents. It goes beyond a single engine and looks at the entire system.\n\nHighlights:\n\n* GKE Inference Gateway: How it cuts tail latency by 60% &amp; boosts throughput 40% with model-aware routing (KV cache, LoRA).\n* vLLM on GPUs &amp; TPUs: Using vLLM as a unified layer to serve models across different hardware, including a look at the insane interconnects on Cloud TPUs.\n* The Future is llm-d: A breakdown of the new Google/Red Hat project for disaggregated inference (separating prefill/decode stages).\n* Planetary-Scale Networking: The role of a global Anycast network and 42+ regions in minimizing latency for users everywhere.\n* Managing Capacity &amp; Cost: Using GKE Custom Compute Classes to build a resilient and cost-effective mix of Spot, On-demand, and Reserved instances.\n\nFull article with architecture diagrams &amp; walkthroughs:\n\n[https://medium.com/google-cloud/scaling-inference-to-billions-of-users-and-agents-516d5d9f5da7](https://medium.com/google-cloud/scaling-inference-to-billions-of-users-and-agents-516d5d9f5da7)\n\nLet me know what you think!\n\n*(Disclaimer: I work at Google Cloud.)*",
          "author_fullname": "t2_43kcp65r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Scaling Inference To Billions of Users And Agents",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9tnj5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753537307,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;Just published a deep dive on the full infrastructure stack required to scale LLM inference to billions of users and agents. It goes beyond a single engine and looks at the entire system.&lt;/p&gt;\n\n&lt;p&gt;Highlights:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;GKE Inference Gateway: How it cuts tail latency by 60% &amp;amp; boosts throughput 40% with model-aware routing (KV cache, LoRA).&lt;/li&gt;\n&lt;li&gt;vLLM on GPUs &amp;amp; TPUs: Using vLLM as a unified layer to serve models across different hardware, including a look at the insane interconnects on Cloud TPUs.&lt;/li&gt;\n&lt;li&gt;The Future is llm-d: A breakdown of the new Google/Red Hat project for disaggregated inference (separating prefill/decode stages).&lt;/li&gt;\n&lt;li&gt;Planetary-Scale Networking: The role of a global Anycast network and 42+ regions in minimizing latency for users everywhere.&lt;/li&gt;\n&lt;li&gt;Managing Capacity &amp;amp; Cost: Using GKE Custom Compute Classes to build a resilient and cost-effective mix of Spot, On-demand, and Reserved instances.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Full article with architecture diagrams &amp;amp; walkthroughs:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/google-cloud/scaling-inference-to-billions-of-users-and-agents-516d5d9f5da7\"&gt;https://medium.com/google-cloud/scaling-inference-to-billions-of-users-and-agents-516d5d9f5da7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think!&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;(Disclaimer: I work at Google Cloud.)&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/7JS3Ya3b6rSOwFZeMG-XxVZkJJv_W5OFh2T4WbpzeG0.png?auto=webp&amp;s=03e21a1bc3ab1617cd4ffe76d71c7c70efbc5cd0",
                  "width": 1200,
                  "height": 1200
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/7JS3Ya3b6rSOwFZeMG-XxVZkJJv_W5OFh2T4WbpzeG0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=58f812f9bd6c9bebf70c1af318f89680a57d02cc",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/7JS3Ya3b6rSOwFZeMG-XxVZkJJv_W5OFh2T4WbpzeG0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7803a71a8d13a2ea1df78345f28d2b72c03065e1",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/7JS3Ya3b6rSOwFZeMG-XxVZkJJv_W5OFh2T4WbpzeG0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=44d2bb07bca9e3359cb00e3b44c2bb19e82d326a",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/7JS3Ya3b6rSOwFZeMG-XxVZkJJv_W5OFh2T4WbpzeG0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=63e55fc492279919c4c0a2beee0bfb4315a9c01c",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/7JS3Ya3b6rSOwFZeMG-XxVZkJJv_W5OFh2T4WbpzeG0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=81d6c21e619a1ba63093f138a5b114a0d152f5f9",
                    "width": 960,
                    "height": 960
                  },
                  {
                    "url": "https://external-preview.redd.it/7JS3Ya3b6rSOwFZeMG-XxVZkJJv_W5OFh2T4WbpzeG0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=060190a2d09b97087c4351bc02783504ce618f26",
                    "width": 1080,
                    "height": 1080
                  }
                ],
                "variants": {},
                "id": "7JS3Ya3b6rSOwFZeMG-XxVZkJJv_W5OFh2T4WbpzeG0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9tnj5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "m4r1k_",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753537307,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying to test out iklcpp and the new Qwen3 235B non thinking. I'm using Unsloth UD-Q4_K_XL quant. My system is 64Gb DDR4 ram and 2x 16Gb GPUs. I have previously tested this split gguf with latest release of koboldcpp. But with iklcpp, I'm getting memory allocation failure.\n\nBasically I'm using mmap as I don't have enough Ram+Vram. \n\nFor kcpp, I use the following settings:\n```\nkobold --model AI/LLM/Qwen3/Qwen3-235B-A228-Instruct-2507-UD-Q4_KXL-00001-of00003.gguf \\ --contextsize 65536 \\\n--blasbatchsize 2048 \\\n--tensor_split 0.5 0.5 \\\n--usecuda nommq \\\n--gpulayers 999 \\\n--flashattention \\\n--overridetensors \"([0-9]+).ffn_.*_exps.weight=CPU\"  \\\n--usemmap \\\n--threads 24\n```\n\nWith this, I get about 10+10Gib vram usage on my two GPUs. Model loads and works, however slow it might be. \n\nI compiled iklcpp using the following instructions:\n\n```\n# Install build dependencies and cuda toolkit as needed\n\n# Clone\ngit clone https://github.com/ikawrakow/ik_llama.cpp\ncd ik_llama.cpp\n\n# Configure CUDA+CPU Backend (I used this) \ncmake -B ./build -DGGML_CUDA=ON -DGGML_BLAS=OFF\n\n# *or* Configure CPU Only Backend\ncmake -B ./build -DGGML_CUDA=OFF -DGGML_BLAS=OFF\n\n# Build\ncmake --build ./build --config Release -j $(nproc)\n\n# Confirm\n./build/bin/llama-server --version\nversion: 3597 (68a5b604)\n```\n\nNow if I try to use the gguf with iklcpp with the following command:\n```\n./AI/ik_llama.cpp/build/bin/llama-server \\\n-m AI/LLM/Qwen3/Qwen3-235B-A22B-Instruct-2507-UD-Q4_K_XL-00001-of-00003.gguf \\\n-t 20 \\\n-c 65536 \\\n-b 4096 \\\n-ub 4096 \\\n-fa \\\n-ot \"([0-9]+).ffn_.*_exps.weight=CPU\" \\\n-ngl 95 \\\n-sm layer \\\n-ts 1,1 \\\n-amb 512 \\\n-fmoe 1\n```\n\nI get the following error:\n```\nllama_new_context_with_model: n_ctx      = 65536\nllama_new_context_with_model: n_batch    = 4096\nllama_new_context_with_model: n_ubatch   = 4096\nllama_new_context_with_model: flash_attn = 1\nllama_new_context_with_model: mla_attn   = 0\nllama_new_context_with_model: attn_max_b = 512\nllama_new_context_with_model: fused_moe  = 1\nllama_new_context_with_model: ser        = -1, 0\nllama_new_context_with_model: freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =  6144.00 MiB\nllama_kv_cache_init:      CUDA1 KV buffer size =  5888.00 MiB\nllama_new_context_with_model: KV self size  = 12032.00 MiB, K (f16): 6016.00 MiB, V (f16): 6016.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     1.16 MiB\nllama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 523616.00 MiB on device 0: cudaMalloc failed: out of memory\nggml_gallocr_reserve_n: failed to allocate CUDA0 buffer of size 549051165696\nllama_new_context_with_model: failed to allocate compute buffers\nllama_init_from_gpt_params: error: failed to create context with model 'AI/LLM/Qwen3/Qwen3-235B-A22B-Instruct-2507-UD-Q4_K_XL-0000\n1-of-00003.gguf'\nERR [              load_model] unable to load model | tid=\"140606057730048\" timestamp=1753561505 model=\"AI/LLM/Qwen3/Qwen3-235B-A\n22B-Instruct-2507-UD-Q4_K_XL-00001-of-00003.gguf\"\nfish: Job 1, './AI/ik_llama.cpp/build/bin/lla' terminated by signal -m AI/LLM/Qwen3/Qwen3-235B-A22B (-t 20 \\)\nfish: Job -c 65536 \\, '-b 4096 \\' terminated by signal -ub 4096 \\ (-fa \\)\nfish: Job -ot \"([0-9]+).ffn_.*_exps.weigh, '-ngl 95 \\' terminated by signal -sm layer \\ (-ts 1,1 \\)\nfish: Job -amb 512 \\, '-fmoe' terminated by signal SIGSEGV (Address boundary error)\n```\n\nI'm guessing the issue is with the pipeline parallelism n_copies = 4. But I couldn't find any flag to turn it off.\n\nI would appreciate any explanation of the issue and advice regarding getting this working. Thank you. \n\n\nEdit: solved, needed `DGGML_SCHED_MAX_COPIES=1` as build option. ",
          "author_fullname": "t2_9nex5np2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ik_llama.cpp help!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma41wu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753567668,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753563267,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to test out iklcpp and the new Qwen3 235B non thinking. I&amp;#39;m using Unsloth UD-Q4_K_XL quant. My system is 64Gb DDR4 ram and 2x 16Gb GPUs. I have previously tested this split gguf with latest release of koboldcpp. But with iklcpp, I&amp;#39;m getting memory allocation failure.&lt;/p&gt;\n\n&lt;p&gt;Basically I&amp;#39;m using mmap as I don&amp;#39;t have enough Ram+Vram. &lt;/p&gt;\n\n&lt;p&gt;For kcpp, I use the following settings:\n&lt;code&gt;\nkobold --model AI/LLM/Qwen3/Qwen3-235B-A228-Instruct-2507-UD-Q4_KXL-00001-of00003.gguf \\ --contextsize 65536 \\\n--blasbatchsize 2048 \\\n--tensor_split 0.5 0.5 \\\n--usecuda nommq \\\n--gpulayers 999 \\\n--flashattention \\\n--overridetensors &amp;quot;([0-9]+).ffn_.*_exps.weight=CPU&amp;quot;  \\\n--usemmap \\\n--threads 24\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;With this, I get about 10+10Gib vram usage on my two GPUs. Model loads and works, however slow it might be. &lt;/p&gt;\n\n&lt;p&gt;I compiled iklcpp using the following instructions:&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;h1&gt;Install build dependencies and cuda toolkit as needed&lt;/h1&gt;\n\n&lt;h1&gt;Clone&lt;/h1&gt;\n\n&lt;p&gt;git clone &lt;a href=\"https://github.com/ikawrakow/ik_llama.cpp\"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt;\ncd ik_llama.cpp&lt;/p&gt;\n\n&lt;h1&gt;Configure CUDA+CPU Backend (I used this)&lt;/h1&gt;\n\n&lt;p&gt;cmake -B ./build -DGGML_CUDA=ON -DGGML_BLAS=OFF&lt;/p&gt;\n\n&lt;h1&gt;&lt;em&gt;or&lt;/em&gt; Configure CPU Only Backend&lt;/h1&gt;\n\n&lt;p&gt;cmake -B ./build -DGGML_CUDA=OFF -DGGML_BLAS=OFF&lt;/p&gt;\n\n&lt;h1&gt;Build&lt;/h1&gt;\n\n&lt;p&gt;cmake --build ./build --config Release -j $(nproc)&lt;/p&gt;\n\n&lt;h1&gt;Confirm&lt;/h1&gt;\n\n&lt;p&gt;./build/bin/llama-server --version\nversion: 3597 (68a5b604)\n```&lt;/p&gt;\n\n&lt;p&gt;Now if I try to use the gguf with iklcpp with the following command:\n&lt;code&gt;\n./AI/ik_llama.cpp/build/bin/llama-server \\\n-m AI/LLM/Qwen3/Qwen3-235B-A22B-Instruct-2507-UD-Q4_K_XL-00001-of-00003.gguf \\\n-t 20 \\\n-c 65536 \\\n-b 4096 \\\n-ub 4096 \\\n-fa \\\n-ot &amp;quot;([0-9]+).ffn_.*_exps.weight=CPU&amp;quot; \\\n-ngl 95 \\\n-sm layer \\\n-ts 1,1 \\\n-amb 512 \\\n-fmoe 1\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I get the following error:\n&lt;code&gt;\nllama_new_context_with_model: n_ctx      = 65536\nllama_new_context_with_model: n_batch    = 4096\nllama_new_context_with_model: n_ubatch   = 4096\nllama_new_context_with_model: flash_attn = 1\nllama_new_context_with_model: mla_attn   = 0\nllama_new_context_with_model: attn_max_b = 512\nllama_new_context_with_model: fused_moe  = 1\nllama_new_context_with_model: ser        = -1, 0\nllama_new_context_with_model: freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =  6144.00 MiB\nllama_kv_cache_init:      CUDA1 KV buffer size =  5888.00 MiB\nllama_new_context_with_model: KV self size  = 12032.00 MiB, K (f16): 6016.00 MiB, V (f16): 6016.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     1.16 MiB\nllama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 523616.00 MiB on device 0: cudaMalloc failed: out of memory\nggml_gallocr_reserve_n: failed to allocate CUDA0 buffer of size 549051165696\nllama_new_context_with_model: failed to allocate compute buffers\nllama_init_from_gpt_params: error: failed to create context with model &amp;#39;AI/LLM/Qwen3/Qwen3-235B-A22B-Instruct-2507-UD-Q4_K_XL-0000\n1-of-00003.gguf&amp;#39;\nERR [              load_model] unable to load model | tid=&amp;quot;140606057730048&amp;quot; timestamp=1753561505 model=&amp;quot;AI/LLM/Qwen3/Qwen3-235B-A\n22B-Instruct-2507-UD-Q4_K_XL-00001-of-00003.gguf&amp;quot;\nfish: Job 1, &amp;#39;./AI/ik_llama.cpp/build/bin/lla&amp;#39; terminated by signal -m AI/LLM/Qwen3/Qwen3-235B-A22B (-t 20 \\)\nfish: Job -c 65536 \\, &amp;#39;-b 4096 \\&amp;#39; terminated by signal -ub 4096 \\ (-fa \\)\nfish: Job -ot &amp;quot;([0-9]+).ffn_.*_exps.weigh, &amp;#39;-ngl 95 \\&amp;#39; terminated by signal -sm layer \\ (-ts 1,1 \\)\nfish: Job -amb 512 \\, &amp;#39;-fmoe&amp;#39; terminated by signal SIGSEGV (Address boundary error)\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m guessing the issue is with the pipeline parallelism n_copies = 4. But I couldn&amp;#39;t find any flag to turn it off.&lt;/p&gt;\n\n&lt;p&gt;I would appreciate any explanation of the issue and advice regarding getting this working. Thank you. &lt;/p&gt;\n\n&lt;p&gt;Edit: solved, needed &lt;code&gt;DGGML_SCHED_MAX_COPIES=1&lt;/code&gt; as build option. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/d-Wkvq34ZRzACVPwHO_DAPDTtuOd0WoROIS_xIxGbqQ.png?auto=webp&amp;s=6e0a61fe992b8503db41032a1bd82d39cfd4f716",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/d-Wkvq34ZRzACVPwHO_DAPDTtuOd0WoROIS_xIxGbqQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f9eee7355afb465bfdb476b246b097f0a0153b4",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/d-Wkvq34ZRzACVPwHO_DAPDTtuOd0WoROIS_xIxGbqQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=39c878657e2261a4938ec3ba26bd53b57f5a89c9",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/d-Wkvq34ZRzACVPwHO_DAPDTtuOd0WoROIS_xIxGbqQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eea7730ed14db82624c81e1d19b49d3d315fc245",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/d-Wkvq34ZRzACVPwHO_DAPDTtuOd0WoROIS_xIxGbqQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b0c6e108cbe3e0d3ed12257fd8b9943594383239",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/d-Wkvq34ZRzACVPwHO_DAPDTtuOd0WoROIS_xIxGbqQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ca14e51a272d2532a144d6b2e4453d5222de5f88",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/d-Wkvq34ZRzACVPwHO_DAPDTtuOd0WoROIS_xIxGbqQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a2dc4fb0d9648fe2bc8b7af0ae5e80923d6c4129",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "d-Wkvq34ZRzACVPwHO_DAPDTtuOd0WoROIS_xIxGbqQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ma41wu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "lacerating_aura",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma41wu/ik_llamacpp_help/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma41wu/ik_llamacpp_help/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753563267,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A few weeks ago (2 weeks), I shared a 23 question survey with my online community. With all the buzz around new model announcements and the \"AGI is just around the corner\" hype, I wanted to hear directly from people in the field to understand the real picture of Large Language Models (LLMs).\n\nI'm grateful to all **26 participants** who took the time to fill out the survey. We had 72 visits to the survey, and 26 completed responses (after removing one spam entry).\n\nNote: I still need to summarize the open-ended answers, and will share those insights in a later post.\n\n# Here are the results:\n\nhttps://preview.redd.it/nhi6wunfn8ff1.png?width=1634&amp;format=png&amp;auto=webp&amp;s=6ec4d4a31c1047f354a6813dcd889077f0e06cb7\n\nhttps://preview.redd.it/wi7zhv7kn8ff1.png?width=1394&amp;format=png&amp;auto=webp&amp;s=f0ef9a676bddcc4da1f5b722a755baaa35d4072f\n\nhttps://preview.redd.it/gn7ecotln8ff1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=2001c0e759c39ee2702f35608c66a732eec64167\n\nhttps://preview.redd.it/r1ekc61nn8ff1.png?width=1412&amp;format=png&amp;auto=webp&amp;s=cf081f61084d75640f73bfb8fdd68e9a1e2e3308\n\nhttps://preview.redd.it/1nqywtmnn8ff1.png?width=1348&amp;format=png&amp;auto=webp&amp;s=aa2a983347ec9339c9d0f29360f9d48ae0c58090\n\nhttps://preview.redd.it/dzm2tq9on8ff1.png?width=1404&amp;format=png&amp;auto=webp&amp;s=920f45ca1d91218b3f9ff13d0f42fec0b2f086e8\n\nhttps://preview.redd.it/5wz98zuon8ff1.png?width=1198&amp;format=png&amp;auto=webp&amp;s=ccca6af30a91b9352d2a35ce0e5f298de18607b5\n\nhttps://preview.redd.it/w6brj3dpn8ff1.png?width=1266&amp;format=png&amp;auto=webp&amp;s=5c81480f675892bfb81485546ac241463bec3c7e\n\nhttps://preview.redd.it/lsrxy13qn8ff1.png?width=1270&amp;format=png&amp;auto=webp&amp;s=edc9a2ea7b3eb49f7cf274e8903dd4328d95ebb1\n\nhttps://preview.redd.it/x4c7zilqn8ff1.png?width=1322&amp;format=png&amp;auto=webp&amp;s=bdba0fea27a0342ef2880266961e3b080bbda7ad\n\nhttps://preview.redd.it/a2q40qwrn8ff1.png?width=1178&amp;format=png&amp;auto=webp&amp;s=fc4beee55a3ae25fee780cc2b992651dac1113b4\n\n  \nKey Suggestions:\n\n* Our current questions are **too open-ended**; we need to ask more specific questions in the future.\n* We also need to include questions about **proprietary models**.\n\nThanks again to everyone who participated.",
          "author_fullname": "t2_dtzmsoy3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "My Attempt to Understand local LLM Landscape (Survey Results)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "wi7zhv7kn8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 76,
                  "x": 108,
                  "u": "https://preview.redd.it/wi7zhv7kn8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8d5d4b841a29a47f57a0a645153064db12dd3b5"
                },
                {
                  "y": 152,
                  "x": 216,
                  "u": "https://preview.redd.it/wi7zhv7kn8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6fd3bba31f9db82d8ae3f1e399bcd19ad62b6f2e"
                },
                {
                  "y": 225,
                  "x": 320,
                  "u": "https://preview.redd.it/wi7zhv7kn8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=571e61d0b8e1b1a440398fb4556da7436c0449f8"
                },
                {
                  "y": 451,
                  "x": 640,
                  "u": "https://preview.redd.it/wi7zhv7kn8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d8548bf5c5200ccc01ccafb4181c52a912647388"
                },
                {
                  "y": 677,
                  "x": 960,
                  "u": "https://preview.redd.it/wi7zhv7kn8ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d0d433fd924a8fa919cb81d7db877a699bee3f87"
                },
                {
                  "y": 762,
                  "x": 1080,
                  "u": "https://preview.redd.it/wi7zhv7kn8ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1c9fca609acd59ed3b54236e92debacfbbfe551f"
                }
              ],
              "s": {
                "y": 984,
                "x": 1394,
                "u": "https://preview.redd.it/wi7zhv7kn8ff1.png?width=1394&amp;format=png&amp;auto=webp&amp;s=f0ef9a676bddcc4da1f5b722a755baaa35d4072f"
              },
              "id": "wi7zhv7kn8ff1"
            },
            "w6brj3dpn8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/w6brj3dpn8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b465639065466759a1ea5c9e70b2a056f7b1a732"
                },
                {
                  "y": 135,
                  "x": 216,
                  "u": "https://preview.redd.it/w6brj3dpn8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e011bbbba63c37a44ae8ea4cba7cb3c59ae4543f"
                },
                {
                  "y": 200,
                  "x": 320,
                  "u": "https://preview.redd.it/w6brj3dpn8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf57d40531c7ced3bc7a4bcde4bf7294a0f6f385"
                },
                {
                  "y": 400,
                  "x": 640,
                  "u": "https://preview.redd.it/w6brj3dpn8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b0a51aa1c656f866de8f040e9005f05df87bc298"
                },
                {
                  "y": 600,
                  "x": 960,
                  "u": "https://preview.redd.it/w6brj3dpn8ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2eca37940d61ef05394d8a4b309f7cd7038fec57"
                },
                {
                  "y": 675,
                  "x": 1080,
                  "u": "https://preview.redd.it/w6brj3dpn8ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ecb19ba15f8ad11937f3119aefceb6f0c103e964"
                }
              ],
              "s": {
                "y": 792,
                "x": 1266,
                "u": "https://preview.redd.it/w6brj3dpn8ff1.png?width=1266&amp;format=png&amp;auto=webp&amp;s=5c81480f675892bfb81485546ac241463bec3c7e"
              },
              "id": "w6brj3dpn8ff1"
            },
            "a2q40qwrn8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 35,
                  "x": 108,
                  "u": "https://preview.redd.it/a2q40qwrn8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1e726a7621a6adde60057cbaeeb1ec9c38b10648"
                },
                {
                  "y": 71,
                  "x": 216,
                  "u": "https://preview.redd.it/a2q40qwrn8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d7d66168eda7222536424d709a4652613534b7fa"
                },
                {
                  "y": 106,
                  "x": 320,
                  "u": "https://preview.redd.it/a2q40qwrn8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e143bb25b8adb83b7bed648b03b60c5657ceb6a"
                },
                {
                  "y": 212,
                  "x": 640,
                  "u": "https://preview.redd.it/a2q40qwrn8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=99420b92f96aa47cd493339e7d65231cc0019291"
                },
                {
                  "y": 319,
                  "x": 960,
                  "u": "https://preview.redd.it/a2q40qwrn8ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e55e988c836137a78687c0a7d88ac0d7c71aaceb"
                },
                {
                  "y": 359,
                  "x": 1080,
                  "u": "https://preview.redd.it/a2q40qwrn8ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0951d1b577e1502a53078d219a076fa30f22121e"
                }
              ],
              "s": {
                "y": 392,
                "x": 1178,
                "u": "https://preview.redd.it/a2q40qwrn8ff1.png?width=1178&amp;format=png&amp;auto=webp&amp;s=fc4beee55a3ae25fee780cc2b992651dac1113b4"
              },
              "id": "a2q40qwrn8ff1"
            },
            "5wz98zuon8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 37,
                  "x": 108,
                  "u": "https://preview.redd.it/5wz98zuon8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd3741b5b010faa1d02206934db765b9afbfdf95"
                },
                {
                  "y": 75,
                  "x": 216,
                  "u": "https://preview.redd.it/5wz98zuon8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b72ed3fbd23ab8f085aec3bcead2fa9ace856f8c"
                },
                {
                  "y": 111,
                  "x": 320,
                  "u": "https://preview.redd.it/5wz98zuon8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5241b600b101f67489c2d359c464d7a79ab2750d"
                },
                {
                  "y": 222,
                  "x": 640,
                  "u": "https://preview.redd.it/5wz98zuon8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=577954319f1ab57bf0f9651d94a53a25eaf36a70"
                },
                {
                  "y": 333,
                  "x": 960,
                  "u": "https://preview.redd.it/5wz98zuon8ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b6f8f4eb684802d0e2974604c4b9c1ad07e84954"
                },
                {
                  "y": 375,
                  "x": 1080,
                  "u": "https://preview.redd.it/5wz98zuon8ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=56f6696092dd45b5fde4832f47658e2876db391c"
                }
              ],
              "s": {
                "y": 416,
                "x": 1198,
                "u": "https://preview.redd.it/5wz98zuon8ff1.png?width=1198&amp;format=png&amp;auto=webp&amp;s=ccca6af30a91b9352d2a35ce0e5f298de18607b5"
              },
              "id": "5wz98zuon8ff1"
            },
            "r1ekc61nn8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 117,
                  "x": 108,
                  "u": "https://preview.redd.it/r1ekc61nn8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f6f7a1d4a13249d3e82b411441bfe18bd211984d"
                },
                {
                  "y": 234,
                  "x": 216,
                  "u": "https://preview.redd.it/r1ekc61nn8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b8c29907b679025b8e9732b9602bda9cc9f70839"
                },
                {
                  "y": 347,
                  "x": 320,
                  "u": "https://preview.redd.it/r1ekc61nn8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=30d4f25fc59b0efa6b1415f85061930075e3642d"
                },
                {
                  "y": 694,
                  "x": 640,
                  "u": "https://preview.redd.it/r1ekc61nn8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ca16f9d1fbd21b3a84bf5595060616d147a53eef"
                },
                {
                  "y": 1041,
                  "x": 960,
                  "u": "https://preview.redd.it/r1ekc61nn8ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0dacc71298563b34645975fd600af1d25d5e8e46"
                },
                {
                  "y": 1171,
                  "x": 1080,
                  "u": "https://preview.redd.it/r1ekc61nn8ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0566cef95164a985d2e1c75108289bf880bf5333"
                }
              ],
              "s": {
                "y": 1532,
                "x": 1412,
                "u": "https://preview.redd.it/r1ekc61nn8ff1.png?width=1412&amp;format=png&amp;auto=webp&amp;s=cf081f61084d75640f73bfb8fdd68e9a1e2e3308"
              },
              "id": "r1ekc61nn8ff1"
            },
            "x4c7zilqn8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 75,
                  "x": 108,
                  "u": "https://preview.redd.it/x4c7zilqn8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ed683a95edfc40f39f508150524ff1271a59676"
                },
                {
                  "y": 151,
                  "x": 216,
                  "u": "https://preview.redd.it/x4c7zilqn8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=37b57f4c2ba3bfc4ac570badc77743248455ef8e"
                },
                {
                  "y": 224,
                  "x": 320,
                  "u": "https://preview.redd.it/x4c7zilqn8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a7fbbd354246ae0fb2a7b4d4c907b5879ad7bb98"
                },
                {
                  "y": 448,
                  "x": 640,
                  "u": "https://preview.redd.it/x4c7zilqn8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=00b99b3b9636f51d7d906737dce157166aa15a09"
                },
                {
                  "y": 672,
                  "x": 960,
                  "u": "https://preview.redd.it/x4c7zilqn8ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5753d6d9f1bb3109e12ed7423cfbbc4dd02e3bfe"
                },
                {
                  "y": 756,
                  "x": 1080,
                  "u": "https://preview.redd.it/x4c7zilqn8ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3b286b400d4e99a9672b1dbc91d3e770e585d956"
                }
              ],
              "s": {
                "y": 926,
                "x": 1322,
                "u": "https://preview.redd.it/x4c7zilqn8ff1.png?width=1322&amp;format=png&amp;auto=webp&amp;s=bdba0fea27a0342ef2880266961e3b080bbda7ad"
              },
              "id": "x4c7zilqn8ff1"
            },
            "1nqywtmnn8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 128,
                  "x": 108,
                  "u": "https://preview.redd.it/1nqywtmnn8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a785fbab2169ce4628a2b2e4794bbb43f45cb7ab"
                },
                {
                  "y": 256,
                  "x": 216,
                  "u": "https://preview.redd.it/1nqywtmnn8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=98a77a485ced88c8983856e89cae6bdcedf7856b"
                },
                {
                  "y": 379,
                  "x": 320,
                  "u": "https://preview.redd.it/1nqywtmnn8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2e138de7299e2449579a8de3c332389c1825d125"
                },
                {
                  "y": 759,
                  "x": 640,
                  "u": "https://preview.redd.it/1nqywtmnn8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a457307b766742441149a22bd9201c1f351856a7"
                },
                {
                  "y": 1139,
                  "x": 960,
                  "u": "https://preview.redd.it/1nqywtmnn8ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3cb901231018cdae90ab40129bb4979bfed2def6"
                },
                {
                  "y": 1281,
                  "x": 1080,
                  "u": "https://preview.redd.it/1nqywtmnn8ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4a5fe6eb6eff42fca1834993a2f1f44a8a339fe9"
                }
              ],
              "s": {
                "y": 1600,
                "x": 1348,
                "u": "https://preview.redd.it/1nqywtmnn8ff1.png?width=1348&amp;format=png&amp;auto=webp&amp;s=aa2a983347ec9339c9d0f29360f9d48ae0c58090"
              },
              "id": "1nqywtmnn8ff1"
            },
            "gn7ecotln8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 75,
                  "x": 108,
                  "u": "https://preview.redd.it/gn7ecotln8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c5e608feb240127867258ad6fcdb43912147db9"
                },
                {
                  "y": 150,
                  "x": 216,
                  "u": "https://preview.redd.it/gn7ecotln8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3fb90e162c3405fb9cd6e53520915a37f7ed3c41"
                },
                {
                  "y": 222,
                  "x": 320,
                  "u": "https://preview.redd.it/gn7ecotln8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f31102d68a8e84288a5d3261c61252dfab040240"
                },
                {
                  "y": 445,
                  "x": 640,
                  "u": "https://preview.redd.it/gn7ecotln8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3d781130485dd578be9ec7625fb7541ab070617"
                },
                {
                  "y": 668,
                  "x": 960,
                  "u": "https://preview.redd.it/gn7ecotln8ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c1b2ab107359d854c8cb660bdd0d68c5d90ceefb"
                },
                {
                  "y": 751,
                  "x": 1080,
                  "u": "https://preview.redd.it/gn7ecotln8ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9e18742a45806f88c40590621eef30653d5b46bf"
                }
              ],
              "s": {
                "y": 1044,
                "x": 1500,
                "u": "https://preview.redd.it/gn7ecotln8ff1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=2001c0e759c39ee2702f35608c66a732eec64167"
              },
              "id": "gn7ecotln8ff1"
            },
            "dzm2tq9on8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 70,
                  "x": 108,
                  "u": "https://preview.redd.it/dzm2tq9on8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5bf28f0fe49054ffbeb6e7b6376e9083efe48240"
                },
                {
                  "y": 140,
                  "x": 216,
                  "u": "https://preview.redd.it/dzm2tq9on8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2cc35519da3a5633f8360ed05e929286fb7eb919"
                },
                {
                  "y": 208,
                  "x": 320,
                  "u": "https://preview.redd.it/dzm2tq9on8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=683cbdd9397ace1633771079c84ab2551f256c41"
                },
                {
                  "y": 416,
                  "x": 640,
                  "u": "https://preview.redd.it/dzm2tq9on8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7804747e0a9c449909601d90b2d21e92aae274b3"
                },
                {
                  "y": 624,
                  "x": 960,
                  "u": "https://preview.redd.it/dzm2tq9on8ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cfcabde7be377dcff550c63750c27ea80a04d251"
                },
                {
                  "y": 703,
                  "x": 1080,
                  "u": "https://preview.redd.it/dzm2tq9on8ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=03d7e3a5f27caccbf66612d33719cad31c0e15d6"
                }
              ],
              "s": {
                "y": 914,
                "x": 1404,
                "u": "https://preview.redd.it/dzm2tq9on8ff1.png?width=1404&amp;format=png&amp;auto=webp&amp;s=920f45ca1d91218b3f9ff13d0f42fec0b2f086e8"
              },
              "id": "dzm2tq9on8ff1"
            },
            "nhi6wunfn8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 65,
                  "x": 108,
                  "u": "https://preview.redd.it/nhi6wunfn8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=84896f9bcd3e2768e4920301b3bec2b64bcf7fd6"
                },
                {
                  "y": 130,
                  "x": 216,
                  "u": "https://preview.redd.it/nhi6wunfn8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7b35406cd98a3422f4935403eaa923d1f7bf21d"
                },
                {
                  "y": 193,
                  "x": 320,
                  "u": "https://preview.redd.it/nhi6wunfn8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d012ed3a3edf01287665550f3e158562dc7f328a"
                },
                {
                  "y": 386,
                  "x": 640,
                  "u": "https://preview.redd.it/nhi6wunfn8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4f92e918d55338ae7418381d2565347f6d14446a"
                },
                {
                  "y": 579,
                  "x": 960,
                  "u": "https://preview.redd.it/nhi6wunfn8ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f3c71d2beb61692979fbb699ee9dab516c7d771"
                },
                {
                  "y": 651,
                  "x": 1080,
                  "u": "https://preview.redd.it/nhi6wunfn8ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=95761bdd2b79f131d2c86c6c8dfbd0d890b7101a"
                }
              ],
              "s": {
                "y": 986,
                "x": 1634,
                "u": "https://preview.redd.it/nhi6wunfn8ff1.png?width=1634&amp;format=png&amp;auto=webp&amp;s=6ec4d4a31c1047f354a6813dcd889077f0e06cb7"
              },
              "id": "nhi6wunfn8ff1"
            },
            "lsrxy13qn8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 78,
                  "x": 108,
                  "u": "https://preview.redd.it/lsrxy13qn8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c84ffbc0ced8042b09fbc2fc946caaf3cb412e5c"
                },
                {
                  "y": 156,
                  "x": 216,
                  "u": "https://preview.redd.it/lsrxy13qn8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=25b568231aff62d9697bfcbf4705b5564586b445"
                },
                {
                  "y": 231,
                  "x": 320,
                  "u": "https://preview.redd.it/lsrxy13qn8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc6614bad1306989903f5a2f2abfbef803f6195d"
                },
                {
                  "y": 462,
                  "x": 640,
                  "u": "https://preview.redd.it/lsrxy13qn8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8bc8fdedcaeb2fac76698d114cdf99f66c6e00a5"
                },
                {
                  "y": 693,
                  "x": 960,
                  "u": "https://preview.redd.it/lsrxy13qn8ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=48b6b3b6fb6a37e6614ffd2a867ea320f01cd30d"
                },
                {
                  "y": 780,
                  "x": 1080,
                  "u": "https://preview.redd.it/lsrxy13qn8ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dd89c0eeff4a9b346335f38fe845de2d3f62077e"
                }
              ],
              "s": {
                "y": 918,
                "x": 1270,
                "u": "https://preview.redd.it/lsrxy13qn8ff1.png?width=1270&amp;format=png&amp;auto=webp&amp;s=edc9a2ea7b3eb49f7cf274e8903dd4328d95ebb1"
              },
              "id": "lsrxy13qn8ff1"
            }
          },
          "name": "t3_1m9woxb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/cymzgufUv1SZsoTI2vGjU6tUlc_m9Q8JUI7Ye-hau_w.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753545020,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A few weeks ago (2 weeks), I shared a 23 question survey with my online community. With all the buzz around new model announcements and the &amp;quot;AGI is just around the corner&amp;quot; hype, I wanted to hear directly from people in the field to understand the real picture of Large Language Models (LLMs).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m grateful to all &lt;strong&gt;26 participants&lt;/strong&gt; who took the time to fill out the survey. We had 72 visits to the survey, and 26 completed responses (after removing one spam entry).&lt;/p&gt;\n\n&lt;p&gt;Note: I still need to summarize the open-ended answers, and will share those insights in a later post.&lt;/p&gt;\n\n&lt;h1&gt;Here are the results:&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nhi6wunfn8ff1.png?width=1634&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ec4d4a31c1047f354a6813dcd889077f0e06cb7\"&gt;https://preview.redd.it/nhi6wunfn8ff1.png?width=1634&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ec4d4a31c1047f354a6813dcd889077f0e06cb7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wi7zhv7kn8ff1.png?width=1394&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f0ef9a676bddcc4da1f5b722a755baaa35d4072f\"&gt;https://preview.redd.it/wi7zhv7kn8ff1.png?width=1394&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f0ef9a676bddcc4da1f5b722a755baaa35d4072f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/gn7ecotln8ff1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2001c0e759c39ee2702f35608c66a732eec64167\"&gt;https://preview.redd.it/gn7ecotln8ff1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2001c0e759c39ee2702f35608c66a732eec64167&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/r1ekc61nn8ff1.png?width=1412&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf081f61084d75640f73bfb8fdd68e9a1e2e3308\"&gt;https://preview.redd.it/r1ekc61nn8ff1.png?width=1412&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf081f61084d75640f73bfb8fdd68e9a1e2e3308&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1nqywtmnn8ff1.png?width=1348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa2a983347ec9339c9d0f29360f9d48ae0c58090\"&gt;https://preview.redd.it/1nqywtmnn8ff1.png?width=1348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa2a983347ec9339c9d0f29360f9d48ae0c58090&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dzm2tq9on8ff1.png?width=1404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=920f45ca1d91218b3f9ff13d0f42fec0b2f086e8\"&gt;https://preview.redd.it/dzm2tq9on8ff1.png?width=1404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=920f45ca1d91218b3f9ff13d0f42fec0b2f086e8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5wz98zuon8ff1.png?width=1198&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ccca6af30a91b9352d2a35ce0e5f298de18607b5\"&gt;https://preview.redd.it/5wz98zuon8ff1.png?width=1198&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ccca6af30a91b9352d2a35ce0e5f298de18607b5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/w6brj3dpn8ff1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c81480f675892bfb81485546ac241463bec3c7e\"&gt;https://preview.redd.it/w6brj3dpn8ff1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c81480f675892bfb81485546ac241463bec3c7e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lsrxy13qn8ff1.png?width=1270&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=edc9a2ea7b3eb49f7cf274e8903dd4328d95ebb1\"&gt;https://preview.redd.it/lsrxy13qn8ff1.png?width=1270&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=edc9a2ea7b3eb49f7cf274e8903dd4328d95ebb1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/x4c7zilqn8ff1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bdba0fea27a0342ef2880266961e3b080bbda7ad\"&gt;https://preview.redd.it/x4c7zilqn8ff1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bdba0fea27a0342ef2880266961e3b080bbda7ad&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/a2q40qwrn8ff1.png?width=1178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc4beee55a3ae25fee780cc2b992651dac1113b4\"&gt;https://preview.redd.it/a2q40qwrn8ff1.png?width=1178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc4beee55a3ae25fee780cc2b992651dac1113b4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Key Suggestions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Our current questions are &lt;strong&gt;too open-ended&lt;/strong&gt;; we need to ask more specific questions in the future.&lt;/li&gt;\n&lt;li&gt;We also need to include questions about &lt;strong&gt;proprietary models&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks again to everyone who participated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m9woxb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kidupstart",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9woxb/my_attempt_to_understand_local_llm_landscape/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9woxb/my_attempt_to_understand_local_llm_landscape/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753545020,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I run a LLM for home use, like sorting big text files. Nothing fancy, just more or less boring administrative stuff. I use Qwen3-30B-A3B-128K-UD-Q6\\_K\\_XL for this (by Unsloth) on a CPU only environment (Mini PC with Ryzen and 64GB RAM). I can load and use about 55GB of RAM, so eg. a 45GB LLM + 8GB for data aka context, but big models are very slow (below 1token/s). The A3B model is refreshingly fast (several token/s) but takes ages before it really works - aka reasoning. Although it's sometimes helpful, it also often eats all the given 32k/64k or 128k context before it outputs any results. And it also repeats and repeats and repeats the same train of thoughts. No\\_think is faster (if it works at all) but also worse in the results, so - any alternatives? This A3B model is really usable fast, but takes soooo long to think...",
          "author_fullname": "t2_3q3msk1c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anything as fast as Qwen A3B?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9ujwe",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753539717,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I run a LLM for home use, like sorting big text files. Nothing fancy, just more or less boring administrative stuff. I use Qwen3-30B-A3B-128K-UD-Q6_K_XL for this (by Unsloth) on a CPU only environment (Mini PC with Ryzen and 64GB RAM). I can load and use about 55GB of RAM, so eg. a 45GB LLM + 8GB for data aka context, but big models are very slow (below 1token/s). The A3B model is refreshingly fast (several token/s) but takes ages before it really works - aka reasoning. Although it&amp;#39;s sometimes helpful, it also often eats all the given 32k/64k or 128k context before it outputs any results. And it also repeats and repeats and repeats the same train of thoughts. No_think is faster (if it works at all) but also worse in the results, so - any alternatives? This A3B model is really usable fast, but takes soooo long to think...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9ujwe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pogo4Fufu",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9ujwe/anything_as_fast_as_qwen_a3b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9ujwe/anything_as_fast_as_qwen_a3b/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753539717,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been working on a extension that Allows you to use your LLM from any page on the browser, now I added the capability of pulling and deleting models directly from the browser\n\nIf you want to help me or star my project here is the link (100% open-source):  \n[https://github.com/Aletech-Solutions/XandAI-Extension](https://github.com/Aletech-Solutions/XandAI-Extension)  \n  \n",
          "author_fullname": "t2_lduvmp6r0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Now you can pull LLM models directly from the browser using XandAI extension",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma2r4d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753559951,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a extension that Allows you to use your LLM from any page on the browser, now I added the capability of pulling and deleting models directly from the browser&lt;/p&gt;\n\n&lt;p&gt;If you want to help me or star my project here is the link (100% open-source):&lt;br/&gt;\n&lt;a href=\"https://github.com/Aletech-Solutions/XandAI-Extension\"&gt;https://github.com/Aletech-Solutions/XandAI-Extension&lt;/a&gt;  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/yp9kcsLE0b5eEfAA34Kw6OoIldEMhA7bxI4wXZrUv18.png?auto=webp&amp;s=0b73250d018c6a8b2133f4cd3448a68d77ba542e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/yp9kcsLE0b5eEfAA34Kw6OoIldEMhA7bxI4wXZrUv18.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3307ff783a901de9a927798ebf0e50d1d99c17fd",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/yp9kcsLE0b5eEfAA34Kw6OoIldEMhA7bxI4wXZrUv18.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d1ce952dd81a611a417b73ce4cd966fb4025395",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/yp9kcsLE0b5eEfAA34Kw6OoIldEMhA7bxI4wXZrUv18.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=220b52803af30bbfe2497f9bce8a30314e72930a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/yp9kcsLE0b5eEfAA34Kw6OoIldEMhA7bxI4wXZrUv18.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6c8fd5d3f489a2b1f80cbf70e36067b43588bc88",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/yp9kcsLE0b5eEfAA34Kw6OoIldEMhA7bxI4wXZrUv18.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a93d79c9cd398d355d1db814dced9f21c744260e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/yp9kcsLE0b5eEfAA34Kw6OoIldEMhA7bxI4wXZrUv18.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=abeaff383826dd2133cc39dee211d59e3a5d6e69",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "yp9kcsLE0b5eEfAA34Kw6OoIldEMhA7bxI4wXZrUv18"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ma2r4d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sea-Reception-2697",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma2r4d/now_you_can_pull_llm_models_directly_from_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma2r4d/now_you_can_pull_llm_models_directly_from_the/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753559951,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "have bunch of documents that have this grid like formation and i wanted to build a script to extract the info in json format 1.B,D 2.B 3. A,B,E.....etc tried all the ai models basically tried multiple ocr tools tesseract kraken i even tried Docling but i couldnt get it to work any suggestions? thanxs\n\nhttps://preview.redd.it/lqj7z7v21bff1.png?width=283&amp;format=png&amp;auto=webp&amp;s=e0c02b761c9c77923d817dfbc8fbd9bdb3d61edb\n\n",
          "author_fullname": "t2_7ork9m9r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "WHAT SHOULD I USE?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "lqj7z7v21bff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/lqj7z7v21bff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a9461d6b4cb7139cf7d0a439fec4fda34c5cd4b8"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/lqj7z7v21bff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aee941d9e3366b72c864bd71523a4b02fad1782e"
                }
              ],
              "s": {
                "y": 618,
                "x": 283,
                "u": "https://preview.redd.it/lqj7z7v21bff1.png?width=283&amp;format=png&amp;auto=webp&amp;s=e0c02b761c9c77923d817dfbc8fbd9bdb3d61edb"
              },
              "id": "lqj7z7v21bff1"
            }
          },
          "name": "t3_1ma7oyv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/l-_C-FYyCM6-5B0nt8DkshBPonxNuq_7PQENOnq52kQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753573087,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;have bunch of documents that have this grid like formation and i wanted to build a script to extract the info in json format 1.B,D 2.B 3. A,B,E.....etc tried all the ai models basically tried multiple ocr tools tesseract kraken i even tried Docling but i couldnt get it to work any suggestions? thanxs&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lqj7z7v21bff1.png?width=283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0c02b761c9c77923d817dfbc8fbd9bdb3d61edb\"&gt;https://preview.redd.it/lqj7z7v21bff1.png?width=283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0c02b761c9c77923d817dfbc8fbd9bdb3d61edb&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ma7oyv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Champ4real",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma7oyv/what_should_i_use/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma7oyv/what_should_i_use/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753573087,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been out of the scene for the past few months.\n\nShould I use lmstudio? ollama? llamacpp?\n\nOr ik\\_llama? vllm? lmdeploy?\n\nI have a 4090 + 96 GB of ram and Ryzen 9 7900 and my goal is to hit 100k context with pp times &lt;5 seconds and models 7B to 32B. Possible?",
          "author_fullname": "t2_o087g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the fastest backend for local long context (100k+)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9uoa7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753540045,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been out of the scene for the past few months.&lt;/p&gt;\n\n&lt;p&gt;Should I use lmstudio? ollama? llamacpp?&lt;/p&gt;\n\n&lt;p&gt;Or ik_llama? vllm? lmdeploy?&lt;/p&gt;\n\n&lt;p&gt;I have a 4090 + 96 GB of ram and Ryzen 9 7900 and my goal is to hit 100k context with pp times &amp;lt;5 seconds and models 7B to 32B. Possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9uoa7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "trithilon",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9uoa7/whats_the_fastest_backend_for_local_long_context/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9uoa7/whats_the_fastest_backend_for_local_long_context/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753540045,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "(Rig lol) Ive got a 2x 3090 with 128gb of Ram on a 16 core ryzen 9. What should I use so that I can fully load the GPUs and also the CPU/RAM? Will ollama automatically use what I put in front of it?\n\nI need to be able to use it to provide a local API on my network.",
          "author_fullname": "t2_2zxxuvbk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What inference engine should I use to fully use my budget rug?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1madt6e",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753592339,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Rig lol) Ive got a 2x 3090 with 128gb of Ram on a 16 core ryzen 9. What should I use so that I can fully load the GPUs and also the CPU/RAM? Will ollama automatically use what I put in front of it?&lt;/p&gt;\n\n&lt;p&gt;I need to be able to use it to provide a local API on my network.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1madt6e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bidet_enthusiast",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753592339,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'd like to introduce a high-quality Japanese version of TTS that I've created through continuous pre-learning and post-training with orpheus.\n\n[https://huggingface.co/webbigdata/VoiceCore](https://huggingface.co/webbigdata/VoiceCore)\n\nFindings for those who are trying to create TTS in languages other than English\n\nI think that various TTS models use various neural codecs. This time, I used SNAC 24khz, which is used by orpheus-tts.\n\nSNAC is trained only in English. It is very high performance, but I noticed that there is a tendency for noise to be added to high-pitched voices such as surprise and joy of Japanese women.\n\nI noticed this after a lot of work was completed, so I decided to release it as it is as a preview version. When selecting a codec, I think it is better to first check whether it can handle emotional voices as well as normal voices.\n\nThank you meta/llama 3.2, canopylabs, and snac.\n\nFeedback is welcome.\n\nThank you!",
          "author_fullname": "t2_6sew99etq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "webbigdata/VoiceCore: Japanese voice version of canopylabs/orpheus-tts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9ml0y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753512259,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to introduce a high-quality Japanese version of TTS that I&amp;#39;ve created through continuous pre-learning and post-training with orpheus.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/webbigdata/VoiceCore\"&gt;https://huggingface.co/webbigdata/VoiceCore&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Findings for those who are trying to create TTS in languages other than English&lt;/p&gt;\n\n&lt;p&gt;I think that various TTS models use various neural codecs. This time, I used SNAC 24khz, which is used by orpheus-tts.&lt;/p&gt;\n\n&lt;p&gt;SNAC is trained only in English. It is very high performance, but I noticed that there is a tendency for noise to be added to high-pitched voices such as surprise and joy of Japanese women.&lt;/p&gt;\n\n&lt;p&gt;I noticed this after a lot of work was completed, so I decided to release it as it is as a preview version. When selecting a codec, I think it is better to first check whether it can handle emotional voices as well as normal voices.&lt;/p&gt;\n\n&lt;p&gt;Thank you meta/llama 3.2, canopylabs, and snac.&lt;/p&gt;\n\n&lt;p&gt;Feedback is welcome.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/uFYNIoBfPPAfuk5C31nO4L2HdlD3pLd1JhqRMfIL6NQ.png?auto=webp&amp;s=2ae82e239f4c3b3831b7353351cbef6e3bc8ba77",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/uFYNIoBfPPAfuk5C31nO4L2HdlD3pLd1JhqRMfIL6NQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8af70d876318a37e56084cd5d2f690d937ab70c1",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/uFYNIoBfPPAfuk5C31nO4L2HdlD3pLd1JhqRMfIL6NQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=afddce4493c61691207bd2b51d7e4c4502e74a3e",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/uFYNIoBfPPAfuk5C31nO4L2HdlD3pLd1JhqRMfIL6NQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1f74255680481b8b263bf5804c86250d07707b4",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/uFYNIoBfPPAfuk5C31nO4L2HdlD3pLd1JhqRMfIL6NQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6510f02fc900bb217077a770be169591fbc0cf1",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/uFYNIoBfPPAfuk5C31nO4L2HdlD3pLd1JhqRMfIL6NQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cdb07477f93611dddb3aca6244e275835ed4ff70",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/uFYNIoBfPPAfuk5C31nO4L2HdlD3pLd1JhqRMfIL6NQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7294049b9f0b2d71704d5885bc22b3678335431e",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "uFYNIoBfPPAfuk5C31nO4L2HdlD3pLd1JhqRMfIL6NQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9ml0y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dahara111",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9ml0y/webbigdatavoicecore_japanese_voice_version_of/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9ml0y/webbigdatavoicecore_japanese_voice_version_of/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753512259,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": " Were excited to introduce Qwen3-235B-A22B-Thinking-2507  our most advanced reasoning model yet!\n\nOver the past 3 months, weve significantly scaled and enhanced the thinking capability of Qwen3, achieving:\n Improved performance in logical reasoning, math, science &amp; coding\n Better general skills: instruction following, tool use, alignment\n 256K native context for deep, long-form understanding\n\n Built exclusively for thinking mode, with no need to enable it manually. The model now natively supports extended reasoning chains for maximum depth and accuracy.",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-Thinking-2507 released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vegq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 823,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 823,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/-odbY7J30GjzUj_7jlWdKXiqJnZvfwCCllktRqnbgQw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753438585,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt; Were excited to introduce Qwen3-235B-A22B-Thinking-2507  our most advanced reasoning model yet!&lt;/p&gt;\n\n&lt;p&gt;Over the past 3 months, weve significantly scaled and enhanced the thinking capability of Qwen3, achieving:\n Improved performance in logical reasoning, math, science &amp;amp; coding\n Better general skills: instruction following, tool use, alignment\n 256K native context for deep, long-form understanding&lt;/p&gt;\n\n&lt;p&gt; Built exclusively for thinking mode, with no need to enable it manually. The model now natively supports extended reasoning chains for maximum depth and accuracy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/bvx1dbl5xzef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?auto=webp&amp;s=859c619548c1493932ad87e55f7f58a1af5e10a9",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=12b042c0d833ea5fda0bb3962502543415136139",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f46d3176f8fa24281464889257c33a01a1436c1a",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f8f68cc4715e6804c2c5837be4369857e2df0466",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f579818ebd6748b55b90f802c28f4d37095432e",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=140ba50cbe31d7a36d36327b99a4151addfcc085",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=71a7110710b3d8cba38090a6e670da187ba8f0a7",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "DoQkFL1CfB5iTwd4k2RZEMaeKWH49DDXr_m3yklloUY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8vegq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 173,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vegq/qwen3235ba22bthinking2507_released/",
          "stickied": false,
          "url": "https://i.redd.it/bvx1dbl5xzef1.jpeg",
          "subreddit_subscribers": 505252,
          "created_utc": 1753438585,
          "num_crossposts": 4,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My question is what version of my python does chatter tts need to run correctly. I think I saw somewhere saying it needs version 3.10.8 but I also have stable diffusion running on my computer which becomes buggy if I change from 3.10.6. Would chatterbox still function fine on 3.10.6 or would I need to change it",
          "author_fullname": "t2_1hhil4cbg7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chatterbox Tts python version",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma6b7j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753569173,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My question is what version of my python does chatter tts need to run correctly. I think I saw somewhere saying it needs version 3.10.8 but I also have stable diffusion running on my computer which becomes buggy if I change from 3.10.6. Would chatterbox still function fine on 3.10.6 or would I need to change it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ma6b7j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "StrangeMan060",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma6b7j/chatterbox_tts_python_version/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma6b7j/chatterbox_tts_python_version/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753569173,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "With the release of GLM-4.5 and GLM-4.5-Air (both large MoE models), Zhipu has mentioned that they are also considering upgrading their 9B model if theres enough community interest in a small model.\n\nThis potential small model would be much more accessible than the planned GLM-4.5 models which would likely be far too large to run on most consumer hardware. Personally super excited for this as it would make a great base for finetuning",
          "author_fullname": "t2_1f194h3luj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5-9B?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9fuf9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 56,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 56,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753490431,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the release of GLM-4.5 and GLM-4.5-Air (both large MoE models), Zhipu has mentioned that they are also considering upgrading their 9B model if theres enough community interest in a small model.&lt;/p&gt;\n\n&lt;p&gt;This potential small model would be much more accessible than the planned GLM-4.5 models which would likely be far too large to run on most consumer hardware. Personally super excited for this as it would make a great base for finetuning&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9fuf9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrfakename0",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9fuf9/glm459b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9fuf9/glm459b/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753490431,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "That's seems like good bang/buck, BUT\n\nI am not knowledgeble about the limitations of these cards.\n\nWhat works, what doesn't? \nDrivers available, etc.\n\nOn what kind of platform could I use how many of these?",
          "author_fullname": "t2_k9k5w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD MI50 @ 100",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma5wq2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753568085,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s seems like good bang/buck, BUT&lt;/p&gt;\n\n&lt;p&gt;I am not knowledgeble about the limitations of these cards.&lt;/p&gt;\n\n&lt;p&gt;What works, what doesn&amp;#39;t? \nDrivers available, etc.&lt;/p&gt;\n\n&lt;p&gt;On what kind of platform could I use how many of these?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ma5wq2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BrainOnLoan",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma5wq2/amd_mi50_100/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma5wq2/amd_mi50_100/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753568085,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looks like we will get smaller instruct and reasoning variants of Qwen3 next week. Hopefully smaller Qwen3 coder variants aswell.",
          "author_fullname": "t2_aedi2k9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Smaller Qwen Models next week!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 120,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8w7ny",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 655,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 655,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/6kSnt6WBnJfIBSCL6q0LkQ73C2HHdl70wbVC4gMqel8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753441468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looks like we will get smaller instruct and reasoning variants of Qwen3 next week. Hopefully smaller Qwen3 coder variants aswell.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/752ts71q50ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/752ts71q50ff1.png?auto=webp&amp;s=cd553f34eb742e1d85420c6d88ba6b8cb1d3b9d6",
                  "width": 1220,
                  "height": 1052
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9445aa998ff7b1cb74e082152702795b220a5ac",
                    "width": 108,
                    "height": 93
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=de63b169d17a3c50132d99540acd46ec84af351c",
                    "width": 216,
                    "height": 186
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebafb9a27014643332158cfd5bce11fa7ce928bb",
                    "width": 320,
                    "height": 275
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=27677972e2a6faf4ae42e2c72e03cfbb90ab79cb",
                    "width": 640,
                    "height": 551
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dbb199b19983285065aac667c6d68d707942ae1d",
                    "width": 960,
                    "height": 827
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aff7f0acf9e11e6cd46908b27417dd44a8c4e224",
                    "width": 1080,
                    "height": 931
                  }
                ],
                "variants": {},
                "id": "Uxg0LnkLD_KDODwI8dd_rf7OYZXf6oVvIyFguNn4CcI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8w7ny",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "R46H4V",
          "discussion_type": null,
          "num_comments": 50,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8w7ny/smaller_qwen_models_next_week/",
          "stickied": false,
          "url": "https://i.redd.it/752ts71q50ff1.png",
          "subreddit_subscribers": 505252,
          "created_utc": 1753441468,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, recently we support rekas ai models in uzu engine. Pretty nice model. It shows good performance across all tasks and truly open source. I was able to get almost 16 t/s on my Mac studio with Ultra chip. Highly recommend to try. ",
          "author_fullname": "t2_am0r9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Reka AI models support in uzu engine",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4p0yz1wz14ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 128,
                  "x": 108,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=36d40ff398a85b6a39c79224ffddfc09eed21c15"
                },
                {
                  "y": 256,
                  "x": 216,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=30c3e9e51c86094f782c551bca8f414951b75f43"
                },
                {
                  "y": 379,
                  "x": 320,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4aad8a712011b66ef8aeccc9c81819b541974e01"
                },
                {
                  "y": 759,
                  "x": 640,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ddbf108ac7dcc66e4176ebaeac81580b1b28b4f2"
                },
                {
                  "y": 1139,
                  "x": 960,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=aa2931c03d7bd0b5093dce97885c12549bf6323e"
                },
                {
                  "y": 1282,
                  "x": 1080,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=834d2b109d53329203b2a5be882ffe65f195c7d9"
                }
              ],
              "s": {
                "y": 2048,
                "x": 1725,
                "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=1725&amp;format=pjpg&amp;auto=webp&amp;s=8c7c356b82f1f6ca05e733b2891ee093e8167694"
              },
              "id": "4p0yz1wz14ff1"
            },
            "t6wtfgwz14ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=addc90584f3d6ade4a36295de42ee6dc5b54dd0c"
                },
                {
                  "y": 95,
                  "x": 216,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1b0ce1a18524c47547285a545915c55a8fc8ee41"
                },
                {
                  "y": 141,
                  "x": 320,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a22bd5151d83d4eb08ac8e1076b99a54534c048d"
                },
                {
                  "y": 283,
                  "x": 640,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3a44d4f2806f502111ec8b1b8d62274e8bee42b1"
                },
                {
                  "y": 424,
                  "x": 960,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e28dc031ec52f98a47c6c2dbe703b453d63b599b"
                },
                {
                  "y": 477,
                  "x": 1080,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3fc56b654a49aca496c13c3936531b19135621ef"
                }
              ],
              "s": {
                "y": 722,
                "x": 1632,
                "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=1632&amp;format=pjpg&amp;auto=webp&amp;s=7f198172f1ac57c4ddc796fdc37624ddc0123511"
              },
              "id": "t6wtfgwz14ff1"
            }
          },
          "name": "t3_1m9f7lq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 56,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "4p0yz1wz14ff1",
                "id": 714080367
              },
              {
                "media_id": "t6wtfgwz14ff1",
                "id": 714080368
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 56,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/afA01kAaHMzrELSiWVNCsofMUALmXIzdtd4uSzHvlv4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753488641,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, recently we support rekas ai models in uzu engine. Pretty nice model. It shows good performance across all tasks and truly open source. I was able to get almost 16 t/s on my Mac studio with Ultra chip. Highly recommend to try. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m9f7lq",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9f7lq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "darkolorin",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9f7lq/reka_ai_models_support_in_uzu_engine/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m9f7lq",
          "subreddit_subscribers": 505252,
          "created_utc": 1753488641,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just hosted a mistral model for the first time. tried to havei t speak dutch and it hallucinated a lot of words and grammar. what model would be a bit more seamless when instructed to speak other languages similar to gpt 4o/claude etc?",
          "author_fullname": "t2_1pp6frj307",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "new to all this, best local llm for multilingual (dutch)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma5359",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753565944,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just hosted a mistral model for the first time. tried to havei t speak dutch and it hallucinated a lot of words and grammar. what model would be a bit more seamless when instructed to speak other languages similar to gpt 4o/claude etc?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ma5359",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Internal_Patience297",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma5359/new_to_all_this_best_local_llm_for_multilingual/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma5359/new_to_all_this_best_local_llm_for_multilingual/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753565944,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am working on a chatbot system that offers different services &amp; one of the things I am wondering about is how different input files/type are handled? for example, I want my agent to handle different kinds of files (docx, pdf, excel, pngs,...) and in different quantities (for example, the user uploads a folder of files).\n\nWould such implementation require manual handling for each case? or is there a better way to do this, for example, an MCP server? Please feel free to point out any wrong assumptions on my end; I'm working with Qwen VL currently, it is able to process pngs,jpegs fine with a little bit of preprocessing, but for other inputs (pdfs, docx, csvs, excel sheets,...) do I need to customize the preprocessing for each? and if so, what format would be better used for the llm to understand (for excel VS. csv for example).\n\nAny help/tips is appreciated, thank you.",
          "author_fullname": "t2_1avlzvhbnl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to handle different input types",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma4oqz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753564903,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a chatbot system that offers different services &amp;amp; one of the things I am wondering about is how different input files/type are handled? for example, I want my agent to handle different kinds of files (docx, pdf, excel, pngs,...) and in different quantities (for example, the user uploads a folder of files).&lt;/p&gt;\n\n&lt;p&gt;Would such implementation require manual handling for each case? or is there a better way to do this, for example, an MCP server? Please feel free to point out any wrong assumptions on my end; I&amp;#39;m working with Qwen VL currently, it is able to process pngs,jpegs fine with a little bit of preprocessing, but for other inputs (pdfs, docx, csvs, excel sheets,...) do I need to customize the preprocessing for each? and if so, what format would be better used for the llm to understand (for excel VS. csv for example).&lt;/p&gt;\n\n&lt;p&gt;Any help/tips is appreciated, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ma4oqz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Worldly-Algae7541",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma4oqz/how_to_handle_different_input_types/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma4oqz/how_to_handle_different_input_types/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753564903,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm a beginner to all this but I want to practice fine tuning and gaining general knowledge on local ai overall dose anyone have any suggestions on where to learn? Or if there's someone with experience that's willing to share general insights it would be greatly appreciated ",
          "author_fullname": "t2_i5w9033e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Beginner suggestions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma426d",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753563286,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a beginner to all this but I want to practice fine tuning and gaining general knowledge on local ai overall dose anyone have any suggestions on where to learn? Or if there&amp;#39;s someone with experience that&amp;#39;s willing to share general insights it would be greatly appreciated &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ma426d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Budget-Management698",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma426d/beginner_suggestions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma426d/beginner_suggestions/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753563286,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/64acjiuoscff1.png?width=709&amp;format=png&amp;auto=webp&amp;s=b2684299a3979cf7065d74f1c5d0f52703200e75\n\n",
          "author_fullname": "t2_9qrzczg1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "this actually made me feel so relieved haha",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 116,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "64acjiuoscff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 89,
                  "x": 108,
                  "u": "https://preview.redd.it/64acjiuoscff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=50cfbe89f811ee94102727c8cbce55f5d8494d40"
                },
                {
                  "y": 179,
                  "x": 216,
                  "u": "https://preview.redd.it/64acjiuoscff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=64875c8948998e7412cf2366567e82f1fc07f0ee"
                },
                {
                  "y": 265,
                  "x": 320,
                  "u": "https://preview.redd.it/64acjiuoscff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b6d1600b5bb64a3696e4da6f3eb1cd4654f3c923"
                },
                {
                  "y": 530,
                  "x": 640,
                  "u": "https://preview.redd.it/64acjiuoscff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=20b1dcce5f28cffefc52634493ae32d11d55bacf"
                }
              ],
              "s": {
                "y": 588,
                "x": 709,
                "u": "https://preview.redd.it/64acjiuoscff1.png?width=709&amp;format=png&amp;auto=webp&amp;s=b2684299a3979cf7065d74f1c5d0f52703200e75"
              },
              "id": "64acjiuoscff1"
            }
          },
          "name": "t3_1maeg4y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.13,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/9py5bEaOewfdSUIBnCSEcWfQNXpZKK0HUaexpuPR-MA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753594609,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/64acjiuoscff1.png?width=709&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2684299a3979cf7065d74f1c5d0f52703200e75\"&gt;https://preview.redd.it/64acjiuoscff1.png?width=709&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2684299a3979cf7065d74f1c5d0f52703200e75&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1maeg4y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Current_Housing_7294",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maeg4y/this_actually_made_me_feel_so_relieved_haha/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maeg4y/this_actually_made_me_feel_so_relieved_haha/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753594609,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello guys,  \nSo I have been developing a NL to SQL multi agent system using langgraph and llama 3:8b.  \nLately I read at some places and the official docs that 8b version is not capable of maitaining regular conversations with tool calling.  \nI need some suggestions on if I should use any other version of llama which supports tool calling. Tool calling is needed because I need some way to generate visuals/ answer very complex queries etc.  \nMaybe there is a hack or I am completely missing something.  \nThanks for the suggestions. ",
          "author_fullname": "t2_kws1zhj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tool calling support in Llama 3 8b",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma3hmd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753561832,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,&lt;br/&gt;\nSo I have been developing a NL to SQL multi agent system using langgraph and llama 3:8b.&lt;br/&gt;\nLately I read at some places and the official docs that 8b version is not capable of maitaining regular conversations with tool calling.&lt;br/&gt;\nI need some suggestions on if I should use any other version of llama which supports tool calling. Tool calling is needed because I need some way to generate visuals/ answer very complex queries etc.&lt;br/&gt;\nMaybe there is a hack or I am completely missing something.&lt;br/&gt;\nThanks for the suggestions. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ma3hmd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "codingpinscher",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma3hmd/tool_calling_support_in_llama_3_8b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma3hmd/tool_calling_support_in_llama_3_8b/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753561832,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Testing Qwen2.5-VL-7B for pill/imprint text extraction.\n\nWondering if any of you would know of a vLLM that would work well for this use case.\n\nLooking for best options for pharmaceutical OCR (imprint codes, dosages) that are:\n- More accurate \n- Easier RunPod deployment\n- Better price/performance\n\nAny experience with LLaVA, CogVLM, or others for this use case?",
          "author_fullname": "t2_1iti0qaxaf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best vLLM for pill imprint/textOCR?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma89au",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753574706,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Testing Qwen2.5-VL-7B for pill/imprint text extraction.&lt;/p&gt;\n\n&lt;p&gt;Wondering if any of you would know of a vLLM that would work well for this use case.&lt;/p&gt;\n\n&lt;p&gt;Looking for best options for pharmaceutical OCR (imprint codes, dosages) that are:\n- More accurate \n- Easier RunPod deployment\n- Better price/performance&lt;/p&gt;\n\n&lt;p&gt;Any experience with LLaVA, CogVLM, or others for this use case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ma89au",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Virtual_Attitude2025",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma89au/best_vllm_for_pill_imprinttextocr/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma89au/best_vllm_for_pill_imprinttextocr/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753574706,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I know zenith is allegedly an openai or kimi model, but I've not found anything about summit?",
          "author_fullname": "t2_1fzopogahb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New model on lmarena called summit?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9saif",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753533466,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know zenith is allegedly an openai or kimi model, but I&amp;#39;ve not found anything about summit?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9saif",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hereitisguys9888",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9saif/new_model_on_lmarena_called_summit/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9saif/new_model_on_lmarena_called_summit/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753533466,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello  friends!\nI have a rig with 1TB RAM and one A100 80 GB. What task would you assign to a couple of python programmers, who doesn't have any idea about ML/LLMs, for 2 weeks to complete or to gain new skill/knowledge?",
          "author_fullname": "t2_banbmed5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Task for python dev",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma2vjq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753560253,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello  friends!\nI have a rig with 1TB RAM and one A100 80 GB. What task would you assign to a couple of python programmers, who doesn&amp;#39;t have any idea about ML/LLMs, for 2 weeks to complete or to gain new skill/knowledge?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ma2vjq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GoldCompetition7722",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma2vjq/task_for_python_dev/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma2vjq/task_for_python_dev/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753560253,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It seems the updated model doesnt enclose thinking in &lt;think&gt;&lt;/think&gt; tags.\nWhich means you cant collapse thinking window in gui apps like LM studio.",
          "author_fullname": "t2_jqxb4pte",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Think tags missing in Qwen3-235B-A22B-Thinking-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9qt65",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753528651,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems the updated model doesnt enclose thinking in &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; tags.\nWhich means you cant collapse thinking window in gui apps like LM studio.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9qt65",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Conversation9561",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9qt65/think_tags_missing_in_qwen3235ba22bthinking2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9qt65/think_tags_missing_in_qwen3235ba22bthinking2507/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753528651,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been seeing some hype around the upcoming RTX 5060 (Blackwell series), and I wanted to throw this out to folks doing serious local inference: how does it *really* stack up against the tried-and-tested 4090?  \nIf your goal is real local AI use (fast generation, agent chains, even fine-tuning), dont let the generational number fool you  **the 4090 still obliterates the 5060** in every practical sense.",
          "author_fullname": "t2_uaotuj04",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RTX 4090 vs RTX 5060 ....Is the 5060 even worth considering for local LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mahjoo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.21,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753606445,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been seeing some hype around the upcoming RTX 5060 (Blackwell series), and I wanted to throw this out to folks doing serious local inference: how does it &lt;em&gt;really&lt;/em&gt; stack up against the tried-and-tested 4090?&lt;br/&gt;\nIf your goal is real local AI use (fast generation, agent chains, even fine-tuning), dont let the generational number fool you  &lt;strong&gt;the 4090 still obliterates the 5060&lt;/strong&gt; in every practical sense.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mahjoo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Edge2098",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mahjoo/rtx_4090_vs_rtx_5060_is_the_5060_even_worth/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mahjoo/rtx_4090_vs_rtx_5060_is_the_5060_even_worth/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753606445,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "There have been many models released this year already and have lost track of which models are better and for what.\n\n**Does anyone have some resource or spreadsheet that collates the results of many models on many benchmarks?**\n\nI'm slightly more interested in open-weights model results, but I think it's important to have data for closed source as well for comparison.\n\nI've tried to look myself, but the following resources aren't what I'm looking for:\n\n* [vellum.ai/llm-leaderboard](http://vellum.ai/llm-leaderboard) \\- not enough models or benchmarks covered\n* [artificialanalysis.ai](http://artificialanalysis.ai) \\- does cover lots of models, but only uses single number for intelligence\n* [https://dubesor.de/benchtable](https://dubesor.de/benchtable) \\- no official benchmarks used\n* [https://llm-stats.com/](https://llm-stats.com/) \\- not many benchmarks covered",
          "author_fullname": "t2_2ys57apx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone created a table of collated benchmark results of many LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9p9kg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753522802,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There have been many models released this year already and have lost track of which models are better and for what.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Does anyone have some resource or spreadsheet that collates the results of many models on many benchmarks?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m slightly more interested in open-weights model results, but I think it&amp;#39;s important to have data for closed source as well for comparison.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried to look myself, but the following resources aren&amp;#39;t what I&amp;#39;m looking for:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"http://vellum.ai/llm-leaderboard\"&gt;vellum.ai/llm-leaderboard&lt;/a&gt; - not enough models or benchmarks covered&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"http://artificialanalysis.ai\"&gt;artificialanalysis.ai&lt;/a&gt; - does cover lots of models, but only uses single number for intelligence&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dubesor.de/benchtable\"&gt;https://dubesor.de/benchtable&lt;/a&gt; - no official benchmarks used&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://llm-stats.com/\"&gt;https://llm-stats.com/&lt;/a&gt; - not many benchmarks covered&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4mltMPuoIx8FMW1Y60sN3h17rSzmD3D6Q8bt5cARpzY.png?auto=webp&amp;s=694be80e81c347654e4a0cc9eab6ad4723b65311",
                  "width": 1165,
                  "height": 627
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4mltMPuoIx8FMW1Y60sN3h17rSzmD3D6Q8bt5cARpzY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=01b76279937283a6f65fb2f22bacafcfd57327f0",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/4mltMPuoIx8FMW1Y60sN3h17rSzmD3D6Q8bt5cARpzY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c32f4a5cf158408de0486e01f1bb633dac191c9e",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/4mltMPuoIx8FMW1Y60sN3h17rSzmD3D6Q8bt5cARpzY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0e6c25d22c39ee81cd1fed7e9caf0295ccb0a828",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/4mltMPuoIx8FMW1Y60sN3h17rSzmD3D6Q8bt5cARpzY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e5ff7437f75185162c16b9fa223ebb95d12680c5",
                    "width": 640,
                    "height": 344
                  },
                  {
                    "url": "https://external-preview.redd.it/4mltMPuoIx8FMW1Y60sN3h17rSzmD3D6Q8bt5cARpzY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b8fe4d3636c03fffa93f2686ad89c21e63cd305e",
                    "width": 960,
                    "height": 516
                  },
                  {
                    "url": "https://external-preview.redd.it/4mltMPuoIx8FMW1Y60sN3h17rSzmD3D6Q8bt5cARpzY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=286fe8030eb3bac68c389e001bb59d162a1b2e9f",
                    "width": 1080,
                    "height": 581
                  }
                ],
                "variants": {},
                "id": "4mltMPuoIx8FMW1Y60sN3h17rSzmD3D6Q8bt5cARpzY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9p9kg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JawGBoi",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9p9kg/has_anyone_created_a_table_of_collated_benchmark/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9p9kg/has_anyone_created_a_table_of_collated_benchmark/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753522802,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just finished uploading and perplexity testing some new ik\\_llama.cpp quants. Despite the random github takedown (and subsequent restoring) ik\\_llama.cpp is going strong!\n\nik just refreshed the IQ4\\_KSS 4.0 bpw non-linear quantization for faster performance and great perplexity so this quant hits a sweet spot at \\~114GiB allowing 2x64GB DDR5 gaming rigs with a single GPU to run it with decently long context lengths.\n\nAlso ik\\_llama.cpp recently had some PRs to improve tool/function calling.\n\nIf you have more RAM, check out my larger Qwen3-Coder-480B-A35B-Instruct-GGUF quants if that is your thing.\n\nCheers!",
          "author_fullname": "t2_n321yfw5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "IQ4_KSS 114 GiB and more ik_llama.cpp exclusive quants!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9cp2n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": "#bbbdbf",
          "ups": 42,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 42,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=c5722281288ad17c5425d7d73e761d8eafbf6b87",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753481963,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just finished uploading and perplexity testing some new ik_llama.cpp quants. Despite the random github takedown (and subsequent restoring) ik_llama.cpp is going strong!&lt;/p&gt;\n\n&lt;p&gt;ik just refreshed the IQ4_KSS 4.0 bpw non-linear quantization for faster performance and great perplexity so this quant hits a sweet spot at ~114GiB allowing 2x64GB DDR5 gaming rigs with a single GPU to run it with decently long context lengths.&lt;/p&gt;\n\n&lt;p&gt;Also ik_llama.cpp recently had some PRs to improve tool/function calling.&lt;/p&gt;\n\n&lt;p&gt;If you have more RAM, check out my larger Qwen3-Coder-480B-A35B-Instruct-GGUF quants if that is your thing.&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/ubergarm/Qwen3-235B-A22B-Thinking-2507-GGUF",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?auto=webp&amp;s=f0dc878274992b77737eb6b8b4cf221787490f16",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a69f6d9c0fc9888f89ac3d1e39ffbc13bdff2b89",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4746d87fa4ee43c4c2ad291169131df06a3880bb",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=64c7166a9e592474054818234e6a8fed62af8aca",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=14512857abb7d9ebb3c411f65e1639efb84a8b4a",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce689ee00af8c47b796b25b64ba7f36e852011a9",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=941654186ea54e774e0ff104ec12535879418828",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9cp2n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VoidAlchemy",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m9cp2n/iq4_kss_114_gib_and_more_ik_llamacpp_exclusive/",
          "stickied": false,
          "url": "https://huggingface.co/ubergarm/Qwen3-235B-A22B-Thinking-2507-GGUF",
          "subreddit_subscribers": 505252,
          "created_utc": 1753481963,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yo, Im new to this whole local AI model thing. My setups got 16GB RAM and a GTX1650 with 4GB VRAMyeah, I know its weak.\n\nI started with the model **mythomax-l2-13b.Q5\\_K\\_S.gguf** (yeah, kinda overkill for my setup) running on **oobabooga/text-generation-webui**. First time I tried it, everything worked finechat mode was dope, characters were on point, RAM was maxed but I still had 12GB free, VRAM full, all good.\n\nThen I killed the console to shut it down (thought that was normal), but when I booted it back up the next time, everything went to hell. Now its crazy slow, RAMs almost completely eaten (less than 500MB free), and the chat mode feels dumblike just a generic AI assistant.\n\nI tried lowering `ctx-size`, still the same issue: RAM full, performance trash. I even deleted the entire **oobabooga/text-generation-webui** folder to start fresh, but when I reopened the WebUI, nothing changedlike my old settings and chats were still there. Tried deleting all chats thinking maybe it was token bloat, but nope, same problem.\n\nAnyone got any suggestions to fix this?",
          "author_fullname": "t2_1quxz8adxt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "hay everyone I'm new here help please",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma8iez",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.17,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753575468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yo, Im new to this whole local AI model thing. My setups got 16GB RAM and a GTX1650 with 4GB VRAMyeah, I know its weak.&lt;/p&gt;\n\n&lt;p&gt;I started with the model &lt;strong&gt;mythomax-l2-13b.Q5_K_S.gguf&lt;/strong&gt; (yeah, kinda overkill for my setup) running on &lt;strong&gt;oobabooga/text-generation-webui&lt;/strong&gt;. First time I tried it, everything worked finechat mode was dope, characters were on point, RAM was maxed but I still had 12GB free, VRAM full, all good.&lt;/p&gt;\n\n&lt;p&gt;Then I killed the console to shut it down (thought that was normal), but when I booted it back up the next time, everything went to hell. Now its crazy slow, RAMs almost completely eaten (less than 500MB free), and the chat mode feels dumblike just a generic AI assistant.&lt;/p&gt;\n\n&lt;p&gt;I tried lowering &lt;code&gt;ctx-size&lt;/code&gt;, still the same issue: RAM full, performance trash. I even deleted the entire &lt;strong&gt;oobabooga/text-generation-webui&lt;/strong&gt; folder to start fresh, but when I reopened the WebUI, nothing changedlike my old settings and chats were still there. Tried deleting all chats thinking maybe it was token bloat, but nope, same problem.&lt;/p&gt;\n\n&lt;p&gt;Anyone got any suggestions to fix this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ma8iez",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "-Fibon4cci",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma8iez/hay_everyone_im_new_here_help_please/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma8iez/hay_everyone_im_new_here_help_please/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753575468,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I know GPUs can be connected (like via NVLink or just multiple GPUs in one system), but can their **VRAM** be **combined**?\n\n\n\nHeres my use case: I have two GTX 1060 6GB cards, and theoretically together they give me 12GB of VRAM.\n\n\n\n**Question**  can I run a model (like an LLM or SDXL) that requires more than 6GB (or even 8B+ params) using both cards? Or am I still limited to just 6GB because the VRAM isnt shared?",
          "author_fullname": "t2_odgywsgtq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help understanding GPU VRAM pooling  can I combine VRAM across GPUs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9rmry",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753531408,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I know GPUs can be connected (like via NVLink or just multiple GPUs in one system), but can their &lt;strong&gt;VRAM&lt;/strong&gt; be &lt;strong&gt;combined&lt;/strong&gt;?&lt;/p&gt;\n\n&lt;p&gt;Heres my use case: I have two GTX 1060 6GB cards, and theoretically together they give me 12GB of VRAM.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;  can I run a model (like an LLM or SDXL) that requires more than 6GB (or even 8B+ params) using both cards? Or am I still limited to just 6GB because the VRAM isnt shared?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9rmry",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Recent-Bother5388",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9rmry/need_help_understanding_gpu_vram_pooling_can_i/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9rmry/need_help_understanding_gpu_vram_pooling_can_i/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753531408,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It seems much effort was spent to improve quantization by the community trying to fit a dense model in VRAM so it didnt tick along at 2 tokens a second. Many even bought multiple cards to have more VRAM. \n\nNow many new models are MoEs, where the average Joe sits hopelessly at his computer with a couple of consumer cards and 32 gb of RAM. Obviously lots of system RAM is cheaper than lots of VRAM but the larger MoEs have as many active parameters as some dense models of years past.\n\nHow likely are we to see improvements that can take Qwen 3s massive MoE and cut it down with similar performance but at a dense 72b size? Or the new ERNIE? Or Deepseek?\n\nNvidia has done some pruning of dense models, and it seems likely that a MoE has less efficiency since it performs just a little better than the dense models.  So it seems likely to me  as a layman. \n\nAnyone familiar with efforts towards economic solutions that could compress MoEs in ways other than quantization? Does anyone with a better grasp of the architecture think its possible? What challenges might there be what solutions might exist love your thoughts!",
          "author_fullname": "t2_dissgzyl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "There has been a lot of efforts in the past to improve  quantization due to the size of dense models are we likely to see improvements like pruning and/or distillation with the uprise of huge MoEs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9hycx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753496736,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems much effort was spent to improve quantization by the community trying to fit a dense model in VRAM so it didnt tick along at 2 tokens a second. Many even bought multiple cards to have more VRAM. &lt;/p&gt;\n\n&lt;p&gt;Now many new models are MoEs, where the average Joe sits hopelessly at his computer with a couple of consumer cards and 32 gb of RAM. Obviously lots of system RAM is cheaper than lots of VRAM but the larger MoEs have as many active parameters as some dense models of years past.&lt;/p&gt;\n\n&lt;p&gt;How likely are we to see improvements that can take Qwen 3s massive MoE and cut it down with similar performance but at a dense 72b size? Or the new ERNIE? Or Deepseek?&lt;/p&gt;\n\n&lt;p&gt;Nvidia has done some pruning of dense models, and it seems likely that a MoE has less efficiency since it performs just a little better than the dense models.  So it seems likely to me  as a layman. &lt;/p&gt;\n\n&lt;p&gt;Anyone familiar with efforts towards economic solutions that could compress MoEs in ways other than quantization? Does anyone with a better grasp of the architecture think its possible? What challenges might there be what solutions might exist love your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9hycx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "silenceimpaired",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9hycx/there_has_been_a_lot_of_efforts_in_the_past_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9hycx/there_has_been_a_lot_of_efforts_in_the_past_to/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753496736,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello all!\n\nim comparativly new to Local AI but im interrested in a Project of mine that would require a locally hosted AI for inference based on alot of Files with RAG. (or at least that how i envision it at the moment)\n\nthe usecase would be to automatically create \"summaries\" based on the Files in RAG. So no chat and tbh i dont really care about performance as long as it dosn't take like 20min+ for an answer.\n\nMy biggest problem at the moment is, it seems like the models i can run at the moment don't provide enough context for an adequate answer.\n\nSo i have a view questions but the most pressing ones would be:\n\n1. is my problem actually based on the context, or am i doing something completly wrong? If i try to search if RAG is actually part of the provided context for a model i get really contradictory results. Is there some trustworthy source i could read up on?\n2. Would a large Model (with alot of context) based on CPU with 1TB of ram provide better results than a smaller model on a GPU if i never intend to train a model and performance is not necessarily a priority?\n\ni hope someone can enlighten me here and clear up some missunderstandings. thanks!",
          "author_fullname": "t2_k4eh65v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local Machine setup",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9thq6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753536859,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all!&lt;/p&gt;\n\n&lt;p&gt;im comparativly new to Local AI but im interrested in a Project of mine that would require a locally hosted AI for inference based on alot of Files with RAG. (or at least that how i envision it at the moment)&lt;/p&gt;\n\n&lt;p&gt;the usecase would be to automatically create &amp;quot;summaries&amp;quot; based on the Files in RAG. So no chat and tbh i dont really care about performance as long as it dosn&amp;#39;t take like 20min+ for an answer.&lt;/p&gt;\n\n&lt;p&gt;My biggest problem at the moment is, it seems like the models i can run at the moment don&amp;#39;t provide enough context for an adequate answer.&lt;/p&gt;\n\n&lt;p&gt;So i have a view questions but the most pressing ones would be:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;is my problem actually based on the context, or am i doing something completly wrong? If i try to search if RAG is actually part of the provided context for a model i get really contradictory results. Is there some trustworthy source i could read up on?&lt;/li&gt;\n&lt;li&gt;Would a large Model (with alot of context) based on CPU with 1TB of ram provide better results than a smaller model on a GPU if i never intend to train a model and performance is not necessarily a priority?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;i hope someone can enlighten me here and clear up some missunderstandings. thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9thq6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Bloodorem",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9thq6/local_machine_setup/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9thq6/local_machine_setup/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753536859,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_14mlbg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hunyuan (Ex-WizardLM) Dense Model Coming Soon!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m94ls2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 88,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 88,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/jB1CS2LZrHw4qZHwx53ViD8xx1eEncZt0jht4lZ5-yw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753462750,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14878",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?auto=webp&amp;s=0fb34e0f66b8292996122ca9f453b58a37b14813",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1baecdc2dfde1050ccffe3b9db6f4cad84a6a26",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b7d9c1d3dc4da4b79164f82841fe3a8e4f0301c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8e2c1e124ff0e7f72b87f1fd2dae8eacf659993",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=04fdf799a0982d200ae760ce600052e22efe1fd7",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d09d8c9f8137e0686807b5b322768faea8d8935d",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0c6f33fac669b52e589697bc73e7a0408f351898",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "9GvEX7LrasZYgCzEEJMA4dtKp0bfjGuzNOUm65ANRbI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m94ls2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TKGaming_11",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m94ls2/hunyuan_exwizardlm_dense_model_coming_soon/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14878",
          "subreddit_subscribers": 505252,
          "created_utc": 1753462750,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a small dataset which I had trained on Nebius Ai studio and downloaded the files. I then merged the model Llama 3.2-3B instruct and lora adaptor for it. And then when I coverted it in GGUF and loaded on kobaldcpp for test, it giving me this. I am new to all this so if anyone need more information to know the error, please let me know ",
          "author_fullname": "t2_79u553e1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Merged Lora adaptor Model Giving Gibberish as response.  Using Llama 3.2 3B instruct. Dataset trained on Nebius Ai studio. What to do?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 42,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9p7bb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/JpdPpeJ_7kVSSsglwpKR1DkPrBJmly4pwS_UHo8N72I.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753522558,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a small dataset which I had trained on Nebius Ai studio and downloaded the files. I then merged the model Llama 3.2-3B instruct and lora adaptor for it. And then when I coverted it in GGUF and loaded on kobaldcpp for test, it giving me this. I am new to all this so if anyone need more information to know the error, please let me know &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/oj9iadphu6ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/oj9iadphu6ff1.png?auto=webp&amp;s=c2972c93a66d9ceedddeee001ca98e0510cd468e",
                  "width": 1625,
                  "height": 488
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/oj9iadphu6ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a9204efaa1a75ec8eb30cd5a6b003e3499b37518",
                    "width": 108,
                    "height": 32
                  },
                  {
                    "url": "https://preview.redd.it/oj9iadphu6ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=55d6bda53c728cfad9883ea9aff8e267c5502a2e",
                    "width": 216,
                    "height": 64
                  },
                  {
                    "url": "https://preview.redd.it/oj9iadphu6ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=66a1c87c898b9882a89297fd51d7efb6c1f570a3",
                    "width": 320,
                    "height": 96
                  },
                  {
                    "url": "https://preview.redd.it/oj9iadphu6ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=afb8e2b38ca43c2057294a73cbd2fc6542dc5675",
                    "width": 640,
                    "height": 192
                  },
                  {
                    "url": "https://preview.redd.it/oj9iadphu6ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1f388d9fdc773534873635200869e84fdde884b3",
                    "width": 960,
                    "height": 288
                  },
                  {
                    "url": "https://preview.redd.it/oj9iadphu6ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5d410556f87dd4a4da64e61628e4f85f50ae77c3",
                    "width": 1080,
                    "height": 324
                  }
                ],
                "variants": {},
                "id": "mAUoCGTDX2IzYO5VD1nIJ4FheUUNrft3eYnKExXJx_I"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9p7bb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rihuwamidori",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9p7bb/merged_lora_adaptor_model_giving_gibberish_as/",
          "stickied": false,
          "url": "https://i.redd.it/oj9iadphu6ff1.png",
          "subreddit_subscribers": 505252,
          "created_utc": 1753522558,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was told trying to run non-tiny LLM's on a CPU was unusable.  But I got 8.3 token/sec for qwen2.5-coder-32b-instruct Q8 without using the GPU.  38.6 tokens/sec using both 5090's.  Note, I'm getting barely 48% processing usage on the 5090's and wondering what I can do to improve that.\n\nLlama.cpp thread affinity seems to not do anything on Ubuntu. For my CPU's runs I had to do my own fix for this.  I mainly did this to see how well layer overflowing will work for even larger models.  \nThe problem is the nearly continuous stream of new models to try.  \nWas going with qwen2.5-coder-32b-instruct.  \nThen today I see Qwen3-235B-A22B-Thinking-2507-FP8 and just now [Llama-3\\_3-Nemotron-Super-49B-v1\\_5](https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5)   \nToo many choices.\n\n",
          "author_fullname": "t2_rxk6hx4t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "My 7985WX, dual 5090's, and 256GB's of DDR5-6000 has landed.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9itnz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753499460,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was told trying to run non-tiny LLM&amp;#39;s on a CPU was unusable.  But I got 8.3 token/sec for qwen2.5-coder-32b-instruct Q8 without using the GPU.  38.6 tokens/sec using both 5090&amp;#39;s.  Note, I&amp;#39;m getting barely 48% processing usage on the 5090&amp;#39;s and wondering what I can do to improve that.&lt;/p&gt;\n\n&lt;p&gt;Llama.cpp thread affinity seems to not do anything on Ubuntu. For my CPU&amp;#39;s runs I had to do my own fix for this.  I mainly did this to see how well layer overflowing will work for even larger models.&lt;br/&gt;\nThe problem is the nearly continuous stream of new models to try.&lt;br/&gt;\nWas going with qwen2.5-coder-32b-instruct.&lt;br/&gt;\nThen today I see Qwen3-235B-A22B-Thinking-2507-FP8 and just now &lt;a href=\"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\"&gt;Llama-3_3-Nemotron-Super-49B-v1_5&lt;/a&gt;&lt;br/&gt;\nToo many choices.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?auto=webp&amp;s=af31b2002f0236c31cf3c91755fd855ed95ae985",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45e7c8c14055c57d8c62dad0b150faa3212ce087",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=82619b61b919798034ba0f0b798bd1e75640c0b9",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eaf3accf5409bb25bb8728256d4e2f61e2bbbeec",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8ce793850ca6936254a722184eb2367e6423fa1",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e1d1bb2008c0cfb9abdbf16638bc668942167e7",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=313d9b1115102aa6244b349a8e99c1ee840c4702",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9itnz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Guilty-History-9249",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753499460,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7pfgfkis",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Qwen3 on Fiction.liveBench",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m93d0r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 96,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 96,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/VIv9hJCgW9w5E1qzXymkc_nKdHvwujLLN61ek5o0LXE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753459907,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/hvi3tvmjo1ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?auto=webp&amp;s=6258c8c89fb879cded6d316f86467b097f492051",
                  "width": 1568,
                  "height": 2318
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=385e269d9eecbdde591e638694025b90d6447acc",
                    "width": 108,
                    "height": 159
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=419c2fef6a3435f83900687fabdaca18e00bdf81",
                    "width": 216,
                    "height": 319
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=73f13ee604283c4240ce3dc6e2e12c02fbafbcf5",
                    "width": 320,
                    "height": 473
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8c9a4a27cda997bb1fb4bc522e4d9bac9f04231",
                    "width": 640,
                    "height": 946
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ca984a8304eade76aa7c934ed429e1c9a1fcc949",
                    "width": 960,
                    "height": 1419
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1faa0dda9eeec65358a99b5f5dcf6f3256542ec9",
                    "width": 1080,
                    "height": 1596
                  }
                ],
                "variants": {},
                "id": "ZokGAHU7CGwFeHWan5TASqOpdAfZ1F2vcHM0eru8zuw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m93d0r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fictionlive",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m93d0r/new_qwen3_on_fictionlivebench/",
          "stickied": false,
          "url": "https://i.redd.it/hvi3tvmjo1ff1.png",
          "subreddit_subscribers": 505252,
          "created_utc": 1753459907,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is an AI web browser that uses local AI models. It's still very early, FULL of bugs and missing key features as a browser, but still good to play around with it.   \n  \nDownload it from[Github](https://github.com/nuance-dev/Web)\n\nNote: AI features only work with M series chips.",
          "author_fullname": "t2_7tlxcyy6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I created an open-source macOS AI browser that uses MLX and Gemma 3n, feel free to fork it!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m903il",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 141,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/fculp27z11ff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1834,
              "scrubber_media_url": "https://v.redd.it/fculp27z11ff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/fculp27z11ff1/DASHPlaylist.mpd?a=1756211237%2CNjI4MWRlMDkyMWY5MDVjYzYyMDRjYzViZTkyYjA4MjI1ZjE4OTA1OWY4OTAzODkxYTg2OWI2NTFlNWEwY2EwOA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 55,
              "hls_url": "https://v.redd.it/fculp27z11ff1/HLSPlaylist.m3u8?a=1756211237%2CNTViODIzMzczMDgzMWFlZTI2Njc3YmNlYWU3MWJlNDk5NWFhYjQ1ZmUwYmNhYzM5MDUyNThhYzI0MTMwNzc1Yg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 141,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=140&amp;height=82&amp;crop=140:82,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=ec09409f07401537e98387108dfed9051e1440fa",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753452396,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is an AI web browser that uses local AI models. It&amp;#39;s still very early, FULL of bugs and missing key features as a browser, but still good to play around with it.   &lt;/p&gt;\n\n&lt;p&gt;Download it from&lt;a href=\"https://github.com/nuance-dev/Web\"&gt;Github&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Note: AI features only work with M series chips.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/fculp27z11ff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?format=pjpg&amp;auto=webp&amp;s=7c21e43bb9a5da5da2c2010ce93da12804258bf5",
                  "width": 2844,
                  "height": 1674
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fcdf109b4fdfd5cf54e3e2e866680aebaec3a5ae",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=112befd5fa6112b6cb09d0adc2d4a9d58cfe354c",
                    "width": 216,
                    "height": 127
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6be30ac241df05f59330c9fd5fd4452821ab66fc",
                    "width": 320,
                    "height": 188
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d5ddaa6122638b83743d7370ed78028c80cb5d52",
                    "width": 640,
                    "height": 376
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a46f147a34e2d2ab597964ba48f2a74696b5c102",
                    "width": 960,
                    "height": 565
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ed6db97d6de07a2a5b3adbb6c2277382e6257018",
                    "width": 1080,
                    "height": 635
                  }
                ],
                "variants": {},
                "id": "NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m903il",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sirjoaco",
          "discussion_type": null,
          "num_comments": 38,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m903il/i_created_an_opensource_macos_ai_browser_that/",
          "stickied": false,
          "url": "https://v.redd.it/fculp27z11ff1",
          "subreddit_subscribers": 505252,
          "created_utc": 1753452396,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/fculp27z11ff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1834,
              "scrubber_media_url": "https://v.redd.it/fculp27z11ff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/fculp27z11ff1/DASHPlaylist.mpd?a=1756211237%2CNjI4MWRlMDkyMWY5MDVjYzYyMDRjYzViZTkyYjA4MjI1ZjE4OTA1OWY4OTAzODkxYTg2OWI2NTFlNWEwY2EwOA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 55,
              "hls_url": "https://v.redd.it/fculp27z11ff1/HLSPlaylist.m3u8?a=1756211237%2CNTViODIzMzczMDgzMWFlZTI2Njc3YmNlYWU3MWJlNDk5NWFhYjQ1ZmUwYmNhYzM5MDUyNThhYzI0MTMwNzc1Yg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Or do both of them work great with all mcp servers? I have only really used mcp with claude desktop, and I especially like the knowledge graph memory server",
          "author_fullname": "t2_i5os0v0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "For MCP is LMstudio or Ollama better?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9xwo5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753547982,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Or do both of them work great with all mcp servers? I have only really used mcp with claude desktop, and I especially like the knowledge graph memory server&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9xwo5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "thebadslime",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9xwo5/for_mcp_is_lmstudio_or_ollama_better/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9xwo5/for_mcp_is_lmstudio_or_ollama_better/",
          "subreddit_subscribers": 505252,
          "created_utc": 1753547982,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}