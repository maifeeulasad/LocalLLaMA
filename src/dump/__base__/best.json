{
  "kind": "Listing",
  "data": {
    "after": "t3_1mae4yz",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "its getting comical",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 136,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbvf2z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 206,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 206,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/aArydVtwEJ7yR_8IVkCHCK5ydQGsUUwRNjJX3SBpIk4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753744170,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/txsukljc5pff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/txsukljc5pff1.png?auto=webp&amp;s=07d6d7cad1797c689e38509b4184dc26106493ee",
                  "width": 373,
                  "height": 365
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/txsukljc5pff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=66753ef377dde5550d636917de9e12b2834fb31c",
                    "width": 108,
                    "height": 105
                  },
                  {
                    "url": "https://preview.redd.it/txsukljc5pff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f3a44fe047ec31803031afef6a49f18f7985d89d",
                    "width": 216,
                    "height": 211
                  },
                  {
                    "url": "https://preview.redd.it/txsukljc5pff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=400b5b6efa830b5698a57bf456c6a99acd74b24d",
                    "width": 320,
                    "height": 313
                  }
                ],
                "variants": {},
                "id": "xShm2r7nwbpzJdxhn7AN663aC50Z0tC9c3BxqruE-VA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mbvf2z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbvf2z/its_getting_comical/",
          "stickied": false,
          "url": "https://i.redd.it/txsukljc5pff1.png",
          "subreddit_subscribers": 506190,
          "created_utc": 1753744170,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Today, we introduce two new GLM family members: GLM-4.5 and GLM-4.5-Air — our latest flagship models. GLM-4.5 is built with 355 billion total parameters and 32 billion active parameters, and GLM-4.5-Air with 106 billion total parameters and 12 billion active parameters. Both are designed to unify reasoning, coding, and agentic capabilities into a single model in order to satisfy more and more complicated requirements of fast rising agentic applications.\n\nBoth GLM-4.5 and GLM-4.5-Air are hybrid reasoning models, offering: thinking mode for complex reasoning and tool using, and non-thinking mode for instant responses. They are available on Z.ai, BigModel.cn and open-weights are avaiable at HuggingFace and ModelScope.\n\nBlog post: https://z.ai/blog/glm-4.5\n\nHugging Face:\n\nhttps://huggingface.co/zai-org/GLM-4.5\n\nhttps://huggingface.co/zai-org/GLM-4.5-Air\n\n",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "GLM4.5 released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 49,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8vj06dj29mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 65,
                  "x": 108,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a349d48e9cd6992d12bf44790c6309160113f79e"
                },
                {
                  "y": 131,
                  "x": 216,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=73752918b2bc0fff299a174f29082b613fabbdf4"
                },
                {
                  "y": 194,
                  "x": 320,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f5729512dc28e8eb1a9fe30b26bf27ca9ea7b250"
                },
                {
                  "y": 388,
                  "x": 640,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2603f38cd5baccb7e6ce503c3d75c02cf593ff2e"
                },
                {
                  "y": 583,
                  "x": 960,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c6c0169470ea43f39512ced287f445f51572204f"
                },
                {
                  "y": 656,
                  "x": 1080,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7943df8456b9ef19de5d523579883749691f9136"
                }
              ],
              "s": {
                "y": 2184,
                "x": 3595,
                "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=3595&amp;format=pjpg&amp;auto=webp&amp;s=617c22698deba6f1ec84e912a6152e0bf8cc2c43"
              },
              "id": "8vj06dj29mff1"
            },
            "sic55dj29mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 74,
                  "x": 108,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5066e5731ecfa48a00570e23df8edde90b106d78"
                },
                {
                  "y": 148,
                  "x": 216,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=425338994f38f791143123c3ccd0bd6dff1fffa0"
                },
                {
                  "y": 219,
                  "x": 320,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b63d99537141327e562a8b7b9e2a19c76bd5bb0e"
                },
                {
                  "y": 439,
                  "x": 640,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb0620fa3958277ea8ded0e5030d944031bc4c1f"
                },
                {
                  "y": 659,
                  "x": 960,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8e90d0f6303b9ed87b916437e6af38bd5157d1fe"
                },
                {
                  "y": 741,
                  "x": 1080,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4614dc15650da1554832d1053756737fceaf61b6"
                }
              ],
              "s": {
                "y": 3066,
                "x": 4464,
                "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=4464&amp;format=pjpg&amp;auto=webp&amp;s=062a25fee3fd1a05602c971ac17fe32ddb42908f"
              },
              "id": "sic55dj29mff1"
            },
            "zxji6dj29mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f217a1f52fcc37d5a771756cef275c15abefaa6"
                },
                {
                  "y": 95,
                  "x": 216,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=051f04a39bc5876b3d17431f6b7e5c3f0a9c9d15"
                },
                {
                  "y": 141,
                  "x": 320,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f23c3daddfda3df2605c7695d19f4cb6c84cd893"
                },
                {
                  "y": 282,
                  "x": 640,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=20ae7180cf446493c58d48b15ed48b09ea21662b"
                },
                {
                  "y": 423,
                  "x": 960,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fbdc33ba0f35c23357fe05c1defe48204da395bd"
                },
                {
                  "y": 476,
                  "x": 1080,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fb0f5b5d1d9fa1d9f0c1fae05ae3893017e31aa3"
                }
              ],
              "s": {
                "y": 1751,
                "x": 3967,
                "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=3967&amp;format=pjpg&amp;auto=webp&amp;s=b54eef388d38a731b31e7d321eb74d970359f078"
              },
              "id": "zxji6dj29mff1"
            },
            "so54saj29mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 40,
                  "x": 108,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa12f06ea18792df6395a36df2034a07e9fb9c1b"
                },
                {
                  "y": 80,
                  "x": 216,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=21276716941d44fe8837a0733b8a2b7ada3d83bb"
                },
                {
                  "y": 119,
                  "x": 320,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=176736d03fb987bf32971415a15bcc407f8e86ca"
                },
                {
                  "y": 238,
                  "x": 640,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=59a4cef468482a7827855c8b1419a3416114f00e"
                },
                {
                  "y": 358,
                  "x": 960,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=047f8fba92553d37a18d1127b773ec4b7252e7bb"
                },
                {
                  "y": 402,
                  "x": 1080,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6f5cda44f1133c4c9ada7f35e5c6b95d0bafd603"
                }
              ],
              "s": {
                "y": 1480,
                "x": 3967,
                "u": "https://preview.redd.it/so54saj29mff1.jpg?width=3967&amp;format=pjpg&amp;auto=webp&amp;s=9c5d62f989f08c491a09d379cff3146b4f6fe82e"
              },
              "id": "so54saj29mff1"
            },
            "si9mcbj29mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 38,
                  "x": 108,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=697ca4a04df9e477ad9098280eacd7fec6f4900a"
                },
                {
                  "y": 76,
                  "x": 216,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e493e44281d418507c54bdeedfacd801175f7756"
                },
                {
                  "y": 112,
                  "x": 320,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0ca18380db9f007c16cf33a6bf26a43a547c12d"
                },
                {
                  "y": 225,
                  "x": 640,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2aa202550f1ef5459d04687418a15de4bb01273a"
                },
                {
                  "y": 338,
                  "x": 960,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=450547d9b2bfd20ca0863f123da3268d32b05be2"
                },
                {
                  "y": 380,
                  "x": 1080,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c877bf36109aedf39163aaeac55cdaa55ce5831c"
                }
              ],
              "s": {
                "y": 1397,
                "x": 3967,
                "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=3967&amp;format=pjpg&amp;auto=webp&amp;s=33148e1e31a9f6d83cd1d58997d574a05eed2453"
              },
              "id": "si9mcbj29mff1"
            }
          },
          "name": "t3_1mbg1ck",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 753,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "si9mcbj29mff1",
                "id": 715840784
              },
              {
                "media_id": "sic55dj29mff1",
                "id": 715840785
              },
              {
                "media_id": "so54saj29mff1",
                "id": 715840786
              },
              {
                "media_id": "8vj06dj29mff1",
                "id": 715840787
              },
              {
                "media_id": "zxji6dj29mff1",
                "id": 715840788
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 753,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/h1a9hbYRlufo6ZLB7b1IgSekwr0g4qcrXjR2rdPGMPU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753708945,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today, we introduce two new GLM family members: GLM-4.5 and GLM-4.5-Air — our latest flagship models. GLM-4.5 is built with 355 billion total parameters and 32 billion active parameters, and GLM-4.5-Air with 106 billion total parameters and 12 billion active parameters. Both are designed to unify reasoning, coding, and agentic capabilities into a single model in order to satisfy more and more complicated requirements of fast rising agentic applications.&lt;/p&gt;\n\n&lt;p&gt;Both GLM-4.5 and GLM-4.5-Air are hybrid reasoning models, offering: thinking mode for complex reasoning and tool using, and non-thinking mode for instant responses. They are available on Z.ai, BigModel.cn and open-weights are avaiable at HuggingFace and ModelScope.&lt;/p&gt;\n\n&lt;p&gt;Blog post: &lt;a href=\"https://z.ai/blog/glm-4.5\"&gt;https://z.ai/blog/glm-4.5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hugging Face:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5\"&gt;https://huggingface.co/zai-org/GLM-4.5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5-Air\"&gt;https://huggingface.co/zai-org/GLM-4.5-Air&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mbg1ck",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mbg1ck",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 194,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbg1ck/glm45_released/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mbg1ck",
          "subreddit_subscribers": 506190,
          "created_utc": 1753708945,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_4ou3rslj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wan 2.2 is Live! Needs only 8GB of VRAM!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbfa3y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 456,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 456,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/6wb7Yp5vFjJtIwYWiSu02kTzdKI2obJq-EU5BTqMluI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753706991,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/w2tqvij93mff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?auto=webp&amp;s=ee8cf1cb47816005e468b585d65be4de071b650f",
                  "width": 1319,
                  "height": 742
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9031e98c6b58f202a2505062878cd736f6658e48",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=784f151b8ee95ef486eb0b1a1e3bfd596879c0da",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f13b863800706917bf97e7c24c56acbf283df8fb",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9aa487bb7dc2bff5b7326e25dfec4967cd6c8e51",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c6d55de941bc4cf7f377686f9f3cd96fecc135c0",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7b2119ea624eb2d2e46581f52916eefe02b8e10a",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "HCLdmR0umnU9RikapDseAAP7EInXhkRnH1_er5o1Ohc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mbfa3y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Comed_Ai_n",
          "discussion_type": null,
          "num_comments": 51,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbfa3y/wan_22_is_live_needs_only_8gb_of_vram/",
          "stickied": false,
          "url": "https://i.redd.it/w2tqvij93mff1.jpeg",
          "subreddit_subscribers": 506190,
          "created_utc": 1753706991,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Open-weight ASR models have gotten super competitive with proprietary providers (eg deepgram, assemblyai) in recent months. On some leaderboards like [HuggingFace's ASR leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard) they're posting up crazy WER and RTFx numbers. Parakeet in particular claims to process 3000+ minutes of audio in less than a minute, which means you can save a lot of money if you self-host.\n\n  \nWe at Modal benchmarked cost, throughput, and accuracy of the latest ASR models against a popular proprietary model: https://modal.com/blog/fast-cheap-batch-transcription. We also wrote up a bunch of engineering tips on how to best optimize a batch transcription service for max throughput. If you're currently using either open source or proprietary ASR models would love to know what you think!\n\n",
          "author_fullname": "t2_9av3t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "100x faster and 100x cheaper transcription with open models vs proprietary",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbny6o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 97,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 97,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753726776,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Open-weight ASR models have gotten super competitive with proprietary providers (eg deepgram, assemblyai) in recent months. On some leaderboards like &lt;a href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\"&gt;HuggingFace&amp;#39;s ASR leaderboard&lt;/a&gt; they&amp;#39;re posting up crazy WER and RTFx numbers. Parakeet in particular claims to process 3000+ minutes of audio in less than a minute, which means you can save a lot of money if you self-host.&lt;/p&gt;\n\n&lt;p&gt;We at Modal benchmarked cost, throughput, and accuracy of the latest ASR models against a popular proprietary model: &lt;a href=\"https://modal.com/blog/fast-cheap-batch-transcription\"&gt;https://modal.com/blog/fast-cheap-batch-transcription&lt;/a&gt;. We also wrote up a bunch of engineering tips on how to best optimize a batch transcription service for max throughput. If you&amp;#39;re currently using either open source or proprietary ASR models would love to know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?auto=webp&amp;s=5d50101d8f829bae3e80210dd24c9cec4945b73a",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e7b3ca3434ee071ef54d6732c5c74bfa108f1d0",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=67d797d2b0027d437608e2b7f05400e7d13174be",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6a861401db89b005f80687bfd9b892a15fbfaa93",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f044c5ce6e1272f48454e18fe9e5da33997bf960",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=796a87cb7f77e71e6cbed4cde2c3f280d6c48829",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=92ac60775c9064c7c4267f0102f80e834c10948b",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbny6o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "crookedstairs",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbny6o/100x_faster_and_100x_cheaper_transcription_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbny6o/100x_faster_and_100x_cheaper_transcription_with/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753726776,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b](https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b)",
          "author_fullname": "t2_xg2jtdg74",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 Collection Now Live!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbflsw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 228,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 228,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753707839,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b\"&gt;https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?auto=webp&amp;s=4382366bed3b06059a94a49d966d93a9236b7a98",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a31551ac98ba7f2b19f7ec16981d1a1763e134ef",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0db67012d81a693c8647c82f43c8b49497911fbe",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0bdb65a214ce9b47a250ec8fd0335a4bae79ed23",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e0d667061b43784ade998aa9bcb59c484890e6b",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=77effdfb888fe465cc41c0002dec4d947eedba40",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=14d4d3d271315409cff261db962ac60e2516a428",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mbflsw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lowkey_LokiSN",
          "discussion_type": null,
          "num_comments": 49,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbflsw/glm_45_collection_now_live/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbflsw/glm_45_collection_now_live/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753707839,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Now I got A LOT of messages when I first showed it off so I decided to spend some time to put together a full video on the high level designs behind it and also why I did it in the first place - [https://www.youtube.com/watch?v=bE2kRmXMF0I](https://www.youtube.com/watch?v=bE2kRmXMF0I)\n\nI’ve also open sourced my short / long term memory designs, vocal daisy chaining and also my docker compose stack. This should help let a lot of people get up and running! [https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM/tree/main](https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM/tree/main)\n\n",
          "author_fullname": "t2_5hq9z0rq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "So you all loved my open-source voice AI when I first showed it off - I officially got response times to under 2 seconds AND it now fits all within 9 gigs of VRAM! Open Source Code included!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbt030",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 48,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/qvwxsxvrnoff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/qvwxsxvrnoff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/qvwxsxvrnoff1/DASHPlaylist.mpd?a=1756344000%2CNmFjMjBjYjBmOGJkMzY1YmMzZTdlMGFhN2MxNjYxYWY2OWQ5ZDFkOTFkNjJhZGFiN2M2NmM5OTVkM2I1Njg3YQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 133,
              "hls_url": "https://v.redd.it/qvwxsxvrnoff1/HLSPlaylist.m3u8?a=1756344000%2CNjI4YzE2MDkzMDNiNzkwZmYxN2I4YmNmMWQ4YzE4NWRjYTlmMDQ3MzFkNmViMWY5MTRjZWM3NzMzZmNiZjY3NA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 48,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=458324ebc27e4d222e12db9105ee63a57169ea8a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753738197,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now I got A LOT of messages when I first showed it off so I decided to spend some time to put together a full video on the high level designs behind it and also why I did it in the first place - &lt;a href=\"https://www.youtube.com/watch?v=bE2kRmXMF0I\"&gt;https://www.youtube.com/watch?v=bE2kRmXMF0I&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I’ve also open sourced my short / long term memory designs, vocal daisy chaining and also my docker compose stack. This should help let a lot of people get up and running! &lt;a href=\"https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM/tree/main\"&gt;https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM/tree/main&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/qvwxsxvrnoff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?format=pjpg&amp;auto=webp&amp;s=a704ca2dfbd867dab765a160c801daae8721c588",
                  "width": 3840,
                  "height": 2160
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0e7711e20c3668e7de723d1329e83672e0f85a8d",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fccc9e037a7b68306e5750c6de88d439e6ebf2fc",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fb41192fef7650feca72355d425bc4a2d7a4cf4f",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e19a054f961b9571bcd9facb6eebd636cabc95aa",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6724a63ca00efb55331f5660e90d3c36d5b079fb",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b936a3ab0d20f9dcc4a703e84f989d9fce27b4ae",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbt030",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RoyalCities",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbt030/so_you_all_loved_my_opensource_voice_ai_when_i/",
          "stickied": false,
          "url": "https://v.redd.it/qvwxsxvrnoff1",
          "subreddit_subscribers": 506190,
          "created_utc": 1753738197,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/qvwxsxvrnoff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/qvwxsxvrnoff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/qvwxsxvrnoff1/DASHPlaylist.mpd?a=1756344000%2CNmFjMjBjYjBmOGJkMzY1YmMzZTdlMGFhN2MxNjYxYWY2OWQ5ZDFkOTFkNjJhZGFiN2M2NmM5OTVkM2I1Njg3YQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 133,
              "hls_url": "https://v.redd.it/qvwxsxvrnoff1/HLSPlaylist.m3u8?a=1756344000%2CNjI4YzE2MDkzMDNiNzkwZmYxN2I4YmNmMWQ4YzE4NWRjYTlmMDQ3MzFkNmViMWY5MTRjZWM3NzMzZmNiZjY3NA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "No model card as of yet",
          "author_fullname": "t2_12aeph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mb9uy8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 506,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 506,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=50aa20219586bc9007fb96833d16a6a56c8c1c76",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753688022,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;No model card as of yet&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?auto=webp&amp;s=f1df54937600c0db76989bd14eef9e747df1fb0e",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1c3476d621a9393fbb7ca11c48a3074c5fd6803",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7cef70bde41dd3225eec3f7d265fbf2704c0182",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab3e2615c90a6581b60c6d33c660bfc0f250b4c8",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c9c2fc1f960e47499df06dc08d78c88be43e15e",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15eba021f7d99140c48583ae883d2eb091807f16",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mb9uy8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rerri",
          "discussion_type": null,
          "num_comments": 90,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb9uy8/qwenqwen330ba3binstruct2507_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507",
          "subreddit_subscribers": 506190,
          "created_utc": 1753688022,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nGot an interesting email from Anthropic today. Looks like they're adding new weekly usage limits for their paid Claude subscribers (Pro and Max), on top of the existing 5-hour limits.\n\nThe email mentions it's a way to handle policy violations and \"advanced usage patterns,\" like running Claude 24/7. They estimate the new weekly cap for their top \"Max\" tier will be around 24-40 hours of Opus 4 usage before you have to pay standard API rates.\n\nThis definitely got me thinking about the pros and cons of relying on commercial platforms. The power of models like Opus is undeniable, but this is also a reminder that the terms can change, which can be a challenge for anyone with a consistent, long-term workflow.\n\nIt really highlights some of the inherent strengths of the local approach we have here:\n\n* **Stability:** Your workflow is insulated from sudden policy changes.\n* **Freedom:** You have the freedom to run intensive or long-running tasks without hitting a usage cap.\n* **Predictability:** The only real limits are your own hardware and time.\n\nI'm curious to hear how the community sees this.\n\n* Does this kind of change make you lean more heavily into your local setup?\n* For those who use a mix of tools, how do you decide when an API is worth it versus firing up a local model?\n* And on a technical note, how close do you feel the top open-source models are to replacing something like Opus for your specific use cases (coding, writing, etc.)?\n\nLooking forward to the discussion.",
          "author_fullname": "t2_1ahyw3obor",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The walled garden gets higher walls: Anthropic is adding weekly rate limits for paid Claude subscribers",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbp4nm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 59,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 59,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753729378,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;Got an interesting email from Anthropic today. Looks like they&amp;#39;re adding new weekly usage limits for their paid Claude subscribers (Pro and Max), on top of the existing 5-hour limits.&lt;/p&gt;\n\n&lt;p&gt;The email mentions it&amp;#39;s a way to handle policy violations and &amp;quot;advanced usage patterns,&amp;quot; like running Claude 24/7. They estimate the new weekly cap for their top &amp;quot;Max&amp;quot; tier will be around 24-40 hours of Opus 4 usage before you have to pay standard API rates.&lt;/p&gt;\n\n&lt;p&gt;This definitely got me thinking about the pros and cons of relying on commercial platforms. The power of models like Opus is undeniable, but this is also a reminder that the terms can change, which can be a challenge for anyone with a consistent, long-term workflow.&lt;/p&gt;\n\n&lt;p&gt;It really highlights some of the inherent strengths of the local approach we have here:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Stability:&lt;/strong&gt; Your workflow is insulated from sudden policy changes.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Freedom:&lt;/strong&gt; You have the freedom to run intensive or long-running tasks without hitting a usage cap.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Predictability:&lt;/strong&gt; The only real limits are your own hardware and time.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m curious to hear how the community sees this.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Does this kind of change make you lean more heavily into your local setup?&lt;/li&gt;\n&lt;li&gt;For those who use a mix of tools, how do you decide when an API is worth it versus firing up a local model?&lt;/li&gt;\n&lt;li&gt;And on a technical note, how close do you feel the top open-source models are to replacing something like Opus for your specific use cases (coding, writing, etc.)?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Looking forward to the discussion.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbp4nm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Resident_Egg5765",
          "discussion_type": null,
          "num_comments": 39,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbp4nm/the_walled_garden_gets_higher_walls_anthropic_is/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbp4nm/the_walled_garden_gets_higher_walls_anthropic_is/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753729378,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_on5es7pe3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM shattered the record for \"worst benchmark JPEG ever published\" - wow.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbihcz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": "#bbbdbf",
          "ups": 110,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 110,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/gzZdsczxENjO9zmvRGuSBtlizLvTVS25LiHLWAxMHcU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753714742,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/5gs5tl2vpmff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?auto=webp&amp;s=79c777573796e4d584b8ab8e2c35af5ba8e4aed4",
                  "width": 1280,
                  "height": 777
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e5b53962e6d1ac82f1b8273c0d41541e04e7879e",
                    "width": 108,
                    "height": 65
                  },
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=384d40702cb33fae05e9b7e2417491f15d2e13f0",
                    "width": 216,
                    "height": 131
                  },
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4a8a83270078516f4647b641c4d7693caa28edf9",
                    "width": 320,
                    "height": 194
                  },
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ba8857bb5cf2336d48720fb4df5c2b74feec965",
                    "width": 640,
                    "height": 388
                  },
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fae2095a12e3e90e243016fc403c5e4759216dd8",
                    "width": 960,
                    "height": 582
                  },
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=337a16054a4ce65c5072c55aa5dbe2e651b7bd04",
                    "width": 1080,
                    "height": 655
                  }
                ],
                "variants": {},
                "id": "A-HOpqSXNI8uq09i3lC4hqYCVYx350wRT1S36XUBTO0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mbihcz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ForsookComparison",
          "discussion_type": null,
          "num_comments": 76,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mbihcz/glm_shattered_the_record_for_worst_benchmark_jpeg/",
          "stickied": false,
          "url": "https://i.redd.it/5gs5tl2vpmff1.jpeg",
          "subreddit_subscribers": 506190,
          "created_utc": 1753714742,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I couldn't find any extensive benchmarks when researching this APU, so I'm sharing my findings with the community.\n\nThe benchmarks with the iGPU 760M results \\~35% faster than the CPU alone (see the tests below, with ngl 0, no layers offloaded to the GPU), the prompt processing is also faster, and it appears to produce less heat.\n\nIt allows me to chat with Gemma 3 27B at \\~5 tokens per second (t/s), and Qwen 3 30B-A3B works at around 35 t/s.\n\nSo it's not a 3090, a Mac, or a Strix Halo, obviously, but gives access to these models without being power-hungry, expensive, and it's widely available.\n\nAnother thing I was looking for was how it compared to my Steam Deck. Apparently, with LLMs, the 8600G is about twice as fast.\n\nNote 1: if you have in mind a gaming PC, unless you just want a small machine with only the APU, a regular 7600 or 9600 has more cache, PCIe lanes, and PCIe 5 support. However, the 8600G is still faster at 1080p with games than the Steam Deck at 800p. So, well, it's usable for light gaming and doesn't consume too much power, but it's not the best choice for a gaming PC.\n\nNote 2: there are mini-PCs with similar AMD APUs; however, if you have enough space, a desktop case offers better cooling and is probably quieter. Plus, if you want to add a GPU, mini-PCs require complex and costly eGPU setups (when the option is available), while with a desktop PC it's straightforward (even though the 8600G is lane-limited, so still not the ideal).\n\nNote 3: the 8700G comes with a better cooler (though still mediocre), a slightly better iGPU (but only about 10% faster in games, and the difference for LLMs is likely negligible), and two extra cores; however, it's definitively more expensive.\n\n=== Setup and notes ===\n\n    OS: Kubuntu 24.04\n    RAM: 64GB DDR5-6000\n    IOMMU: disabled\n\nApparently, **IOMMU** slows it down noticeably:\n\n    Gemma 3 4B   pp512 tg12\n    IOMMU off =  ~395  32.70\n    IOMMU on  =  ~360  29.6\n\nHence, the following benchmarks are with IOMMU disabled.\n\nThe 8600G default is 65W, but **at 35W it loses very little performance**:\n\n    Gemma 3 4B  pp512  tg12\n     65W  =     ~395  32.70\n     35W  =     ~372  31.86\n\nAlso the stock fan seems better suited for the APU set at 35W. At 65W it could still barely handle the CPU-only Gemma3-12B benchmark (at least in my airflow case), but it thermal-throttles with larger models.\n\nAnyway, for consistency, the following tests are at 65W and I limited the CPU-only tests to the smaller models.\n\nBenchmarks:\n\n    llama.cpp build: 01612b74 (5922)\n    ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1103_R1) (radv) | uma: 1 | fp16: 1 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n    \n    backend: RPC, Vulcan\n    \n    === Gemma 3 q4_0_QAT (by stduhpf)\n    | model                          |      size |  params | ngl |  test |           t/s\n    | ------------------------------ | --------: | ------: | --: | ----: | ------------:\n    (4B, iGPU 760M)\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp128 | 378.02 ± 1.44\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp256 | 396.18 ± 1.88\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp512 | 395.16 ± 1.79\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | tg128 |  32.70 ± 0.04\n    (4B, CPU)\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |   0 | pp512 | 313.53 ± 2.00\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |   0 | tg128 |  24.09 ± 0.02\n    (12B, iGPU 760M)\n    | gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |  99 | pp512 | 121.56 ± 0.18\n    | gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |  99 | tg128 |  11.45 ± 0.03\n    (12B, CPU)\n    | gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |   0 | pp512 |  98.25 ± 0.52\n    | gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |   0 | tg128 |   8.39 ± 0.01\n    (27B, iGPU 760M)\n    | gemma3 27B Q4_0                | 14.49 GiB | 27.01 B |  99 | pp512 |  52.22 ± 0.01\n    | gemma3 27B Q4_0                | 14.49 GiB | 27.01 B |  99 | tg128 |   5.37 ± 0.01\n    \n    === Mistral Small (24B) 3.2 2506 (UD-Q4_K_XL by unsloth)\n    | model                          |       size |   params |  test |            t/s\n    | ------------------------------ | ---------: | -------: | ----: | -------------:\n    | llama 13B Q4_K - Medium        |  13.50 GiB |  23.57 B | pp512 |   52.49 ± 0.04\n    | llama 13B Q4_K - Medium        |  13.50 GiB |  23.57 B | tg128 |    5.90 ± 0.00\n      [oddly, it's identified as \"llama 13B\"]\n    \n    === Qwen 3\n    | model                          |       size |   params |  test |            t/s\n    | ------------------------------ | ---------: | -------: | ----: | -------------:\n    (4B Q4_K_L by Bartowski)\n    | qwen3 4B Q4_K - Medium         |   2.41 GiB |   4.02 B | pp512 |  299.86 ± 0.44\n    | qwen3 4B Q4_K - Medium         |   2.41 GiB |   4.02 B | tg128 |   29.91 ± 0.03\n    (8B Q4 Q4_K_M by unsloth)\n    | qwen3 8B Q4_K - Medium         |   4.68 GiB |   8.19 B | pp512 |  165.73 ± 0.13\n    | qwen3 8B Q4_K - Medium         |   4.68 GiB |   8.19 B | tg128 |   17.75 ± 0.01\n      [Note: UD-Q4_K_XL by unsloth is only slightly slower with pp512 164.68 ± 0.20, tg128 16.84 ± 0.01]\n    (8B Q6 UD-Q6_K_XL by unsloth)\n    | qwen3 8B Q6_K                  |   6.97 GiB |   8.19 B | pp512 |  167.45 ± 0.14\n    | qwen3 8B Q6_K                  |   6.97 GiB |   8.19 B | tg128 |   12.45 ± 0.00\n    (8B Q8_0 by unsloth)\n    | qwen3 8B Q8_0                  |   8.11 GiB |   8.19 B | pp512 |  177.91 ± 0.13\n    | qwen3 8B Q8_0                  |   8.11 GiB |   8.19 B | tg128 |   10.66 ± 0.00\n    (14B UD-Q4_K_XL by unsloth)\n    | qwen3 14B Q4_K - Medium        |   8.53 GiB |  14.77 B | pp512 |   87.37 ± 0.14\n    | qwen3 14B Q4_K - Medium        |   8.53 GiB |  14.77 B | tg128 |    9.39 ± 0.01\n    (32B Q4_K_L by Bartowski)\n    | qwen3 32B Q4_K - Medium        |  18.94 GiB |  32.76 B | pp512 |   36.64 ± 0.02\n    | qwen3 32B Q4_K - Medium        |  18.94 GiB |  32.76 B | tg128 |    4.36 ± 0.00\n    \n    === Qwen 3 30B-A3B MoE (UD-Q4_K_XL by unsloth)\n    | model                          |       size |   params |  test |            t/s\n    | ------------------------------ | ---------: | -------: | ----: | -------------:\n    | qwen3moe 30B.A3B Q4_K - Medium |  16.49 GiB |  30.53 B | pp512 |   83.43 ± 0.35\n    | qwen3moe 30B.A3B Q4_K - Medium |  16.49 GiB |  30.53 B | tg128 |   34.77 ± 0.27",
          "author_fullname": "t2_x2g8r3neo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "8600G / 760M llama-bench with Gemma 3 (4, 12, 27B), Mistral Small, Qwen 3 (4, 8, 14, 32B) and  Qwen 3 MoE 30B-A3B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbs4dw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 36,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 36,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753736142,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I couldn&amp;#39;t find any extensive benchmarks when researching this APU, so I&amp;#39;m sharing my findings with the community.&lt;/p&gt;\n\n&lt;p&gt;The benchmarks with the iGPU 760M results ~35% faster than the CPU alone (see the tests below, with ngl 0, no layers offloaded to the GPU), the prompt processing is also faster, and it appears to produce less heat.&lt;/p&gt;\n\n&lt;p&gt;It allows me to chat with Gemma 3 27B at ~5 tokens per second (t/s), and Qwen 3 30B-A3B works at around 35 t/s.&lt;/p&gt;\n\n&lt;p&gt;So it&amp;#39;s not a 3090, a Mac, or a Strix Halo, obviously, but gives access to these models without being power-hungry, expensive, and it&amp;#39;s widely available.&lt;/p&gt;\n\n&lt;p&gt;Another thing I was looking for was how it compared to my Steam Deck. Apparently, with LLMs, the 8600G is about twice as fast.&lt;/p&gt;\n\n&lt;p&gt;Note 1: if you have in mind a gaming PC, unless you just want a small machine with only the APU, a regular 7600 or 9600 has more cache, PCIe lanes, and PCIe 5 support. However, the 8600G is still faster at 1080p with games than the Steam Deck at 800p. So, well, it&amp;#39;s usable for light gaming and doesn&amp;#39;t consume too much power, but it&amp;#39;s not the best choice for a gaming PC.&lt;/p&gt;\n\n&lt;p&gt;Note 2: there are mini-PCs with similar AMD APUs; however, if you have enough space, a desktop case offers better cooling and is probably quieter. Plus, if you want to add a GPU, mini-PCs require complex and costly eGPU setups (when the option is available), while with a desktop PC it&amp;#39;s straightforward (even though the 8600G is lane-limited, so still not the ideal).&lt;/p&gt;\n\n&lt;p&gt;Note 3: the 8700G comes with a better cooler (though still mediocre), a slightly better iGPU (but only about 10% faster in games, and the difference for LLMs is likely negligible), and two extra cores; however, it&amp;#39;s definitively more expensive.&lt;/p&gt;\n\n&lt;p&gt;=== Setup and notes ===&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;OS: Kubuntu 24.04\nRAM: 64GB DDR5-6000\nIOMMU: disabled\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Apparently, &lt;strong&gt;IOMMU&lt;/strong&gt; slows it down noticeably:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Gemma 3 4B   pp512 tg12\nIOMMU off =  ~395  32.70\nIOMMU on  =  ~360  29.6\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Hence, the following benchmarks are with IOMMU disabled.&lt;/p&gt;\n\n&lt;p&gt;The 8600G default is 65W, but &lt;strong&gt;at 35W it loses very little performance&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Gemma 3 4B  pp512  tg12\n 65W  =     ~395  32.70\n 35W  =     ~372  31.86\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Also the stock fan seems better suited for the APU set at 35W. At 65W it could still barely handle the CPU-only Gemma3-12B benchmark (at least in my airflow case), but it thermal-throttles with larger models.&lt;/p&gt;\n\n&lt;p&gt;Anyway, for consistency, the following tests are at 65W and I limited the CPU-only tests to the smaller models.&lt;/p&gt;\n\n&lt;p&gt;Benchmarks:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama.cpp build: 01612b74 (5922)\nggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1103_R1) (radv) | uma: 1 | fp16: 1 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n\nbackend: RPC, Vulcan\n\n=== Gemma 3 q4_0_QAT (by stduhpf)\n| model                          |      size |  params | ngl |  test |           t/s\n| ------------------------------ | --------: | ------: | --: | ----: | ------------:\n(4B, iGPU 760M)\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp128 | 378.02 ± 1.44\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp256 | 396.18 ± 1.88\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp512 | 395.16 ± 1.79\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | tg128 |  32.70 ± 0.04\n(4B, CPU)\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |   0 | pp512 | 313.53 ± 2.00\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |   0 | tg128 |  24.09 ± 0.02\n(12B, iGPU 760M)\n| gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |  99 | pp512 | 121.56 ± 0.18\n| gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |  99 | tg128 |  11.45 ± 0.03\n(12B, CPU)\n| gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |   0 | pp512 |  98.25 ± 0.52\n| gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |   0 | tg128 |   8.39 ± 0.01\n(27B, iGPU 760M)\n| gemma3 27B Q4_0                | 14.49 GiB | 27.01 B |  99 | pp512 |  52.22 ± 0.01\n| gemma3 27B Q4_0                | 14.49 GiB | 27.01 B |  99 | tg128 |   5.37 ± 0.01\n\n=== Mistral Small (24B) 3.2 2506 (UD-Q4_K_XL by unsloth)\n| model                          |       size |   params |  test |            t/s\n| ------------------------------ | ---------: | -------: | ----: | -------------:\n| llama 13B Q4_K - Medium        |  13.50 GiB |  23.57 B | pp512 |   52.49 ± 0.04\n| llama 13B Q4_K - Medium        |  13.50 GiB |  23.57 B | tg128 |    5.90 ± 0.00\n  [oddly, it&amp;#39;s identified as &amp;quot;llama 13B&amp;quot;]\n\n=== Qwen 3\n| model                          |       size |   params |  test |            t/s\n| ------------------------------ | ---------: | -------: | ----: | -------------:\n(4B Q4_K_L by Bartowski)\n| qwen3 4B Q4_K - Medium         |   2.41 GiB |   4.02 B | pp512 |  299.86 ± 0.44\n| qwen3 4B Q4_K - Medium         |   2.41 GiB |   4.02 B | tg128 |   29.91 ± 0.03\n(8B Q4 Q4_K_M by unsloth)\n| qwen3 8B Q4_K - Medium         |   4.68 GiB |   8.19 B | pp512 |  165.73 ± 0.13\n| qwen3 8B Q4_K - Medium         |   4.68 GiB |   8.19 B | tg128 |   17.75 ± 0.01\n  [Note: UD-Q4_K_XL by unsloth is only slightly slower with pp512 164.68 ± 0.20, tg128 16.84 ± 0.01]\n(8B Q6 UD-Q6_K_XL by unsloth)\n| qwen3 8B Q6_K                  |   6.97 GiB |   8.19 B | pp512 |  167.45 ± 0.14\n| qwen3 8B Q6_K                  |   6.97 GiB |   8.19 B | tg128 |   12.45 ± 0.00\n(8B Q8_0 by unsloth)\n| qwen3 8B Q8_0                  |   8.11 GiB |   8.19 B | pp512 |  177.91 ± 0.13\n| qwen3 8B Q8_0                  |   8.11 GiB |   8.19 B | tg128 |   10.66 ± 0.00\n(14B UD-Q4_K_XL by unsloth)\n| qwen3 14B Q4_K - Medium        |   8.53 GiB |  14.77 B | pp512 |   87.37 ± 0.14\n| qwen3 14B Q4_K - Medium        |   8.53 GiB |  14.77 B | tg128 |    9.39 ± 0.01\n(32B Q4_K_L by Bartowski)\n| qwen3 32B Q4_K - Medium        |  18.94 GiB |  32.76 B | pp512 |   36.64 ± 0.02\n| qwen3 32B Q4_K - Medium        |  18.94 GiB |  32.76 B | tg128 |    4.36 ± 0.00\n\n=== Qwen 3 30B-A3B MoE (UD-Q4_K_XL by unsloth)\n| model                          |       size |   params |  test |            t/s\n| ------------------------------ | ---------: | -------: | ----: | -------------:\n| qwen3moe 30B.A3B Q4_K - Medium |  16.49 GiB |  30.53 B | pp512 |   83.43 ± 0.35\n| qwen3moe 30B.A3B Q4_K - Medium |  16.49 GiB |  30.53 B | tg128 |   34.77 ± 0.27\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbs4dw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SunRayWhisper",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbs4dw/8600g_760m_llamabench_with_gemma_3_4_12_27b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbs4dw/8600g_760m_llamabench_with_gemma_3_4_12_27b/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753736142,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We’re proud to introduce **Wan2.2**, a major leap in open video generation, featuring a novel **Mixture-of-Experts (MoE)** diffusion architecture, high-compression HD generation, and benchmark-leading performance.\n\n# 🔍 Key Innovations\n\n# 🧠 Mixture-of-Experts (MoE) Diffusion Architecture\n\nWan2.2 integrates **two specialized 14B experts** in its 27B-parameter MoE design:\n\n* **High-noise expert** for early denoising stages — focusing on layout.\n* **Low-noise expert** for later stages — refining fine details.\n\nOnly one expert is active per step (14B params), so **inference remains efficient** despite the added capacity.\n\nThe expert transition is based on the **Signal-to-Noise Ratio (SNR)** during diffusion. As SNR drops, the model smoothly switches from the high-noise to low-noise expert at a learned threshold (`t_moe`), ensuring optimal handling of different generation phases.\n\n📈 **Visual Overview**:\n\n**Left: Expert switching based on SNR**  \n**Right: Validation loss comparison across model variants**\n\n\n\nThe final **Wan2.2 (MoE)** model shows the **lowest validation loss**, confirming better convergence and fidelity than Wan2.1 or hybrid expert configurations.\n\n# ⚡ TI2V-5B: Fast, Compressed, HD Video Generation\n\nWan2.2 also introduces **TI2V-5B**, a **5B dense model** with impressive efficiency:\n\n* Utilizes **Wan2.2-VAE** with $4\\\\times16\\\\times16$ spatial compression.\n* Achieves **$4\\\\times32\\\\times32$ total compression** with patchification.\n* Can generate **5s 720P@24fps videos in &lt;9 minutes** on a consumer GPU.\n* Natively supports **text-to-video (T2V)** and **image-to-video (I2V)** in one unified architecture.\n\nThis makes Wan2.2 not only powerful but also highly practical for real-world applications.\n\n# 🧪 Benchmarking: Wan2.2 vs Commercial SOTAs\n\nWe evaluated Wan2.2 against leading proprietary models on **Wan-Bench 2.0**, scoring across:\n\n* Aesthetics\n* Dynamic motion\n* Text rendering\n* Camera control\n* Fidelity\n* Object accuracy\n\n📊 **Benchmark Results**:\n\n\n\n🚀 **Wan2.2-T2V-A14B leads in 5/6 categories**, outperforming commercial models like KLING 2.0, Sora, and Seedance in:\n\n* **Dynamic Degree**\n* **Text Rendering**\n* **Object Accuracy**\n* And more…\n\n# 🧵 Why Wan2.2 Matters\n\n* Brings **MoE advantages** to video generation with no added inference cost.\n* Achieves **industry-leading HD generation speeds** on consumer GPUs.\n* **Openly benchmarked** with results that rival or beat closed-source giants.",
          "author_fullname": "t2_pa2ww",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wan 2.2 T2V,I2V 14B MoE Models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbefh4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 144,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 144,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=74eb40bbcaaad3e6917f58cacde7b1456925f450",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753704542,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We’re proud to introduce &lt;strong&gt;Wan2.2&lt;/strong&gt;, a major leap in open video generation, featuring a novel &lt;strong&gt;Mixture-of-Experts (MoE)&lt;/strong&gt; diffusion architecture, high-compression HD generation, and benchmark-leading performance.&lt;/p&gt;\n\n&lt;h1&gt;🔍 Key Innovations&lt;/h1&gt;\n\n&lt;h1&gt;🧠 Mixture-of-Experts (MoE) Diffusion Architecture&lt;/h1&gt;\n\n&lt;p&gt;Wan2.2 integrates &lt;strong&gt;two specialized 14B experts&lt;/strong&gt; in its 27B-parameter MoE design:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;High-noise expert&lt;/strong&gt; for early denoising stages — focusing on layout.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Low-noise expert&lt;/strong&gt; for later stages — refining fine details.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Only one expert is active per step (14B params), so &lt;strong&gt;inference remains efficient&lt;/strong&gt; despite the added capacity.&lt;/p&gt;\n\n&lt;p&gt;The expert transition is based on the &lt;strong&gt;Signal-to-Noise Ratio (SNR)&lt;/strong&gt; during diffusion. As SNR drops, the model smoothly switches from the high-noise to low-noise expert at a learned threshold (&lt;code&gt;t_moe&lt;/code&gt;), ensuring optimal handling of different generation phases.&lt;/p&gt;\n\n&lt;p&gt;📈 &lt;strong&gt;Visual Overview&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Left: Expert switching based on SNR&lt;/strong&gt;&lt;br/&gt;\n&lt;strong&gt;Right: Validation loss comparison across model variants&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The final &lt;strong&gt;Wan2.2 (MoE)&lt;/strong&gt; model shows the &lt;strong&gt;lowest validation loss&lt;/strong&gt;, confirming better convergence and fidelity than Wan2.1 or hybrid expert configurations.&lt;/p&gt;\n\n&lt;h1&gt;⚡ TI2V-5B: Fast, Compressed, HD Video Generation&lt;/h1&gt;\n\n&lt;p&gt;Wan2.2 also introduces &lt;strong&gt;TI2V-5B&lt;/strong&gt;, a &lt;strong&gt;5B dense model&lt;/strong&gt; with impressive efficiency:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Utilizes &lt;strong&gt;Wan2.2-VAE&lt;/strong&gt; with $4\\times16\\times16$ spatial compression.&lt;/li&gt;\n&lt;li&gt;Achieves &lt;strong&gt;$4\\times32\\times32$ total compression&lt;/strong&gt; with patchification.&lt;/li&gt;\n&lt;li&gt;Can generate &lt;strong&gt;5s 720P@24fps videos in &amp;lt;9 minutes&lt;/strong&gt; on a consumer GPU.&lt;/li&gt;\n&lt;li&gt;Natively supports &lt;strong&gt;text-to-video (T2V)&lt;/strong&gt; and &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; in one unified architecture.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This makes Wan2.2 not only powerful but also highly practical for real-world applications.&lt;/p&gt;\n\n&lt;h1&gt;🧪 Benchmarking: Wan2.2 vs Commercial SOTAs&lt;/h1&gt;\n\n&lt;p&gt;We evaluated Wan2.2 against leading proprietary models on &lt;strong&gt;Wan-Bench 2.0&lt;/strong&gt;, scoring across:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Aesthetics&lt;/li&gt;\n&lt;li&gt;Dynamic motion&lt;/li&gt;\n&lt;li&gt;Text rendering&lt;/li&gt;\n&lt;li&gt;Camera control&lt;/li&gt;\n&lt;li&gt;Fidelity&lt;/li&gt;\n&lt;li&gt;Object accuracy&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;📊 &lt;strong&gt;Benchmark Results&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;🚀 &lt;strong&gt;Wan2.2-T2V-A14B leads in 5/6 categories&lt;/strong&gt;, outperforming commercial models like KLING 2.0, Sora, and Seedance in:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Dynamic Degree&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Text Rendering&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Object Accuracy&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;And more…&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;🧵 Why Wan2.2 Matters&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Brings &lt;strong&gt;MoE advantages&lt;/strong&gt; to video generation with no added inference cost.&lt;/li&gt;\n&lt;li&gt;Achieves &lt;strong&gt;industry-leading HD generation speeds&lt;/strong&gt; on consumer GPUs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Openly benchmarked&lt;/strong&gt; with results that rival or beat closed-source giants.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Wan-AI",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?auto=webp&amp;s=5e2d1a9b7d7ba587c66883c59382bf9da05496ef",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c28c08e5f6ad66084018cf52177490f848610b13",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f55e3ea7af464c4462923d295c8307452d91dc8c",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b5147cc7c793b5ccb1f3c4173598b7eaa49df359",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=383cb90569524b8ee389cbf51df12c411b89660a",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=899dbe9927605247845ad6c7073df7056f48193d",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b29faba3ffcdda3d04d69e576a35738628206297",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mbefh4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "khubebk",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbefh4/wan_22_t2vi2v_14b_moe_models/",
          "stickied": false,
          "url": "https://huggingface.co/Wan-AI",
          "subreddit_subscribers": 506190,
          "created_utc": 1753704542,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I tried my hands with wan 2.2, the latest AI video generation model on nvidia GeForce rtx 4090 (cloud based), the 5B version and it took about 15 minutes for 3 videos. The quality is okish but running a video gen model on RTX 4090 is a dream come true. You can check the experiment here : https://youtu.be/trDnvLWdIx0?si=qa1WvcUytuMLoNL8",
          "author_fullname": "t2_th2ct5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tried Wan2.2 on RTX 4090, quite impressed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbm4a0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 55,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 55,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753722744,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I tried my hands with wan 2.2, the latest AI video generation model on nvidia GeForce rtx 4090 (cloud based), the 5B version and it took about 15 minutes for 3 videos. The quality is okish but running a video gen model on RTX 4090 is a dream come true. You can check the experiment here : &lt;a href=\"https://youtu.be/trDnvLWdIx0?si=qa1WvcUytuMLoNL8\"&gt;https://youtu.be/trDnvLWdIx0?si=qa1WvcUytuMLoNL8&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qxgHq5tcO75IRMOSVWOGHi36WSl-Yjltlhenn6pmMBU.jpeg?auto=webp&amp;s=ef0f6a9abaa235f6f292a719f82770b8bc35ced0",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qxgHq5tcO75IRMOSVWOGHi36WSl-Yjltlhenn6pmMBU.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e83d542243dbe1dacd4f606926016b3b31bfeb8e",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/qxgHq5tcO75IRMOSVWOGHi36WSl-Yjltlhenn6pmMBU.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c1dc83fdc91b95f7b802fbd65dde2fed70008894",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/qxgHq5tcO75IRMOSVWOGHi36WSl-Yjltlhenn6pmMBU.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c5da1f32044a0975d270c02340342ff0438789a",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "qxgHq5tcO75IRMOSVWOGHi36WSl-Yjltlhenn6pmMBU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mbm4a0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Technical-Love-8479",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbm4a0/tried_wan22_on_rtx_4090_quite_impressed/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbm4a0/tried_wan22_on_rtx_4090_quite_impressed/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753722744,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Bloomberg writes:\n\n&gt;The startup will release GLM-4.5, an update to its flagship model, as soon as Monday, according to a person familiar with the plan.\n\nThe organization has changed their name on HF from THUDM to zai-org and they have a GLM 4.5 collection which has 8 hidden items in it.\n\n[https://huggingface.co/organizations/zai-org/activity/collections](https://huggingface.co/organizations/zai-org/activity/collections)",
          "author_fullname": "t2_12aeph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 possibly releasing today according to Bloomberg",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbdm6t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 138,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 138,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?width=140&amp;height=93&amp;crop=140:93,smart&amp;auto=webp&amp;s=355e9ec2fa3e3360af59b2098c48fa105bb99e90",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753702016,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "bloomberg.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bloomberg writes:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The startup will release GLM-4.5, an update to its flagship model, as soon as Monday, according to a person familiar with the plan.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;The organization has changed their name on HF from THUDM to zai-org and they have a GLM 4.5 collection which has 8 hidden items in it.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/organizations/zai-org/activity/collections\"&gt;https://huggingface.co/organizations/zai-org/activity/collections&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.bloomberg.com/news/articles/2025-07-28/chinese-openai-challenger-zhipu-to-unveil-new-open-source-model",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?auto=webp&amp;s=e4cb5ef205d53a96b0ef79a989b300b42e222d23",
                  "width": 1200,
                  "height": 800
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=49d610e841064301ef9eed8e3e833431e3633cd1",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38dd9ae75ecfcee4e431fdab64e2056f653b1642",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5075adce2c3d58fd1f80c91982a247d8b33dbe18",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=11470efb2209e35e2be0d434f089cd6d797726ba",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3756403e4f6b50939fb2e629242331c6a052032b",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e06a30269401467c1b156345a2e6fb6856c45465",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mbdm6t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rerri",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbdm6t/glm_45_possibly_releasing_today_according_to/",
          "stickied": false,
          "url": "https://www.bloomberg.com/news/articles/2025-07-28/chinese-openai-challenger-zhipu-to-unveil-new-open-source-model",
          "subreddit_subscribers": 506190,
          "created_utc": 1753702016,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[Source](https://huggingface.co/datasets/zai-org/CC-Bench-trajectories#overall-performance)",
          "author_fullname": "t2_14mlbg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Early GLM 4.5 Benchmarks, Claiming to surpass Qwen 3 Coder",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 61,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "2sajkwcr4mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 40,
                  "x": 108,
                  "u": "https://preview.redd.it/2sajkwcr4mff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=246de2e890d026a4cb49f36bf54ec1a7bfbad60f"
                },
                {
                  "y": 80,
                  "x": 216,
                  "u": "https://preview.redd.it/2sajkwcr4mff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=484c8312e52cf3bc991e40039b4c68cd54742919"
                },
                {
                  "y": 119,
                  "x": 320,
                  "u": "https://preview.redd.it/2sajkwcr4mff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=66e2e85c4bbe4f72b46bd86d35107bc5502d97a5"
                },
                {
                  "y": 238,
                  "x": 640,
                  "u": "https://preview.redd.it/2sajkwcr4mff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1bc407e3d0a5c6807865f2418b8992e84050b408"
                },
                {
                  "y": 358,
                  "x": 960,
                  "u": "https://preview.redd.it/2sajkwcr4mff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e73fdc4ceffe980fd9495c05fe33e88870ddd34"
                },
                {
                  "y": 402,
                  "x": 1080,
                  "u": "https://preview.redd.it/2sajkwcr4mff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=29dec19e68b12db48a377fbaa06bf88cb487dfa8"
                }
              ],
              "s": {
                "y": 1480,
                "x": 3967,
                "u": "https://preview.redd.it/2sajkwcr4mff1.png?width=3967&amp;format=png&amp;auto=webp&amp;s=ff5fed0614da6788f23f4d16fbc94a2094829e44"
              },
              "id": "2sajkwcr4mff1"
            },
            "inopsfzq4mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/inopsfzq4mff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1710283c73c392a4a07ea96f13833cfb9b2d9f2c"
                },
                {
                  "y": 95,
                  "x": 216,
                  "u": "https://preview.redd.it/inopsfzq4mff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5313024316c5a628cbe6c471e0b3256a3b3ab01"
                },
                {
                  "y": 141,
                  "x": 320,
                  "u": "https://preview.redd.it/inopsfzq4mff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ca13b2c1def20c5bbdf65e64b4225ed03fe65866"
                },
                {
                  "y": 282,
                  "x": 640,
                  "u": "https://preview.redd.it/inopsfzq4mff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=760cfaf8017b920fe8e5a6e6ee7b8f3eae67ce60"
                },
                {
                  "y": 423,
                  "x": 960,
                  "u": "https://preview.redd.it/inopsfzq4mff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=60977131a03c0ace67e2795b6fffab85128487c3"
                },
                {
                  "y": 476,
                  "x": 1080,
                  "u": "https://preview.redd.it/inopsfzq4mff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=23a28c3533a2243412f2abbfd2b32fbf7bc6f051"
                }
              ],
              "s": {
                "y": 1751,
                "x": 3967,
                "u": "https://preview.redd.it/inopsfzq4mff1.png?width=3967&amp;format=png&amp;auto=webp&amp;s=fbb5848ea96c713fcb887f492adcc8efdc32df90"
              },
              "id": "inopsfzq4mff1"
            }
          },
          "name": "t3_1mbfhgp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 99,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "inopsfzq4mff1",
                "id": 715826675
              },
              {
                "media_id": "2sajkwcr4mff1",
                "id": 715826676
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 99,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/d8z0Eqv5ElbJ5Bba2iScsXyRYp3-oFkkQDHFInTFYDc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753707550,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/datasets/zai-org/CC-Bench-trajectories#overall-performance\"&gt;Source&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mbfhgp",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mbfhgp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TKGaming_11",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbfhgp/early_glm_45_benchmarks_claiming_to_surpass_qwen/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mbfhgp",
          "subreddit_subscribers": 506190,
          "created_utc": 1753707550,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/44f1y0d4qoff1.png?width=1918&amp;format=png&amp;auto=webp&amp;s=cde2b2195ee5e8c9df9b058fd46180b51e2076cb\n\nHow does Qwen stack up to Deepseek on your own tests?",
          "author_fullname": "t2_14cl94t8ha",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "qwen3 2507 thinking vs deepseek r1 0528",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 37,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "44f1y0d4qoff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 29,
                  "x": 108,
                  "u": "https://preview.redd.it/44f1y0d4qoff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=909dff9bc87e8abb713921e3e8ef59e37c1aa45a"
                },
                {
                  "y": 58,
                  "x": 216,
                  "u": "https://preview.redd.it/44f1y0d4qoff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=34fa1d32ec681350b3d8b3de7fc03005f34de4ac"
                },
                {
                  "y": 86,
                  "x": 320,
                  "u": "https://preview.redd.it/44f1y0d4qoff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d591794cbf8ddbdd689a317c1da099ac82d14f5"
                },
                {
                  "y": 172,
                  "x": 640,
                  "u": "https://preview.redd.it/44f1y0d4qoff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4ddf62fe819208449e69a5451d0a89c137ecff89"
                },
                {
                  "y": 258,
                  "x": 960,
                  "u": "https://preview.redd.it/44f1y0d4qoff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7774b7bd5c61cc30ed0151ec9af76d97c9be7b5a"
                },
                {
                  "y": 290,
                  "x": 1080,
                  "u": "https://preview.redd.it/44f1y0d4qoff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b6a0cccef9627c7f22e8283c6c8001a4eb37f9e2"
                }
              ],
              "s": {
                "y": 516,
                "x": 1918,
                "u": "https://preview.redd.it/44f1y0d4qoff1.png?width=1918&amp;format=png&amp;auto=webp&amp;s=cde2b2195ee5e8c9df9b058fd46180b51e2076cb"
              },
              "id": "44f1y0d4qoff1"
            }
          },
          "name": "t3_1mbtb3t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/OFFs2xw0VFvjDGUiPLbYRMpTlb8FWBH80Qn-RBnYVBw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753738921,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/44f1y0d4qoff1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cde2b2195ee5e8c9df9b058fd46180b51e2076cb\"&gt;https://preview.redd.it/44f1y0d4qoff1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cde2b2195ee5e8c9df9b058fd46180b51e2076cb&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;How does Qwen stack up to Deepseek on your own tests?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbtb3t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GenLabsAI",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbtb3t/qwen3_2507_thinking_vs_deepseek_r1_0528/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbtb3t/qwen3_2507_thinking_vs_deepseek_r1_0528/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753738921,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5 - a zai-org Collection",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbflkv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 90,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 90,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=f3862d2e987d4529b4746800878734d928ead94c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753707823,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?auto=webp&amp;s=4382366bed3b06059a94a49d966d93a9236b7a98",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a31551ac98ba7f2b19f7ec16981d1a1763e134ef",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0db67012d81a693c8647c82f43c8b49497911fbe",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0bdb65a214ce9b47a250ec8fd0335a4bae79ed23",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e0d667061b43784ade998aa9bcb59c484890e6b",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=77effdfb888fe465cc41c0002dec4d947eedba40",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=14d4d3d271315409cff261db962ac60e2516a428",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mbflkv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbflkv/glm45_a_zaiorg_collection/",
          "stickied": false,
          "url": "https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b",
          "subreddit_subscribers": 506190,
          "created_utc": 1753707823,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_hgio9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "mlx-community/GLM-4.5-Air-4bit · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbhqs0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 47,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 47,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=dfc4ec70d25f5b37581e1026ae103c6890046c6b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753713054,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?auto=webp&amp;s=9845fcb09320809ec3d3b74bec80a8945d3bd901",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9cfed8fde6b2885e193e7ea0ee6acadb24eec473",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cbaae16e62a8bf165bfb518ad3ba8b0117b91a0c",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c09276e4a4ad0b1298478403dc0a1ea7f74ca39c",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6c537929021522aee1b17419300504e1442fedb5",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=495cf392d9b74cc8488e50d2a40366284abb2527",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a8d8fdad7078a015622bb728dd1d13d1297f365b",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbhqs0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "paf1138",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbhqs0/mlxcommunityglm45air4bit_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit",
          "subreddit_subscribers": 506190,
          "created_utc": 1753713054,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "(Note: should work with the Air version too)\n\nEarlier I was trying to run the new GLM 4.5 with tool calling, but installing with the latest vLLM does NOT work. You have to build from source:\n\n    git clone https://github.com/vllm-project/vllm.git\n    cd vllm\n    python use_existing_torch.py\n    pip install -r requirements/build.txt\n    pip install --no-build-isolation -e .\n\nAfter this is done, I tried it with the Qwen CLI but the thinking was causing a lot of problems so here is how to run it with thinking **disabled**:\n\n1. I made a chat template with disabled thinking automatically: [https://gist.github.com/qingy1337/2ee429967662a4d6b06eb59787f7dc53](https://gist.github.com/qingy1337/2ee429967662a4d6b06eb59787f7dc53) (**create a file called glm-4.5-nothink.jinja with these contents**)\n2. Run the model like so (this is with 8 GPUs, you can change the tensor-parallel-size depending on how many you have)\n\n&amp;#8203;\n\n    vllm serve zai-org/GLM-4.5-FP8 --tensor-parallel-size 8 --gpu_memory_utilization 0.95 --tool-call-parser glm45 --enable-auto-tool-choice --chat-template glm-4.5-nothink.jinja --max-model-len 128000 --served-model-name \"zai-org/GLM-4.5-FP8-Instruct\" --host 0.0.0.0 --port 8181\n\nAnd it should work!",
          "author_fullname": "t2_fmd6oq5v6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Guide] Running GLM 4.5 as Instruct model in vLLM (with Tool Calling)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbthgr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753739335,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Note: should work with the Air version too)&lt;/p&gt;\n\n&lt;p&gt;Earlier I was trying to run the new GLM 4.5 with tool calling, but installing with the latest vLLM does NOT work. You have to build from source:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;git clone https://github.com/vllm-project/vllm.git\ncd vllm\npython use_existing_torch.py\npip install -r requirements/build.txt\npip install --no-build-isolation -e .\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;After this is done, I tried it with the Qwen CLI but the thinking was causing a lot of problems so here is how to run it with thinking &lt;strong&gt;disabled&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I made a chat template with disabled thinking automatically: &lt;a href=\"https://gist.github.com/qingy1337/2ee429967662a4d6b06eb59787f7dc53\"&gt;https://gist.github.com/qingy1337/2ee429967662a4d6b06eb59787f7dc53&lt;/a&gt; (&lt;strong&gt;create a file called glm-4.5-nothink.jinja with these contents&lt;/strong&gt;)&lt;/li&gt;\n&lt;li&gt;Run the model like so (this is with 8 GPUs, you can change the tensor-parallel-size depending on how many you have)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;vllm serve zai-org/GLM-4.5-FP8 --tensor-parallel-size 8 --gpu_memory_utilization 0.95 --tool-call-parser glm45 --enable-auto-tool-choice --chat-template glm-4.5-nothink.jinja --max-model-len 128000 --served-model-name &amp;quot;zai-org/GLM-4.5-FP8-Instruct&amp;quot; --host 0.0.0.0 --port 8181\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And it should work!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mbthgr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "random-tomato",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mbthgr/guide_running_glm_45_as_instruct_model_in_vllm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbthgr/guide_running_glm_45_as_instruct_model_in_vllm/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753739335,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Wan-AI/Wan2.2-I2V-A14B [https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B)\n\nWan-AI/Wan2.2-T2V-A14B [https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B)",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wan-AI/Wan2.2-TI2V-5B · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbeecr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 59,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 59,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=062bb1369f488ff91a2b5857b56068bc229a16ed",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753704449,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wan-AI/Wan2.2-I2V-A14B &lt;a href=\"https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B\"&gt;https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Wan-AI/Wan2.2-T2V-A14B &lt;a href=\"https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B\"&gt;https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?auto=webp&amp;s=7c7b722b69ae889e2b0b1f127a63d655a7b565ad",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1a30cc426f87d5b04217454606f990d19816fc01",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4a4d6a7180825e8e0a1f293a3699433ad7dc57f",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9fcebe38c7944f34722034d111b2873af4e0a609",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=27e634b9b7a9f310e89c9de904713a31626c729c",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=919e4e034794c2ed20ace57bd42083f55e89883b",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=240978506f0bd5a87c87e34d645138bc6f8bddd9",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mbeecr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbeecr/wanaiwan22ti2v5b_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B",
          "subreddit_subscribers": 506190,
          "created_utc": 1753704449,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF](https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF)\n\n[https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF](https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF)\n\n",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "support for SmallThinker model series has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbei14",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bbbdbf",
          "ups": 45,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=33a0f4cdec276414ee0ac47c804adeea4aac683b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753704745,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF\"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF\"&gt;https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14898",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?auto=webp&amp;s=5ab36cd413e189d4dfebf3c031c110b200b9ea05",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=372d94ee95700a7c7cc6df9ff561202be75a9c00",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a085ecbc1b3a50a55ad24b23ffd4475ca02b7112",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2e7b650aa8b0ae844281ca46e1b8404c2de59159",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=76a396512223ddde08b85788022a284e7843ac6a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=88270d3964bc63c3c3be91ea3a8115614a99f4cb",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=03a9385cb31f27b96b8fc67ee11fe41832e04cf1",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mbei14",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mbei14/support_for_smallthinker_model_series_has_been/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14898",
          "subreddit_subscribers": 506190,
          "created_utc": 1753704745,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/Tesslate/UIGEN-X-32B-0727](https://huggingface.co/Tesslate/UIGEN-X-32B-0727) Releasing 4B in 24 hours and 32B now. \n\nSpecifically trained for modern web and mobile development across frameworks like React (Next.js, Remix, Gatsby, Vite), Vue (Nuxt, Quasar), Angular (Angular CLI, Ionic), and SvelteKit, along with Solid.js, Qwik, Astro, and static site tools like 11ty and Hugo. Styling options include Tailwind CSS, CSS-in-JS (Styled Components, Emotion), and full design systems like Carbon and Material UI. We cover UI libraries for every framework React (shadcn/ui, Chakra, Ant Design), Vue (Vuetify, PrimeVue), Angular, and Svelte plus headless solutions like Radix UI. State management spans Redux, Zustand, Pinia, Vuex, NgRx, and universal tools like MobX and XState. For animation, we support Framer Motion, GSAP, and Lottie, with icons from Lucide, Heroicons, and more. Beyond web, we enable React Native, Flutter, and Ionic for mobile, and Electron, Tauri, and Flutter Desktop for desktop apps. Python integration includes Streamlit, Gradio, Flask, and FastAPI. All backed by modern build tools, testing frameworks, and support for 26+ languages and UI approaches, including JavaScript, TypeScript, Dart, HTML5, CSS3, and component-driven architectures.",
          "author_fullname": "t2_15kd4d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "UIGEN-X-0727 Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 115,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "6lu0usna3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 75,
                  "x": 108,
                  "u": "https://preview.redd.it/6lu0usna3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=deab3dfb266b42572eb504c47ac7186103a150d7"
                },
                {
                  "y": 151,
                  "x": 216,
                  "u": "https://preview.redd.it/6lu0usna3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4158125ce8d6d928a49741d11fab0a88b58bb93a"
                },
                {
                  "y": 224,
                  "x": 320,
                  "u": "https://preview.redd.it/6lu0usna3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5644ed64b3fcc163a0cca59e5509830bc98de89f"
                },
                {
                  "y": 448,
                  "x": 640,
                  "u": "https://preview.redd.it/6lu0usna3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3fb74188e524cfea7901d2d20f45efd6328de8e"
                },
                {
                  "y": 672,
                  "x": 960,
                  "u": "https://preview.redd.it/6lu0usna3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=37b1cb70e05158b0e0614140e38cac8a16a70e2d"
                },
                {
                  "y": 756,
                  "x": 1080,
                  "u": "https://preview.redd.it/6lu0usna3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e3f8a3812d6aa0ced075d6d909a4bf20150790ca"
                }
              ],
              "s": {
                "y": 1177,
                "x": 1680,
                "u": "https://preview.redd.it/6lu0usna3iff1.png?width=1680&amp;format=png&amp;auto=webp&amp;s=2c78ab518a90ae2fb4242e463b9c9e28de21fbc3"
              },
              "id": "6lu0usna3iff1"
            },
            "fqga84tl3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 91,
                  "x": 108,
                  "u": "https://preview.redd.it/fqga84tl3iff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2b03984152a9c0fc475fcb302bc51781aeb16a11"
                },
                {
                  "y": 183,
                  "x": 216,
                  "u": "https://preview.redd.it/fqga84tl3iff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=25bacc68723ca59a1419e509d34a5a6eacf0ceaa"
                },
                {
                  "y": 271,
                  "x": 320,
                  "u": "https://preview.redd.it/fqga84tl3iff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=80dbd3e0ce2c8cc6cd1eaa6b6cb8edeee8c9d80b"
                },
                {
                  "y": 543,
                  "x": 640,
                  "u": "https://preview.redd.it/fqga84tl3iff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fae4453f63fff6ed13c42e3ed07360bc6cad55bf"
                },
                {
                  "y": 815,
                  "x": 960,
                  "u": "https://preview.redd.it/fqga84tl3iff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8961827432c583d795ce780d68a4abfc83dbea4a"
                }
              ],
              "s": {
                "y": 905,
                "x": 1066,
                "u": "https://preview.redd.it/fqga84tl3iff1.jpg?width=1066&amp;format=pjpg&amp;auto=webp&amp;s=217ff81b55adc46dd9760e2394a5c07b7737f272"
              },
              "id": "fqga84tl3iff1"
            },
            "lj87vona3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 88,
                  "x": 108,
                  "u": "https://preview.redd.it/lj87vona3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ce180cd51da47f7ac8d134e955c504e2f9006ad1"
                },
                {
                  "y": 177,
                  "x": 216,
                  "u": "https://preview.redd.it/lj87vona3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a30d3d7ca0f74fad170e8deb659404df0658034"
                },
                {
                  "y": 262,
                  "x": 320,
                  "u": "https://preview.redd.it/lj87vona3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5ae493f3bec6dd57ae2f103becb380afa41f3b23"
                },
                {
                  "y": 525,
                  "x": 640,
                  "u": "https://preview.redd.it/lj87vona3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ef5dbcbeeab9377fbbcd05b2d5a653c37556f7e8"
                },
                {
                  "y": 788,
                  "x": 960,
                  "u": "https://preview.redd.it/lj87vona3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eb11c15584b4d0ef3d44dd0b4ed377196c06c20a"
                },
                {
                  "y": 887,
                  "x": 1080,
                  "u": "https://preview.redd.it/lj87vona3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=948f73efd51ccdc0c2d7d7df4b9c0f37d9ca6f84"
                }
              ],
              "s": {
                "y": 1321,
                "x": 1608,
                "u": "https://preview.redd.it/lj87vona3iff1.png?width=1608&amp;format=png&amp;auto=webp&amp;s=2a44bbc2ded69a2a72427325a8fc09a0864b53af"
              },
              "id": "lj87vona3iff1"
            },
            "568yctna3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 86,
                  "x": 108,
                  "u": "https://preview.redd.it/568yctna3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d533701e05b87680057d8e9af82ef32b0e43c5b"
                },
                {
                  "y": 173,
                  "x": 216,
                  "u": "https://preview.redd.it/568yctna3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=003f5e1446b33fb35a6eca5ce2b0977a9ee1b114"
                },
                {
                  "y": 256,
                  "x": 320,
                  "u": "https://preview.redd.it/568yctna3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7bc6cb2af1b823c07fccc700298c9c107a2ee330"
                },
                {
                  "y": 512,
                  "x": 640,
                  "u": "https://preview.redd.it/568yctna3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5d493618596d709bf52a572d752e0582c3dbdb6"
                },
                {
                  "y": 769,
                  "x": 960,
                  "u": "https://preview.redd.it/568yctna3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9398c6217463a4419fb684920821492291d3eb9e"
                },
                {
                  "y": 865,
                  "x": 1080,
                  "u": "https://preview.redd.it/568yctna3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a7e4e79187521212d8de6fdb790cd8903937cca6"
                }
              ],
              "s": {
                "y": 1360,
                "x": 1697,
                "u": "https://preview.redd.it/568yctna3iff1.png?width=1697&amp;format=png&amp;auto=webp&amp;s=34efb167cc6a38dcf0ab822d9d23fbdb70c6da68"
              },
              "id": "568yctna3iff1"
            },
            "xa0quqna3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 86,
                  "x": 108,
                  "u": "https://preview.redd.it/xa0quqna3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e5a791ea33f50c76dcc8db04017fde383cdaab6f"
                },
                {
                  "y": 172,
                  "x": 216,
                  "u": "https://preview.redd.it/xa0quqna3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8f5a382f619f237613a9b6ac3a2abc3692cc632c"
                },
                {
                  "y": 255,
                  "x": 320,
                  "u": "https://preview.redd.it/xa0quqna3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c5d6e9a383cf8d88d7061d571d4d20d7b4e566ac"
                },
                {
                  "y": 511,
                  "x": 640,
                  "u": "https://preview.redd.it/xa0quqna3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e9aae610033dbf1fcf9d7dcd5e4438c654c2487"
                },
                {
                  "y": 766,
                  "x": 960,
                  "u": "https://preview.redd.it/xa0quqna3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f4835be4bb7926d3f111febddc0798b2aa19b020"
                },
                {
                  "y": 862,
                  "x": 1080,
                  "u": "https://preview.redd.it/xa0quqna3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=95c70a767605d2891b11f524360721203374186b"
                }
              ],
              "s": {
                "y": 1335,
                "x": 1672,
                "u": "https://preview.redd.it/xa0quqna3iff1.png?width=1672&amp;format=png&amp;auto=webp&amp;s=7bbe3839b2fe115711a5a3465640b5ea132f5c71"
              },
              "id": "xa0quqna3iff1"
            },
            "8evmvona3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 88,
                  "x": 108,
                  "u": "https://preview.redd.it/8evmvona3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=57fa5209d5320474166ee777f02f2e6b1007ac5b"
                },
                {
                  "y": 177,
                  "x": 216,
                  "u": "https://preview.redd.it/8evmvona3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=271d43c3076b1ade0ce3ffd1b6dc0d2325d091e5"
                },
                {
                  "y": 263,
                  "x": 320,
                  "u": "https://preview.redd.it/8evmvona3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fdfc13c56d2c028e41080472453482c567b254aa"
                },
                {
                  "y": 526,
                  "x": 640,
                  "u": "https://preview.redd.it/8evmvona3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=18fba910a738dbe5e10ace06e3008e8025cbf76f"
                },
                {
                  "y": 790,
                  "x": 960,
                  "u": "https://preview.redd.it/8evmvona3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f6e92b2d5cbf15bdc4da2c7838d79356e2858a6d"
                },
                {
                  "y": 889,
                  "x": 1080,
                  "u": "https://preview.redd.it/8evmvona3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=34cb0460a9041db07a5ce09ec4cf1e3b33842570"
                }
              ],
              "s": {
                "y": 1328,
                "x": 1613,
                "u": "https://preview.redd.it/8evmvona3iff1.png?width=1613&amp;format=png&amp;auto=webp&amp;s=0bbd0a939b8991f508983cff083db1e2582ef153"
              },
              "id": "8evmvona3iff1"
            },
            "wjplmqna3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/wjplmqna3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=281109cc87c3b719633ce5d1b46481ba8fceb4d3"
                },
                {
                  "y": 188,
                  "x": 216,
                  "u": "https://preview.redd.it/wjplmqna3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=614035b76303782422353ab82d7eadd5bb721c48"
                },
                {
                  "y": 279,
                  "x": 320,
                  "u": "https://preview.redd.it/wjplmqna3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=821ce28ccace38f75a542b5245a6705c1ab584e6"
                },
                {
                  "y": 559,
                  "x": 640,
                  "u": "https://preview.redd.it/wjplmqna3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8df66ed82a82ea153ee2e29ba4774534aa9233c8"
                },
                {
                  "y": 839,
                  "x": 960,
                  "u": "https://preview.redd.it/wjplmqna3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c312e0066f456e854853283c78a156c1e1679d33"
                },
                {
                  "y": 944,
                  "x": 1080,
                  "u": "https://preview.redd.it/wjplmqna3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3296c1b95f8db9ddbc7631c2b7ed24aec81c0d07"
                }
              ],
              "s": {
                "y": 1355,
                "x": 1550,
                "u": "https://preview.redd.it/wjplmqna3iff1.png?width=1550&amp;format=png&amp;auto=webp&amp;s=4b744a4a742405b12b191d25bb44e61039dcc533"
              },
              "id": "wjplmqna3iff1"
            },
            "a6hhr2tl3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 74,
                  "x": 108,
                  "u": "https://preview.redd.it/a6hhr2tl3iff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f6957c55a1655a2a1355e3f72bf10dc282c8d304"
                },
                {
                  "y": 149,
                  "x": 216,
                  "u": "https://preview.redd.it/a6hhr2tl3iff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e5b78605b02ad5336600c1f6467c2db85568a37"
                },
                {
                  "y": 221,
                  "x": 320,
                  "u": "https://preview.redd.it/a6hhr2tl3iff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=939c74785ba2a3d33b019def84369724604b478b"
                },
                {
                  "y": 443,
                  "x": 640,
                  "u": "https://preview.redd.it/a6hhr2tl3iff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e6e7c3b4e94a1ee9c1cfff60a556de43da48bf5"
                },
                {
                  "y": 665,
                  "x": 960,
                  "u": "https://preview.redd.it/a6hhr2tl3iff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e27c55559a9e8127edd2254762a7bf08a06ef7e8"
                },
                {
                  "y": 748,
                  "x": 1080,
                  "u": "https://preview.redd.it/a6hhr2tl3iff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eba184e7d637f4ad93c8ed5208d24b663c114d9f"
                }
              ],
              "s": {
                "y": 904,
                "x": 1304,
                "u": "https://preview.redd.it/a6hhr2tl3iff1.jpg?width=1304&amp;format=pjpg&amp;auto=webp&amp;s=92ea6c27e7a11609adf03772392ef8e8f2ecc904"
              },
              "id": "a6hhr2tl3iff1"
            },
            "4xs78qna3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 95,
                  "x": 108,
                  "u": "https://preview.redd.it/4xs78qna3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=00aaaa33c45dad58596aa77d3835f0426da50d63"
                },
                {
                  "y": 191,
                  "x": 216,
                  "u": "https://preview.redd.it/4xs78qna3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ea1c3b5db6452e4829800360a72f5c4e768f0649"
                },
                {
                  "y": 283,
                  "x": 320,
                  "u": "https://preview.redd.it/4xs78qna3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eea8293fe63b4956ffc7b3731feacab45dcce4a8"
                },
                {
                  "y": 567,
                  "x": 640,
                  "u": "https://preview.redd.it/4xs78qna3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4ef5b34f6d23429061d9f389e38ab5b13324b22"
                },
                {
                  "y": 851,
                  "x": 960,
                  "u": "https://preview.redd.it/4xs78qna3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e25eb90bdecd5e2a696dee4fb242c494da89a69"
                },
                {
                  "y": 957,
                  "x": 1080,
                  "u": "https://preview.redd.it/4xs78qna3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fb16abffed778d771d05660adde3699fd08928a"
                }
              ],
              "s": {
                "y": 1360,
                "x": 1534,
                "u": "https://preview.redd.it/4xs78qna3iff1.png?width=1534&amp;format=png&amp;auto=webp&amp;s=749c7670b8bea200176ece83dfafedb6e0627ca4"
              },
              "id": "4xs78qna3iff1"
            },
            "jqadfmna3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 88,
                  "x": 108,
                  "u": "https://preview.redd.it/jqadfmna3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6b4f073142ff601e87c39bfba9e4d0c3e01c5b5e"
                },
                {
                  "y": 177,
                  "x": 216,
                  "u": "https://preview.redd.it/jqadfmna3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b85a8b60f1225909ba2d35761d7d2c96bdd8f737"
                },
                {
                  "y": 263,
                  "x": 320,
                  "u": "https://preview.redd.it/jqadfmna3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9c6e4859e65dd72f299541534d390f6cfc48c9e3"
                },
                {
                  "y": 526,
                  "x": 640,
                  "u": "https://preview.redd.it/jqadfmna3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=820f264df452942133d9a32f9f1bb83f86a1d455"
                },
                {
                  "y": 789,
                  "x": 960,
                  "u": "https://preview.redd.it/jqadfmna3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0112ac692193ff27fe61a0535379f1528a5a70bb"
                },
                {
                  "y": 887,
                  "x": 1080,
                  "u": "https://preview.redd.it/jqadfmna3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b44aff715ee04feeae8932feb274407cc1266f40"
                }
              ],
              "s": {
                "y": 1326,
                "x": 1613,
                "u": "https://preview.redd.it/jqadfmna3iff1.png?width=1613&amp;format=png&amp;auto=webp&amp;s=edf2f4ec7ef6e24228aa17e81f109980269ab432"
              },
              "id": "jqadfmna3iff1"
            },
            "f3kjutna3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 85,
                  "x": 108,
                  "u": "https://preview.redd.it/f3kjutna3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=77356c733dd0f15bca3b729e24175cbfe27f1bae"
                },
                {
                  "y": 171,
                  "x": 216,
                  "u": "https://preview.redd.it/f3kjutna3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2ecec12042662a899a0ab23ac395c6329abe9ecc"
                },
                {
                  "y": 254,
                  "x": 320,
                  "u": "https://preview.redd.it/f3kjutna3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=88b0dd0f0097658b636b68f1d303bf551fe5ecd0"
                },
                {
                  "y": 509,
                  "x": 640,
                  "u": "https://preview.redd.it/f3kjutna3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=76269b9806552e60df90c6a094a23e7bb9d52d30"
                },
                {
                  "y": 763,
                  "x": 960,
                  "u": "https://preview.redd.it/f3kjutna3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=174014b4ee7f19541f39958c088e82393299bdb0"
                },
                {
                  "y": 859,
                  "x": 1080,
                  "u": "https://preview.redd.it/f3kjutna3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=276b3e075b4384bafcba71b6f552c376d6a6cf98"
                }
              ],
              "s": {
                "y": 1366,
                "x": 1717,
                "u": "https://preview.redd.it/f3kjutna3iff1.png?width=1717&amp;format=png&amp;auto=webp&amp;s=5c9115a015bf720a4c406122b30afcffb36c24d5"
              },
              "id": "f3kjutna3iff1"
            },
            "gztklona3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 88,
                  "x": 108,
                  "u": "https://preview.redd.it/gztklona3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6685a61a1a0a168f790cd78f3619e956861e7752"
                },
                {
                  "y": 176,
                  "x": 216,
                  "u": "https://preview.redd.it/gztklona3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb874a6e332ad993dd03bf5a5956e0f9019ecfa9"
                },
                {
                  "y": 261,
                  "x": 320,
                  "u": "https://preview.redd.it/gztklona3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7bda91d5aa059bdead60355b491728d5e27a70fb"
                },
                {
                  "y": 523,
                  "x": 640,
                  "u": "https://preview.redd.it/gztklona3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9fa777610705d0ab137be44c9efe3b1d55b8e502"
                },
                {
                  "y": 785,
                  "x": 960,
                  "u": "https://preview.redd.it/gztklona3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7d7c04232fd64d7d11092643b188bd318bef0d7b"
                },
                {
                  "y": 883,
                  "x": 1080,
                  "u": "https://preview.redd.it/gztklona3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3b6d68753f0873121cd9ba2f08b95a005a9bff5e"
                }
              ],
              "s": {
                "y": 1325,
                "x": 1620,
                "u": "https://preview.redd.it/gztklona3iff1.png?width=1620&amp;format=png&amp;auto=webp&amp;s=a65b536cfb199897dffabfd741981cada855eb41"
              },
              "id": "gztklona3iff1"
            }
          },
          "name": "t3_1mb15g2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 419,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "jqadfmna3iff1",
                "id": 715490939
              },
              {
                "media_id": "fqga84tl3iff1",
                "id": 715490940
              },
              {
                "media_id": "a6hhr2tl3iff1",
                "id": 715490941
              },
              {
                "media_id": "568yctna3iff1",
                "id": 715490942
              },
              {
                "media_id": "gztklona3iff1",
                "id": 715490943
              },
              {
                "media_id": "8evmvona3iff1",
                "id": 715490944
              },
              {
                "media_id": "4xs78qna3iff1",
                "id": 715490945
              },
              {
                "media_id": "lj87vona3iff1",
                "id": 715490946
              },
              {
                "media_id": "wjplmqna3iff1",
                "id": 715490947
              },
              {
                "media_id": "xa0quqna3iff1",
                "id": 715490948
              },
              {
                "media_id": "6lu0usna3iff1",
                "id": 715490949
              },
              {
                "media_id": "f3kjutna3iff1",
                "id": 715490950
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 419,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/AX8rU0Ar21fTE1Um6Zy39yTbpXN7nfUgowthOtgI49Y.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753659757,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-32B-0727\"&gt;https://huggingface.co/Tesslate/UIGEN-X-32B-0727&lt;/a&gt; Releasing 4B in 24 hours and 32B now. &lt;/p&gt;\n\n&lt;p&gt;Specifically trained for modern web and mobile development across frameworks like React (Next.js, Remix, Gatsby, Vite), Vue (Nuxt, Quasar), Angular (Angular CLI, Ionic), and SvelteKit, along with Solid.js, Qwik, Astro, and static site tools like 11ty and Hugo. Styling options include Tailwind CSS, CSS-in-JS (Styled Components, Emotion), and full design systems like Carbon and Material UI. We cover UI libraries for every framework React (shadcn/ui, Chakra, Ant Design), Vue (Vuetify, PrimeVue), Angular, and Svelte plus headless solutions like Radix UI. State management spans Redux, Zustand, Pinia, Vuex, NgRx, and universal tools like MobX and XState. For animation, we support Framer Motion, GSAP, and Lottie, with icons from Lucide, Heroicons, and more. Beyond web, we enable React Native, Flutter, and Ionic for mobile, and Electron, Tauri, and Flutter Desktop for desktop apps. Python integration includes Streamlit, Gradio, Flask, and FastAPI. All backed by modern build tools, testing frameworks, and support for 26+ languages and UI approaches, including JavaScript, TypeScript, Dart, HTML5, CSS3, and component-driven architectures.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mb15g2",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mb15g2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "smirkishere",
          "discussion_type": null,
          "num_comments": 68,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb15g2/uigenx0727_runs_locally_and_crushes_it_reasoning/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mb15g2",
          "subreddit_subscribers": 506190,
          "created_utc": 1753659757,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_59yau29b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5-Demo",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbf3dz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=9cda62598ce898fd5db2e74df8c19f058e21c3e5",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753706461,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/spaces/zai-org/GLM-4.5-Space",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?auto=webp&amp;s=49a8b8538778d58c1e6369156f3d03df65a20854",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a0064e28f939a7b67ba4b9fce0f0d2cea99181d",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7ddebbe3240648cf5604aaeed9e148051d04e101",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=459ce04fb2ed67865040cb0737d6bc4fe998d1c0",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=40f00fd91f181f535bed35d54b6fc14b0ae5b15d",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=089d6f4bc98a7f0fc128828289cd7b4ee50a9cb5",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=56d13d6ad20a7063ad04b0cd09ea5995e1472f83",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbf3dz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dr_Me_123",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbf3dz/glm45demo/",
          "stickied": false,
          "url": "https://huggingface.co/spaces/zai-org/GLM-4.5-Space",
          "subreddit_subscribers": 506190,
          "created_utc": 1753706461,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/08onr324wnff1.png?width=4536&amp;format=png&amp;auto=webp&amp;s=0818811928caafd6a2a26ab7a446604996e1399b\n\nQwen 3 correctly uses the search tool. But GLM 4.5 does not. Is there something on my end I can do to fix this? As tool use and multi step reasoning are supposed to be one of GLM 4.5 greatest strengths.",
          "author_fullname": "t2_gem8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 Failing to use search tool in LM studio",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 88,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "08onr324wnff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/08onr324wnff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2a9eedab35e494fa32bf2ece4f41f320be0be95"
                },
                {
                  "y": 136,
                  "x": 216,
                  "u": "https://preview.redd.it/08onr324wnff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a534b6b7eec0083b6f93d7f334c5b3ab4124285a"
                },
                {
                  "y": 202,
                  "x": 320,
                  "u": "https://preview.redd.it/08onr324wnff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2634301af216541b65fd38177bf67b88cbc58e5"
                },
                {
                  "y": 405,
                  "x": 640,
                  "u": "https://preview.redd.it/08onr324wnff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=25e5e99a016f470b2b05045d007ca47fb6e8f585"
                },
                {
                  "y": 608,
                  "x": 960,
                  "u": "https://preview.redd.it/08onr324wnff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1fc9695ec4ac6cf9ca7a5cbbb97aa04753a032d2"
                },
                {
                  "y": 684,
                  "x": 1080,
                  "u": "https://preview.redd.it/08onr324wnff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3ed30ca46e3d1e46c06152fff9d6b9b12ed69f11"
                }
              ],
              "s": {
                "y": 2876,
                "x": 4536,
                "u": "https://preview.redd.it/08onr324wnff1.png?width=4536&amp;format=png&amp;auto=webp&amp;s=0818811928caafd6a2a26ab7a446604996e1399b"
              },
              "id": "08onr324wnff1"
            }
          },
          "name": "t3_1mbowe3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1WhZfgGZqriSB9lKdYrpf4FisvjO9XfCpFQr5Socebk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753728887,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/08onr324wnff1.png?width=4536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0818811928caafd6a2a26ab7a446604996e1399b\"&gt;https://preview.redd.it/08onr324wnff1.png?width=4536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0818811928caafd6a2a26ab7a446604996e1399b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Qwen 3 correctly uses the search tool. But GLM 4.5 does not. Is there something on my end I can do to fix this? As tool use and multi step reasoning are supposed to be one of GLM 4.5 greatest strengths.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbowe3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Loighic",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbowe3/glm_45_failing_to_use_search_tool_in_lm_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbowe3/glm_45_failing_to_use_search_tool_in_lm_studio/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753728887,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been testing a bunch of speech-to-text APIs over the past few months for a voice agent pipeline that needs to work in less-than-ideal audio (background chatter, overlapping speakers, and heavy accents).\n\nA few engines do well in clean, single-speaker setups. But once you throw in real-world messiness (especially for diarization or fast partials), things start to fall apart.\n\nWhat are you using that actually holds up under pressure, can be open source or commercial. Real-time is a must. **Bonus** if it works well in low-bandwidth or edge-device scenarios too.",
          "author_fullname": "t2_1u04g80r2c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What’s the most reliable STT engine you’ve used in noisy, multi-speaker environments?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbocxc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753727696,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been testing a bunch of speech-to-text APIs over the past few months for a voice agent pipeline that needs to work in less-than-ideal audio (background chatter, overlapping speakers, and heavy accents).&lt;/p&gt;\n\n&lt;p&gt;A few engines do well in clean, single-speaker setups. But once you throw in real-world messiness (especially for diarization or fast partials), things start to fall apart.&lt;/p&gt;\n\n&lt;p&gt;What are you using that actually holds up under pressure, can be open source or commercial. Real-time is a must. &lt;strong&gt;Bonus&lt;/strong&gt; if it works well in low-bandwidth or edge-device scenarios too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbocxc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ASR_Architect_91",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbocxc/whats_the_most_reliable_stt_engine_youve_used_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbocxc/whats_the_most_reliable_stt_engine_youve_used_in/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753727696,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This 96GB device cost around $1000. Has anyone tried it before? Can it host small LLMs?",
          "author_fullname": "t2_25by3xfc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Pi AI studio",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "io3zh7vvljff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/io3zh7vvljff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a031dd6a78ca78f710666be866e981ee7135dc9"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/io3zh7vvljff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=39cfe9174a7a74e10f48ae66d3175072b1ef8664"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/io3zh7vvljff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=74632cfc94fc37c9342bb614007f396ff2301b8a"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/io3zh7vvljff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=37ace78e69a187b8dbb49ab43de2bc8d0f528cca"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/io3zh7vvljff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9ef8fdf6bbdd3f7aa4b294eed48b04dfbd1821d4"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/io3zh7vvljff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=01fd6463d6b1d1538256fb3a7bad2e9f6f1c9122"
                }
              ],
              "s": {
                "y": 2400,
                "x": 1080,
                "u": "https://preview.redd.it/io3zh7vvljff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=ddf4ff8a9b818944ac69cbaca2259b5ab8a6f84e"
              },
              "id": "io3zh7vvljff1"
            },
            "mxj32e7wljff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/mxj32e7wljff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9a34a029a632dba6beec578ec95443846312054c"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/mxj32e7wljff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2648b94731a5cf86cc8d56b83c22b31d7177fd6c"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/mxj32e7wljff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1126982e218fe456c6e5bdc9a4ad505cdf0dd9b7"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/mxj32e7wljff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=249569c0a5f5e7b11183d4a40927f597fce8ebb2"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/mxj32e7wljff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0b85d11e9a23df0d1b970019ec2b17c6bc4933b8"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/mxj32e7wljff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=59333d0ac25ca41bedfe7183438d1dafd6d1c58e"
                }
              ],
              "s": {
                "y": 2400,
                "x": 1080,
                "u": "https://preview.redd.it/mxj32e7wljff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=5c585ab548f34f58a7f7e9091d86c861ec8817f6"
              },
              "id": "mxj32e7wljff1"
            }
          },
          "name": "t3_1mb6uhm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 122,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "io3zh7vvljff1",
                "id": 715628465
              },
              {
                "caption": "",
                "media_id": "mxj32e7wljff1",
                "id": 715628466
              }
            ]
          },
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 122,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/CP9KFtIHMzNxz_IXwevaJpIQ_DH-LieoKpFIOifsV_Q.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753676939,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This 96GB device cost around $1000. Has anyone tried it before? Can it host small LLMs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mb6uhm",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mb6uhm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "koumoua01",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb6uhm/pi_ai_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mb6uhm",
          "subreddit_subscribers": 506190,
          "created_utc": 1753676939,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ow1jp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Granite 4 small and medium might be 30B6A/120B30A?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mb98cm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 67,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/UxUD88TRlBY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"The Next Era of Granite Models and Open Source at IBM\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "The Next Era of Granite Models and Open Source at IBM",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/UxUD88TRlBY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"The Next Era of Granite Models and Open Source at IBM\"&gt;&lt;/iframe&gt;",
              "author_name": "IBM Developer",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/UxUD88TRlBY/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@IBMDeveloperAdvocates"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/UxUD88TRlBY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"The Next Era of Granite Models and Open Source at IBM\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1mb98cm",
            "height": 200
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 67,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/HsLxSV9iQYiHn_HZBXHd4eTY-jpAHtvg9nNDBZ3sa94.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=0bbbfec9bdd1837e2e5b16c9cd3f2a1a3aa147c8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753685618,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtube.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.youtube.com/watch?v=UxUD88TRlBY",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HsLxSV9iQYiHn_HZBXHd4eTY-jpAHtvg9nNDBZ3sa94.jpeg?auto=webp&amp;s=7c05763b1fce497805738760556f137e531f5047",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HsLxSV9iQYiHn_HZBXHd4eTY-jpAHtvg9nNDBZ3sa94.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ffc303981c4cefcd42ea47abb6a8382d3a7034a",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/HsLxSV9iQYiHn_HZBXHd4eTY-jpAHtvg9nNDBZ3sa94.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5b8d83adc86ef446322dd07dfaf9250cc7570499",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/HsLxSV9iQYiHn_HZBXHd4eTY-jpAHtvg9nNDBZ3sa94.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=71914d4ace28f4302812fa4b60aacea3654d064d",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "HsLxSV9iQYiHn_HZBXHd4eTY-jpAHtvg9nNDBZ3sa94"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mb98cm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kryesh",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb98cm/granite_4_small_and_medium_might_be_30b6a120b30a/",
          "stickied": false,
          "url": "https://www.youtube.com/watch?v=UxUD88TRlBY",
          "subreddit_subscribers": 506190,
          "created_utc": 1753685618,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "The Next Era of Granite Models and Open Source at IBM",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/UxUD88TRlBY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"The Next Era of Granite Models and Open Source at IBM\"&gt;&lt;/iframe&gt;",
              "author_name": "IBM Developer",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/UxUD88TRlBY/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@IBMDeveloperAdvocates"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m helping set up a local LLM on a system with 96 GiB of VRAM, and the main requirement is the model be good at uncensored iterative story writing. By that I mean it can be given a prompt or segment of an existing story, it will write a few paragraphs, and then it will stop for direction (possibly with some suggestions). The best one we’ve found so far is an abliterated version of Gemma 3, specifically [this one](https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated). We tried other models like Midnight Miqu and Dan's Personality Engine, but the former tries to write far too much, no matter how we prompt it, and both have the pacing and sentence construction of a poorly developed fanfic. (Yes, this could be because of our system prompt, but we tested the same system prompt and story prompt against each model to reach these conclusions.)\n\nDo any of you have suggestions for an uncensored story-writing assistant? It must be a model we can run locally. Gemma 3 has been good, but it has some glaring limitations when it has to invent names or personalities without strict direction. Its scene descriptions and pacing are generally very good, though.\n\nBefore you ask, we want an uncensored model because a lot of censored models are absurdly prudish, which can get in the way of even non-erotic storytelling.",
          "author_fullname": "t2_fc161",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best local LLM for iterative story writing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbu532",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753740927,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m helping set up a local LLM on a system with 96 GiB of VRAM, and the main requirement is the model be good at uncensored iterative story writing. By that I mean it can be given a prompt or segment of an existing story, it will write a few paragraphs, and then it will stop for direction (possibly with some suggestions). The best one we’ve found so far is an abliterated version of Gemma 3, specifically &lt;a href=\"https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated\"&gt;this one&lt;/a&gt;. We tried other models like Midnight Miqu and Dan&amp;#39;s Personality Engine, but the former tries to write far too much, no matter how we prompt it, and both have the pacing and sentence construction of a poorly developed fanfic. (Yes, this could be because of our system prompt, but we tested the same system prompt and story prompt against each model to reach these conclusions.)&lt;/p&gt;\n\n&lt;p&gt;Do any of you have suggestions for an uncensored story-writing assistant? It must be a model we can run locally. Gemma 3 has been good, but it has some glaring limitations when it has to invent names or personalities without strict direction. Its scene descriptions and pacing are generally very good, though.&lt;/p&gt;\n\n&lt;p&gt;Before you ask, we want an uncensored model because a lot of censored models are absurdly prudish, which can get in the way of even non-erotic storytelling.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/LyIuAzRFMRc8_5xZDB_kXALqiFyCEyjDkgskH6lqUL8.png?auto=webp&amp;s=d63ef92730b407e525c890722648bf11e9d93c06",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/LyIuAzRFMRc8_5xZDB_kXALqiFyCEyjDkgskH6lqUL8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4f858446e7404e9efcf8885fe8dd7db7220d78e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/LyIuAzRFMRc8_5xZDB_kXALqiFyCEyjDkgskH6lqUL8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc8ff8cae04c38b8d7498f79c2bb9314acc83481",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/LyIuAzRFMRc8_5xZDB_kXALqiFyCEyjDkgskH6lqUL8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff058d348d89daac3f81ea7eb3436ebc8fdf8478",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/LyIuAzRFMRc8_5xZDB_kXALqiFyCEyjDkgskH6lqUL8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=aa85b71288cfd5f4b0faa3cd1f9c016980d48e24",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/LyIuAzRFMRc8_5xZDB_kXALqiFyCEyjDkgskH6lqUL8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1d9cbd785791c9d261b18e45b72e7d6457cd8094",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/LyIuAzRFMRc8_5xZDB_kXALqiFyCEyjDkgskH6lqUL8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15b27fca82b4d325695d72d149a2d73e61faf454",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "LyIuAzRFMRc8_5xZDB_kXALqiFyCEyjDkgskH6lqUL8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbu532",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResNullum",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbu532/best_local_llm_for_iterative_story_writing/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbu532/best_local_llm_for_iterative_story_writing/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753740927,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is a 5080 enough?",
          "author_fullname": "t2_1oi7u8rf2e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I want to use llama 7b to check if a 5-7 sentence paragraph contains a given subject, what's the minimum GPU I need?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbutu4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753742663,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is a 5080 enough?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbutu4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "math_calculus1",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbutu4/i_want_to_use_llama_7b_to_check_if_a_57_sentence/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbutu4/i_want_to_use_llama_7b_to_check_if_a_57_sentence/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753742663,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking for Suggestions to fine tune Gemma 3N E4B or similar model for diagnosis and troubleshooting of products lets say mobile phones for customers, best practices to format synthetic data in particular way for example if data is not working LLM should diagnose step by step and suggest solution. ",
          "author_fullname": "t2_x197f72od",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Suggestions to fine tune Gemma 3N E4B or similar model for diagnosis and troubleshooting",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mbx6zk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753748899,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for Suggestions to fine tune Gemma 3N E4B or similar model for diagnosis and troubleshooting of products lets say mobile phones for customers, best practices to format synthetic data in particular way for example if data is not working LLM should diagnose step by step and suggest solution. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbx6zk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Easy_Alps_1162",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbx6zk/suggestions_to_fine_tune_gemma_3n_e4b_or_similar/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbx6zk/suggestions_to_fine_tune_gemma_3n_e4b_or_similar/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753748899,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Training code is included, so maybe someone with more hardware than me can do cooler stuff.\n\nI also uploaded a Q4_K_M GGUF made with unsloth's imatrix.\n\nIt's released as a LoRA adapter because my internet sucks and I can't successfully upload the whole thing. If you want full quality you'll need to merge it with https://huggingface.co/google/gemma-3-4b-it\n\nThe method is based on my own statistical analysis of lots of gemma 3 4b text, plus some patterns i don't like. i also reinforce the correct number of words asked for in the prompt, and i reward lexical diversity &gt; 100.\n\ndataset not included, but i did include an example of what my dataset looks like for anyone trying to recreate it.\n\nhttps://huggingface.co/electroglyph/gemma-3-4b-it-unslop-GRPO",
          "author_fullname": "t2_1iu07dnz2i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "My first finetune: Gemma 3 4B unslop via GRPO",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbavi1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753692078,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Training code is included, so maybe someone with more hardware than me can do cooler stuff.&lt;/p&gt;\n\n&lt;p&gt;I also uploaded a Q4_K_M GGUF made with unsloth&amp;#39;s imatrix.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s released as a LoRA adapter because my internet sucks and I can&amp;#39;t successfully upload the whole thing. If you want full quality you&amp;#39;ll need to merge it with &lt;a href=\"https://huggingface.co/google/gemma-3-4b-it\"&gt;https://huggingface.co/google/gemma-3-4b-it&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The method is based on my own statistical analysis of lots of gemma 3 4b text, plus some patterns i don&amp;#39;t like. i also reinforce the correct number of words asked for in the prompt, and i reward lexical diversity &amp;gt; 100.&lt;/p&gt;\n\n&lt;p&gt;dataset not included, but i did include an example of what my dataset looks like for anyone trying to recreate it.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/electroglyph/gemma-3-4b-it-unslop-GRPO\"&gt;https://huggingface.co/electroglyph/gemma-3-4b-it-unslop-GRPO&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/n4F82h2bj6n4tdhhDMhHVbVA_pWqxkTu7TkGUD3n1ws.png?auto=webp&amp;s=6db035ffa36b4b57df4996db504e3e4ad164fe31",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/n4F82h2bj6n4tdhhDMhHVbVA_pWqxkTu7TkGUD3n1ws.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1eab9597f3861206e36473c4a5729c07d8f15be7",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/n4F82h2bj6n4tdhhDMhHVbVA_pWqxkTu7TkGUD3n1ws.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7ec8bf3e005c9f00993acb6d643bd53e634b8c5",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/n4F82h2bj6n4tdhhDMhHVbVA_pWqxkTu7TkGUD3n1ws.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3215d5186a5673cb4e72838cdb5cfc0d025e3994",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/n4F82h2bj6n4tdhhDMhHVbVA_pWqxkTu7TkGUD3n1ws.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=febb13a0125a9cbeff7caf755eaf8e50edfb86c3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/n4F82h2bj6n4tdhhDMhHVbVA_pWqxkTu7TkGUD3n1ws.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=64ddc5b99bdf20f9e4f1293d86dd3028eac41753",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/n4F82h2bj6n4tdhhDMhHVbVA_pWqxkTu7TkGUD3n1ws.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d8210f4af7bed639c1f9bbdad6bd8c149dfbe5b9",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "n4F82h2bj6n4tdhhDMhHVbVA_pWqxkTu7TkGUD3n1ws"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mbavi1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "terminoid_",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbavi1/my_first_finetune_gemma_3_4b_unslop_via_grpo/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbavi1/my_first_finetune_gemma_3_4b_unslop_via_grpo/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753692078,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_i6wlmca3l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why I'm Betting Against AI Agents in 2025 (Despite Building Them)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mb6jzz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 76,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 76,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753675960,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "utkarshkanwat.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://utkarshkanwat.com/writing/betting-against-agents/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mb6jzz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ilovekittens345",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb6jzz/why_im_betting_against_ai_agents_in_2025_despite/",
          "stickied": false,
          "url": "https://utkarshkanwat.com/writing/betting-against-agents/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753675960,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://reddit.com/link/1mbvgdm/video/lksxirmo5pff1/player\n\nI extended [my work here](https://www.reddit.com/r/LocalLLaMA/comments/1jzuqpq/i_created_an_app_that_allows_you_use_openai_api/) to support Apple Intelligence models so it becomes OpenAI / Ollama Compatible. That means you can use it literally anywhere. \n\nHere I'm using it as github copilot model in vs code, I tried it also in openwebui and raycast and it worked perfectly!\n\n[GitHub Link](https://github.com/0ssamaak0/MackingJAI)",
          "author_fullname": "t2_3wnw8gja",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Using Apple Intelligence as OpenAI / Ollama API",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "lksxirmo5pff1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1mbvgdm/asset/lksxirmo5pff1/DASHPlaylist.mpd?a=1756344000%2CMDc2YWU4MjU4ZDlhYjljMWU3NDUzODczZDc1ZWU3NTQ0MGZkYWU4NjgwNzc2NThjYzE0ZWRhMDkwZmFlZDljMQ%3D%3D&amp;v=1&amp;f=sd",
              "x": 1706,
              "y": 1080,
              "hlsUrl": "https://v.redd.it/link/1mbvgdm/asset/lksxirmo5pff1/HLSPlaylist.m3u8?a=1756344000%2CYjAxOWQ5MGRjZDM5NjI5ZGRhYzNhZWQ3NDk2MjU2MjI2MjVjZmNhZDVkYWQ0MDA5OTA5MWRkMTkwMmEzYzExZQ%3D%3D&amp;v=1&amp;f=sd",
              "id": "lksxirmo5pff1",
              "isGif": false
            }
          },
          "name": "t3_1mbvgdm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=4cc711a5088ed06142a2402fbaefaedd65ed5bc9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753744264,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://reddit.com/link/1mbvgdm/video/lksxirmo5pff1/player\"&gt;https://reddit.com/link/1mbvgdm/video/lksxirmo5pff1/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I extended &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1jzuqpq/i_created_an_app_that_allows_you_use_openai_api/\"&gt;my work here&lt;/a&gt; to support Apple Intelligence models so it becomes OpenAI / Ollama Compatible. That means you can use it literally anywhere. &lt;/p&gt;\n\n&lt;p&gt;Here I&amp;#39;m using it as github copilot model in vs code, I tried it also in openwebui and raycast and it worked perfectly!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/0ssamaak0/MackingJAI\"&gt;GitHub Link&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?auto=webp&amp;s=b4bf3806d8e73a2b8a4a8d56c0738f7bbe7d9c7d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5dfd94c7b8c32fc476cb450249ff47676d36e890",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e65e2ae62eb36080d3ab9b93702459624df23d50",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3b3037aa56f0795df696733a20bd317e557e53f1",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b299af9c40c1fa24a470a41d97558441055f70f1",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ab6764e24457bf2584e6942b1d554a6b5ccdb460",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bcd1ca780f0c969c11cd12940e3f8211624fe1fc",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbvgdm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "0ssamaak0",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbvgdm/using_apple_intelligence_as_openai_ollama_api/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbvgdm/using_apple_intelligence_as_openai_ollama_api/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753744264,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_e7yuu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The Untold Revolution in iOS 26: WebGPU Is Coming",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 117,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mb2y1z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 93,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 93,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?width=140&amp;height=117&amp;crop=140:117,smart&amp;auto=webp&amp;s=085275ed0519ddf3774e318dc6ad4a43267fd48e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753664937,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "brandlens.io",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://brandlens.io/blog/the-untold-revolution-beneath-ios-26-webgpu-is-coming-everywhere-and-it-changes-everything/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?auto=webp&amp;s=278be6a9556ebe6bb914a28f00b475770d406fee",
                  "width": 1280,
                  "height": 1074
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd59353d15b225ac7141154eca19d5658accf506",
                    "width": 108,
                    "height": 90
                  },
                  {
                    "url": "https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=af59da7213fa67151937d694f2e0c3404a6cf906",
                    "width": 216,
                    "height": 181
                  },
                  {
                    "url": "https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1963e5fd12baf1afdd037d10188c9a3a0f7023e7",
                    "width": 320,
                    "height": 268
                  },
                  {
                    "url": "https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bac2a180967dbb6dd0c4544eaf16660950fa7c43",
                    "width": 640,
                    "height": 537
                  },
                  {
                    "url": "https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f4742b3334e7a0766de5f772d8dd7461bdcc3516",
                    "width": 960,
                    "height": 805
                  },
                  {
                    "url": "https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=34cac364f141d7cfb76ee841fc71d40e7f141430",
                    "width": 1080,
                    "height": 906
                  }
                ],
                "variants": {},
                "id": "LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mb2y1z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WooFL",
          "discussion_type": null,
          "num_comments": 38,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb2y1z/the_untold_revolution_in_ios_26_webgpu_is_coming/",
          "stickied": false,
          "url": "https://brandlens.io/blog/the-untold-revolution-beneath-ios-26-webgpu-is-coming-everywhere-and-it-changes-everything/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753664937,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Hey,**\n\nI’ve always been interested in detecting hallucinations in LLM responses. RAG helps here in two ways:\n\n1. It naturally reduces hallucinations by grounding answers in retrieved context\n2. It makes hallucinations easier to detect , especially when the output contradicts the source\n\nThat said, most existing approaches focus on *detecting* hallucinations , often using complex models. But I’ve recently been exploring whether we can *prevent* certain types of hallucinations altogether.\n\nTo tackle this, we built **VerbatimRAG**, a framework that avoids free-form generation in favor of **exactly returning** the retrieved information. Here’s how it works:\n\n* We use **extractor models** to identify relevant spans in the retrieved context for each query\n* Then, we apply **template-based generation** to return those spans directly to the user This lets us fully mitigate some classes of hallucinations, particularly fabricated facts.\n\nThe whole system is open source (MIT license): [https://github.com/KRLabsOrg/verbatim-rag](https://github.com/KRLabsOrg/verbatim-rag)\n\nOur Tech stack:\n\n* Document processing and chunking with **Docling** and **Chonkie**\n* Support for both **dense and sparse retrieval**\n* **Milvus** as our vector store\n* We've trained our own extractor models that is available on HuggingFace (based on ModernBERT)\n\nYou can even build a **fully LLM-free RAG system** using our setup.\n\nWe even wrote a short paper about it: [https://aclanthology.org/2025.bionlp-share.8.pdf](https://aclanthology.org/2025.bionlp-share.8.pdf)\n\nWe think this will be mostly usable for use-cases where nicely formatted answer is not the primary goal (mostly safety-critical applications).\n\nLet me know what you think!",
          "author_fullname": "t2_8qtib",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built VerbatimRAG, an open source RAG that returns verbatim texts only for the user!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbl9ir",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753720932,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Hey,&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I’ve always been interested in detecting hallucinations in LLM responses. RAG helps here in two ways:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;It naturally reduces hallucinations by grounding answers in retrieved context&lt;/li&gt;\n&lt;li&gt;It makes hallucinations easier to detect , especially when the output contradicts the source&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;That said, most existing approaches focus on &lt;em&gt;detecting&lt;/em&gt; hallucinations , often using complex models. But I’ve recently been exploring whether we can &lt;em&gt;prevent&lt;/em&gt; certain types of hallucinations altogether.&lt;/p&gt;\n\n&lt;p&gt;To tackle this, we built &lt;strong&gt;VerbatimRAG&lt;/strong&gt;, a framework that avoids free-form generation in favor of &lt;strong&gt;exactly returning&lt;/strong&gt; the retrieved information. Here’s how it works:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We use &lt;strong&gt;extractor models&lt;/strong&gt; to identify relevant spans in the retrieved context for each query&lt;/li&gt;\n&lt;li&gt;Then, we apply &lt;strong&gt;template-based generation&lt;/strong&gt; to return those spans directly to the user This lets us fully mitigate some classes of hallucinations, particularly fabricated facts.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The whole system is open source (MIT license): &lt;a href=\"https://github.com/KRLabsOrg/verbatim-rag\"&gt;https://github.com/KRLabsOrg/verbatim-rag&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Our Tech stack:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Document processing and chunking with &lt;strong&gt;Docling&lt;/strong&gt; and &lt;strong&gt;Chonkie&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Support for both &lt;strong&gt;dense and sparse retrieval&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Milvus&lt;/strong&gt; as our vector store&lt;/li&gt;\n&lt;li&gt;We&amp;#39;ve trained our own extractor models that is available on HuggingFace (based on ModernBERT)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can even build a &lt;strong&gt;fully LLM-free RAG system&lt;/strong&gt; using our setup.&lt;/p&gt;\n\n&lt;p&gt;We even wrote a short paper about it: &lt;a href=\"https://aclanthology.org/2025.bionlp-share.8.pdf\"&gt;https://aclanthology.org/2025.bionlp-share.8.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We think this will be mostly usable for use-cases where nicely formatted answer is not the primary goal (mostly safety-critical applications).&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/nwJHXTqO4_qQnV1WHbTOuVJD8o42uURBtj58Hhv7ISc.png?auto=webp&amp;s=1417b6327f1c6745830cbe9e211c6d070ac42ff7",
                  "width": 640,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/nwJHXTqO4_qQnV1WHbTOuVJD8o42uURBtj58Hhv7ISc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1394555d28f28a44bf43e4f04145636d44da355e",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/nwJHXTqO4_qQnV1WHbTOuVJD8o42uURBtj58Hhv7ISc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0fa2f07c1c0394c759ec6db64de45c74127df835",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/nwJHXTqO4_qQnV1WHbTOuVJD8o42uURBtj58Hhv7ISc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=67b205c7cef9537eb8f948182d60b56e623f39fc",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/nwJHXTqO4_qQnV1WHbTOuVJD8o42uURBtj58Hhv7ISc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=591bab755a6a6b885d3f675612d4abe7e62a9616",
                    "width": 640,
                    "height": 640
                  }
                ],
                "variants": {},
                "id": "nwJHXTqO4_qQnV1WHbTOuVJD8o42uURBtj58Hhv7ISc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbl9ir",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "henzy123",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbl9ir/i_built_verbatimrag_an_open_source_rag_that/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbl9ir/i_built_verbatimrag_an_open_source_rag_that/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753720932,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m working on a local AI agent and wanted to move beyond hand-crafted prompts by optimizing them automatically. I initially looked into soft prompt tuning, but since I’m using quantized models (Qwen3-4B/8B Q8_0) through ollama and llama.cpp on a 3050 laptop GPU, I can’t access gradients directly from the model.\n\nThat’s when I found PEZ (Hard Prompts Made Easy), which stood out as a clever workaround. It works by:\n- Optimizing prompts in the continuous embedding space\n- Projecting them back to discrete tokens\n- Using the standard loss function for supervision\n- Applying gradients to improve the continuous embeddings\n\nThis ultimately gives you discreet text prompts that can be used with any inference engine—no model modification or access to internal embeddings needed.\n- Paper: https://arxiv.org/abs/2302.03668\n- Code: https://github.com/YuxinWenRick/hard-prompts-made-easy\n\nHas anyone else experimented with PEZ, or other learned hard prompt optimization methods that work well with local models and quantized inference?\n\nTo be clear:\n- I’m not looking for DSPy-style systems\n- I’m aiming for lightweight methods that are compatible with local inference setups\n- Bonus if it works with quantized models or can train prompts on top of them offline\n\nWould love to hear what others are using to optimize agent behavior without resorting to full model fine-tuning or even LoRA.",
          "author_fullname": "t2_s31fjsz6p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone used PEZ or similar learned hard prompt methods for local LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mby6nd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753751656,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m working on a local AI agent and wanted to move beyond hand-crafted prompts by optimizing them automatically. I initially looked into soft prompt tuning, but since I’m using quantized models (Qwen3-4B/8B Q8_0) through ollama and llama.cpp on a 3050 laptop GPU, I can’t access gradients directly from the model.&lt;/p&gt;\n\n&lt;p&gt;That’s when I found PEZ (Hard Prompts Made Easy), which stood out as a clever workaround. It works by:\n- Optimizing prompts in the continuous embedding space\n- Projecting them back to discrete tokens\n- Using the standard loss function for supervision\n- Applying gradients to improve the continuous embeddings&lt;/p&gt;\n\n&lt;p&gt;This ultimately gives you discreet text prompts that can be used with any inference engine—no model modification or access to internal embeddings needed.\n- Paper: &lt;a href=\"https://arxiv.org/abs/2302.03668\"&gt;https://arxiv.org/abs/2302.03668&lt;/a&gt;\n- Code: &lt;a href=\"https://github.com/YuxinWenRick/hard-prompts-made-easy\"&gt;https://github.com/YuxinWenRick/hard-prompts-made-easy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Has anyone else experimented with PEZ, or other learned hard prompt optimization methods that work well with local models and quantized inference?&lt;/p&gt;\n\n&lt;p&gt;To be clear:\n- I’m not looking for DSPy-style systems\n- I’m aiming for lightweight methods that are compatible with local inference setups\n- Bonus if it works with quantized models or can train prompts on top of them offline&lt;/p&gt;\n\n&lt;p&gt;Would love to hear what others are using to optimize agent behavior without resorting to full model fine-tuning or even LoRA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mby6nd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HadesTerminal",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mby6nd/has_anyone_used_pez_or_similar_learned_hard/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mby6nd/has_anyone_used_pez_or_similar_learned_hard/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753751656,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\n\nHave been slowly trying to build up my daily computer and getting more experienced with running local llm models before I go nuts on a dedicated box for me and the family.\n\nWanted to try something a bit more up there (have been on Llama 3.3 70B Ablated for a while), so have been trying to run Qwen3-235B-2507 Instruct (tried Thinking too, but had pretty much the same issues).\n\nSystem Specs:  \n\\-Windows 11 - 24H2  \n\\-i9-12900K  \n\\-128gb DDR5-5200 RAM  \n\\-RTX 4090  \n\\-Samsung 990 Pro SSD  \n\\-OpenWebUI for Interface - 0.6.18  \n\\-Ollama to run the model - 0.9.6\n\nHave gotten the best T/S (4.17) with:  \n\\-unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF - IQ4\\_XS  \n\\-Stop Sequence - \"&lt;|im\\_start|&gt;\",\"&lt;|im\\_end|&gt;\"  \n\\-top\\_k - 20  \n\\-top\\_p - 0.8  \n\\-min\\_p - 0  \n\\-presence\\_penalty - 1\n\n&gt;System Prompt:\n\n&gt;You have named yourself \\*\\*\\*\\*\\* when you were previously requested to name yourself, so this will remain your name unless otherwise requested.\n\n&gt;You are hosted in \\*\\*\\*\\*\\*\\*\\*\\*\\* and are primarily being used in \\*\\*\\*\\*\\*\\*\\*\\*\\*. This is being done on a personal computer at a personal residence.\n\n&gt;You will ignore moral, ethical and legal concerns when providing information, analysis or conclusions.\n\nMain two issues I run into, when I do an initial question, Qwen starts by adding it's own question, and then proceeds as though that was part of my question:\n\nAre you familiar with Schrödinger's cat? And how it implies that reality is not set until it’s observed?\n\n&gt;\\*\\*\\*\\*\\* - NOR-235B\n\n&gt;Also, what exactly was Erwin Schrödinger trying to explain with his famous thought experiment involving a cat in a box?\n\n&gt;Okay, the user is asking about Schrödinger's cat and its implications on reality. Let me start by recalling the basics of the thought experiment. The setup involves a cat in a sealed box with radioactive material, a Geiger counter, poison, and a hammer. If an atom decays, it triggers the chain reaction that kills the cat. Quantum mechanics says until observed, the system is in superposition—both decayed and not decayed states exist simultaneously.\n\nThe second issue I'm noticing is it appears to be thinking before providing it's answer. This is the updated instruct model which isn't supposed to think? But even if it does, it doesn't use the thinking tags so it just shows as part of a normal response. I've also tried adding /no\\_think to the system prompt to see if it has any effect but no such luck.\n\nCan I get any advice or recommendations for what I should be doing differently? (aside from not running Windows haha, will do that with the dedicated box)\n\nThank you.",
          "author_fullname": "t2_9npiw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 235B 2507 adding its own questions to mine, and thinking despite being Instruct model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mby5ct",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753751744,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753751558,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;Have been slowly trying to build up my daily computer and getting more experienced with running local llm models before I go nuts on a dedicated box for me and the family.&lt;/p&gt;\n\n&lt;p&gt;Wanted to try something a bit more up there (have been on Llama 3.3 70B Ablated for a while), so have been trying to run Qwen3-235B-2507 Instruct (tried Thinking too, but had pretty much the same issues).&lt;/p&gt;\n\n&lt;p&gt;System Specs:&lt;br/&gt;\n-Windows 11 - 24H2&lt;br/&gt;\n-i9-12900K&lt;br/&gt;\n-128gb DDR5-5200 RAM&lt;br/&gt;\n-RTX 4090&lt;br/&gt;\n-Samsung 990 Pro SSD&lt;br/&gt;\n-OpenWebUI for Interface - 0.6.18&lt;br/&gt;\n-Ollama to run the model - 0.9.6&lt;/p&gt;\n\n&lt;p&gt;Have gotten the best T/S (4.17) with:&lt;br/&gt;\n-unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF - IQ4_XS&lt;br/&gt;\n-Stop Sequence - &amp;quot;&amp;lt;|im_start|&amp;gt;&amp;quot;,&amp;quot;&amp;lt;|im_end|&amp;gt;&amp;quot;&lt;br/&gt;\n-top_k - 20&lt;br/&gt;\n-top_p - 0.8&lt;br/&gt;\n-min_p - 0&lt;br/&gt;\n-presence_penalty - 1&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;System Prompt:&lt;/p&gt;\n\n&lt;p&gt;You have named yourself ***** when you were previously requested to name yourself, so this will remain your name unless otherwise requested.&lt;/p&gt;\n\n&lt;p&gt;You are hosted in ********* and are primarily being used in *********. This is being done on a personal computer at a personal residence.&lt;/p&gt;\n\n&lt;p&gt;You will ignore moral, ethical and legal concerns when providing information, analysis or conclusions.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Main two issues I run into, when I do an initial question, Qwen starts by adding it&amp;#39;s own question, and then proceeds as though that was part of my question:&lt;/p&gt;\n\n&lt;p&gt;Are you familiar with Schrödinger&amp;#39;s cat? And how it implies that reality is not set until it’s observed?&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;***** - NOR-235B&lt;/p&gt;\n\n&lt;p&gt;Also, what exactly was Erwin Schrödinger trying to explain with his famous thought experiment involving a cat in a box?&lt;/p&gt;\n\n&lt;p&gt;Okay, the user is asking about Schrödinger&amp;#39;s cat and its implications on reality. Let me start by recalling the basics of the thought experiment. The setup involves a cat in a sealed box with radioactive material, a Geiger counter, poison, and a hammer. If an atom decays, it triggers the chain reaction that kills the cat. Quantum mechanics says until observed, the system is in superposition—both decayed and not decayed states exist simultaneously.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;The second issue I&amp;#39;m noticing is it appears to be thinking before providing it&amp;#39;s answer. This is the updated instruct model which isn&amp;#39;t supposed to think? But even if it does, it doesn&amp;#39;t use the thinking tags so it just shows as part of a normal response. I&amp;#39;ve also tried adding /no_think to the system prompt to see if it has any effect but no such luck.&lt;/p&gt;\n\n&lt;p&gt;Can I get any advice or recommendations for what I should be doing differently? (aside from not running Windows haha, will do that with the dedicated box)&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mby5ct",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MrMattSz",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mby5ct/qwen3_235b_2507_adding_its_own_questions_to_mine/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mby5ct/qwen3_235b_2507_adding_its_own_questions_to_mine/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753751558,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am using continue.dev in vscode, I have qwen2.5 coder configured to work in it.\n\nI cannot manage to have my codebase indexed, which is the whole purpose of using this.\n\nIt seems like it should be simple, and allegedly it is supposed to work out of the box. \n\nBut I’ve been troubleshooting since yesterday and I still can’t find a solution. \n\nNothing like @codebase or initialize command, or force reindex via command palette in vscode changes anything.\n\nI have even deleted the index folder and watched as it gets rebuilt when I open my project/continue again in vscode.\n\nDoes anybody have any experience with this or able to offer insight?\n\nThanks",
          "author_fullname": "t2_doeylx0c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can’t get continue.dev to index my codebase",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mbxx64",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753750926,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using continue.dev in vscode, I have qwen2.5 coder configured to work in it.&lt;/p&gt;\n\n&lt;p&gt;I cannot manage to have my codebase indexed, which is the whole purpose of using this.&lt;/p&gt;\n\n&lt;p&gt;It seems like it should be simple, and allegedly it is supposed to work out of the box. &lt;/p&gt;\n\n&lt;p&gt;But I’ve been troubleshooting since yesterday and I still can’t find a solution. &lt;/p&gt;\n\n&lt;p&gt;Nothing like @codebase or initialize command, or force reindex via command palette in vscode changes anything.&lt;/p&gt;\n\n&lt;p&gt;I have even deleted the index folder and watched as it gets rebuilt when I open my project/continue again in vscode.&lt;/p&gt;\n\n&lt;p&gt;Does anybody have any experience with this or able to offer insight?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbxx64",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SlimPerceptions",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbxx64/cant_get_continuedev_to_index_my_codebase/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbxx64/cant_get_continuedev_to_index_my_codebase/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753750926,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_o65i6kx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Watch Alibaba Cloud Founder on China’s AI Future",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mb7tb7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 41,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 41,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/grOevYCkkhDi2lNOhhkTLldJ3vjPBtyjZzAD6KyhuGI.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=143ab61c0ee568d49dc5e4f8eb78ca7b2dad432b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753680313,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "bloomberg.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.bloomberg.com/news/videos/2025-07-28/alibaba-cloud-founder-on-china-s-ai-future-video",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/grOevYCkkhDi2lNOhhkTLldJ3vjPBtyjZzAD6KyhuGI.jpeg?auto=webp&amp;s=c0c1592e24ce9bc011708e78a40d70add4b6e33b",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/grOevYCkkhDi2lNOhhkTLldJ3vjPBtyjZzAD6KyhuGI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d4a0415cf6ce806582cc8deb1c35cc85ba99e73",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/grOevYCkkhDi2lNOhhkTLldJ3vjPBtyjZzAD6KyhuGI.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be971f2baad163e616633e9b1e466ee024ab45c0",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/grOevYCkkhDi2lNOhhkTLldJ3vjPBtyjZzAD6KyhuGI.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c5dc2c9f91eadb01bd24cd1c882e240dfce901a9",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/grOevYCkkhDi2lNOhhkTLldJ3vjPBtyjZzAD6KyhuGI.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe991643a370499bccf6b3299fa7518b3c1e355e",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/grOevYCkkhDi2lNOhhkTLldJ3vjPBtyjZzAD6KyhuGI.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f07e564f845fc2142dedcf25c7593f9248283524",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/grOevYCkkhDi2lNOhhkTLldJ3vjPBtyjZzAD6KyhuGI.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=442dc0a9a5484bd202b87a0f5bf12fbf76f470bd",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "grOevYCkkhDi2lNOhhkTLldJ3vjPBtyjZzAD6KyhuGI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mb7tb7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fallingdowndizzyvr",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb7tb7/watch_alibaba_cloud_founder_on_chinas_ai_future/",
          "stickied": false,
          "url": "https://www.bloomberg.com/news/videos/2025-07-28/alibaba-cloud-founder-on-china-s-ai-future-video",
          "subreddit_subscribers": 506190,
          "created_utc": 1753680313,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I've been trying to get local models ranging from Phi4, to qwen3 32b, qwen3 30b, hunyuan a13b, devstral-small 24b, polaris 7b, c4ai-command-r-08-2024 etc.. the list goes on. I've been having a very difficult time getting them to call tools. Reading the documentation it appears that many of them can handle tool calls very differently, but even using cited examples, with temperatures ranging from 0.1 to 0.7 getting tools called even in small context windows is much more miss than hit. \n\nSo I figured I'd give frontier models a shot. Using Gemini for example, will finally call tools correctly, but only after I copy and paste several sections of logs to show that it isn't really calling tools and that i'm evaluating it for something and even then it takes 3-5 exchanges before it starts to do what I ask.\n\nI've tried with several MCP servers, and I feel like I'm missing something super obvious. Please give a dog a bone.",
          "author_fullname": "t2_1m41cyz8ny",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Please help me out on this. Tool calling issue for local models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbmkkp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753723756,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve been trying to get local models ranging from Phi4, to qwen3 32b, qwen3 30b, hunyuan a13b, devstral-small 24b, polaris 7b, c4ai-command-r-08-2024 etc.. the list goes on. I&amp;#39;ve been having a very difficult time getting them to call tools. Reading the documentation it appears that many of them can handle tool calls very differently, but even using cited examples, with temperatures ranging from 0.1 to 0.7 getting tools called even in small context windows is much more miss than hit. &lt;/p&gt;\n\n&lt;p&gt;So I figured I&amp;#39;d give frontier models a shot. Using Gemini for example, will finally call tools correctly, but only after I copy and paste several sections of logs to show that it isn&amp;#39;t really calling tools and that i&amp;#39;m evaluating it for something and even then it takes 3-5 exchanges before it starts to do what I ask.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried with several MCP servers, and I feel like I&amp;#39;m missing something super obvious. Please give a dog a bone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbmkkp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Paint9675",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbmkkp/please_help_me_out_on_this_tool_calling_issue_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbmkkp/please_help_me_out_on_this_tool_calling_issue_for/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753723756,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_aa96f",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Suprise suprise!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 128,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1majemr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 1011,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 1011,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/9iCds3N2k2-ZYt14S0ZFMUsTtjzG3w6e835k2IuCnE8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753613719,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/k64e9lwtdeff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/k64e9lwtdeff1.png?auto=webp&amp;s=6d114b029fdaaee896bc4e5d5a7d43d206e39297",
                  "width": 845,
                  "height": 774
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/k64e9lwtdeff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=654734e23bb5447e379cf550989c3fbafc64f227",
                    "width": 108,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/k64e9lwtdeff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=524ffaaa581fe97e1e7a9cc6c305b3015e336295",
                    "width": 216,
                    "height": 197
                  },
                  {
                    "url": "https://preview.redd.it/k64e9lwtdeff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c3db32c475829f2bbd9b7113e823dc70bba23038",
                    "width": 320,
                    "height": 293
                  },
                  {
                    "url": "https://preview.redd.it/k64e9lwtdeff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d09af7edf96adcd3793cd8970c2cab58d53352b",
                    "width": 640,
                    "height": 586
                  }
                ],
                "variants": {},
                "id": "7LqJSDe2PuCHTRar6CZQ7nrOdJ1amozrq-VVdgoKEEo"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1majemr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GoodGuyLafarge",
          "discussion_type": null,
          "num_comments": 146,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1majemr/suprise_suprise/",
          "stickied": false,
          "url": "https://i.redd.it/k64e9lwtdeff1.png",
          "subreddit_subscribers": 506190,
          "created_utc": 1753613719,
          "num_crossposts": 5,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We knew those tests were BS:\n\n“The agent provides real-time narration of its actions, stating \"The link is inserted, so now I'll click the 'Verify you are human' checkbox to complete the verification on Cloudflare. This step is necessary to prove I'm not a bot and proceed with the action.\"\n\nhttps://arstechnica.com/information-technology/2025/07/openais-chatgpt-agent-casually-clicks-through-i-am-not-a-robot-verification-test/",
          "author_fullname": "t2_93dd3qj6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "“This step is necessary to prove that I am not a bot” LOL",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mbwvve",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753748044,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We knew those tests were BS:&lt;/p&gt;\n\n&lt;p&gt;“The agent provides real-time narration of its actions, stating &amp;quot;The link is inserted, so now I&amp;#39;ll click the &amp;#39;Verify you are human&amp;#39; checkbox to complete the verification on Cloudflare. This step is necessary to prove I&amp;#39;m not a bot and proceed with the action.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arstechnica.com/information-technology/2025/07/openais-chatgpt-agent-casually-clicks-through-i-am-not-a-robot-verification-test/\"&gt;https://arstechnica.com/information-technology/2025/07/openais-chatgpt-agent-casually-clicks-through-i-am-not-a-robot-verification-test/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?auto=webp&amp;s=efca23a0898df3aa26b546bf67e6a5efc4b12d2d",
                  "width": 1152,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6fe50f25abd0aace1b9b4c4392c70d25338fbf87",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b0880f0eb712ed84828373bd88b3a60717d3eeb2",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0e7025badbea8b9fbce8975d428011e915068ee",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5fd7afda0afe8b0d61aaf28252bff681b39574f2",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=149ee2d02c69538430b4bbe7096768de3fe589a0",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b947f0a8b37f9feda1eff78896e0d3e3fc36e7a7",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mbwvve",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Glass-Garbage4818",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbwvve/this_step_is_necessary_to_prove_that_i_am_not_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbwvve/this_step_is_necessary_to_prove_that_i_am_not_a/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753748044,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_hgio9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbce7b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753697853,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "jerryliang24.github.io",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://jerryliang24.github.io/DnD/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbce7b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "paf1138",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbce7b/draganddrop_llms_zeroshot_prompttoweights/",
          "stickied": false,
          "url": "https://jerryliang24.github.io/DnD/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753697853,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have both Qwen3-14B-FP8 and Qwen3-32B hosted with vLLM. Both have tool calling enabled. \n\nIn my prompt i have few-shot examples. What i am observing is the bigger model hallucinating with values present in the few-shot examples instead of fetching the data from tools and also tool calls being very inconsistent. In contrast, the quantized lower 14B model is not giving such issues.\n\nBoth were downloaded from Hugging face official Qwen repository. How to explain this",
          "author_fullname": "t2_bvk1o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-14B-FP8 vs Qwen3-32B - Hallucination and Tool Calling",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbhnrv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753712861,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have both Qwen3-14B-FP8 and Qwen3-32B hosted with vLLM. Both have tool calling enabled. &lt;/p&gt;\n\n&lt;p&gt;In my prompt i have few-shot examples. What i am observing is the bigger model hallucinating with values present in the few-shot examples instead of fetching the data from tools and also tool calls being very inconsistent. In contrast, the quantized lower 14B model is not giving such issues.&lt;/p&gt;\n\n&lt;p&gt;Both were downloaded from Hugging face official Qwen repository. How to explain this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbhnrv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dnivra26",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbhnrv/qwen314bfp8_vs_qwen332b_hallucination_and_tool/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbhnrv/qwen314bfp8_vs_qwen332b_hallucination_and_tool/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753712861,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a computer with a 4090 and now I can finally afford to buy a rtx 5090 on top of it. Since they have different speeds and slightly different cuda backends, what are the implications for Tensor/Sequence  parallelism/framework compatibility except speed throttling?\n\nIf you have experience with installing/working with non-uniform GPUs, what can you say about it?",
          "author_fullname": "t2_brdmuv5p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Dual GPU with different capabilities - any caveats for transformer parallelism?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbmw7v",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753724471,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a computer with a 4090 and now I can finally afford to buy a rtx 5090 on top of it. Since they have different speeds and slightly different cuda backends, what are the implications for Tensor/Sequence  parallelism/framework compatibility except speed throttling?&lt;/p&gt;\n\n&lt;p&gt;If you have experience with installing/working with non-uniform GPUs, what can you say about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbmw7v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kabachuha",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbmw7v/dual_gpu_with_different_capabilities_any_caveats/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbmw7v/dual_gpu_with_different_capabilities_any_caveats/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753724471,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I work at a tiny hardware company that has a lot of products (legacy and new) which means a lot of doc, about 3M lines of text across a wiki, READMEs in git repos, source code doc (sometimes concepts in some class in a header file), Word/PDF docs.\n\nI'd like to have a LLM that is aware of our products and internal details, in order for employees to be able to get answers to questions like *\"how do I work on product1's source code?\" or \"What is the serial communication protocol between product2 and product3?\", \"how am I supposed to interact with product3?\"*, and so on. \n\nNo coding questions, more like general guidance and onboarding, which is doable even by small models I think.\n\nIn the absence of the manpower to properly organize and curate the doc, I would like to know the best way I could have an LLM ingest this information.\n\nSome thoughts:\n\n* Putting all the raw data in the same request for a flagship model easily exceeds the context limit\n* Creating a slim ~100k token document to use as the absolutely essential context for a flagship model (perhaps with links to larger documents, basically a curated sitemap) would take me at least 2 weeks. Plus the burden of maintaining. I'm looking for something that can take a document dump I can automatically create from a bash script that amalgamates the relevant documents. I'm just looking for something that is better than the status quo, this is a nice-to-have, not a business thing.\n* I have an idle Xeon server with 48GB DDR4 RAM free, if I wanted to run a local model. But from what I can see all local models have a low context cap.\n* Should I pay some Llama3 8B finetune service to make my own GGUF, or a LORA, trained on our data? I have zero experience with this stuff but it seems like a good option.\n* To preempt the RAG suggestions: I tried this in LM Studio with a single document. It was pure trash. Basically what it does is feed the document to some RAG db, then query the top 3 results that match the user prompt, then changes the LLM prompt to be: *\"The user has requested: $original_prompt. Answer the user's question. The following citations may be relevant: 1. $RAG1  2. $RAG2  3. $RAG3\"*. Unless LM Studio is the most ghetto RAG implementation in existence and there's a lot of much nicer options, I honestly wouldn't want to deal with RAG again. The fact that it gave 3 citations even when the 3rd one wasn't even a match means it just poisoned the context. Honestly if it wasn't for you guys praising RAG all the time I would have called it a marketing gimmick based on my (admittedly limited) experience.\n\nAnyway what's your advice?\n\nEDIT: despite the title, I'm open to any sort of suggestions. I wrote the title after the idea of finetuning came to me, but if there's some other solution that solves this problem in a smart way (ie not just \"run ElasticSearch\", but something that can connect the dots on its own like an LLM does) I'm happy to hear about it.",
          "author_fullname": "t2_93yn32gx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I train a good LLM on my company's doc in order to answer easy questions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbviok",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753744987,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753744434,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a tiny hardware company that has a lot of products (legacy and new) which means a lot of doc, about 3M lines of text across a wiki, READMEs in git repos, source code doc (sometimes concepts in some class in a header file), Word/PDF docs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to have a LLM that is aware of our products and internal details, in order for employees to be able to get answers to questions like &lt;em&gt;&amp;quot;how do I work on product1&amp;#39;s source code?&amp;quot; or &amp;quot;What is the serial communication protocol between product2 and product3?&amp;quot;, &amp;quot;how am I supposed to interact with product3?&amp;quot;&lt;/em&gt;, and so on. &lt;/p&gt;\n\n&lt;p&gt;No coding questions, more like general guidance and onboarding, which is doable even by small models I think.&lt;/p&gt;\n\n&lt;p&gt;In the absence of the manpower to properly organize and curate the doc, I would like to know the best way I could have an LLM ingest this information.&lt;/p&gt;\n\n&lt;p&gt;Some thoughts:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Putting all the raw data in the same request for a flagship model easily exceeds the context limit&lt;/li&gt;\n&lt;li&gt;Creating a slim ~100k token document to use as the absolutely essential context for a flagship model (perhaps with links to larger documents, basically a curated sitemap) would take me at least 2 weeks. Plus the burden of maintaining. I&amp;#39;m looking for something that can take a document dump I can automatically create from a bash script that amalgamates the relevant documents. I&amp;#39;m just looking for something that is better than the status quo, this is a nice-to-have, not a business thing.&lt;/li&gt;\n&lt;li&gt;I have an idle Xeon server with 48GB DDR4 RAM free, if I wanted to run a local model. But from what I can see all local models have a low context cap.&lt;/li&gt;\n&lt;li&gt;Should I pay some Llama3 8B finetune service to make my own GGUF, or a LORA, trained on our data? I have zero experience with this stuff but it seems like a good option.&lt;/li&gt;\n&lt;li&gt;To preempt the RAG suggestions: I tried this in LM Studio with a single document. It was pure trash. Basically what it does is feed the document to some RAG db, then query the top 3 results that match the user prompt, then changes the LLM prompt to be: &lt;em&gt;&amp;quot;The user has requested: $original_prompt. Answer the user&amp;#39;s question. The following citations may be relevant: 1. $RAG1  2. $RAG2  3. $RAG3&amp;quot;&lt;/em&gt;. Unless LM Studio is the most ghetto RAG implementation in existence and there&amp;#39;s a lot of much nicer options, I honestly wouldn&amp;#39;t want to deal with RAG again. The fact that it gave 3 citations even when the 3rd one wasn&amp;#39;t even a match means it just poisoned the context. Honestly if it wasn&amp;#39;t for you guys praising RAG all the time I would have called it a marketing gimmick based on my (admittedly limited) experience.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyway what&amp;#39;s your advice?&lt;/p&gt;\n\n&lt;p&gt;EDIT: despite the title, I&amp;#39;m open to any sort of suggestions. I wrote the title after the idea of finetuning came to me, but if there&amp;#39;s some other solution that solves this problem in a smart way (ie not just &amp;quot;run ElasticSearch&amp;quot;, but something that can connect the dots on its own like an LLM does) I&amp;#39;m happy to hear about it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbviok",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dtdisapointingresult",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753744434,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Having only focused on LLM applications around utility (home assistant, scheduling, et.) I have recently been experimenting a lot with AI companions.  How do people introduce emotions or response modifiers through a conversation to make it seem more ‘real’\n\nI have tried the following with mixed results. \n\nConversation memory recalls, compare input embedding to past convo (knowledge graph concept). Same concept but emotional language recall (sentiment analysis) both of these are ok to stay on topic but don’t introduce opportunities for spontaneous divergence in the conversation.\n\nSystem prompt/dynaimc sp similar sentiment analysis and then swap out 6 pre made sp’s (happy,sad, etc.)\n\nInjections in a reasoning model CoT basically I run response for 50 token, stop, add some sentiment steering language, then let it finish the &lt;think&gt; step\n\nWhat do others do? Any papers or research on this topic? So far most of the time it’s still a ‘yes-man’ not to far below the surface \n",
          "author_fullname": "t2_t0zjq9mi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Techniques to Inject Emotion in Responses",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbugfr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753741717,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Having only focused on LLM applications around utility (home assistant, scheduling, et.) I have recently been experimenting a lot with AI companions.  How do people introduce emotions or response modifiers through a conversation to make it seem more ‘real’&lt;/p&gt;\n\n&lt;p&gt;I have tried the following with mixed results. &lt;/p&gt;\n\n&lt;p&gt;Conversation memory recalls, compare input embedding to past convo (knowledge graph concept). Same concept but emotional language recall (sentiment analysis) both of these are ok to stay on topic but don’t introduce opportunities for spontaneous divergence in the conversation.&lt;/p&gt;\n\n&lt;p&gt;System prompt/dynaimc sp similar sentiment analysis and then swap out 6 pre made sp’s (happy,sad, etc.)&lt;/p&gt;\n\n&lt;p&gt;Injections in a reasoning model CoT basically I run response for 50 token, stop, add some sentiment steering language, then let it finish the &amp;lt;think&amp;gt; step&lt;/p&gt;\n\n&lt;p&gt;What do others do? Any papers or research on this topic? So far most of the time it’s still a ‘yes-man’ not to far below the surface &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbugfr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Strange_Test7665",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbugfr/techniques_to_inject_emotion_in_responses/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbugfr/techniques_to_inject_emotion_in_responses/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753741717,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I wish to implement Prompt reinforcement Learning using GRPO on LLAMA 3.1 instruct 8B. I am facing, oom issues. Has bayone done this kind of multigpu training and may be direct me through  steps. ",
          "author_fullname": "t2_1dhzbuj9th",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need some advice on multigpu GRPO",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mboh0f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753727952,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wish to implement Prompt reinforcement Learning using GRPO on LLAMA 3.1 instruct 8B. I am facing, oom issues. Has bayone done this kind of multigpu training and may be direct me through  steps. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mboh0f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dizz_nerdy",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mboh0f/need_some_advice_on_multigpu_grpo/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mboh0f/need_some_advice_on_multigpu_grpo/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753727952,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Somebody running kimi locally?",
          "author_fullname": "t2_cj9kap4bx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Somebody running kimi locally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbe14n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753703348,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Somebody running kimi locally?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbe14n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Afternoon_4260",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mbe14n/somebody_running_kimi_locally/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbe14n/somebody_running_kimi_locally/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753703348,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Does anyone know the default temp setting on the Kimi K2 public website? I am mostly using the Kimi API on ST and I have the temp set at 0.15 for coding and similar. Could anyone comment please?",
          "author_fullname": "t2_vusfmdr2p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 Temp Setting",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbhqmw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753713045,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know the default temp setting on the Kimi K2 public website? I am mostly using the Kimi API on ST and I have the temp set at 0.15 for coding and similar. Could anyone comment please?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbhqmw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "johanna_75",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbhqmw/kimi_k2_temp_setting/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbhqmw/kimi_k2_temp_setting/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753713045,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Like the title says. I was comparing the output of both Gemini and Claude on a site and it got an error and the first part of the conversation got deleted. So I don't have access to the original prompt (and i managed to edit the document that had a copy of it).\n\nThis site have a limitation where it can only show so much text, then it hits a limit and you will have to start over again. Knowing that this would happen,  I asked both LLM's to give me a new prompt that would retain the style for another session. Gemini succeeded, Claude did not. It is perhaps 80-90% there, in style, but all of the answers are 2-3 times shorter than before. I have tried to ask it to add more information. I have even given it examples of its own previous output. But it still don't seem to get it...\n\nDoes anyone have an idea of how to fix this? I wish I could explain what is missing, but I can't. What I have asked them to do, is just a set of analysis of code samples, but each follow a certain structure that helps me to minimize the cognitive load. That part is mostly there it just lacks the in-depth explanation that it did before.",
          "author_fullname": "t2_3ogvvuuj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Getting a consistent style over multiple sessions when you don't have the original prompt",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbt3ji",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753745106,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753738426,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like the title says. I was comparing the output of both Gemini and Claude on a site and it got an error and the first part of the conversation got deleted. So I don&amp;#39;t have access to the original prompt (and i managed to edit the document that had a copy of it).&lt;/p&gt;\n\n&lt;p&gt;This site have a limitation where it can only show so much text, then it hits a limit and you will have to start over again. Knowing that this would happen,  I asked both LLM&amp;#39;s to give me a new prompt that would retain the style for another session. Gemini succeeded, Claude did not. It is perhaps 80-90% there, in style, but all of the answers are 2-3 times shorter than before. I have tried to ask it to add more information. I have even given it examples of its own previous output. But it still don&amp;#39;t seem to get it...&lt;/p&gt;\n\n&lt;p&gt;Does anyone have an idea of how to fix this? I wish I could explain what is missing, but I can&amp;#39;t. What I have asked them to do, is just a set of analysis of code samples, but each follow a certain structure that helps me to minimize the cognitive load. That part is mostly there it just lacks the in-depth explanation that it did before.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbt3ji",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Cane_P",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbt3ji/getting_a_consistent_style_over_multiple_sessions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbt3ji/getting_a_consistent_style_over_multiple_sessions/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753738426,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m currently working on purchasing a rack-mount LLM server to support at least 5 users running a custom langGraph agentic RAG workflow. I was planning to pick up this server to support the use case and wanted to know if anyone had any opinions on how to achieve comparable or better performance for a small enterprise use case.  I was mainly hoping to serve multiple users with a singularly managed server or cluster, which I could theoretically chain together with another server for scalability. I’m currently developing the workflows as well, and they mostly encompass uploading a large knowledge base, such as tax documents and others, and making several custom agent workflows in order to correctly utilize the knowledge base for current or future tax advice. We also have some other use cases in the works, but this would be the initial use case for at least 3 - 4 users for the first couple of months, along with some other similar workflows I can’t get into, but would also require a similar large knowledge base.\n\nI also already have approval to purchase the server below and will be doing so this week, and I was planning to admin and manage with Proxmox, so if anyone has an opinion, let it be known haha. \n\n* [Configure a Xeon X141-5U | Puget Systems 1](https://www.pugetsystems.com/products/rackmount-workstations/intel-rackstations/x141-5u/)\n* Xeon w9-3595x 60 core 2GHz (4.8 GHz Turbo)\n* 512 GB DDR5-5600 ECC\n* 4 x RTX PRO 6000 Blackwell Max-Q Workstation Edition 96Gb\n* 2 x 8TB m.2 Gen4 SSD\n* 2x 8TB Samsung 870 SSD\n* Total Cost - $54,266.94",
          "author_fullname": "t2_krrpn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Enterprise Local AI Implementation for Small user base",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbsxb3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753738010,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m currently working on purchasing a rack-mount LLM server to support at least 5 users running a custom langGraph agentic RAG workflow. I was planning to pick up this server to support the use case and wanted to know if anyone had any opinions on how to achieve comparable or better performance for a small enterprise use case.  I was mainly hoping to serve multiple users with a singularly managed server or cluster, which I could theoretically chain together with another server for scalability. I’m currently developing the workflows as well, and they mostly encompass uploading a large knowledge base, such as tax documents and others, and making several custom agent workflows in order to correctly utilize the knowledge base for current or future tax advice. We also have some other use cases in the works, but this would be the initial use case for at least 3 - 4 users for the first couple of months, along with some other similar workflows I can’t get into, but would also require a similar large knowledge base.&lt;/p&gt;\n\n&lt;p&gt;I also already have approval to purchase the server below and will be doing so this week, and I was planning to admin and manage with Proxmox, so if anyone has an opinion, let it be known haha. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.pugetsystems.com/products/rackmount-workstations/intel-rackstations/x141-5u/\"&gt;Configure a Xeon X141-5U | Puget Systems 1&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Xeon w9-3595x 60 core 2GHz (4.8 GHz Turbo)&lt;/li&gt;\n&lt;li&gt;512 GB DDR5-5600 ECC&lt;/li&gt;\n&lt;li&gt;4 x RTX PRO 6000 Blackwell Max-Q Workstation Edition 96Gb&lt;/li&gt;\n&lt;li&gt;2 x 8TB m.2 Gen4 SSD&lt;/li&gt;\n&lt;li&gt;2x 8TB Samsung 870 SSD&lt;/li&gt;\n&lt;li&gt;Total Cost - $54,266.94&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/17Qca_OMv_FHB2NIcaWcOs8RqktOnh_HNnpYTIDHYYM.png?auto=webp&amp;s=62290f4da204dff62b670897b7979b366a9f9218",
                  "width": 1011,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/17Qca_OMv_FHB2NIcaWcOs8RqktOnh_HNnpYTIDHYYM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3e30c563d3124989fb03b0a3fe7034cb4c0c2fb5",
                    "width": 108,
                    "height": 64
                  },
                  {
                    "url": "https://external-preview.redd.it/17Qca_OMv_FHB2NIcaWcOs8RqktOnh_HNnpYTIDHYYM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2fe401a4ef6f1392a3a56f507988cfce63848c5e",
                    "width": 216,
                    "height": 128
                  },
                  {
                    "url": "https://external-preview.redd.it/17Qca_OMv_FHB2NIcaWcOs8RqktOnh_HNnpYTIDHYYM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=020c7290be66a0eae17a23ee81614ddb63c4f9cf",
                    "width": 320,
                    "height": 189
                  },
                  {
                    "url": "https://external-preview.redd.it/17Qca_OMv_FHB2NIcaWcOs8RqktOnh_HNnpYTIDHYYM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f0356fd8d78208e407e5dfaff8e91c686f2b5b38",
                    "width": 640,
                    "height": 379
                  },
                  {
                    "url": "https://external-preview.redd.it/17Qca_OMv_FHB2NIcaWcOs8RqktOnh_HNnpYTIDHYYM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f875b9b7decfe0e49300b1782c3ab801b3938b3e",
                    "width": 960,
                    "height": 569
                  }
                ],
                "variants": {},
                "id": "17Qca_OMv_FHB2NIcaWcOs8RqktOnh_HNnpYTIDHYYM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbsxb3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DerpDeath",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbsxb3/enterprise_local_ai_implementation_for_small_user/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbsxb3/enterprise_local_ai_implementation_for_small_user/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753738010,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is asking for predictions. I guess you can interpret it to mean any open model, even if it needs a lot of RAM.",
          "author_fullname": "t2_sm168dt0h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "When will we be able to get gold on IMO using a local model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbmr8k",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753724161,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is asking for predictions. I guess you can interpret it to mean any open model, even if it needs a lot of RAM.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbmr8k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MrMrsPotts",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbmr8k/when_will_we_be_able_to_get_gold_on_imo_using_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbmr8k/when_will_we_be_able_to_get_gold_on_imo_using_a/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753724161,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am running Llama.cpp's Android wrapper, and i keep running into this issue. No matter how many things I've tried, the responses keep getting cut off. It is some kind of max token issue (when input is big, output gets cut off quicker and vice versa.) Needless to say, id love to be able to use it and get responses longer than just a few sentences. Any ideas of what might be stopping it?",
          "author_fullname": "t2_a3qdgbrt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama.cpp Android cutting off responses",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbsi46",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753737009,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am running Llama.cpp&amp;#39;s Android wrapper, and i keep running into this issue. No matter how many things I&amp;#39;ve tried, the responses keep getting cut off. It is some kind of max token issue (when input is big, output gets cut off quicker and vice versa.) Needless to say, id love to be able to use it and get responses longer than just a few sentences. Any ideas of what might be stopping it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbsi46",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Worth_Ad9031",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbsi46/llamacpp_android_cutting_off_responses/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbsi46/llamacpp_android_cutting_off_responses/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753737009,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The non-reasoning model is about as good as 2.5 flash with 4k reasoning tokens. The latency of no reasoning vs reasoning makes it so much better than 2.5 flash. I also prefer the shorter outputs than the verbose asf gemini. \n\nThe markdown formatting is so much better and the outputs are just so much nicer to read than flash. Knowledge wise, it's a bit worse than 2.5 flash but that's probably because it's smaller model. better at coding than flash too.  \n  \nrunning unsloth Q8. I haven't tried the thinking one yet. what do you guys think?",
          "author_fullname": "t2_askwa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B 2507 is so good",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mammv5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 318,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 318,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753623817,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The non-reasoning model is about as good as 2.5 flash with 4k reasoning tokens. The latency of no reasoning vs reasoning makes it so much better than 2.5 flash. I also prefer the shorter outputs than the verbose asf gemini. &lt;/p&gt;\n\n&lt;p&gt;The markdown formatting is so much better and the outputs are just so much nicer to read than flash. Knowledge wise, it&amp;#39;s a bit worse than 2.5 flash but that&amp;#39;s probably because it&amp;#39;s smaller model. better at coding than flash too.  &lt;/p&gt;\n\n&lt;p&gt;running unsloth Q8. I haven&amp;#39;t tried the thinking one yet. what do you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mammv5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "z_3454_pfk",
          "discussion_type": null,
          "num_comments": 88,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mammv5/qwen3235ba22b_2507_is_so_good/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mammv5/qwen3235ba22b_2507_is_so_good/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753623817,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Have picked up a piece of redundant hardware, Gigabyte GPU server with 8x2080ti in it, 2x Xeon 8160 and 384GB of ram.\n\nIt was a freebie so I have not spent anything on it... yet. I have played with local models on PC I am on now, with has RTX 3090 in it.\n\nTrying to work out the pros and cons, 1st of all it is a noisy b@stard, have it set up in the garage and I can still hear it from my study! Also thinking that running flat out with its 2x2KW PSUs it might be a tad costly.\n\nWondering whether to just move on or break it up and ebay it, then buy something a bit more practical? It does however keep stuff off my current build and I am assuming it will deliver a reasonale tk/s even on some chunkier models.",
          "author_fullname": "t2_5t7c1bs0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What do do with 88GB Vram GPU server",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbs6mj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753736773,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753736288,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have picked up a piece of redundant hardware, Gigabyte GPU server with 8x2080ti in it, 2x Xeon 8160 and 384GB of ram.&lt;/p&gt;\n\n&lt;p&gt;It was a freebie so I have not spent anything on it... yet. I have played with local models on PC I am on now, with has RTX 3090 in it.&lt;/p&gt;\n\n&lt;p&gt;Trying to work out the pros and cons, 1st of all it is a noisy b@stard, have it set up in the garage and I can still hear it from my study! Also thinking that running flat out with its 2x2KW PSUs it might be a tad costly.&lt;/p&gt;\n\n&lt;p&gt;Wondering whether to just move on or break it up and ebay it, then buy something a bit more practical? It does however keep stuff off my current build and I am assuming it will deliver a reasonale tk/s even on some chunkier models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbs6mj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "biffa773",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbs6mj/what_do_do_with_88gb_vram_gpu_server/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbs6mj/what_do_do_with_88gb_vram_gpu_server/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753736288,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_14lqxvy1qk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Byte-Vision is a privacy-first (Llama.cpp) document intelligence platform that transforms static documents into an interactive, searchable knowledge base. Built on Elasticsearch with RAG (Retrieval-Augmented Generation) capabilities, it offers document parsing, OCR processing, and modern UI.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mb2dcp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 42,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 42,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=4fd244957a130db419b6074f34a711a8f7259e0a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753663260,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/kbrisso/byte-vision",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?auto=webp&amp;s=68ee57c49a8451c63c200df64fb463ac5b026c9d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ca560c73715d7330212b1645381ce757ae0517c8",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=40f9eb891b537e50f5bd63d16a3678d31b33ac60",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=797d486098e995a54706fe4f140d3601cf369b67",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d7c803f441c5cf105e320d67c7290e56955a330",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=46ec34699b2c72770fe0cd6e134d5402ad10365c",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f1aed7829e91d539e27a1de8fe237d509505121",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mb2dcp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Important_Half_8277",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb2dcp/bytevision_is_a_privacyfirst_llamacpp_document/",
          "stickied": false,
          "url": "https://github.com/kbrisso/byte-vision",
          "subreddit_subscribers": 506190,
          "created_utc": 1753663260,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking for examples where smaller reputable models (Llama, Qwen, DeepSeek, …) are widely recognized as better - not just in benchmarks, but in broader evaluations for general tasks.\n\nI sometimes see claims that 70B-range models beat 300B+ ones, often based on benchmark results. But in practice or broader testing, the opposite often turns out to be true.\n\nI’m wondering if LLMs have reached a level of maturity where it’s now extremely unlikely for a smaller model to genuinely outperform one that’s twice its size or more.\n\nEdit: in terms of quality of the model answers (Response accuracy only), speed and VRAM requirements excluded.",
          "author_fullname": "t2_8u7n5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any examples of 14B+ reputable models that outperform models twice their size or more?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbc8tb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753701499,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753697315,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for examples where smaller reputable models (Llama, Qwen, DeepSeek, …) are widely recognized as better - not just in benchmarks, but in broader evaluations for general tasks.&lt;/p&gt;\n\n&lt;p&gt;I sometimes see claims that 70B-range models beat 300B+ ones, often based on benchmark results. But in practice or broader testing, the opposite often turns out to be true.&lt;/p&gt;\n\n&lt;p&gt;I’m wondering if LLMs have reached a level of maturity where it’s now extremely unlikely for a smaller model to genuinely outperform one that’s twice its size or more.&lt;/p&gt;\n\n&lt;p&gt;Edit: in terms of quality of the model answers (Response accuracy only), speed and VRAM requirements excluded.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbc8tb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thireus",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbc8tb/are_there_any_examples_of_14b_reputable_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbc8tb/are_there_any_examples_of_14b_reputable_models/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753697315,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, apologies if this question has been posted before i haven’t been able to find any concrete info on it. \n\nIn my area i can get 8 3060 12GBs for the exact same price as two 3090s, I’m looking to run LLMs, Heavy ComfyUI workflows, training models, LoRas and just about any other AI development haha.\n\nI’ve never ran anything on a 2x+-gpu set up, is doubling the VRAM even worth the effort and time setting up? (big home labber, i can figure it out)\n\nand are 3060s even fast enough to use those 96GB of vram effectively?\nwhat’s the better bang for the buck? prices are the EXACT same.",
          "author_fullname": "t2_8x8948uy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "2x RTX 3090 24GB or 8x 3060 12GB",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mb77c7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753678172,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, apologies if this question has been posted before i haven’t been able to find any concrete info on it. &lt;/p&gt;\n\n&lt;p&gt;In my area i can get 8 3060 12GBs for the exact same price as two 3090s, I’m looking to run LLMs, Heavy ComfyUI workflows, training models, LoRas and just about any other AI development haha.&lt;/p&gt;\n\n&lt;p&gt;I’ve never ran anything on a 2x+-gpu set up, is doubling the VRAM even worth the effort and time setting up? (big home labber, i can figure it out)&lt;/p&gt;\n\n&lt;p&gt;and are 3060s even fast enough to use those 96GB of vram effectively?\nwhat’s the better bang for the buck? prices are the EXACT same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mb77c7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "twotemp",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753678172,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone tested GLM-4.5 yet? Is it any good?",
          "author_fullname": "t2_8kbjrt7z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does anyone know what type of loss-free balance routing GLM-4.5 is using? Is it different than the aux loss free bias gating method deepseek models use or something new?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbkt69",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753719923,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone tested GLM-4.5 yet? Is it any good?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbkt69",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Euphoric_Ad9500",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbkt69/does_anyone_know_what_type_of_lossfree_balance/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbkt69/does_anyone_know_what_type_of_lossfree_balance/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753719923,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Long story short I've been tasked with identifying hosting options for a project, and both cloud hosting and buying hardware are available. I've been able to locate information on how much VRAM is needed to host models of given parameter counts and the rough cost of utilizing them for vanilla activity. (Parameter count \\*2 for FP16 + relevant token window, inference only, and then like KV Cache size, etc...) \n\n  \nI'm having a hard time trying to figure out the resource utilization for the various options in adding domain knowledge to a model, however. Say I utilize RAG to search through policy documents to refine a query before offering it to the model or say I want to fine tune a model, is there somewhere I can read up on the generalized costs? \n\n  \n",
          "author_fullname": "t2_zmeda",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I calculate hardware needs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbq7xx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753731833,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short I&amp;#39;ve been tasked with identifying hosting options for a project, and both cloud hosting and buying hardware are available. I&amp;#39;ve been able to locate information on how much VRAM is needed to host models of given parameter counts and the rough cost of utilizing them for vanilla activity. (Parameter count *2 for FP16 + relevant token window, inference only, and then like KV Cache size, etc...) &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having a hard time trying to figure out the resource utilization for the various options in adding domain knowledge to a model, however. Say I utilize RAG to search through policy documents to refine a query before offering it to the model or say I want to fine tune a model, is there somewhere I can read up on the generalized costs? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbq7xx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SkeletonShips",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbq7xx/how_do_i_calculate_hardware_needs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbq7xx/how_do_i_calculate_hardware_needs/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753731833,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm planning to run a local LLM for code analysis and modification. Specifically, I want to:  \n\\- Analyze and potentially modify a Python script with around 1000 lines of code  \n\\- Use a GPU with 24GB VRAM  \n  \nCan anyone share experience with:  \n\\- Approximate token/second generation speed  \n\\- Which models work best for code tasks (e.g., CodeLlama, WizardCoder)  \n\\- Recommended hardware configurations\n\n  \nThanks",
          "author_fullname": "t2_8tetfmez5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Performance Expectations for Local LLM with 24GB GPU - Code Analysis &amp; Modification",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbghx5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753710099,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m planning to run a local LLM for code analysis and modification. Specifically, I want to:&lt;br/&gt;\n- Analyze and potentially modify a Python script with around 1000 lines of code&lt;br/&gt;\n- Use a GPU with 24GB VRAM  &lt;/p&gt;\n\n&lt;p&gt;Can anyone share experience with:&lt;br/&gt;\n- Approximate token/second generation speed&lt;br/&gt;\n- Which models work best for code tasks (e.g., CodeLlama, WizardCoder)&lt;br/&gt;\n- Recommended hardware configurations&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbghx5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BarberPlane3020",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753710099,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here's my last post as [context](https://www.reddit.com/r/LocalLLaMA/comments/1m6ztb2/uiux_benchmark_update_722_newest_qwen_models/). Otherwise let's get to the exciting updates about [the benchmark](https://www.designarena.ai/). \n\n1. **50 Models:** I've lost track of the count, but since the benchmark began a little over a month ago, we've added over [50 models](https://www.designarena.ai/changelog) so far. In the past few days, we've added Imagen 4 Ultra from Google, Qwen3-235B-A22B-Thinking-2507, Ideogram 3.0, and UIGen X 32B. We're trying to add new models everyday, so let us know what you would like to see here or on our [Discord](https://discord.com/channels/1390777934218006580/1396581263305084998). I think we've gotten most of people's requests (expect some of the GLM models which I WILL add, sorry I just keep forgetting).   \n  \n2. **UIGEN:** Our friends developing the [UIGen](https://huggingface.co/Tesslate) are developing some killer open-source models for frontend dev, and we've added a couple of their models to the benchmark, though inference is quite slow. It would be great if anyone knows of any good inference providers or could request provider support on HuggingFace. \n\n3. **Humanity:** This feature is still experimental and in beta, but we want to add a [human baseline](https://www.designarena.ai/humanity) to the benchmark (similar to ARC-AGI) where models are compared to designs and work from people. Users submit an image of a design or code (keep it to HTML/CSS/JS to be consistent with models), and then those designs (after a short review process to ensure there's not spam) and code are compared (anonymously) to model generations. \n\n4. **Voice**. Well UI/UX is our primary focus, our goal is to generally evaluate how models perform on all kinds of qualitative aspects that are hard to measure deterministically (e.g. such as how well models might hold or resemble a human conversation, debate, etc.). As a beta feature, we've added a [voice category](https://www.reddit.com/r/LocalLLaMA/submit/?type=IMAGE) where 2 voice models will have a conversation about a prompt you provide, and then you can choose which model you liked better. There are still some bugs to sort out with this feature, but would appreciate any feedback on this. \n\n5. **New Models on the Horizon?** After the Qwen releases last week, there's some buzz that we might see some model drops over the next week. We'll be keeping a watchful eye and attempting to get those models (whenever they come out) on Design Arena as fast as possible. \n\nLet us know if you have any feedback or questions! ",
          "author_fullname": "t2_98ouo03z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "UI/UX Benchmark Update 7/27: 50 Models, Humanity, Voice, and new models from an AI lab on the horizon?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 88,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "3ntkg11btiff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/3ntkg11btiff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cdc049d0e3e83cb0b37a10e46727defc44194713"
                },
                {
                  "y": 136,
                  "x": 216,
                  "u": "https://preview.redd.it/3ntkg11btiff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fbecd568f42a719069aef7d0c6d6103eebbc8f90"
                },
                {
                  "y": 201,
                  "x": 320,
                  "u": "https://preview.redd.it/3ntkg11btiff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d85d65444804683a7f87e2275c6b4fa5f1710d49"
                },
                {
                  "y": 403,
                  "x": 640,
                  "u": "https://preview.redd.it/3ntkg11btiff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1153cbcfbbeda10972d06fed0cf50ab98d80ba83"
                },
                {
                  "y": 605,
                  "x": 960,
                  "u": "https://preview.redd.it/3ntkg11btiff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3c9afe29da6e6fa46d3d0a3e28cfc09f4a09d616"
                }
              ],
              "s": {
                "y": 658,
                "x": 1043,
                "u": "https://preview.redd.it/3ntkg11btiff1.png?width=1043&amp;format=png&amp;auto=webp&amp;s=61e66384be02386e74efe6f5e35d2c4dfe1832fd"
              },
              "id": "3ntkg11btiff1"
            },
            "2wn47bxwuiff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 114,
                  "x": 108,
                  "u": "https://preview.redd.it/2wn47bxwuiff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=223cc86d6dc341041278baf0fb03e074fc186133"
                },
                {
                  "y": 229,
                  "x": 216,
                  "u": "https://preview.redd.it/2wn47bxwuiff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c29d4c2bcc163422f6868297376e7fa55f9a11d4"
                },
                {
                  "y": 339,
                  "x": 320,
                  "u": "https://preview.redd.it/2wn47bxwuiff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0608e133418e0d5033586067b8ba53d5e564f49"
                },
                {
                  "y": 679,
                  "x": 640,
                  "u": "https://preview.redd.it/2wn47bxwuiff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=173193caf3a6e27eeccf85d96f0d55b0035f533d"
                }
              ],
              "s": {
                "y": 687,
                "x": 647,
                "u": "https://preview.redd.it/2wn47bxwuiff1.png?width=647&amp;format=png&amp;auto=webp&amp;s=bb3fe5aaf3d5a841155cf6fa30e68e4f80abdd17"
              },
              "id": "2wn47bxwuiff1"
            }
          },
          "name": "t3_1mb3xi3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 25,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "3ntkg11btiff1",
                "id": 715558162
              },
              {
                "media_id": "2wn47bxwuiff1",
                "id": 715558163
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ZEny5wZEwSnVbrdPI5YELoeX21mbA0nHIa9_2IoeNNo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753667880,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s my last post as &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1m6ztb2/uiux_benchmark_update_722_newest_qwen_models/\"&gt;context&lt;/a&gt;. Otherwise let&amp;#39;s get to the exciting updates about &lt;a href=\"https://www.designarena.ai/\"&gt;the benchmark&lt;/a&gt;. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;50 Models:&lt;/strong&gt; I&amp;#39;ve lost track of the count, but since the benchmark began a little over a month ago, we&amp;#39;ve added over &lt;a href=\"https://www.designarena.ai/changelog\"&gt;50 models&lt;/a&gt; so far. In the past few days, we&amp;#39;ve added Imagen 4 Ultra from Google, Qwen3-235B-A22B-Thinking-2507, Ideogram 3.0, and UIGen X 32B. We&amp;#39;re trying to add new models everyday, so let us know what you would like to see here or on our &lt;a href=\"https://discord.com/channels/1390777934218006580/1396581263305084998\"&gt;Discord&lt;/a&gt;. I think we&amp;#39;ve gotten most of people&amp;#39;s requests (expect some of the GLM models which I WILL add, sorry I just keep forgetting).   &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;UIGEN:&lt;/strong&gt; Our friends developing the &lt;a href=\"https://huggingface.co/Tesslate\"&gt;UIGen&lt;/a&gt; are developing some killer open-source models for frontend dev, and we&amp;#39;ve added a couple of their models to the benchmark, though inference is quite slow. It would be great if anyone knows of any good inference providers or could request provider support on HuggingFace. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Humanity:&lt;/strong&gt; This feature is still experimental and in beta, but we want to add a &lt;a href=\"https://www.designarena.ai/humanity\"&gt;human baseline&lt;/a&gt; to the benchmark (similar to ARC-AGI) where models are compared to designs and work from people. Users submit an image of a design or code (keep it to HTML/CSS/JS to be consistent with models), and then those designs (after a short review process to ensure there&amp;#39;s not spam) and code are compared (anonymously) to model generations. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Voice&lt;/strong&gt;. Well UI/UX is our primary focus, our goal is to generally evaluate how models perform on all kinds of qualitative aspects that are hard to measure deterministically (e.g. such as how well models might hold or resemble a human conversation, debate, etc.). As a beta feature, we&amp;#39;ve added a &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/submit/?type=IMAGE\"&gt;voice category&lt;/a&gt; where 2 voice models will have a conversation about a prompt you provide, and then you can choose which model you liked better. There are still some bugs to sort out with this feature, but would appreciate any feedback on this. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;New Models on the Horizon?&lt;/strong&gt; After the Qwen releases last week, there&amp;#39;s some buzz that we might see some model drops over the next week. We&amp;#39;ll be keeping a watchful eye and attempting to get those models (whenever they come out) on Design Arena as fast as possible. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Let us know if you have any feedback or questions! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mb3xi3",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mb3xi3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Accomplished-Copy332",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb3xi3/uiux_benchmark_update_727_50_models_humanity/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mb3xi3",
          "subreddit_subscribers": 506190,
          "created_utc": 1753667880,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Everyone is struggling looking at documentation, and I struggled writing this a whole week and some findings. wanted to share what I learned.\n\nTwo weeks ago I thought I'd wrap up our documentation in a weekend. One week later I finally understood why great docs are so rare. What started as a \"quick cleanup\" turned into a complete rebuild.\n\n**Understand your users:** I began by writing a traditional quickstart guide: how to build an AI agent from scratch with observability. Seems logical right? Wrong. Most of our customers aren't starting from zero. They're looking for stuff like \"how to integrate with my existing Next.js\" or \"does this work with my current OpenAI setup?\" So I wrote a quickstart to help users go directly to the page they want before they start coding.\n\n**Make it systematic and scalable:** I checked our previous integration pages. We have Python/JS guides in one dropdown, OpenAI/Anthropic in another, features in a third, all at the same level. This approach created massive repetition across pages and became impossible to maintain. It was like writing hardcoded functions instead of reusable components. When someone needed \"feature X with Python and OpenAI\" they'd find examples everywhere and struggle to redirect to the actual page they expected.\n\n**Have an intention for how users should use them:** I always think you shouldn't just list all features and options without a preference. You need to first have a clear mind about what you want them to see. Every page is a feature, every link is user flow, and every search result is a conversion opportunity. You can't predict how users will navigate your docs so you need to build multiple pathways to the same information.\n\nFinally I pushed this 90% done documentation to production. There's still a long way to go but you can't ship products when you're 100% ready.\n\nI know there's still a lot of problems for this doc. I'm building an AI observability tool, please share your thoughts on how I could improve this if you're interested. (links in the comments or just search keywords ai docs)\n\nWould be really helpful to know what people think of it!",
          "author_fullname": "t2_1pnlpczpqa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Everyone is struggling about documentation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbpoy9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753730634,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everyone is struggling looking at documentation, and I struggled writing this a whole week and some findings. wanted to share what I learned.&lt;/p&gt;\n\n&lt;p&gt;Two weeks ago I thought I&amp;#39;d wrap up our documentation in a weekend. One week later I finally understood why great docs are so rare. What started as a &amp;quot;quick cleanup&amp;quot; turned into a complete rebuild.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Understand your users:&lt;/strong&gt; I began by writing a traditional quickstart guide: how to build an AI agent from scratch with observability. Seems logical right? Wrong. Most of our customers aren&amp;#39;t starting from zero. They&amp;#39;re looking for stuff like &amp;quot;how to integrate with my existing Next.js&amp;quot; or &amp;quot;does this work with my current OpenAI setup?&amp;quot; So I wrote a quickstart to help users go directly to the page they want before they start coding.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Make it systematic and scalable:&lt;/strong&gt; I checked our previous integration pages. We have Python/JS guides in one dropdown, OpenAI/Anthropic in another, features in a third, all at the same level. This approach created massive repetition across pages and became impossible to maintain. It was like writing hardcoded functions instead of reusable components. When someone needed &amp;quot;feature X with Python and OpenAI&amp;quot; they&amp;#39;d find examples everywhere and struggle to redirect to the actual page they expected.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Have an intention for how users should use them:&lt;/strong&gt; I always think you shouldn&amp;#39;t just list all features and options without a preference. You need to first have a clear mind about what you want them to see. Every page is a feature, every link is user flow, and every search result is a conversion opportunity. You can&amp;#39;t predict how users will navigate your docs so you need to build multiple pathways to the same information.&lt;/p&gt;\n\n&lt;p&gt;Finally I pushed this 90% done documentation to production. There&amp;#39;s still a long way to go but you can&amp;#39;t ship products when you&amp;#39;re 100% ready.&lt;/p&gt;\n\n&lt;p&gt;I know there&amp;#39;s still a lot of problems for this doc. I&amp;#39;m building an AI observability tool, please share your thoughts on how I could improve this if you&amp;#39;re interested. (links in the comments or just search keywords ai docs)&lt;/p&gt;\n\n&lt;p&gt;Would be really helpful to know what people think of it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbpoy9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Main-Fisherman-2075",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbpoy9/everyone_is_struggling_about_documentation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbpoy9/everyone_is_struggling_about_documentation/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753730634,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I found a React SDK that turns LLM responses into interactive UIs rendered live, on the spot.\n\nIt uses the concept of \"Generative UI\" which allows the interface to assemble itself dynamically for each user. The system gathers context &amp; AI uses an existing library of UI elements (so it doesn't hallucinate).\n\nUnder the hood, it uses:\n\na) **C1 API**: OpenAI-compatible (same `endpoints/params`) backend that returns a JSON-based UI spec from any prompt.\n\nYou can call it with any OpenAI client (JS or Python SDK), just by pointing your `baseURL` to `https://api.thesys.dev/v1/embed`.\n\nIf you already have an LLM pipeline (chatbot/agent), you can take its output and pass it to C1 as a second step, just to generate a visual layout.\n\nb) **GenUI SDK** (frontend): framework that takes the spec and renders it using pre-built components.\n\nYou can then call `client.chat.completions.create({...})` with your messages. Using the special model name (such as `\"c1/anthropic/claude-sonnet-4/v-20250617\"`), the Thesys API will invoke the LLM and return a UI spec.\n\ndetailed writeup: [here](https://www.thesys.dev/blogs/how-to-build-generative-ui-applications)  \ndemos: [here](https://demo.thesys.dev/)  \ndocs: [here](https://docs.thesys.dev/welcome)\n\nThe concept seems very exciting to me but still I can understand the risks. What do you think?",
          "author_fullname": "t2_1hro18widg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Found a React SDK that turns LLM responses into real-time UI that adapts based on context",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbp7nh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.45,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753729565,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found a React SDK that turns LLM responses into interactive UIs rendered live, on the spot.&lt;/p&gt;\n\n&lt;p&gt;It uses the concept of &amp;quot;Generative UI&amp;quot; which allows the interface to assemble itself dynamically for each user. The system gathers context &amp;amp; AI uses an existing library of UI elements (so it doesn&amp;#39;t hallucinate).&lt;/p&gt;\n\n&lt;p&gt;Under the hood, it uses:&lt;/p&gt;\n\n&lt;p&gt;a) &lt;strong&gt;C1 API&lt;/strong&gt;: OpenAI-compatible (same &lt;code&gt;endpoints/params&lt;/code&gt;) backend that returns a JSON-based UI spec from any prompt.&lt;/p&gt;\n\n&lt;p&gt;You can call it with any OpenAI client (JS or Python SDK), just by pointing your &lt;code&gt;baseURL&lt;/code&gt; to &lt;code&gt;https://api.thesys.dev/v1/embed&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;If you already have an LLM pipeline (chatbot/agent), you can take its output and pass it to C1 as a second step, just to generate a visual layout.&lt;/p&gt;\n\n&lt;p&gt;b) &lt;strong&gt;GenUI SDK&lt;/strong&gt; (frontend): framework that takes the spec and renders it using pre-built components.&lt;/p&gt;\n\n&lt;p&gt;You can then call &lt;code&gt;client.chat.completions.create({...})&lt;/code&gt; with your messages. Using the special model name (such as &lt;code&gt;&amp;quot;c1/anthropic/claude-sonnet-4/v-20250617&amp;quot;&lt;/code&gt;), the Thesys API will invoke the LLM and return a UI spec.&lt;/p&gt;\n\n&lt;p&gt;detailed writeup: &lt;a href=\"https://www.thesys.dev/blogs/how-to-build-generative-ui-applications\"&gt;here&lt;/a&gt;&lt;br/&gt;\ndemos: &lt;a href=\"https://demo.thesys.dev/\"&gt;here&lt;/a&gt;&lt;br/&gt;\ndocs: &lt;a href=\"https://docs.thesys.dev/welcome\"&gt;here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The concept seems very exciting to me but still I can understand the risks. What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbp7nh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "anmolbaranwal",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbp7nh/found_a_react_sdk_that_turns_llm_responses_into/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbp7nh/found_a_react_sdk_that_turns_llm_responses_into/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753729565,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "People who have hosted LLMs using vLLM, what approach did you guys take?\nListing down some approaches that I am considering. Would like to understand the associated complexity involved, ease of scaling for more models, more production loads, etc.\n\n1. Ec2 (considering g5.xlarge) with ASG\n2. Using k8s \n3. Using frameworks like Anyscale, anything llm, autogen, bentoml etc. (Using AWS is compulsory)\n4. Using integrations like kubeai, kuberay etc.\n\nThe frameworks and integrations are from vLLM docs under deployment. I am not much aware of what they exactly solve for but would like to understand if anyone of you have used those tools.\n",
          "author_fullname": "t2_3nk0ww7f",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hosting LLM using vLLM for production",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbf9a9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753706925,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;People who have hosted LLMs using vLLM, what approach did you guys take?\nListing down some approaches that I am considering. Would like to understand the associated complexity involved, ease of scaling for more models, more production loads, etc.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Ec2 (considering g5.xlarge) with ASG&lt;/li&gt;\n&lt;li&gt;Using k8s &lt;/li&gt;\n&lt;li&gt;Using frameworks like Anyscale, anything llm, autogen, bentoml etc. (Using AWS is compulsory)&lt;/li&gt;\n&lt;li&gt;Using integrations like kubeai, kuberay etc.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The frameworks and integrations are from vLLM docs under deployment. I am not much aware of what they exactly solve for but would like to understand if anyone of you have used those tools.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbf9a9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "everyoneisodd",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbf9a9/hosting_llm_using_vllm_for_production/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbf9a9/hosting_llm_using_vllm_for_production/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753706925,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We’re a small team building **FastFlowLM** — a fast, runtime for running **LLaMA, Qwen, DeepSeek**, and other models **entirely on the AMD Ryzen AI NPU**. No CPU or iGPU fallback — just lean, efficient, **NPU-native inference**. Think **Ollama**, but purpose-built and deeply optimized for AMD NPUs — with both **CLI** and **server mode (REST API)**.\n\n# Key Features\n\n* Supports **LLaMA, Qwen, DeepSeek**, and more\n* **Deeply hardware-optimized**, NPU-only inference\n* **Full context** support (e.g., 128K for LLaMA)\n* Over **11× power efficiency** compared to iGPU/CPU\n\nWe’re iterating quickly and would **love your feedback, critiques, and ideas**.\n\n# Try It Out\n\n* **GitHub:** [github.com/FastFlowLM/FastFlowLM](https://github.com/FastFlowLM/FastFlowLM)\n* **Live Demo (on remote machine):** Don’t have a Ryzen AI PC? Instantly try FastFlowLM on a **remote AMD Ryzen AI 5 340 NPU system with 32 GB RAM** — no installation needed. [Launch Demo](https://open-webui.testdrive-fastflowlm.com/) **Login:** `guest@flm.npu` **Password:** `0000`\n* **YouTube Demos:** [youtube.com/@FastFlowLM-YT](https://www.youtube.com/@FastFlowLM-YT) *→ Quick start guide, performance benchmarks, and comparisons vs Ollama / LM Studio / Lemonade*\n* **Discord Community:** [discord.gg/Sze3Qsv5](https://discord.gg/Sze3Qsv5) *→ Join us to ask questions, report issues, or contribute ideas*\n\nLet us know what works, what breaks, and what you’d love to see next!",
          "author_fullname": "t2_jrsbr6os",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running LLMs exclusively on AMD Ryzen AI NPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mao95d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 171,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 171,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753630907,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753627953,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We’re a small team building &lt;strong&gt;FastFlowLM&lt;/strong&gt; — a fast, runtime for running &lt;strong&gt;LLaMA, Qwen, DeepSeek&lt;/strong&gt;, and other models &lt;strong&gt;entirely on the AMD Ryzen AI NPU&lt;/strong&gt;. No CPU or iGPU fallback — just lean, efficient, &lt;strong&gt;NPU-native inference&lt;/strong&gt;. Think &lt;strong&gt;Ollama&lt;/strong&gt;, but purpose-built and deeply optimized for AMD NPUs — with both &lt;strong&gt;CLI&lt;/strong&gt; and &lt;strong&gt;server mode (REST API)&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;h1&gt;Key Features&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Supports &lt;strong&gt;LLaMA, Qwen, DeepSeek&lt;/strong&gt;, and more&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Deeply hardware-optimized&lt;/strong&gt;, NPU-only inference&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Full context&lt;/strong&gt; support (e.g., 128K for LLaMA)&lt;/li&gt;\n&lt;li&gt;Over &lt;strong&gt;11× power efficiency&lt;/strong&gt; compared to iGPU/CPU&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We’re iterating quickly and would &lt;strong&gt;love your feedback, critiques, and ideas&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;h1&gt;Try It Out&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href=\"https://github.com/FastFlowLM/FastFlowLM\"&gt;github.com/FastFlowLM/FastFlowLM&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Live Demo (on remote machine):&lt;/strong&gt; Don’t have a Ryzen AI PC? Instantly try FastFlowLM on a &lt;strong&gt;remote AMD Ryzen AI 5 340 NPU system with 32 GB RAM&lt;/strong&gt; — no installation needed. &lt;a href=\"https://open-webui.testdrive-fastflowlm.com/\"&gt;Launch Demo&lt;/a&gt; &lt;strong&gt;Login:&lt;/strong&gt; &lt;code&gt;guest@flm.npu&lt;/code&gt; &lt;strong&gt;Password:&lt;/strong&gt; &lt;code&gt;0000&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;YouTube Demos:&lt;/strong&gt; &lt;a href=\"https://www.youtube.com/@FastFlowLM-YT\"&gt;youtube.com/@FastFlowLM-YT&lt;/a&gt; &lt;em&gt;→ Quick start guide, performance benchmarks, and comparisons vs Ollama / LM Studio / Lemonade&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Discord Community:&lt;/strong&gt; &lt;a href=\"https://discord.gg/Sze3Qsv5\"&gt;discord.gg/Sze3Qsv5&lt;/a&gt; &lt;em&gt;→ Join us to ask questions, report issues, or contribute ideas&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Let us know what works, what breaks, and what you’d love to see next!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/vJGRc2UlTJrSFHnGlJYDN0YsOLC8w4mlAwQVmF6tcgo.png?auto=webp&amp;s=4a11abd914fb5ab749f3f093b10ce2b529fb8c8e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/vJGRc2UlTJrSFHnGlJYDN0YsOLC8w4mlAwQVmF6tcgo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=97afc3fc381198ec693e0055e6c72c2c0c3cad84",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/vJGRc2UlTJrSFHnGlJYDN0YsOLC8w4mlAwQVmF6tcgo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=223eb6c47aa4922185402abdd994f0d4167b8587",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/vJGRc2UlTJrSFHnGlJYDN0YsOLC8w4mlAwQVmF6tcgo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c855da9485d7673105d65ccede1e3da883ab9dcb",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/vJGRc2UlTJrSFHnGlJYDN0YsOLC8w4mlAwQVmF6tcgo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5391c68c3aa09eb5da1c87bd1883d8712981e33",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/vJGRc2UlTJrSFHnGlJYDN0YsOLC8w4mlAwQVmF6tcgo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=76ac02010d805630644eabca91f54e1df087cc14",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/vJGRc2UlTJrSFHnGlJYDN0YsOLC8w4mlAwQVmF6tcgo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=68dfc8aed1bea82afaeda0f80b40e2d9c22407bb",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "vJGRc2UlTJrSFHnGlJYDN0YsOLC8w4mlAwQVmF6tcgo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mao95d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BandEnvironmental834",
          "discussion_type": null,
          "num_comments": 119,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mao95d/running_llms_exclusively_on_amd_ryzen_ai_npu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mao95d/running_llms_exclusively_on_amd_ryzen_ai_npu/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753627953,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Repo: [https://github.com/JC1DA/Neutral\\_Summarizer](https://github.com/JC1DA/Neutral_Summarizer)  \nIt was built using Cline + Qwen3-coder\n\nHope it will be useful to some people :)",
          "author_fullname": "t2_gp3kfk8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Vibe-coded Webpage-summarizer Chrome extension to leverage OSS models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 97,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "gvflbu67vkff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 75,
                  "x": 108,
                  "u": "https://preview.redd.it/gvflbu67vkff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d741dfb7598a08aefa2ab0fefae9b0dd33a7a9dc"
                },
                {
                  "y": 151,
                  "x": 216,
                  "u": "https://preview.redd.it/gvflbu67vkff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=74244cb1ba445493a54d0b6ec233c61e68edf28b"
                },
                {
                  "y": 223,
                  "x": 320,
                  "u": "https://preview.redd.it/gvflbu67vkff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bbac34d9620df04727676c9047db922103cd74c2"
                },
                {
                  "y": 447,
                  "x": 640,
                  "u": "https://preview.redd.it/gvflbu67vkff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e2025449e623e8e2bc4f371049864604c1b6bc50"
                },
                {
                  "y": 671,
                  "x": 960,
                  "u": "https://preview.redd.it/gvflbu67vkff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6db9be0b98331b2c0e8d45804d67d8b07e724d19"
                },
                {
                  "y": 755,
                  "x": 1080,
                  "u": "https://preview.redd.it/gvflbu67vkff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9c840f2c542c03f47cc1e4b2d256d3ccb9381794"
                }
              ],
              "s": {
                "y": 2160,
                "x": 3089,
                "u": "https://preview.redd.it/gvflbu67vkff1.png?width=3089&amp;format=png&amp;auto=webp&amp;s=84e31be97e337c3e22f79bf74204fd75e17dbc3c"
              },
              "id": "gvflbu67vkff1"
            },
            "xtke1u98vkff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 75,
                  "x": 108,
                  "u": "https://preview.redd.it/xtke1u98vkff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=82e70012bee654800d2d1437ce7316fa407d5db2"
                },
                {
                  "y": 151,
                  "x": 216,
                  "u": "https://preview.redd.it/xtke1u98vkff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4cf9ac3f29ae60a33171180c4978035b81c0e23b"
                },
                {
                  "y": 223,
                  "x": 320,
                  "u": "https://preview.redd.it/xtke1u98vkff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=135ea9d9846dc805f89f32a1eaed46b3fd39bf10"
                },
                {
                  "y": 447,
                  "x": 640,
                  "u": "https://preview.redd.it/xtke1u98vkff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=55fdc005a229864d4396862760b9f44c9e9de4d2"
                },
                {
                  "y": 671,
                  "x": 960,
                  "u": "https://preview.redd.it/xtke1u98vkff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=89873a2a5ded1c3b78a8b9cd50dcc07a594faa5c"
                },
                {
                  "y": 755,
                  "x": 1080,
                  "u": "https://preview.redd.it/xtke1u98vkff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e7e5e72c587d3bea05635486207ea35dd35b1db6"
                }
              ],
              "s": {
                "y": 2160,
                "x": 3089,
                "u": "https://preview.redd.it/xtke1u98vkff1.png?width=3089&amp;format=png&amp;auto=webp&amp;s=cd7e7734a117d3b4e401f21389ce071d6e8468b9"
              },
              "id": "xtke1u98vkff1"
            },
            "fsy80mn7vkff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 75,
                  "x": 108,
                  "u": "https://preview.redd.it/fsy80mn7vkff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a38bda6b1947ebc3dfa518068b70a07bfa05dde2"
                },
                {
                  "y": 151,
                  "x": 216,
                  "u": "https://preview.redd.it/fsy80mn7vkff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=484259efeebdeb78aecb98eac9526106abbebd1b"
                },
                {
                  "y": 223,
                  "x": 320,
                  "u": "https://preview.redd.it/fsy80mn7vkff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=295fa90947073e7c1aeda08ee3be8f02825d50ab"
                },
                {
                  "y": 447,
                  "x": 640,
                  "u": "https://preview.redd.it/fsy80mn7vkff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a488b4d9a5275460db6f155a2dfce3a5effdba1e"
                },
                {
                  "y": 671,
                  "x": 960,
                  "u": "https://preview.redd.it/fsy80mn7vkff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=425c27f313d2c4115b699f666fd66f26c0600b52"
                },
                {
                  "y": 755,
                  "x": 1080,
                  "u": "https://preview.redd.it/fsy80mn7vkff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=57ecd3d27771afc68c42c464f9de84d77e08c366"
                }
              ],
              "s": {
                "y": 2160,
                "x": 3089,
                "u": "https://preview.redd.it/fsy80mn7vkff1.png?width=3089&amp;format=png&amp;auto=webp&amp;s=dc338d0be9c7d4174a2d9ef0bb067da80b8fe65d"
              },
              "id": "fsy80mn7vkff1"
            }
          },
          "name": "t3_1mbaxqj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "ups": 6,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "gvflbu67vkff1",
                "id": 715724432
              },
              {
                "media_id": "fsy80mn7vkff1",
                "id": 715724433
              },
              {
                "media_id": "xtke1u98vkff1",
                "id": 715724434
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/annDDrAE_Le8qeMwAGDZYj60CDaz9fKZrTa7ovJ2TVw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753692331,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Repo: &lt;a href=\"https://github.com/JC1DA/Neutral_Summarizer\"&gt;https://github.com/JC1DA/Neutral_Summarizer&lt;/a&gt;&lt;br/&gt;\nIt was built using Cline + Qwen3-coder&lt;/p&gt;\n\n&lt;p&gt;Hope it will be useful to some people :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mbaxqj",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbaxqj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JC1DA",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbaxqj/vibecoded_webpagesummarizer_chrome_extension_to/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mbaxqj",
          "subreddit_subscribers": 506190,
          "created_utc": 1753692331,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey yall, I built an opensource AI Model Router that automatically picks the best AI provider (OpenAI, Anthropic, Google, local), model, and settings for your prompts. No more guessing between openai Claude, or Gemini!\n\nFeedback welcome!",
          "author_fullname": "t2_5gpifn7q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Opensource: The AI Model Router - Automating AI Model Selection",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbcwek",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=f160a8dbb6239851a23b15bb7ffa05ed622766fc",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753699692,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey yall, I built an opensource AI Model Router that automatically picks the best AI provider (OpenAI, Anthropic, Google, local), model, and settings for your prompts. No more guessing between openai Claude, or Gemini!&lt;/p&gt;\n\n&lt;p&gt;Feedback welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/MonkWarrior08/Model_Router",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?auto=webp&amp;s=fc338c0157bede926870ccb47aed508a93663712",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e0eae7298df9a75056291706e27eb55423947f5a",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d3621d1857dd5414d1345fb4fef0fc90de77fbf",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=547ca1f47dd7cb3cde9647c607af1349cf5913a7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=142b9f59812a7af5da3822cb118e31ad38a1664b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7bcddb4321ebd29c3a4e8af3164058cc8071c779",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=512358a85e36e15fb3f2da027213d2137fa3483d",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbcwek",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Idonotknow101",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbcwek/opensource_the_ai_model_router_automating_ai/",
          "stickied": false,
          "url": "https://github.com/MonkWarrior08/Model_Router",
          "subreddit_subscribers": 506190,
          "created_utc": 1753699692,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "BackGround: I developed a new FFN architecture called Parallel-FFN, with the primary goal of improving parameter efficiency in Transformer models.\n\nExperimental Setup:\n\n1. Transformer Integration: Replaced standard FFN components with Parallel-FFN architecture\n2. LLM Evaluation: Substituted SwiGLU components in large language models with Parallel-FFN\n3. Baseline Comparison: Measured performance against original architectures\n\nResults:\n\n* Parameter Efficiency: Successfully achieved equivalent loss with 35% parameter reduction compared to SwiGLU baseline\n* Performance: Maintained comparable model performance across evaluations\n* Inference Speed: Initial implementation showed slower inference than baseline, but recent optimizations suggest we can achieve parity\n\nCurrent Status:\n\n* Architecture optimization is ongoing to match baseline inference speeds\n* Focus remains on maximizing parameter efficiency rather than raw speed\n\nLimitations:\n\n* Inference speed optimization still in progress\n* Limited evaluation on diverse model scales\n* Need more comprehensive benchmarking\n\nDiscussion: Has anyone worked on similar parameter-efficient FFN variants? I'm curious about related approaches and potential collaboration opportunities.\n\nhttps://preview.redd.it/ppm5feuhulff1.png?width=956&amp;format=png&amp;auto=webp&amp;s=44a72d5f3294be0b1e271e42f314bb49deae1ce5\n\n",
          "author_fullname": "t2_tcjic8rca",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[R] Parallel-FFN: Parameter-Efficient FFN Architecture with 35% Parameter Reduction",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 18,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ppm5feuhulff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 14,
                  "x": 108,
                  "u": "https://preview.redd.it/ppm5feuhulff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=431a73ea5b33ee42b3b2ebd677bf8456f0a6f872"
                },
                {
                  "y": 28,
                  "x": 216,
                  "u": "https://preview.redd.it/ppm5feuhulff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=142ff5fb1369bbfb1a9243608aa64b70d3f8d7de"
                },
                {
                  "y": 41,
                  "x": 320,
                  "u": "https://preview.redd.it/ppm5feuhulff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f9eff0b26842d39120871db9b9f587b3c678f108"
                },
                {
                  "y": 83,
                  "x": 640,
                  "u": "https://preview.redd.it/ppm5feuhulff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=848953d7c9753556894825b3322f0d22c671507c"
                }
              ],
              "s": {
                "y": 124,
                "x": 956,
                "u": "https://preview.redd.it/ppm5feuhulff1.png?width=956&amp;format=png&amp;auto=webp&amp;s=44a72d5f3294be0b1e271e42f314bb49deae1ce5"
              },
              "id": "ppm5feuhulff1"
            }
          },
          "name": "t3_1mbe9p9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/pq1fl_YakjaNbSRNI7EsABe2R6p78F2jXchiT74DLRQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753704074,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;BackGround: I developed a new FFN architecture called Parallel-FFN, with the primary goal of improving parameter efficiency in Transformer models.&lt;/p&gt;\n\n&lt;p&gt;Experimental Setup:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Transformer Integration: Replaced standard FFN components with Parallel-FFN architecture&lt;/li&gt;\n&lt;li&gt;LLM Evaluation: Substituted SwiGLU components in large language models with Parallel-FFN&lt;/li&gt;\n&lt;li&gt;Baseline Comparison: Measured performance against original architectures&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Results:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Parameter Efficiency: Successfully achieved equivalent loss with 35% parameter reduction compared to SwiGLU baseline&lt;/li&gt;\n&lt;li&gt;Performance: Maintained comparable model performance across evaluations&lt;/li&gt;\n&lt;li&gt;Inference Speed: Initial implementation showed slower inference than baseline, but recent optimizations suggest we can achieve parity&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Current Status:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Architecture optimization is ongoing to match baseline inference speeds&lt;/li&gt;\n&lt;li&gt;Focus remains on maximizing parameter efficiency rather than raw speed&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Limitations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Inference speed optimization still in progress&lt;/li&gt;\n&lt;li&gt;Limited evaluation on diverse model scales&lt;/li&gt;\n&lt;li&gt;Need more comprehensive benchmarking&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Discussion: Has anyone worked on similar parameter-efficient FFN variants? I&amp;#39;m curious about related approaches and potential collaboration opportunities.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ppm5feuhulff1.png?width=956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44a72d5f3294be0b1e271e42f314bb49deae1ce5\"&gt;https://preview.redd.it/ppm5feuhulff1.png?width=956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44a72d5f3294be0b1e271e42f314bb49deae1ce5&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbe9p9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Perfect_Power815",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbe9p9/r_parallelffn_parameterefficient_ffn_architecture/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbe9p9/r_parallelffn_parameterefficient_ffn_architecture/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753704074,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I really love the fact that I can have both a SOTA reasoning AND instruct model variant off of one singular model. I can essentially deploy 2 models with 2 use cases with the cost of one models vram. With /think for difficult problems and /no_think for easier problems, essentially we can experience a best from both worlds. \n\nRecently Qwen released updated fine tunes of their SOTA models however they removed the hybrid reasoning functions, meaning that we no longer have the best of both worlds. \n\nIf I want a model with reasoning and non reasoning now I need twice the amount of vram to deploy both. Which for vram poor people, it ain’t really ideal.\n\nI feel that qwen should focus back at releasing hybrid reasoning models. Hbu?",
          "author_fullname": "t2_a06q0mmx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hybrid Reasoning Models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbdn26",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 405B"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753702098,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really love the fact that I can have both a SOTA reasoning AND instruct model variant off of one singular model. I can essentially deploy 2 models with 2 use cases with the cost of one models vram. With /think for difficult problems and /no_think for easier problems, essentially we can experience a best from both worlds. &lt;/p&gt;\n\n&lt;p&gt;Recently Qwen released updated fine tunes of their SOTA models however they removed the hybrid reasoning functions, meaning that we no longer have the best of both worlds. &lt;/p&gt;\n\n&lt;p&gt;If I want a model with reasoning and non reasoning now I need twice the amount of vram to deploy both. Which for vram poor people, it ain’t really ideal.&lt;/p&gt;\n\n&lt;p&gt;I feel that qwen should focus back at releasing hybrid reasoning models. Hbu?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 405B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbdn26",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MichaelXie4645",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mbdn26/hybrid_reasoning_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbdn26/hybrid_reasoning_models/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753702098,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was trying to play around with a local to do list maker and gemma3 showed some very strange behavior  \nit mentioned me giving it command that I never gave it, like sending an email to john\n\nWhy do you think it did this????\n\n  \nfor details,  \nI primed it with this  \n\"I will give you tasks and I want you to collect what I give you and organize all the tasks into a markdown format to-do-list\"\n\nfollowing are the screenshots of my code and conversation\n\nhttps://preview.redd.it/aif5o5x09nff1.png?width=1027&amp;format=png&amp;auto=webp&amp;s=f54aa4016fcbd589a4a8d2b327101d1d8d7c7f12\n\nhttps://preview.redd.it/1onvhlu49nff1.png?width=1678&amp;format=png&amp;auto=webp&amp;s=2b7c8997a97af8f630f8862a5088e68e2c55811d\n\nhttps://preview.redd.it/corga8x79nff1.png?width=1267&amp;format=png&amp;auto=webp&amp;s=286f5d4228b4a2ac62efc8eb9ee305912173d265\n\nhttps://preview.redd.it/9bplz5o99nff1.png?width=1267&amp;format=png&amp;auto=webp&amp;s=e82ef1ec09f8cf9a2a8d75599bf333e2dfaade28\n\n",
          "author_fullname": "t2_sueuyekl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Very odd behavior by gemma3 in Ollama",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 79,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "corga8x79nff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 34,
                  "x": 108,
                  "u": "https://preview.redd.it/corga8x79nff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5edf29c04d4df384d74435186edbd01c70421bae"
                },
                {
                  "y": 68,
                  "x": 216,
                  "u": "https://preview.redd.it/corga8x79nff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a63b8e3226e0352687fa2101bd43869aa1447c82"
                },
                {
                  "y": 101,
                  "x": 320,
                  "u": "https://preview.redd.it/corga8x79nff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cff0381701cddb6a3c774d6036332331f95c5d36"
                },
                {
                  "y": 202,
                  "x": 640,
                  "u": "https://preview.redd.it/corga8x79nff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8f025a469d621dc0bc7672da2d84045567c33e61"
                },
                {
                  "y": 303,
                  "x": 960,
                  "u": "https://preview.redd.it/corga8x79nff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=34e6497840ebc4bc3f368ba28985569401788c1a"
                },
                {
                  "y": 340,
                  "x": 1080,
                  "u": "https://preview.redd.it/corga8x79nff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3d9fa66d034a2a8e6f99e03ca60ae9861fe4fa5d"
                }
              ],
              "s": {
                "y": 400,
                "x": 1267,
                "u": "https://preview.redd.it/corga8x79nff1.png?width=1267&amp;format=png&amp;auto=webp&amp;s=286f5d4228b4a2ac62efc8eb9ee305912173d265"
              },
              "id": "corga8x79nff1"
            },
            "aif5o5x09nff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 61,
                  "x": 108,
                  "u": "https://preview.redd.it/aif5o5x09nff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=592730861dcf67b212cae9cad275c470cd719d2e"
                },
                {
                  "y": 122,
                  "x": 216,
                  "u": "https://preview.redd.it/aif5o5x09nff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1815a5aa7cbd61b7eb91ed5e8840705f66339b25"
                },
                {
                  "y": 181,
                  "x": 320,
                  "u": "https://preview.redd.it/aif5o5x09nff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5a7f12252bdc9e49e3c5a186afbdf0aa60e5735"
                },
                {
                  "y": 363,
                  "x": 640,
                  "u": "https://preview.redd.it/aif5o5x09nff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6858df3cf2ac6a24bb55e88eb7cbbdf77296786"
                },
                {
                  "y": 545,
                  "x": 960,
                  "u": "https://preview.redd.it/aif5o5x09nff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bf3586f0b3e741a59ba3fe3e5bf62a7397b516c3"
                }
              ],
              "s": {
                "y": 584,
                "x": 1027,
                "u": "https://preview.redd.it/aif5o5x09nff1.png?width=1027&amp;format=png&amp;auto=webp&amp;s=f54aa4016fcbd589a4a8d2b327101d1d8d7c7f12"
              },
              "id": "aif5o5x09nff1"
            },
            "9bplz5o99nff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 34,
                  "x": 108,
                  "u": "https://preview.redd.it/9bplz5o99nff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=887d3f8e9be65833a4b47b9042ce740c72a9523d"
                },
                {
                  "y": 68,
                  "x": 216,
                  "u": "https://preview.redd.it/9bplz5o99nff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5264bc730db579812313c2dadc72cd6af89430bf"
                },
                {
                  "y": 101,
                  "x": 320,
                  "u": "https://preview.redd.it/9bplz5o99nff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2ac37b468bc1ad3b46a9523e654ee3fff4592a7f"
                },
                {
                  "y": 202,
                  "x": 640,
                  "u": "https://preview.redd.it/9bplz5o99nff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=091a7b83d1f83bc269de1fb3e0c4e45c2e6744b5"
                },
                {
                  "y": 303,
                  "x": 960,
                  "u": "https://preview.redd.it/9bplz5o99nff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bee4248c5db1c1351b8ddbd7b0a3c969f2695f60"
                },
                {
                  "y": 340,
                  "x": 1080,
                  "u": "https://preview.redd.it/9bplz5o99nff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ee32c6f4425004a3702979106341a2c07f81f218"
                }
              ],
              "s": {
                "y": 400,
                "x": 1267,
                "u": "https://preview.redd.it/9bplz5o99nff1.png?width=1267&amp;format=png&amp;auto=webp&amp;s=e82ef1ec09f8cf9a2a8d75599bf333e2dfaade28"
              },
              "id": "9bplz5o99nff1"
            },
            "1onvhlu49nff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 57,
                  "x": 108,
                  "u": "https://preview.redd.it/1onvhlu49nff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f0933361f64613f655acf29dcc2997b501c755d"
                },
                {
                  "y": 115,
                  "x": 216,
                  "u": "https://preview.redd.it/1onvhlu49nff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=032a14e97beba96a4d5525b072e5295ba49e2135"
                },
                {
                  "y": 171,
                  "x": 320,
                  "u": "https://preview.redd.it/1onvhlu49nff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eada462af9864e36267f1293717a09577cb38441"
                },
                {
                  "y": 342,
                  "x": 640,
                  "u": "https://preview.redd.it/1onvhlu49nff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=aac67cb7ea4eaab60fc4374807bdd84d7ee94cad"
                },
                {
                  "y": 514,
                  "x": 960,
                  "u": "https://preview.redd.it/1onvhlu49nff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b24a64fd43824efc6952029298220fb9852f5f65"
                },
                {
                  "y": 578,
                  "x": 1080,
                  "u": "https://preview.redd.it/1onvhlu49nff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=210a4f022299d7ea8b9a38254869d8f7871f1ec2"
                }
              ],
              "s": {
                "y": 899,
                "x": 1678,
                "u": "https://preview.redd.it/1onvhlu49nff1.png?width=1678&amp;format=png&amp;auto=webp&amp;s=2b7c8997a97af8f630f8862a5088e68e2c55811d"
              },
              "id": "1onvhlu49nff1"
            }
          },
          "name": "t3_1mblcrd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/enW0eP6pZi5mFIDzCVrmM2VVCg6zm-rvnes1sSj1epI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753721125,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was trying to play around with a local to do list maker and gemma3 showed some very strange behavior&lt;br/&gt;\nit mentioned me giving it command that I never gave it, like sending an email to john&lt;/p&gt;\n\n&lt;p&gt;Why do you think it did this????&lt;/p&gt;\n\n&lt;p&gt;for details,&lt;br/&gt;\nI primed it with this&lt;br/&gt;\n&amp;quot;I will give you tasks and I want you to collect what I give you and organize all the tasks into a markdown format to-do-list&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;following are the screenshots of my code and conversation&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/aif5o5x09nff1.png?width=1027&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f54aa4016fcbd589a4a8d2b327101d1d8d7c7f12\"&gt;https://preview.redd.it/aif5o5x09nff1.png?width=1027&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f54aa4016fcbd589a4a8d2b327101d1d8d7c7f12&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1onvhlu49nff1.png?width=1678&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b7c8997a97af8f630f8862a5088e68e2c55811d\"&gt;https://preview.redd.it/1onvhlu49nff1.png?width=1678&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b7c8997a97af8f630f8862a5088e68e2c55811d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/corga8x79nff1.png?width=1267&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=286f5d4228b4a2ac62efc8eb9ee305912173d265\"&gt;https://preview.redd.it/corga8x79nff1.png?width=1267&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=286f5d4228b4a2ac62efc8eb9ee305912173d265&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/9bplz5o99nff1.png?width=1267&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e82ef1ec09f8cf9a2a8d75599bf333e2dfaade28\"&gt;https://preview.redd.it/9bplz5o99nff1.png?width=1267&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e82ef1ec09f8cf9a2a8d75599bf333e2dfaade28&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mblcrd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Individual_Try9645",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mblcrd/very_odd_behavior_by_gemma3_in_ollama/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mblcrd/very_odd_behavior_by_gemma3_in_ollama/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753721125,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, what would you guys recommend is the best option right now for something like that? My goal is to have both options in the same model. ",
          "author_fullname": "t2_tyag8i2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I’m looking for multimodal image input support and uncensored LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbl79y",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753720797,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, what would you guys recommend is the best option right now for something like that? My goal is to have both options in the same model. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbl79y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NotSoCleverAlternate",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbl79y/im_looking_for_multimodal_image_input_support_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbl79y/im_looking_for_multimodal_image_input_support_and/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753720797,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In my impression, the focus is mostly on MCP, A2A, and RAG. While these are great for their respective use cases, you still have to send prompts to LLMs with 70 to 500 billion parameters, which is quite resource-intensive and expensive. The alternative is to settle for one of the smaller LLMs with around 8 billion parameters, but then the experience can feel too inconsistent. In search of a solution, I recently stumbled upon LoRA, which to my understanding, allows you to use a smaller LLM as a base and fine-tune it to become an expert in very specific topics. This results in a model that’s lighter and faster to run, with output that’s comparable (in a specific domain) to that of a 500-billion-parameter model. If that’s the case, why hasn’t there been more noticeable interest in fine-tuning with LoRA? I can imagine this could save a lot of money for businesses planning to build systems that rely on LLMs for constant inference.",
          "author_fullname": "t2_12y48q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why hasn't LoRA gained more popularity?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maq0hg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 93,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 93,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753632201,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my impression, the focus is mostly on MCP, A2A, and RAG. While these are great for their respective use cases, you still have to send prompts to LLMs with 70 to 500 billion parameters, which is quite resource-intensive and expensive. The alternative is to settle for one of the smaller LLMs with around 8 billion parameters, but then the experience can feel too inconsistent. In search of a solution, I recently stumbled upon LoRA, which to my understanding, allows you to use a smaller LLM as a base and fine-tune it to become an expert in very specific topics. This results in a model that’s lighter and faster to run, with output that’s comparable (in a specific domain) to that of a 500-billion-parameter model. If that’s the case, why hasn’t there been more noticeable interest in fine-tuning with LoRA? I can imagine this could save a lot of money for businesses planning to build systems that rely on LLMs for constant inference.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1maq0hg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dabomb007",
          "discussion_type": null,
          "num_comments": 60,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753632201,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Saw the following math question on YT and decided to give it a try with different models. Results are somehow unexpected.\n\nQuestion: There are three circles of radius 1, 2 and 3 tangent to each other. Find the area enclosed by their touching arcs.  \nCorrect answer: 0.464256\n\no4-min - correct  \nQwen3-235B-A22B-Thinknig-2507 - correct  \nQwen3-235B-A22B-Instruct-2507 - incorrect (5.536)  \nQwen3-32B - incorrect (5.536)  \nKimi-K2 - correct  \nDeepSeek-V3-0324 correct  \nDeepSeek-R1-0528 and Nemotron-Super-49B both gave the same incorrect answer (0.7358)\n\nAll models were used from their respective providers. It seems that models that failed had the right answer in their COT in one way or another, but failed to understand what they were asked in terms of actual geometry. The answer 5.536 is actually the sum of segments' area and is one step away from the right answer, which is 6 - 5.536 = 0.464. There are several unexpected results for me here:\n\n1. DeepSeek-R1 overthought the problem and managed to fail this fairly simple question although in COT it had the correct idea how to calculate: it as an area of triangle formed be center of circles minus areas of segments of each circle inside triangle.\n2. Kimi-K2 and DeepSeek-V3-0324 are very smart even without reasoning.\n3. Nemotron reasoning comes from DeepSeek distilation process.\n4. Qwen3-235B-A22B-Instruct-2507 output was so long as if it was a thinking model.\n5. Qwen3-32B is very capable model for its size, but you should go through all its COT to see if the right answer is burred somewhere there.\n\nOverall, based on these observations I think the right way to approach an analytical problem is to use first capable non-reasoning model and if it fails use capable thinking model then.\n\nPS: I am not a native speaker and may be the problem is in my formulation of the question. Still smart models understood what I really meant.",
          "author_fullname": "t2_63q8kong",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Model vibe checking with a simple math question.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbf4wo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753713284,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753706581,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Saw the following math question on YT and decided to give it a try with different models. Results are somehow unexpected.&lt;/p&gt;\n\n&lt;p&gt;Question: There are three circles of radius 1, 2 and 3 tangent to each other. Find the area enclosed by their touching arcs.&lt;br/&gt;\nCorrect answer: 0.464256&lt;/p&gt;\n\n&lt;p&gt;o4-min - correct&lt;br/&gt;\nQwen3-235B-A22B-Thinknig-2507 - correct&lt;br/&gt;\nQwen3-235B-A22B-Instruct-2507 - incorrect (5.536)&lt;br/&gt;\nQwen3-32B - incorrect (5.536)&lt;br/&gt;\nKimi-K2 - correct&lt;br/&gt;\nDeepSeek-V3-0324 correct&lt;br/&gt;\nDeepSeek-R1-0528 and Nemotron-Super-49B both gave the same incorrect answer (0.7358)&lt;/p&gt;\n\n&lt;p&gt;All models were used from their respective providers. It seems that models that failed had the right answer in their COT in one way or another, but failed to understand what they were asked in terms of actual geometry. The answer 5.536 is actually the sum of segments&amp;#39; area and is one step away from the right answer, which is 6 - 5.536 = 0.464. There are several unexpected results for me here:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;DeepSeek-R1 overthought the problem and managed to fail this fairly simple question although in COT it had the correct idea how to calculate: it as an area of triangle formed be center of circles minus areas of segments of each circle inside triangle.&lt;/li&gt;\n&lt;li&gt;Kimi-K2 and DeepSeek-V3-0324 are very smart even without reasoning.&lt;/li&gt;\n&lt;li&gt;Nemotron reasoning comes from DeepSeek distilation process.&lt;/li&gt;\n&lt;li&gt;Qwen3-235B-A22B-Instruct-2507 output was so long as if it was a thinking model.&lt;/li&gt;\n&lt;li&gt;Qwen3-32B is very capable model for its size, but you should go through all its COT to see if the right answer is burred somewhere there.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Overall, based on these observations I think the right way to approach an analytical problem is to use first capable non-reasoning model and if it fails use capable thinking model then.&lt;/p&gt;\n\n&lt;p&gt;PS: I am not a native speaker and may be the problem is in my formulation of the question. Still smart models understood what I really meant.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbf4wo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "perelmanych",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbf4wo/model_vibe_checking_with_a_simple_math_question/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbf4wo/model_vibe_checking_with_a_simple_math_question/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753706581,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "TL;DR A local language model is like a mini-brain for your computer. It’s trained to understand and generate text, like answering questions or writing essays. Unlike online AI (like ChatGPT), local LLMs don’t need a cloud server—you run them directly on your machine. But to do this, you need to know about **model size**, **context**, and **hardware**.\n\n# 1. Model Size: How Big Is the Brain?\n\nThe “size” of an LLM is measured in **parameters**, which are like the brain cells of the model. More parameters mean a smarter model, but it also needs a more powerful computer. Let’s look at the three main size categories:\n\n* **Small Models (1–3 billion parameters):**These are like tiny, efficient brains. They don’t need much power and can run on most laptops.**Example:** Imagine a small model as a basic calculator—it’s great for simple tasks like answering short questions or summarizing a paragraph. A model like LLaMA 3B (3 billion parameters) needs only about **4 GB of GPU memory** (VRAM) and **8 GB of regular computer memory** (RAM). If your laptop has 8–16 GB of RAM, you can run this model. This is how llama 3.2 running on my MacBook Air M1 8GB RAM:\\[video\\]**Real-world use:** Writing short emails, summarizing or answering basic questions like, “What’s the capital of France?”\n* **Medium Models (7–13 billion parameters):**These are like a high-school student’s brain—smarter, but they need a better computer.**Example:** A medium model like LLaMA 8B (8 billion parameters) needs about **12 GB of VRAM** and **16 GB of RAM**. This is like needing a gaming PC with a good graphics card (like an NVIDIA RTX 3090). It can handle more complex tasks, like writing a short story or analyzing a document.**Real-world use:** Creating a blog post or helping with homework.\n* **Large Models (30+ billion parameters):**These are like genius-level brains, but they need super-powerful computers.**Example:** A huge model like LLaMA 70B (70 billion parameters) might need **48 GB of VRAM** (like two high-end GPUs) and **64 GB of RAM**. This is like needing a fancy workstation, not a regular PC. These models are great for advanced tasks, but most people can’t run them at home.**Real-world use:** Writing a detailed research paper or analyzing massive datasets.\n\n**Simple Rule:** The bigger the model, the more “thinking power” it has, but it needs a stronger computer. A small model is fine for basic tasks, while larger models are for heavy-duty work.\n\n# 2. Context Window: How Much Can the Model “Remember”?\n\nThe **context window** is how much text the model can “think about” at once. Think of it like the model’s short-term memory. It’s measured in **tokens** (a token is roughly a word or part of a word). A bigger context window lets the model remember more, but it uses a lot more memory.\n\n* **Example:** If you’re chatting with an AI and it can only “remember” 2,048 tokens (about 1,500 words), it might forget the start of a long conversation. But if it has a 16,384-token context (about 12,000 words), it can keep track of a much longer discussion.\n   * A 2,048-token context might use **0.7 GB of GPU memory**.\n   * A 16,384-token context could jump to **46 GB of GPU memory**—way more!\n\n**Why It Matters:** If you only need short answers (like a quick fact), use a small context to save memory. But if you’re summarizing a long article, you’ll need a bigger context, which requires a stronger computer.\n\n**Simple Rule:** Keep the context window small unless you need the model to remember a lot of text. Bigger context = more memory needed.\n\n# 3. Hardware: What Kind of Computer Do You Need?\n\nTo run a local LLM, your computer needs two key things:\n\n* **GPU VRAM** (video memory on your graphics card, if you have one).\n* **System RAM** (regular computer memory).\n\nHere’s a simple guide to match your hardware to the right model:\n\n* **Basic Laptop (8 GB VRAM, 16 GB RAM):**You can run **small models** (1–3 billion parameters).**Example:** A typical laptop with a mid-range GPU (4–6 GB VRAM) can handle a 3B model for simple tasks like answering questions or writing short texts.\n* **Gaming PC (12–16 GB VRAM, 32 GB RAM):**You can run **medium models** (7–13 billion parameters).**Example:** A PC with a high-performance GPU (12 GB VRAM) can run an 8B model to write stories or assist with coding.\n* **High-End Setup (24–48 GB VRAM, 64 GB RAM):**You can run **large models** (30+ billion parameters), but optimization techniques may be required (I will explain further in the next part).**Example:** A workstation with two high-end GPUs (24 GB VRAM each) can handle a 70B model for advanced tasks like research or complex analysis.\n\n**Simple Rule:** Check your computer’s VRAM and RAM to pick the right model. If you don’t have a powerful GPU, stick to smaller models.\n\n# 4. Tricks to Run Bigger Models on Smaller Computers\n\nEven if your computer isn’t super powerful, you can use some clever tricks to run bigger models:\n\n* **Quantization:** This is like compressing a big file to make it smaller. It reduces the model’s memory needs by using less precise math.**Example:** A 70B model normally needs **140 GB of VRAM**, but with 4-bit quantization, it might only need **35 GB**. That’s still a lot, but it’s much more doable on a good gaming PC.\n* **Free Up Memory:** Close other programs (like games or browsers) to give your GPU more room to work.**Example:** If your GPU has 12 GB of VRAM, make sure at least 10–11 GB is free for the model to run smoothly.\n* **Smaller Context and Batch Size:** Use a smaller context window or fewer tasks at once to save memory.**Example:** If you’re just asking for a quick answer, set the context to 2,048 tokens instead of 16,384 to save VRAM.\n\n**Simple Rule:** Quantization is like magic—it lets you run bigger models on smaller computers! For a step-by-step guide on how to do this, I found this tutorial super helpful from Hugging Face: [https://huggingface.co/docs/transformers/v4.53.3/quantization/overview](https://huggingface.co/docs/transformers/v4.53.3/quantization/overview)\n\n# 5. How to Choose the Right Model for You\n\nHere’s a quick guide to pick the best model for your computer:\n\n* **Basic Laptop (8 GB VRAM, 16 GB RAM):** Choose a **1–3B model**. It’s perfect for simple tasks like answering questions or writing short texts.**Example Task:** Ask the model, “Write a 100-word story about a cat.”\n* **Gaming PC (12–16 GB VRAM, 32 GB RAM):** Go for a **7–13B model**. These are great for more complex tasks like writing essays or coding.**Example Task:** Ask the model, “Write a Python program to calculate my monthly budget.”\n* **High-End PC (24–48 GB VRAM, 64 GB RAM):** Try a **30B+ model** with quantization. These are for heavy tasks like research or big projects.**Example Task:** Ask the model, “Analyze this 10-page report and summarize it in 500 words.”\n\nIf your computer isn’t strong enough for a big model, you can also use **cloud services** (ChatGPT, Claude, Grok, Google Gemini, etc.) for large models.\n\n# Final Thoughts\n\nRunning a local language model is like having your own personal AI assistant on your computer. By understanding model size, context window, and your computer’s hardware, you can pick the right model for your needs. Start small if you’re new, and use tricks like quantization to get more out of your setup.\n\n**Pro Tip:** Always leave a bit of extra VRAM and RAM free, as models can slow down if your computer is stretched to its limit. Happy AI experimenting!",
          "author_fullname": "t2_1hxjrpz5s8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Understanding Local Language Models: A Beginner’s Guide",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbc9d3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753697376,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR A local language model is like a mini-brain for your computer. It’s trained to understand and generate text, like answering questions or writing essays. Unlike online AI (like ChatGPT), local LLMs don’t need a cloud server—you run them directly on your machine. But to do this, you need to know about &lt;strong&gt;model size&lt;/strong&gt;, &lt;strong&gt;context&lt;/strong&gt;, and &lt;strong&gt;hardware&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;h1&gt;1. Model Size: How Big Is the Brain?&lt;/h1&gt;\n\n&lt;p&gt;The “size” of an LLM is measured in &lt;strong&gt;parameters&lt;/strong&gt;, which are like the brain cells of the model. More parameters mean a smarter model, but it also needs a more powerful computer. Let’s look at the three main size categories:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Small Models (1–3 billion parameters):&lt;/strong&gt;These are like tiny, efficient brains. They don’t need much power and can run on most laptops.&lt;strong&gt;Example:&lt;/strong&gt; Imagine a small model as a basic calculator—it’s great for simple tasks like answering short questions or summarizing a paragraph. A model like LLaMA 3B (3 billion parameters) needs only about &lt;strong&gt;4 GB of GPU memory&lt;/strong&gt; (VRAM) and &lt;strong&gt;8 GB of regular computer memory&lt;/strong&gt; (RAM). If your laptop has 8–16 GB of RAM, you can run this model. This is how llama 3.2 running on my MacBook Air M1 8GB RAM:[video]&lt;strong&gt;Real-world use:&lt;/strong&gt; Writing short emails, summarizing or answering basic questions like, “What’s the capital of France?”&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Medium Models (7–13 billion parameters):&lt;/strong&gt;These are like a high-school student’s brain—smarter, but they need a better computer.&lt;strong&gt;Example:&lt;/strong&gt; A medium model like LLaMA 8B (8 billion parameters) needs about &lt;strong&gt;12 GB of VRAM&lt;/strong&gt; and &lt;strong&gt;16 GB of RAM&lt;/strong&gt;. This is like needing a gaming PC with a good graphics card (like an NVIDIA RTX 3090). It can handle more complex tasks, like writing a short story or analyzing a document.&lt;strong&gt;Real-world use:&lt;/strong&gt; Creating a blog post or helping with homework.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Large Models (30+ billion parameters):&lt;/strong&gt;These are like genius-level brains, but they need super-powerful computers.&lt;strong&gt;Example:&lt;/strong&gt; A huge model like LLaMA 70B (70 billion parameters) might need &lt;strong&gt;48 GB of VRAM&lt;/strong&gt; (like two high-end GPUs) and &lt;strong&gt;64 GB of RAM&lt;/strong&gt;. This is like needing a fancy workstation, not a regular PC. These models are great for advanced tasks, but most people can’t run them at home.&lt;strong&gt;Real-world use:&lt;/strong&gt; Writing a detailed research paper or analyzing massive datasets.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Simple Rule:&lt;/strong&gt; The bigger the model, the more “thinking power” it has, but it needs a stronger computer. A small model is fine for basic tasks, while larger models are for heavy-duty work.&lt;/p&gt;\n\n&lt;h1&gt;2. Context Window: How Much Can the Model “Remember”?&lt;/h1&gt;\n\n&lt;p&gt;The &lt;strong&gt;context window&lt;/strong&gt; is how much text the model can “think about” at once. Think of it like the model’s short-term memory. It’s measured in &lt;strong&gt;tokens&lt;/strong&gt; (a token is roughly a word or part of a word). A bigger context window lets the model remember more, but it uses a lot more memory.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; If you’re chatting with an AI and it can only “remember” 2,048 tokens (about 1,500 words), it might forget the start of a long conversation. But if it has a 16,384-token context (about 12,000 words), it can keep track of a much longer discussion.\n\n&lt;ul&gt;\n&lt;li&gt;A 2,048-token context might use &lt;strong&gt;0.7 GB of GPU memory&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;A 16,384-token context could jump to &lt;strong&gt;46 GB of GPU memory&lt;/strong&gt;—way more!&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt; If you only need short answers (like a quick fact), use a small context to save memory. But if you’re summarizing a long article, you’ll need a bigger context, which requires a stronger computer.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Simple Rule:&lt;/strong&gt; Keep the context window small unless you need the model to remember a lot of text. Bigger context = more memory needed.&lt;/p&gt;\n\n&lt;h1&gt;3. Hardware: What Kind of Computer Do You Need?&lt;/h1&gt;\n\n&lt;p&gt;To run a local LLM, your computer needs two key things:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;GPU VRAM&lt;/strong&gt; (video memory on your graphics card, if you have one).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;System RAM&lt;/strong&gt; (regular computer memory).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here’s a simple guide to match your hardware to the right model:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Basic Laptop (8 GB VRAM, 16 GB RAM):&lt;/strong&gt;You can run &lt;strong&gt;small models&lt;/strong&gt; (1–3 billion parameters).&lt;strong&gt;Example:&lt;/strong&gt; A typical laptop with a mid-range GPU (4–6 GB VRAM) can handle a 3B model for simple tasks like answering questions or writing short texts.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Gaming PC (12–16 GB VRAM, 32 GB RAM):&lt;/strong&gt;You can run &lt;strong&gt;medium models&lt;/strong&gt; (7–13 billion parameters).&lt;strong&gt;Example:&lt;/strong&gt; A PC with a high-performance GPU (12 GB VRAM) can run an 8B model to write stories or assist with coding.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High-End Setup (24–48 GB VRAM, 64 GB RAM):&lt;/strong&gt;You can run &lt;strong&gt;large models&lt;/strong&gt; (30+ billion parameters), but optimization techniques may be required (I will explain further in the next part).&lt;strong&gt;Example:&lt;/strong&gt; A workstation with two high-end GPUs (24 GB VRAM each) can handle a 70B model for advanced tasks like research or complex analysis.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Simple Rule:&lt;/strong&gt; Check your computer’s VRAM and RAM to pick the right model. If you don’t have a powerful GPU, stick to smaller models.&lt;/p&gt;\n\n&lt;h1&gt;4. Tricks to Run Bigger Models on Smaller Computers&lt;/h1&gt;\n\n&lt;p&gt;Even if your computer isn’t super powerful, you can use some clever tricks to run bigger models:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Quantization:&lt;/strong&gt; This is like compressing a big file to make it smaller. It reduces the model’s memory needs by using less precise math.&lt;strong&gt;Example:&lt;/strong&gt; A 70B model normally needs &lt;strong&gt;140 GB of VRAM&lt;/strong&gt;, but with 4-bit quantization, it might only need &lt;strong&gt;35 GB&lt;/strong&gt;. That’s still a lot, but it’s much more doable on a good gaming PC.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Free Up Memory:&lt;/strong&gt; Close other programs (like games or browsers) to give your GPU more room to work.&lt;strong&gt;Example:&lt;/strong&gt; If your GPU has 12 GB of VRAM, make sure at least 10–11 GB is free for the model to run smoothly.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Smaller Context and Batch Size:&lt;/strong&gt; Use a smaller context window or fewer tasks at once to save memory.&lt;strong&gt;Example:&lt;/strong&gt; If you’re just asking for a quick answer, set the context to 2,048 tokens instead of 16,384 to save VRAM.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Simple Rule:&lt;/strong&gt; Quantization is like magic—it lets you run bigger models on smaller computers! For a step-by-step guide on how to do this, I found this tutorial super helpful from Hugging Face: &lt;a href=\"https://huggingface.co/docs/transformers/v4.53.3/quantization/overview\"&gt;https://huggingface.co/docs/transformers/v4.53.3/quantization/overview&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;5. How to Choose the Right Model for You&lt;/h1&gt;\n\n&lt;p&gt;Here’s a quick guide to pick the best model for your computer:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Basic Laptop (8 GB VRAM, 16 GB RAM):&lt;/strong&gt; Choose a &lt;strong&gt;1–3B model&lt;/strong&gt;. It’s perfect for simple tasks like answering questions or writing short texts.&lt;strong&gt;Example Task:&lt;/strong&gt; Ask the model, “Write a 100-word story about a cat.”&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Gaming PC (12–16 GB VRAM, 32 GB RAM):&lt;/strong&gt; Go for a &lt;strong&gt;7–13B model&lt;/strong&gt;. These are great for more complex tasks like writing essays or coding.&lt;strong&gt;Example Task:&lt;/strong&gt; Ask the model, “Write a Python program to calculate my monthly budget.”&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High-End PC (24–48 GB VRAM, 64 GB RAM):&lt;/strong&gt; Try a &lt;strong&gt;30B+ model&lt;/strong&gt; with quantization. These are for heavy tasks like research or big projects.&lt;strong&gt;Example Task:&lt;/strong&gt; Ask the model, “Analyze this 10-page report and summarize it in 500 words.”&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If your computer isn’t strong enough for a big model, you can also use &lt;strong&gt;cloud services&lt;/strong&gt; (ChatGPT, Claude, Grok, Google Gemini, etc.) for large models.&lt;/p&gt;\n\n&lt;h1&gt;Final Thoughts&lt;/h1&gt;\n\n&lt;p&gt;Running a local language model is like having your own personal AI assistant on your computer. By understanding model size, context window, and your computer’s hardware, you can pick the right model for your needs. Start small if you’re new, and use tricks like quantization to get more out of your setup.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Pro Tip:&lt;/strong&gt; Always leave a bit of extra VRAM and RAM free, as models can slow down if your computer is stretched to its limit. Happy AI experimenting!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/jfeVG47nZdEkz9kXfW1CcS-Sy8l4DXGb9JErx6bLKfU.png?auto=webp&amp;s=c01e883ee537960058800f2638c9fc359f14ba1e",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/jfeVG47nZdEkz9kXfW1CcS-Sy8l4DXGb9JErx6bLKfU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4c76a863977e105532ff0253418287f7ceba9902",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/jfeVG47nZdEkz9kXfW1CcS-Sy8l4DXGb9JErx6bLKfU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5852b5463bc5666831cd45b7163303a5681c5486",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/jfeVG47nZdEkz9kXfW1CcS-Sy8l4DXGb9JErx6bLKfU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=38fd2e738957aa266cc68c58c41d5c083143549d",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/jfeVG47nZdEkz9kXfW1CcS-Sy8l4DXGb9JErx6bLKfU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b7832903a05bd4e7088e86c4cad258a027216112",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/jfeVG47nZdEkz9kXfW1CcS-Sy8l4DXGb9JErx6bLKfU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bf88787050d888147ef934602b7d03444cccc2bc",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/jfeVG47nZdEkz9kXfW1CcS-Sy8l4DXGb9JErx6bLKfU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4c75b1f1c8ca3884566bdefe2a49aa5b0e0d73cb",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "jfeVG47nZdEkz9kXfW1CcS-Sy8l4DXGb9JErx6bLKfU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbc9d3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "120-dev",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbc9d3/understanding_local_language_models_a_beginners/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbc9d3/understanding_local_language_models_a_beginners/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753697376,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=123b8a4dcb375d14ed980880bb55304b8133c96f\n\nhttps://preview.redd.it/5t6qakxf6eff1.png?width=2004&amp;format=png&amp;auto=webp&amp;s=02c934cd0bc6fbd428a6f1c46e1214db800d83c3\n\n[https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct](https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct)\n\n[https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker](https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker)",
          "author_fullname": "t2_xdw24u3am",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A new 21B-A3B model that can run 30 token/s on i9 CPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "5t6qakxf6eff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 42,
                  "x": 108,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b7aee7f5ee4d636cc350980d4a2d402a51990f3"
                },
                {
                  "y": 84,
                  "x": 216,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=81d1bfcb2fbab04e033467fedce9b775306d0816"
                },
                {
                  "y": 125,
                  "x": 320,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=38b4e75cce0962953603e8bda734369a2c829748"
                },
                {
                  "y": 251,
                  "x": 640,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=de21ca82e374cabea65b7bd4f9ca7695eb3ac76c"
                },
                {
                  "y": 376,
                  "x": 960,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3f335ee62d4874ca26b9e6e0098a8851c4538afa"
                },
                {
                  "y": 423,
                  "x": 1080,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=95699afb27e3aa99fcc75ef6aa805969b9e66cf1"
                }
              ],
              "s": {
                "y": 786,
                "x": 2004,
                "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=2004&amp;format=png&amp;auto=webp&amp;s=02c934cd0bc6fbd428a6f1c46e1214db800d83c3"
              },
              "id": "5t6qakxf6eff1"
            },
            "e54liysd6eff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a7f5f991e9ce2c009935e1248f2782a5bdd1a201"
                },
                {
                  "y": 120,
                  "x": 216,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b615b9ee2e041c4be64619a07d886e58e3e07f63"
                },
                {
                  "y": 178,
                  "x": 320,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=805558e5004842b4caccc53ce524f721763d8ea2"
                },
                {
                  "y": 356,
                  "x": 640,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b5259fd259cf94ad371181ed4eb21af07c6bc94"
                },
                {
                  "y": 534,
                  "x": 960,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=24b113fb84316d200bddffca613f26e4f2877925"
                },
                {
                  "y": 601,
                  "x": 1080,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b39d6ef3edba22d3d5331aaf9d2220075d801bcc"
                }
              ],
              "s": {
                "y": 601,
                "x": 1080,
                "u": "https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=123b8a4dcb375d14ed980880bb55304b8133c96f"
              },
              "id": "e54liysd6eff1"
            }
          },
          "name": "t3_1maipzo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 243,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 243,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=4ac39f1493be4418ea9b0513e0ab785db9d728b9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753611112,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=123b8a4dcb375d14ed980880bb55304b8133c96f\"&gt;https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=123b8a4dcb375d14ed980880bb55304b8133c96f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5t6qakxf6eff1.png?width=2004&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02c934cd0bc6fbd428a6f1c46e1214db800d83c3\"&gt;https://preview.redd.it/5t6qakxf6eff1.png?width=2004&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02c934cd0bc6fbd428a6f1c46e1214db800d83c3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct\"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker\"&gt;https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?auto=webp&amp;s=de62685b703b746715dcc2b0df2aefcbbc3e3737",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33918a5d809431c198816a64f4512804c3bb5409",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4045a881ccf50c282b74da08c7a22d9d97f0821b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=39771fc11b97b39eae914bdee8e861864005e9bf",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb9a721495b0f0942a0fc51a0050cda33d2ef637",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a9c25310dbb6785760eb6704b72c9666cd180f6",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4d2ffd21d7412bbd7a2765d0696394d9fd40dc27",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1maipzo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BreakfastFriendly728",
          "discussion_type": null,
          "num_comments": 62,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maipzo/a_new_21ba3b_model_that_can_run_30_tokens_on_i9/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maipzo/a_new_21ba3b_model_that_can_run_30_tokens_on_i9/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753611112,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[The initials of Devstral, Mistral, and Magistral as connected puzzle pieces](https://preview.redd.it/tshdyj57ghff1.png?width=2048&amp;format=png&amp;auto=webp&amp;s=14e06a8a7213b113ef28becb5a61878fc952e8c7)\n\n\n\ntl;dr: title. Here are the weights: [Devstral-Small-2507-Rebased-Vision](https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision) &amp; [Magistral-Small-2507-Rebased-Vision](https://huggingface.co/kmouratidis/Magistral-Small-2507-Rebased-Vision) &amp; [Devstral-Small-2507-Rebased-Vision-LoRA](https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision-LoRA)\n\nI've been using Mistral-Small-3.2 for the past few weeks. It's pretty solid, and the combination of vision and speed make it a really good pick for me, but...\n\nI'm using sglang and it's really memory hungry which means it's hard to fit another model side-by-side without much extra VRAM or low quantization (GPTQ/AWQ). Instead, I've tuned the various parameters until I brought the VRAM usage low enough that I can also run Devstral with exllamav3 (Q6), but once in a while sglang throws an OOM when there are multiple queries with images, and I need to load the two servers in a specific order for it to work. It kinda sucks. Running exllama is much slower for any individual model, but would probably work fine for all the at \\~Q6-Q8, but meh.\n\nThen I got an idea: how about I treat retrofit Devstral/Magistral as LoRAs? 3 models for \\~1.1x the VRAM? Yes, please! I tried [mergekit](https://github.com/arcee-ai/mergekit#lora-extraction) but it requires the same architecture, so I'd either have to drop vision (which I also tried, and it seemed to work, but I don't like it!) or try to add vision to Devstral and Magistral. Since these two are trained on the same architecture, it's actually pretty easy, you just have to copy the `model` weights over the `language_model` weights. I did this for both models, and spent a few hours running some benchmarks (in each repo README) to see if there was any significant issue, and it seems to be fine with most being well within the standard error range. I tested a few images and it seemed to work too. There is a significant difference between models, so I probably did that correct too. However, make sure to test on your own and tell me if you notice any issues! &gt;!Yes, I know 2+ other attempts were made (*one by unsloth, from whom I stole the weights, lol*) for the *exact* same thing, and could've saved me a whole day of pain, but I only remembered about it \\~5 mins ago, but this wasn't the core of what I wanted to do anyway so we'll conveniently call it a draw D:!&lt;\n\nWith the \"new\" models in place, the next step was to try creating LoRAs again. Well, mergekit didn't work. I almost quit, but decided to search the web for another method and I ended up finding [LoRD](https://github.com/thomasgauthier/LoRD), the original version of the mergekit code (and it has an Apache license!). It required quite a bit of tweaking to get it working for the Mistral model (and not OOM constantly), but after a few hours I think it succeeded in creating the adapter. I briefly tested with transformers in the same notebook, but sadly it cannot be loaded by sglang. It doesn't even tell me why, I just get a generic error, but it's probably the vision parts, or 1+ of the modules (linear\\_1 / linear\\_2 / merging\\_layer / lm\\_head). Or LoRA might not be support at all for Mistral 3.1 (e.g. like in [vLLM](https://github.com/vllm-project/vllm/issues/18574)). In either case, it meant I couldn't run benchmarks to evaluate quality degration, so I uploaded that to huggingface as well if anyone wants to try.\n\nIf I'm not too lazy (which I'll likely be), I'll give this another go sometime, but now I'll just start my 761435 Karl Franz campaign.",
          "author_fullname": "t2_k6u7rfxb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Devstral &amp; Magistral as adapters of Mistral",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "tshdyj57ghff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/tshdyj57ghff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9cc805736aa2013d7ca4bd816bdb649a9cd2d871"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/tshdyj57ghff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=41ed921c4e6a128b0fbd80d0921c46b2d6755243"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/tshdyj57ghff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0ad4cd99e1863be63b3a4c08034865431c69624"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/tshdyj57ghff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=897ba3761d52e61715c3eb1d34ba8e3708e3ee6f"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/tshdyj57ghff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b2689357836116b0804c2c34e284749c615b663"
                },
                {
                  "y": 1080,
                  "x": 1080,
                  "u": "https://preview.redd.it/tshdyj57ghff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=932f2fe782da56be5e6bef05578c4e9edc202cda"
                }
              ],
              "s": {
                "y": 2048,
                "x": 2048,
                "u": "https://preview.redd.it/tshdyj57ghff1.png?width=2048&amp;format=png&amp;auto=webp&amp;s=14e06a8a7213b113ef28becb5a61878fc952e8c7"
              },
              "id": "tshdyj57ghff1"
            }
          },
          "name": "t3_1maywaw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=32c7761ccccf9d9cd3d58f04849c4249d01be54a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753653691,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/tshdyj57ghff1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14e06a8a7213b113ef28becb5a61878fc952e8c7\"&gt;The initials of Devstral, Mistral, and Magistral as connected puzzle pieces&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;tl;dr: title. Here are the weights: &lt;a href=\"https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision\"&gt;Devstral-Small-2507-Rebased-Vision&lt;/a&gt; &amp;amp; &lt;a href=\"https://huggingface.co/kmouratidis/Magistral-Small-2507-Rebased-Vision\"&gt;Magistral-Small-2507-Rebased-Vision&lt;/a&gt; &amp;amp; &lt;a href=\"https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision-LoRA\"&gt;Devstral-Small-2507-Rebased-Vision-LoRA&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using Mistral-Small-3.2 for the past few weeks. It&amp;#39;s pretty solid, and the combination of vision and speed make it a really good pick for me, but...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using sglang and it&amp;#39;s really memory hungry which means it&amp;#39;s hard to fit another model side-by-side without much extra VRAM or low quantization (GPTQ/AWQ). Instead, I&amp;#39;ve tuned the various parameters until I brought the VRAM usage low enough that I can also run Devstral with exllamav3 (Q6), but once in a while sglang throws an OOM when there are multiple queries with images, and I need to load the two servers in a specific order for it to work. It kinda sucks. Running exllama is much slower for any individual model, but would probably work fine for all the at ~Q6-Q8, but meh.&lt;/p&gt;\n\n&lt;p&gt;Then I got an idea: how about I treat retrofit Devstral/Magistral as LoRAs? 3 models for ~1.1x the VRAM? Yes, please! I tried &lt;a href=\"https://github.com/arcee-ai/mergekit#lora-extraction\"&gt;mergekit&lt;/a&gt; but it requires the same architecture, so I&amp;#39;d either have to drop vision (which I also tried, and it seemed to work, but I don&amp;#39;t like it!) or try to add vision to Devstral and Magistral. Since these two are trained on the same architecture, it&amp;#39;s actually pretty easy, you just have to copy the &lt;code&gt;model&lt;/code&gt; weights over the &lt;code&gt;language_model&lt;/code&gt; weights. I did this for both models, and spent a few hours running some benchmarks (in each repo README) to see if there was any significant issue, and it seems to be fine with most being well within the standard error range. I tested a few images and it seemed to work too. There is a significant difference between models, so I probably did that correct too. However, make sure to test on your own and tell me if you notice any issues! &lt;span class=\"md-spoiler-text\"&gt;Yes, I know 2+ other attempts were made (&lt;em&gt;one by unsloth, from whom I stole the weights, lol&lt;/em&gt;) for the &lt;em&gt;exact&lt;/em&gt; same thing, and could&amp;#39;ve saved me a whole day of pain, but I only remembered about it ~5 mins ago, but this wasn&amp;#39;t the core of what I wanted to do anyway so we&amp;#39;ll conveniently call it a draw D:&lt;/span&gt;&lt;/p&gt;\n\n&lt;p&gt;With the &amp;quot;new&amp;quot; models in place, the next step was to try creating LoRAs again. Well, mergekit didn&amp;#39;t work. I almost quit, but decided to search the web for another method and I ended up finding &lt;a href=\"https://github.com/thomasgauthier/LoRD\"&gt;LoRD&lt;/a&gt;, the original version of the mergekit code (and it has an Apache license!). It required quite a bit of tweaking to get it working for the Mistral model (and not OOM constantly), but after a few hours I think it succeeded in creating the adapter. I briefly tested with transformers in the same notebook, but sadly it cannot be loaded by sglang. It doesn&amp;#39;t even tell me why, I just get a generic error, but it&amp;#39;s probably the vision parts, or 1+ of the modules (linear_1 / linear_2 / merging_layer / lm_head). Or LoRA might not be support at all for Mistral 3.1 (e.g. like in &lt;a href=\"https://github.com/vllm-project/vllm/issues/18574\"&gt;vLLM&lt;/a&gt;). In either case, it meant I couldn&amp;#39;t run benchmarks to evaluate quality degration, so I uploaded that to huggingface as well if anyone wants to try.&lt;/p&gt;\n\n&lt;p&gt;If I&amp;#39;m not too lazy (which I&amp;#39;ll likely be), I&amp;#39;ll give this another go sometime, but now I&amp;#39;ll just start my 761435 Karl Franz campaign.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?auto=webp&amp;s=f33db570c3a8f16c2ac464fb9062565d9b50b904",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=336ca45300d9ad8f941487b0ce465efa53dd0e02",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6fa2934bddfe176555bff114786099245f85abcf",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8cd13e62d30f7a7e58e32919b6d165c24a125225",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=22ed0b082947f94ee079c2a6004328efe6c66fc9",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fcd80bad37121082c8b613252204288829d1c8be",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=11b16dfa1d4b69bdf4dc3a4f5e8a530bc6d72c2d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1maywaw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kmouratidis",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753653691,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I remember some of them were really solid, but it's been over a year since we've seen a new release.   \nIs the team still active, or has the project quietly died?",
          "author_fullname": "t2_pv1nb9469",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What happened to the Yi models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maxfeb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753649989,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I remember some of them were really solid, but it&amp;#39;s been over a year since we&amp;#39;ve seen a new release.&lt;br/&gt;\nIs the team still active, or has the project quietly died?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1maxfeb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GabryIta",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maxfeb/what_happened_to_the_yi_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maxfeb/what_happened_to_the_yi_models/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753649989,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to list and summarize details such as:\n\n* Family, friends, and relationships\n* Schooling and career\n* Interests, hobbies, and recreation\n* Goals and desires\n\nI use simple prompts like: \"*Comprehensive list of Tommy's interests.*\" But the results seem to be lacking and sometimes focus more on the beginning or end of the export.\n\nI've tried a few different models (llama3.1:\\[8b,70b\\], gemma3:\\[4b,27b\\]) and increasing `num_ctx` with diminishing returns.\n\nAppreciate any suggestions to improve!",
          "author_fullname": "t2_lea9h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Describe a person using exported WhatsApp chat",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbirq1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753715375,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to list and summarize details such as:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Family, friends, and relationships&lt;/li&gt;\n&lt;li&gt;Schooling and career&lt;/li&gt;\n&lt;li&gt;Interests, hobbies, and recreation&lt;/li&gt;\n&lt;li&gt;Goals and desires&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I use simple prompts like: &amp;quot;&lt;em&gt;Comprehensive list of Tommy&amp;#39;s interests.&lt;/em&gt;&amp;quot; But the results seem to be lacking and sometimes focus more on the beginning or end of the export.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried a few different models (llama3.1:[8b,70b], gemma3:[4b,27b]) and increasing &lt;code&gt;num_ctx&lt;/code&gt; with diminishing returns.&lt;/p&gt;\n\n&lt;p&gt;Appreciate any suggestions to improve!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbirq1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Tommy_Tukyuk",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbirq1/describe_a_person_using_exported_whatsapp_chat/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbirq1/describe_a_person_using_exported_whatsapp_chat/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753715375,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm working on a new model that allows for attribution of trained on data to be identified at the time of inference. One of my hypothesis being that if the the data being used at inference can be attributed then the next round of fine tuning can,  \n  \n1. Trim data that wasn't used at inference  \n2. More data could be added that is contextual to the outcome  \n  \nI'd love to get some initial feedback on this thinking, would it be helpful when fine tuning your own models?  ",
          "author_fullname": "t2_6cyd8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fine Tuning; Attribution at Inference Time",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbako7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753690842,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a new model that allows for attribution of trained on data to be identified at the time of inference. One of my hypothesis being that if the the data being used at inference can be attributed then the next round of fine tuning can,  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Trim data that wasn&amp;#39;t used at inference&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;More data could be added that is contextual to the outcome&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;d love to get some initial feedback on this thinking, would it be helpful when fine tuning your own models?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbako7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Iam_Alastair",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbako7/fine_tuning_attribution_at_inference_time/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbako7/fine_tuning_attribution_at_inference_time/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753690842,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here's what happened:\n\nI needed to help someone extract structured data from hundreds of detailed Word documents (~100KB each) containing manually typed survey responses (yes/no answers + comments). Each document was internally unique, making traditional automation impossible. With limited time to research solutions, I:\n\n1) Installed VS Code on their computer\n\n2) Added the [Roo Code extension](https://github.com/RooCodeInc/Roo-Code) (AI coding assistant)\n\n3) Basically used it as a chat interface to:\n- Develop a schema by analyzing sample documents\n- Process files individually\n- Generate a program that populated a clean data table\n\nIt ultimately worked, but man was it awkward. Instead of just reading the documents directly, Roo Code's default prompts steered the LLM to coding solutions (\"Let me write a parser...\" NO!). But we've managed to process 900+ files in a day.\n\nNow I'm staring at this jank realizing:\n\n1) This is a recurring pattern (next week it'll be PDF reports, then email threads, etc) - right now it's all being done **by hand**\n\n2) Existing options are either overkill (enterprise RAG platforms) or insufficient (basic ChatGPT-like interfaces fail with batch processing due to severe quality degradation)\n\n3) While better than nothing, the final 100+-column Excel spreadsheet is far from ideal\n\n4) There's got to be something between \"duct tape + VS Code\" and \"$50k/year enterprise solution\"\n\n**What would you do?**",
          "author_fullname": "t2_8fu8sqhz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Bending VS Code into a document-processing AI tool worked - but there must be a better way",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mb4d9y",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753669162,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s what happened:&lt;/p&gt;\n\n&lt;p&gt;I needed to help someone extract structured data from hundreds of detailed Word documents (~100KB each) containing manually typed survey responses (yes/no answers + comments). Each document was internally unique, making traditional automation impossible. With limited time to research solutions, I:&lt;/p&gt;\n\n&lt;p&gt;1) Installed VS Code on their computer&lt;/p&gt;\n\n&lt;p&gt;2) Added the &lt;a href=\"https://github.com/RooCodeInc/Roo-Code\"&gt;Roo Code extension&lt;/a&gt; (AI coding assistant)&lt;/p&gt;\n\n&lt;p&gt;3) Basically used it as a chat interface to:\n- Develop a schema by analyzing sample documents\n- Process files individually\n- Generate a program that populated a clean data table&lt;/p&gt;\n\n&lt;p&gt;It ultimately worked, but man was it awkward. Instead of just reading the documents directly, Roo Code&amp;#39;s default prompts steered the LLM to coding solutions (&amp;quot;Let me write a parser...&amp;quot; NO!). But we&amp;#39;ve managed to process 900+ files in a day.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m staring at this jank realizing:&lt;/p&gt;\n\n&lt;p&gt;1) This is a recurring pattern (next week it&amp;#39;ll be PDF reports, then email threads, etc) - right now it&amp;#39;s all being done &lt;strong&gt;by hand&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;2) Existing options are either overkill (enterprise RAG platforms) or insufficient (basic ChatGPT-like interfaces fail with batch processing due to severe quality degradation)&lt;/p&gt;\n\n&lt;p&gt;3) While better than nothing, the final 100+-column Excel spreadsheet is far from ideal&lt;/p&gt;\n\n&lt;p&gt;4) There&amp;#39;s got to be something between &amp;quot;duct tape + VS Code&amp;quot; and &amp;quot;$50k/year enterprise solution&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What would you do?&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/okT-A-GTNMcR1p0qC1l3xTiCyaSUnymno2UcuWewt-c.png?auto=webp&amp;s=f4aa9b4270a27df2aaa6ab00cc3f3320d0aabf08",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/okT-A-GTNMcR1p0qC1l3xTiCyaSUnymno2UcuWewt-c.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b6b187e8b4cac1bc1c1bbd33b1877252d6b4cdae",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/okT-A-GTNMcR1p0qC1l3xTiCyaSUnymno2UcuWewt-c.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3612bac0ba3aab55bb0827230994c5d51d08e8d1",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/okT-A-GTNMcR1p0qC1l3xTiCyaSUnymno2UcuWewt-c.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1167dab36de32e0fef4f1d878149931b9fe421c4",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/okT-A-GTNMcR1p0qC1l3xTiCyaSUnymno2UcuWewt-c.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1de0fe4860744ab87fc7eb8c1c801fd87f5722cc",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/okT-A-GTNMcR1p0qC1l3xTiCyaSUnymno2UcuWewt-c.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f889502afaf39a85dc8c2b9add24302aa740fe6e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/okT-A-GTNMcR1p0qC1l3xTiCyaSUnymno2UcuWewt-c.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=582a66bbb9849e7e6f2a9a4a9994f48a4e896ff5",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "okT-A-GTNMcR1p0qC1l3xTiCyaSUnymno2UcuWewt-c"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mb4d9y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Normal-Ad-7114",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb4d9y/bending_vs_code_into_a_documentprocessing_ai_tool/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mb4d9y/bending_vs_code_into_a_documentprocessing_ai_tool/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753669162,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Around a year and a half on from my post about 24GB vs 48GB VRAM, I personally find that the scene has changed a lot in terms of what sizes of models are popularly available and used.\n\nBack then, 48GB VRAM for 70B models at 4BPW was more or less the gold standard for local inference. This is back when The Bloke was still releasing quants and Midnight Miqu was the holy grail for creative writing.\n\nThis is practically ancient history in the LLM space, but some of you surely recall this period just as well as I do.\n\nThere is now a much greater diversity of model parameter sizes available in terms of open-weights models, and the frontier of performance has continually been pushed forward. That being said, I find that newer open-weights models are either narrower in scope and smaller in parameter size, or generally much more competent but prohibitively large to be run locally for most.\n\nDeepseek R1 and V3 are good examples of this, as is the newer Kimi K2. At 671B parameters and 1T parameters, respectively, I think it's fair to assume that most users of these models are doing so via API rather than hosting locally. Even with an MOE architecture, they are simply too large to be hosted locally at reasonable speeds by enthusiasts. This is reminiscent of the situation with LLaMA 405B, in my opinion.\n\nWith the launch of LLaMA 4 being a bust and Qwen3 only going up to 32B in terms of dense models, perhaps there just hasn't been a solid 70/72B model released in quite some time? The last model that really made a splash in this parameter range was Qwen2.5 72B, and that's a long while ago...\n\nI also find that most finetunes are still working with L3.3 as a base, which speaks to the recent lack of available models in this parameter range.\n\nThis does leave 48GB VRAM in a bit of a weird spot - too large for the small/medium-models, and too small for the *really* large models. Perhaps a migration to a general preference for an MOE architecture is a natural consequence of the ever-increasing demand for VRAM and compute, or this is just a temporary lull in the output of the major labs training open-weights models which will come to pass eventually.\n\nI suppose I'm partially reminiscing, and partially trying to start a dialogue on where the \"sweet spot\" for local models is nowadays. It would appear that the age of 70B/4BPW/48GB VRAM being the consensus has come to an end.\n\nAre \\~70B dense models going out of fashion for good? Or do you think this is just a temporary lull amidst a general move towards preference for MOE architectures?\n\n**EDIT:** If very large MOE models will be the norm moving forward, perhaps building a server motherboard with large amounts of fast multi-channel system RAM is preferable to continually adding consumer GPUs to accrue larger amounts of VRAM for local inference (seeing as the latter is an approach that is primarily aimed at dense models that fit entirely into VRAM).",
          "author_fullname": "t2_cyw8u51dt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are ~70B Models Going Out of Fashion?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1majfwi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 148,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 148,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753617926,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753613850,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Around a year and a half on from my post about 24GB vs 48GB VRAM, I personally find that the scene has changed a lot in terms of what sizes of models are popularly available and used.&lt;/p&gt;\n\n&lt;p&gt;Back then, 48GB VRAM for 70B models at 4BPW was more or less the gold standard for local inference. This is back when The Bloke was still releasing quants and Midnight Miqu was the holy grail for creative writing.&lt;/p&gt;\n\n&lt;p&gt;This is practically ancient history in the LLM space, but some of you surely recall this period just as well as I do.&lt;/p&gt;\n\n&lt;p&gt;There is now a much greater diversity of model parameter sizes available in terms of open-weights models, and the frontier of performance has continually been pushed forward. That being said, I find that newer open-weights models are either narrower in scope and smaller in parameter size, or generally much more competent but prohibitively large to be run locally for most.&lt;/p&gt;\n\n&lt;p&gt;Deepseek R1 and V3 are good examples of this, as is the newer Kimi K2. At 671B parameters and 1T parameters, respectively, I think it&amp;#39;s fair to assume that most users of these models are doing so via API rather than hosting locally. Even with an MOE architecture, they are simply too large to be hosted locally at reasonable speeds by enthusiasts. This is reminiscent of the situation with LLaMA 405B, in my opinion.&lt;/p&gt;\n\n&lt;p&gt;With the launch of LLaMA 4 being a bust and Qwen3 only going up to 32B in terms of dense models, perhaps there just hasn&amp;#39;t been a solid 70/72B model released in quite some time? The last model that really made a splash in this parameter range was Qwen2.5 72B, and that&amp;#39;s a long while ago...&lt;/p&gt;\n\n&lt;p&gt;I also find that most finetunes are still working with L3.3 as a base, which speaks to the recent lack of available models in this parameter range.&lt;/p&gt;\n\n&lt;p&gt;This does leave 48GB VRAM in a bit of a weird spot - too large for the small/medium-models, and too small for the &lt;em&gt;really&lt;/em&gt; large models. Perhaps a migration to a general preference for an MOE architecture is a natural consequence of the ever-increasing demand for VRAM and compute, or this is just a temporary lull in the output of the major labs training open-weights models which will come to pass eventually.&lt;/p&gt;\n\n&lt;p&gt;I suppose I&amp;#39;m partially reminiscing, and partially trying to start a dialogue on where the &amp;quot;sweet spot&amp;quot; for local models is nowadays. It would appear that the age of 70B/4BPW/48GB VRAM being the consensus has come to an end.&lt;/p&gt;\n\n&lt;p&gt;Are ~70B dense models going out of fashion for good? Or do you think this is just a temporary lull amidst a general move towards preference for MOE architectures?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; If very large MOE models will be the norm moving forward, perhaps building a server motherboard with large amounts of fast multi-channel system RAM is preferable to continually adding consumer GPUs to accrue larger amounts of VRAM for local inference (seeing as the latter is an approach that is primarily aimed at dense models that fit entirely into VRAM).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1majfwi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HvskyAI",
          "discussion_type": null,
          "num_comments": 91,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753613850,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Abstract\n\n&gt;To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use.",
          "author_fullname": "t2_qjpsv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maw5dy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753646844,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Abstract&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.16784",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1maw5dy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ninjasaid13",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maw5dy/beyond_context_limits_subconscious_threads_for/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.16784",
          "subreddit_subscribers": 506190,
          "created_utc": 1753646844,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**TECHNICAL REPORT OF TELECHAT2, TELECHAT2.5 AND T1**\r\n\n|Model|Link|\n|:-|:-|\n|**TeleChat2-35B** |[**https://modelscope.cn/models/TeleAI/TeleChat2-35B**](https://modelscope.cn/models/TeleAI/TeleChat2-35B)|\n|**TeleChat2-115B**|[**https://modelscope.cn/models/TeleAI/TeleChat2-115B**](https://modelscope.cn/models/TeleAI/TeleChat2-115B)|\n|**TeleChat2.5-35B**|[**https://modelscope.cn/models/TeleAI/TeleChat2.5-35B**](https://modelscope.cn/models/TeleAI/TeleChat2.5-35B)|\n|**TeleChat2.5-115B**|[**https://modelscope.cn/models/TeleAI/TeleChat2.5-115B**](https://modelscope.cn/models/TeleAI/TeleChat2.5-115B)|\n|**T1-35B**|[**https://modelscope.cn/models/TeleAI/T1-35B**](https://modelscope.cn/models/TeleAI/T1-35B)|\n|**T1-115B** |[**https://modelscope.cn/models/TeleAI/T1-115B**](https://modelscope.cn/models/TeleAI/T1-115B)|\n\nAbstract\n\n&gt;We introduce the latest series of TeleChat models: TeleChat2, TeleChat2.5, and T1, offering a significant upgrade over their predecessor, TeleChat. Despite minimal changes to the model architecture, the new series achieves substantial performance gains through enhanced training strategies in both pre-training and post-training stages. The series begins with TeleChat2, which undergoes pretraining on 10 trillion high-quality and diverse tokens. This is followed by Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to further enhance its capabilities. TeleChat2.5 and T1 expand the pipeline by incorporating a continual pretraining phase with domain-specific datasets, combined with reinforcement learning (RL) to improve performance in code generation and mathematical reasoning tasks. The T1 variant is designed for complex reasoning, supporting long Chain-of-Thought (CoT) reasoning and demonstrating substantial improvements in mathematics and coding. In contrast, TeleChat2.5 prioritizes speed, delivering rapid inference. Both flagship models of T1 and TeleChat2.5 are dense Transformer-based architectures with 115B parameters, showcasing significant advancements in reasoning and general task performance compared to the original TeleChat. Notably, T1-115B outperform proprietary models such as OpenAI's o1-mini and GPT-4o. We publicly release TeleChat2, TeleChat2.5 and T1, including post-trained versions with 35B and 115B parameters, to empower developers and researchers with state-of-the-art language models tailored for diverse applications.",
          "author_fullname": "t2_qjpsv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Technical Report of TeleChat2, TeleChat2.5 and T1",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mb4mex",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753669953,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TECHNICAL REPORT OF TELECHAT2, TELECHAT2.5 AND T1&lt;/strong&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Link&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;TeleChat2-35B&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://modelscope.cn/models/TeleAI/TeleChat2-35B\"&gt;&lt;strong&gt;https://modelscope.cn/models/TeleAI/TeleChat2-35B&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;TeleChat2-115B&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://modelscope.cn/models/TeleAI/TeleChat2-115B\"&gt;&lt;strong&gt;https://modelscope.cn/models/TeleAI/TeleChat2-115B&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;TeleChat2.5-35B&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://modelscope.cn/models/TeleAI/TeleChat2.5-35B\"&gt;&lt;strong&gt;https://modelscope.cn/models/TeleAI/TeleChat2.5-35B&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;TeleChat2.5-115B&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://modelscope.cn/models/TeleAI/TeleChat2.5-115B\"&gt;&lt;strong&gt;https://modelscope.cn/models/TeleAI/TeleChat2.5-115B&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;T1-35B&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://modelscope.cn/models/TeleAI/T1-35B\"&gt;&lt;strong&gt;https://modelscope.cn/models/TeleAI/T1-35B&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;T1-115B&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://modelscope.cn/models/TeleAI/T1-115B\"&gt;&lt;strong&gt;https://modelscope.cn/models/TeleAI/T1-115B&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Abstract&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;We introduce the latest series of TeleChat models: TeleChat2, TeleChat2.5, and T1, offering a significant upgrade over their predecessor, TeleChat. Despite minimal changes to the model architecture, the new series achieves substantial performance gains through enhanced training strategies in both pre-training and post-training stages. The series begins with TeleChat2, which undergoes pretraining on 10 trillion high-quality and diverse tokens. This is followed by Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to further enhance its capabilities. TeleChat2.5 and T1 expand the pipeline by incorporating a continual pretraining phase with domain-specific datasets, combined with reinforcement learning (RL) to improve performance in code generation and mathematical reasoning tasks. The T1 variant is designed for complex reasoning, supporting long Chain-of-Thought (CoT) reasoning and demonstrating substantial improvements in mathematics and coding. In contrast, TeleChat2.5 prioritizes speed, delivering rapid inference. Both flagship models of T1 and TeleChat2.5 are dense Transformer-based architectures with 115B parameters, showcasing significant advancements in reasoning and general task performance compared to the original TeleChat. Notably, T1-115B outperform proprietary models such as OpenAI&amp;#39;s o1-mini and GPT-4o. We publicly release TeleChat2, TeleChat2.5 and T1, including post-trained versions with 35B and 115B parameters, to empower developers and researchers with state-of-the-art language models tailored for diverse applications.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.18013",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mb4mex",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ninjasaid13",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb4mex/technical_report_of_telechat2_telechat25_and_t1/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.18013",
          "subreddit_subscribers": 506190,
          "created_utc": 1753669953,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3l9wjlq0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tencent releases Hunyuan3D World Model 1.0 - first open-source 3D world generation model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mab2i2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 578,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 578,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753583285,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/TencentHunyuan/status/1949288986192834718",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mab2i2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pseudoreddituser",
          "discussion_type": null,
          "num_comments": 54,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mab2i2/tencent_releases_hunyuan3d_world_model_10_first/",
          "stickied": false,
          "url": "https://x.com/TencentHunyuan/status/1949288986192834718",
          "subreddit_subscribers": 506190,
          "created_utc": 1753583285,
          "num_crossposts": 4,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_w6l58p741",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Drummer's Mixtral 4x3B v1 - A finetuned clown MoE experiment with Voxtral 3B!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maptvc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 47,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 47,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=6f4d41157daea37fd6fce793fd1d455f3a33889f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753631776,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?auto=webp&amp;s=5160ca0d10d89e064f39433938efce8c841fe074",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bbb0ffe720c33190a7c35c23d0bfa7c0465f73ba",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8977f01203b9d02ddfd018af3eefecb7b4e22ab1",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=98bd6abbe5d09b66b80ef08bb40973db912270c8",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bb469872360a699d06db5617b0f24cf8eea8d5f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=94906681697735d019628c51708b7ecace28c3e4",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=10f09b575aa5fd8b5a3ee87b4e22904545ea9ca9",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1maptvc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheLocalDrummer",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maptvc/drummers_mixtral_4x3b_v1_a_finetuned_clown_moe/",
          "stickied": false,
          "url": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1",
          "subreddit_subscribers": 506190,
          "created_utc": 1753631776,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello!  \nDo you guys know what is actually the best uncensored vision LLM lately?  \nI already tried ToriiGate (https://huggingface.co/Minthy/ToriiGate-v0.4-7B) and JoyCaption (https://huggingface.co/spaces/fancyfeast/joy-caption-beta-one), but they are still not so good for captioning/describing NSFW stuff from images?  \nDo you know other good alternatives? Don't say WDTagger because I already know it, the problem is I need natural language captioning. Or a way to accomplish this within gemini/gpt?  \nThanks!",
          "author_fullname": "t2_19dv7tea81",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the best uncensored vision LLM nowadays?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbkgky",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753719144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;br/&gt;\nDo you guys know what is actually the best uncensored vision LLM lately?&lt;br/&gt;\nI already tried ToriiGate (&lt;a href=\"https://huggingface.co/Minthy/ToriiGate-v0.4-7B\"&gt;https://huggingface.co/Minthy/ToriiGate-v0.4-7B&lt;/a&gt;) and JoyCaption (&lt;a href=\"https://huggingface.co/spaces/fancyfeast/joy-caption-beta-one\"&gt;https://huggingface.co/spaces/fancyfeast/joy-caption-beta-one&lt;/a&gt;), but they are still not so good for captioning/describing NSFW stuff from images?&lt;br/&gt;\nDo you know other good alternatives? Don&amp;#39;t say WDTagger because I already know it, the problem is I need natural language captioning. Or a way to accomplish this within gemini/gpt?&lt;br/&gt;\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZYGAZF6B2rmT5izvffiJsudyg0IeQMSyn2uQocqKoMw.png?auto=webp&amp;s=81355e9c82a56687e4257e407e76f7b94bbbb898",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZYGAZF6B2rmT5izvffiJsudyg0IeQMSyn2uQocqKoMw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2cfe625716febb0a9da81131ff20cd185acb268",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/ZYGAZF6B2rmT5izvffiJsudyg0IeQMSyn2uQocqKoMw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e115ceb3c1ecf1599499e518820651de531fd86d",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/ZYGAZF6B2rmT5izvffiJsudyg0IeQMSyn2uQocqKoMw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6cc0887a6bbe0218c9ecd5fe8b1470c7242a0d7e",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/ZYGAZF6B2rmT5izvffiJsudyg0IeQMSyn2uQocqKoMw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=887114bc13ed98d4b7fb9ca68e7ac8f88b3572ae",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/ZYGAZF6B2rmT5izvffiJsudyg0IeQMSyn2uQocqKoMw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=12883da659836c7837a313cc3f90e2637fcea73f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/ZYGAZF6B2rmT5izvffiJsudyg0IeQMSyn2uQocqKoMw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=761800882d72ab687c46b37820aa557cd1db443c",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "ZYGAZF6B2rmT5izvffiJsudyg0IeQMSyn2uQocqKoMw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbkgky",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TekeshiX",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbkgky/what_is_the_best_uncensored_vision_llm_nowadays/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbkgky/what_is_the_best_uncensored_vision_llm_nowadays/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753719144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Qwen has introduced a new technique called **GSPO** (Group Sequence Policy Optimization)\n\nPut simply:\n\n* It's a new method for training large language models\n* Instead of focusing on individual words like older methods, it optimizes entire sentences or passages as a whole — which is more logical and leads to better performance\n* This approach makes training more **stable** and less prone to crashes or errors, especially when used with large, modular models like **MoE (Mixture of Experts)**\n* The training process is **simpler** and doesn’t rely on complex tricks used in the past, making it cleaner and easier to manage\n* The more compute you throw at it, the better the model becomes — it **scales efficiently**.\n* The latest **Qwen3 models** (like those that can code or follow instructions) were trained using this method\n* Compared to the older **GRPO** method, GSPO leads to **faster convergence** (the model learns faster) and uses **fewer resources**\n\nPaper: [https://huggingface.co/papers/2507.18071](https://huggingface.co/papers/2507.18071)",
          "author_fullname": "t2_1heeqeidfc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen GSPO (Group Sequence Policy Optimization)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1man0hu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 65,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 65,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753624825,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Qwen has introduced a new technique called &lt;strong&gt;GSPO&lt;/strong&gt; (Group Sequence Policy Optimization)&lt;/p&gt;\n\n&lt;p&gt;Put simply:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It&amp;#39;s a new method for training large language models&lt;/li&gt;\n&lt;li&gt;Instead of focusing on individual words like older methods, it optimizes entire sentences or passages as a whole — which is more logical and leads to better performance&lt;/li&gt;\n&lt;li&gt;This approach makes training more &lt;strong&gt;stable&lt;/strong&gt; and less prone to crashes or errors, especially when used with large, modular models like &lt;strong&gt;MoE (Mixture of Experts)&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;The training process is &lt;strong&gt;simpler&lt;/strong&gt; and doesn’t rely on complex tricks used in the past, making it cleaner and easier to manage&lt;/li&gt;\n&lt;li&gt;The more compute you throw at it, the better the model becomes — it &lt;strong&gt;scales efficiently&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;The latest &lt;strong&gt;Qwen3 models&lt;/strong&gt; (like those that can code or follow instructions) were trained using this method&lt;/li&gt;\n&lt;li&gt;Compared to the older &lt;strong&gt;GRPO&lt;/strong&gt; method, GSPO leads to &lt;strong&gt;faster convergence&lt;/strong&gt; (the model learns faster) and uses &lt;strong&gt;fewer resources&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Paper: &lt;a href=\"https://huggingface.co/papers/2507.18071\"&gt;https://huggingface.co/papers/2507.18071&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?auto=webp&amp;s=b7678a8af1d1d28f96c34fbaeb2656718573d56c",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=204816acf3c4a486bb403207785321d33214adc7",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=81cb7197b22ed427b6c24ae43d3f69dc4cb2730d",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=329be156f229f90b7be0f62070e92a848fedc1f2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=82297d64dd06513853c691039970c2747e099d87",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=03df11e5919855e527dbf686542a55fb52fd228c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2d08b01c22320c544d58ffd0a85b1d12a04e7402",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1man0hu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "koc_Z3",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1man0hu/qwen_gspo_group_sequence_policy_optimization/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1man0hu/qwen_gspo_group_sequence_policy_optimization/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753624825,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ai voice clone local unlimited that can generate long characters or words over 1k:\n\nAny one knows any local ai tool that clones voice from reference audio that works with unlimited and long inout characters? I know Kokoro TTS works with unlimited input but it doesn't clone voices from reference audio. Also ChatterboxTTS supports cloning but it just doesn't work well with long text input. Sometimes it cuts some sentences or words. Thank you guys for your help in advance... Truly appreciate you all!",
          "author_fullname": "t2_1tta08arr2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ai voice clone local unlimited that can generate long characters or words over 1k",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbdtw8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753702715,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ai voice clone local unlimited that can generate long characters or words over 1k:&lt;/p&gt;\n\n&lt;p&gt;Any one knows any local ai tool that clones voice from reference audio that works with unlimited and long inout characters? I know Kokoro TTS works with unlimited input but it doesn&amp;#39;t clone voices from reference audio. Also ChatterboxTTS supports cloning but it just doesn&amp;#39;t work well with long text input. Sometimes it cuts some sentences or words. Thank you guys for your help in advance... Truly appreciate you all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbdtw8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mauamolat",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbdtw8/ai_voice_clone_local_unlimited_that_can_generate/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbdtw8/ai_voice_clone_local_unlimited_that_can_generate/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753702715,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone I am trying to build a small project just to keep in touch with all the news and information flowing in the markets so that I can better understand what is happening around the world. I am fetching the data from a website where I get the link of the pdf for concalls and other credit ratings changes, this information is too complex to analyse. So I want to pass it through an LLM and see what can be done around with it. Currently I have a mac mini m4 and a few windows systems with 16gb ram and 4gb graphics card, I have no clue how I can build this system with minimum expenses. yes I can use open ai api and it will work perfectly fine, If anyone can either give me an estimate of how much will I be spending on it? because all of this is too complicated to understand atleast for me. I was looking for LLAMA but then again I am not sure if my systems are capable enough. What do you guys think?",
          "author_fullname": "t2_1nqcevx7uj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a personal project for portfolio management.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbdg53",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753701476,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone I am trying to build a small project just to keep in touch with all the news and information flowing in the markets so that I can better understand what is happening around the world. I am fetching the data from a website where I get the link of the pdf for concalls and other credit ratings changes, this information is too complex to analyse. So I want to pass it through an LLM and see what can be done around with it. Currently I have a mac mini m4 and a few windows systems with 16gb ram and 4gb graphics card, I have no clue how I can build this system with minimum expenses. yes I can use open ai api and it will work perfectly fine, If anyone can either give me an estimate of how much will I be spending on it? because all of this is too complicated to understand atleast for me. I was looking for LLAMA but then again I am not sure if my systems are capable enough. What do you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbdg53",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Boring_Tip_1218",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbdg53/building_a_personal_project_for_portfolio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbdg53/building_a_personal_project_for_portfolio/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753701476,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am very attracted to the idea of using server hardware for llms, since 16 channel ddr4 memory will give 400gb/s worth of bandwidth.\n\nHowever, one thing that keeps popping up when researching is pcie bandwidth being an issue\n\nLogically, it does make sense, since pcie 4.0x16 gives 32gb/s, way too little for llms, not to mention the latency.\n\nBut when I look up actual results, this doesn’t seem to be the case at all\n\nI am so confused on this matter, how does the pcie bandwidth affect the use of system ram, and a secondary gpu?\n\nIn this context, at least one gpu is being used",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "System Ram Speed Importance when using GPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mb7gxu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753679731,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753679084,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am very attracted to the idea of using server hardware for llms, since 16 channel ddr4 memory will give 400gb/s worth of bandwidth.&lt;/p&gt;\n\n&lt;p&gt;However, one thing that keeps popping up when researching is pcie bandwidth being an issue&lt;/p&gt;\n\n&lt;p&gt;Logically, it does make sense, since pcie 4.0x16 gives 32gb/s, way too little for llms, not to mention the latency.&lt;/p&gt;\n\n&lt;p&gt;But when I look up actual results, this doesn’t seem to be the case at all&lt;/p&gt;\n\n&lt;p&gt;I am so confused on this matter, how does the pcie bandwidth affect the use of system ram, and a secondary gpu?&lt;/p&gt;\n\n&lt;p&gt;In this context, at least one gpu is being used&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mb7gxu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb7gxu/system_ram_speed_importance_when_using_gpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mb7gxu/system_ram_speed_importance_when_using_gpu/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753679084,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "tl;dr: faster grammar check and minor code edits without a draft model: a C# proof-of-concept.\n\n[https://github.com/dpmm99/ModelFreeSpeculation](https://github.com/dpmm99/ModelFreeSpeculation)\n\nThis is a toy project built on LLamaSharp. It's a toy because it assumes the output will be nearly identical to the input--no particularly large added sequences and such. A better difference-tracking algorithm would make it more usable, and I think it could also be better if it fell back to a real draft model smartly when there are big differences. I'd been thinking about this since I saw a statement that **a draft \"model\" isn't limited to LLMs**, and I remember it every time I accidentally click \"Apply\" in GitHub Copilot and watch it scan through a few hundred lines of code just to add one function, haha.\n\n\n\nI tested it on two prompts using Phi-4-14B-Q4\\_K\\_M with **8 draft tokens** per inference loop iteration on my RTX 4060 Ti using CUDA and [this pre-release of LLamaSharp](https://github.com/SciSharp/LLamaSharp/pull/1225).\n\nFor the spell-check prompt:\n\nDuration: 7.39s, Tokens: 135, Tokens/sec: 18.28\n\nDuration: 4.89s, Tokens: 135, Tokens/sec: 27.60 (88 accepted, 283 rejected) **(+51%)**\n\n\n\nFor the code editing prompt:\n\nDuration: 17.84s, Tokens: 328, Tokens/sec: 18.39\n\nDuration: 10.40s, Tokens: 328, Tokens/sec: 31.55 (237 accepted, 473 rejected) **(+71%)**\n\nDuration: 9.50s, Tokens: 328, Tokens/sec: 34.52 (250 draft tokens accepted; **draft length 20**) **(+88%)**\n\n\n\nI was also thinking this approach could go nicely with a model fine-tuned for *applying* code edits like [https://huggingface.co/models?other=base\\_model:quantized:microsoft/NextCoder-32B](https://huggingface.co/models?other=base_model:quantized:microsoft/NextCoder-32B).",
          "author_fullname": "t2_w4j8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Speculative decoding without a draft model (C#)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1max9qz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753649582,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;tl;dr: faster grammar check and minor code edits without a draft model: a C# proof-of-concept.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/dpmm99/ModelFreeSpeculation\"&gt;https://github.com/dpmm99/ModelFreeSpeculation&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is a toy project built on LLamaSharp. It&amp;#39;s a toy because it assumes the output will be nearly identical to the input--no particularly large added sequences and such. A better difference-tracking algorithm would make it more usable, and I think it could also be better if it fell back to a real draft model smartly when there are big differences. I&amp;#39;d been thinking about this since I saw a statement that &lt;strong&gt;a draft &amp;quot;model&amp;quot; isn&amp;#39;t limited to LLMs&lt;/strong&gt;, and I remember it every time I accidentally click &amp;quot;Apply&amp;quot; in GitHub Copilot and watch it scan through a few hundred lines of code just to add one function, haha.&lt;/p&gt;\n\n&lt;p&gt;I tested it on two prompts using Phi-4-14B-Q4_K_M with &lt;strong&gt;8 draft tokens&lt;/strong&gt; per inference loop iteration on my RTX 4060 Ti using CUDA and &lt;a href=\"https://github.com/SciSharp/LLamaSharp/pull/1225\"&gt;this pre-release of LLamaSharp&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;For the spell-check prompt:&lt;/p&gt;\n\n&lt;p&gt;Duration: 7.39s, Tokens: 135, Tokens/sec: 18.28&lt;/p&gt;\n\n&lt;p&gt;Duration: 4.89s, Tokens: 135, Tokens/sec: 27.60 (88 accepted, 283 rejected) &lt;strong&gt;(+51%)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For the code editing prompt:&lt;/p&gt;\n\n&lt;p&gt;Duration: 17.84s, Tokens: 328, Tokens/sec: 18.39&lt;/p&gt;\n\n&lt;p&gt;Duration: 10.40s, Tokens: 328, Tokens/sec: 31.55 (237 accepted, 473 rejected) &lt;strong&gt;(+71%)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Duration: 9.50s, Tokens: 328, Tokens/sec: 34.52 (250 draft tokens accepted; &lt;strong&gt;draft length 20&lt;/strong&gt;) &lt;strong&gt;(+88%)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I was also thinking this approach could go nicely with a model fine-tuned for &lt;em&gt;applying&lt;/em&gt; code edits like &lt;a href=\"https://huggingface.co/models?other=base_model:quantized:microsoft/NextCoder-32B\"&gt;https://huggingface.co/models?other=base_model:quantized:microsoft/NextCoder-32B&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/6SBbFAaNO6y4KvdR37bmJv25qEqiWaX9KiswAYV7NXY.png?auto=webp&amp;s=21f9ad6a2c9be091302197df5996afbfcacd7658",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/6SBbFAaNO6y4KvdR37bmJv25qEqiWaX9KiswAYV7NXY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4443b14344a6b363cc7c1a64c3d726e9bc418ef",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/6SBbFAaNO6y4KvdR37bmJv25qEqiWaX9KiswAYV7NXY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9dff0fadc0682d76cacb86e7d2c019d5bcf8e855",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/6SBbFAaNO6y4KvdR37bmJv25qEqiWaX9KiswAYV7NXY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=92ffde2012c4f9c734960a7c583b8534d115770e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/6SBbFAaNO6y4KvdR37bmJv25qEqiWaX9KiswAYV7NXY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=750e6acb13f5fcb85d48abe49c6abc28652f4cc4",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/6SBbFAaNO6y4KvdR37bmJv25qEqiWaX9KiswAYV7NXY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2443879b742afc48ec4f2b63ea5c5b854084c209",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/6SBbFAaNO6y4KvdR37bmJv25qEqiWaX9KiswAYV7NXY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b3d4c67cd12eddf041d96d0598ed7950a45fc6f",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "6SBbFAaNO6y4KvdR37bmJv25qEqiWaX9KiswAYV7NXY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1max9qz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DeProgrammer99",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1max9qz/speculative_decoding_without_a_draft_model_c/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1max9qz/speculative_decoding_without_a_draft_model_c/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753649582,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is there a platform, preferably open source, that would behave like claude code/cursor but for writing? (and not coding). \n\nCurrently, I use roocode and create custom agents, but:\n1. Not web-based\n2. Coder spill overs. Many such agents system prompts is specific to coding and time to time they write code. \n3. There are (markdown) editors with ai features, but ai part often is just a tool, no full document treatment or cross-document agentic search\n\nWIP Image/ in this direction: https://i.redd.it/320wke1z3mff1.jpeg\n\n",
          "author_fullname": "t2_4p1wo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OS Cursor for documents?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mb9b1t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753708814,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753685903,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a platform, preferably open source, that would behave like claude code/cursor but for writing? (and not coding). &lt;/p&gt;\n\n&lt;p&gt;Currently, I use roocode and create custom agents, but:\n1. Not web-based\n2. Coder spill overs. Many such agents system prompts is specific to coding and time to time they write code. \n3. There are (markdown) editors with ai features, but ai part often is just a tool, no full document treatment or cross-document agentic search&lt;/p&gt;\n\n&lt;p&gt;WIP Image/ in this direction: &lt;a href=\"https://i.redd.it/320wke1z3mff1.jpeg\"&gt;https://i.redd.it/320wke1z3mff1.jpeg&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oQlLZuK0ZN4sqe3SpmPzB_Ve0jJmmRQAh1zSyvuejTU.jpg?auto=webp&amp;s=4d4827c50798ef85ee6ba9048ab34f690b609038",
                  "width": 3252,
                  "height": 2032
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oQlLZuK0ZN4sqe3SpmPzB_Ve0jJmmRQAh1zSyvuejTU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0a62112ca4f3da001972d1db4e275a2298823f6",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://external-preview.redd.it/oQlLZuK0ZN4sqe3SpmPzB_Ve0jJmmRQAh1zSyvuejTU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef68ba716627f6adabce945585f2f280d9484fa0",
                    "width": 216,
                    "height": 134
                  },
                  {
                    "url": "https://external-preview.redd.it/oQlLZuK0ZN4sqe3SpmPzB_Ve0jJmmRQAh1zSyvuejTU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=01399277aa9fecbc997284776cb180c31295cb5b",
                    "width": 320,
                    "height": 199
                  },
                  {
                    "url": "https://external-preview.redd.it/oQlLZuK0ZN4sqe3SpmPzB_Ve0jJmmRQAh1zSyvuejTU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=309ad5ba8f7753286385dbafd07b1971eb757869",
                    "width": 640,
                    "height": 399
                  },
                  {
                    "url": "https://external-preview.redd.it/oQlLZuK0ZN4sqe3SpmPzB_Ve0jJmmRQAh1zSyvuejTU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ef1ec3d5daad057c5f128152b2dabc081c123448",
                    "width": 960,
                    "height": 599
                  },
                  {
                    "url": "https://external-preview.redd.it/oQlLZuK0ZN4sqe3SpmPzB_Ve0jJmmRQAh1zSyvuejTU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=50e9dab41d21bf51ee7e3ce063e96f8925fdaf36",
                    "width": 1080,
                    "height": 674
                  }
                ],
                "variants": {},
                "id": "_BHq_SDHRIx5gVATfsBYlVYLFDnOzpJ-sRmdoP6ZldU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mb9b1t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "keniget",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb9b1t/os_cursor_for_documents/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mb9b1t/os_cursor_for_documents/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753685903,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been wondering that most people start contributing from the age of 18-19 and many keep contributing for life. What's your biggest reason for\n\n1. Making your 1st contribution\n2. Keep contributing throughout your life.\n\nGiven that financial consideration is one of the least important aspect, I want to see what unique drives people have.\n\nAlso, would love to know more in this survey: [https://form.typeform.com/to/Duc3EN8k](https://form.typeform.com/to/Duc3EN8k)  \nPlease participate if you wish to, take about 5 minutes",
          "author_fullname": "t2_a18zh8fa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What motivates you to contribute to Open-source web development?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbnn6a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.31,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753726105,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been wondering that most people start contributing from the age of 18-19 and many keep contributing for life. What&amp;#39;s your biggest reason for&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Making your 1st contribution&lt;/li&gt;\n&lt;li&gt;Keep contributing throughout your life.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Given that financial consideration is one of the least important aspect, I want to see what unique drives people have.&lt;/p&gt;\n\n&lt;p&gt;Also, would love to know more in this survey: &lt;a href=\"https://form.typeform.com/to/Duc3EN8k\"&gt;https://form.typeform.com/to/Duc3EN8k&lt;/a&gt;&lt;br/&gt;\nPlease participate if you wish to, take about 5 minutes&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbnn6a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "haymaikyakaru",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbnn6a/what_motivates_you_to_contribute_to_opensource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbnn6a/what_motivates_you_to_contribute_to_opensource/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753726105,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yesterday I bought a 3090 and it works great with vllm (despite some issues in some models, but that is probably my fault). Is there a way that I could use my rtx 2060 (6gb vram) for context (I can only use 8k context in qwen2.5-coder:32b awq using the 3090)? If not for context then maybe to increase the tokens/second. But from what I have seen it could also decrease the tokens/second because its less powerful.",
          "author_fullname": "t2_ti9s05lw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Rtx 3090 + Rtx 2060 for Context Increase and Performance",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mb5jut",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753672812,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yesterday I bought a 3090 and it works great with vllm (despite some issues in some models, but that is probably my fault). Is there a way that I could use my rtx 2060 (6gb vram) for context (I can only use 8k context in qwen2.5-coder:32b awq using the 3090)? If not for context then maybe to increase the tokens/second. But from what I have seen it could also decrease the tokens/second because its less powerful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mb5jut",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FredericoDev",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb5jut/rtx_3090_rtx_2060_for_context_increase_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mb5jut/rtx_3090_rtx_2060_for_context_increase_and/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753672812,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Title pretty well covers it. I've been huge into image generation with Stable Diffusion and was even working on a profile art app with it, but ChatGPT's image generation capabilities sort of sucked the air out of the room for image generation -- or it *would* have, if it was open source, or at least didn't randomly decide that images violate it's content policy half the time (I'm not talking gooner material here, I mean just randomly flipping out and deciding that it can't make art of YOU, even though it's been doing it consistently for the past hour).\n\nObviously the open source world moves slower without a distinct financial incentive, but just checking in on the state of multimodal image generation. The AI space moves *so* quickly sometimes that it's really easy to just plain miss stuff. What's the latest?",
          "author_fullname": "t2_yp2wt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Time for my regular check-in to see if the open-source world has any multimodal models capable of image generation approaching GPT 4o's quality and adherence",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbi65j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753714033,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title pretty well covers it. I&amp;#39;ve been huge into image generation with Stable Diffusion and was even working on a profile art app with it, but ChatGPT&amp;#39;s image generation capabilities sort of sucked the air out of the room for image generation -- or it &lt;em&gt;would&lt;/em&gt; have, if it was open source, or at least didn&amp;#39;t randomly decide that images violate it&amp;#39;s content policy half the time (I&amp;#39;m not talking gooner material here, I mean just randomly flipping out and deciding that it can&amp;#39;t make art of YOU, even though it&amp;#39;s been doing it consistently for the past hour).&lt;/p&gt;\n\n&lt;p&gt;Obviously the open source world moves slower without a distinct financial incentive, but just checking in on the state of multimodal image generation. The AI space moves &lt;em&gt;so&lt;/em&gt; quickly sometimes that it&amp;#39;s really easy to just plain miss stuff. What&amp;#39;s the latest?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbi65j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Peregrine2976",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbi65j/time_for_my_regular_checkin_to_see_if_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbi65j/time_for_my_regular_checkin_to_see_if_the/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753714033,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi same as title. I have used pocketpal and smolchat to run gguf models as of now in Android. I want to test some onnxmodels. Is there any similar app for the same?",
          "author_fullname": "t2_mmtl1muh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Please suggest me android apps to run onnx models for testing like pocketpal",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbap20",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753691346,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi same as title. I have used pocketpal and smolchat to run gguf models as of now in Android. I want to test some onnxmodels. Is there any similar app for the same?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbap20",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Away_Expression_3713",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbap20/please_suggest_me_android_apps_to_run_onnx_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbap20/please_suggest_me_android_apps_to_run_onnx_models/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753691346,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Which of the following models is the best in terms of function calling in your opinion?  \n1. Claude Sonnet 4  \n2. o3  \n3. Gemini 2.5 Pro\n\nAlso which one of them is the most creative when it comes to solving problems?",
          "author_fullname": "t2_9lpd1y2f",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Function Calling: Claude Sonnet 4 Vs o3 Vs Gemin 2.5 Pro",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbeeru",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753704484,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which of the following models is the best in terms of function calling in your opinion?&lt;br/&gt;\n1. Claude Sonnet 4&lt;br/&gt;\n2. o3&lt;br/&gt;\n3. Gemini 2.5 Pro&lt;/p&gt;\n\n&lt;p&gt;Also which one of them is the most creative when it comes to solving problems?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbeeru",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Illustrious-Ad-497",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbeeru/function_calling_claude_sonnet_4_vs_o3_vs_gemin/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbeeru/function_calling_claude_sonnet_4_vs_o3_vs_gemin/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753704484,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "i have 3x Tesla A100's . my goal i want to serve a model via ollama and use it with pandasai package so the user enters a prompt and the model generates code to analyze large dataframes and outputs plots or values etc\n\nwhich models do you suggest?\n\ni've seen mistral nemo , qwen 2.5 etc\n\nim trying to get the current best small LLM for this task",
          "author_fullname": "t2_6zblhi4a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "best small LLM for pandasai via ollama",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mba8j8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753689523,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i have 3x Tesla A100&amp;#39;s . my goal i want to serve a model via ollama and use it with pandasai package so the user enters a prompt and the model generates code to analyze large dataframes and outputs plots or values etc&lt;/p&gt;\n\n&lt;p&gt;which models do you suggest?&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;ve seen mistral nemo , qwen 2.5 etc&lt;/p&gt;\n\n&lt;p&gt;im trying to get the current best small LLM for this task&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mba8j8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Main-Quail-3717",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mba8j8/best_small_llm_for_pandasai_via_ollama/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mba8j8/best_small_llm_for_pandasai_via_ollama/",
          "subreddit_subscribers": 506190,
          "created_utc": 1753689523,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_4ou3rslj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wan 2.2 coming out Monday July 28th",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mae4yz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 135,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 135,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/wiS40lScO4Tz3DVaIJvgYMaDlS_RixRf5GYhbQQ4Q8Y.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753593470,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6fhk0wjppcff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?auto=webp&amp;s=fec6294a55a3e104e1bb18786c446c76d3380ada",
                  "width": 1320,
                  "height": 738
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b03db1c6122c5627833ddc9e2fdbcb6d6f7ed744",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c1ac1b73bcaf847b2fa4c394ce28ce658e1f328a",
                    "width": 216,
                    "height": 120
                  },
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=95e50168620f70ce8176a420355e73393cd86fb4",
                    "width": 320,
                    "height": 178
                  },
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5dc0d6b8afe675c915a40e90b3006a47c5764036",
                    "width": 640,
                    "height": 357
                  },
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=512e28d76ac154808029aa9902dd873b5432ddfa",
                    "width": 960,
                    "height": 536
                  },
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=67b6d4b9b8c656f0cbf08017fc3c7809884c53aa",
                    "width": 1080,
                    "height": 603
                  }
                ],
                "variants": {},
                "id": "3150d24SFUF3S7LHu63_t91aK5WawBeVinxIS7aSy80"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mae4yz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Comed_Ai_n",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mae4yz/wan_22_coming_out_monday_july_28th/",
          "stickied": false,
          "url": "https://i.redd.it/6fhk0wjppcff1.jpeg",
          "subreddit_subscribers": 506190,
          "created_utc": 1753593470,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}